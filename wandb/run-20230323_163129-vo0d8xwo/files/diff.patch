diff --git a/.gitmodules b/.gitmodules
new file mode 100644
index 0000000..ffdee8d
--- /dev/null
+++ b/.gitmodules
@@ -0,0 +1,6 @@
+[submodule "MO-Gymnasium"]
+	path = MO-Gymnasium
+	url = git@github.com:Farama-Foundation/MO-Gymnasium.git
+[submodule "morl-baselines"]
+	path = morl-baselines
+	url = git@github.com:LucasAlegre/morl-baselines.git
Submodule MO-Gymnasium 0000000...3a9c3e8 (new submodule)
diff --git a/MO-Gymnasium/.github/FUNDING.yml b/MO-Gymnasium/.github/FUNDING.yml
new file mode 100644
index 0000000..c5d88ac
--- /dev/null
+++ b/MO-Gymnasium/.github/FUNDING.yml
@@ -0,0 +1 @@
+github: Farama-Foundation
diff --git a/MO-Gymnasium/.github/workflows/build-docs-dev.yml b/MO-Gymnasium/.github/workflows/build-docs-dev.yml
new file mode 100644
index 0000000..3a3af90
--- /dev/null
+++ b/MO-Gymnasium/.github/workflows/build-docs-dev.yml
@@ -0,0 +1,46 @@
+name: Build main branch documentation website
+on:
+  push:
+    branches: [main]
+permissions:
+  contents: write
+jobs:
+  docs:
+    name: Generate Website
+    runs-on: ubuntu-latest
+    env:
+      SPHINX_GITHUB_CHANGELOG_TOKEN: ${{ secrets.GITHUB_TOKEN }}
+    steps:
+      - uses: actions/checkout@v3
+
+      - uses: actions/setup-python@v4
+        with:
+            python-version: '3.9'
+
+      - name: Install docs requirements
+        run: pip install -r docs/requirements.txt
+
+      - name: Install MO-Gymnasium
+        run: pip install -e .[all]
+
+      - name: Build env docs
+        run: python docs/_scripts/gen_env_docs.py
+
+      - name: Build
+        run: sphinx-build -b dirhtml -v docs _build
+
+      - name: Move 404
+        run: mv _build/404/index.html _build/404.html
+
+      - name: Update 404 links
+        run: python docs/_scripts/move_404.py _build/404.html
+
+      - name: Remove .doctrees
+        run: rm -r _build/.doctrees
+
+      - name: Upload to GitHub Pages
+        uses: JamesIves/github-pages-deploy-action@v4
+        with:
+          folder: _build
+          target-folder: main
+          clean: false
diff --git a/MO-Gymnasium/.github/workflows/build-docs-version.yml b/MO-Gymnasium/.github/workflows/build-docs-version.yml
new file mode 100644
index 0000000..7df686f
--- /dev/null
+++ b/MO-Gymnasium/.github/workflows/build-docs-version.yml
@@ -0,0 +1,59 @@
+name: Docs Versioning
+on:
+  push:
+    tags:
+      - 'v?*.*.*'
+permissions:
+  contents: write
+jobs:
+  docs:
+    name: Generate Website for new version
+    runs-on: ubuntu-latest
+    env:
+      SPHINX_GITHUB_CHANGELOG_TOKEN: ${{ secrets.GITHUB_TOKEN }}
+    steps:
+      - uses: actions/checkout@v3
+
+      - uses: actions/setup-python@v4
+        with:
+            python-version: '3.9'
+
+      - name: Get tag
+        id: tag
+        uses: dawidd6/action-get-tag@v1
+
+      - name: Install docs requirements
+        run: pip install -r docs/requirements.txt
+
+      - name: Install MO-Gymnasium
+        run: pip install -e .[all]
+
+      - name: Build env docs
+        run: python docs/_scripts/gen_env_docs.py
+
+      - name: Build
+        run: sphinx-build -b dirhtml -v docs _build
+
+      - name: Move 404
+        run: mv _build/404/index.html _build/404.html
+
+      - name: Update 404 links
+        run: python docs/_scripts/move_404.py _build/404.html
+
+      - name: Remove .doctrees
+        run: rm -r _build/.doctrees
+
+      - name: Upload to GitHub Pages
+        uses: JamesIves/github-pages-deploy-action@v4
+        with:
+          folder: _build
+          target-folder: ${{steps.tag.outputs.tag}}
+          clean: false
+
+      - name: Upload to GitHub Pages
+        uses: JamesIves/github-pages-deploy-action@v4
+        with:
+          folder: _build
+          clean-exclude: |
+            *.*.*/
+            main
diff --git a/MO-Gymnasium/.github/workflows/build-publish.yml b/MO-Gymnasium/.github/workflows/build-publish.yml
new file mode 100644
index 0000000..a32ef55
--- /dev/null
+++ b/MO-Gymnasium/.github/workflows/build-publish.yml
@@ -0,0 +1,68 @@
+# This workflow will build and (if release) publish Python distributions to PyPI
+# For more information see:
+#   - https://help.github.com/actions/language-and-framework-guides/using-python-with-github-actions
+#   - https://packaging.python.org/en/latest/guides/publishing-package-distribution-releases-using-github-actions-ci-cd-workflows/
+#
+# derived from https://github.com/Farama-Foundation/PettingZoo/blob/e230f4d80a5df3baf9bd905149f6d4e8ce22be31/.github/workflows/build-publish.yml
+name: build-publish
+
+on:
+  push:
+    branches: [main]
+  pull_request:
+    branches: [main]
+  release:
+    types: [published]
+
+jobs:
+  build-wheels:
+    runs-on: ${{ matrix.os }}
+    strategy:
+      matrix:
+        include:
+        - os: ubuntu-latest
+          python: 37
+          platform: manylinux_x86_64
+        - os: ubuntu-latest
+          python: 38
+          platform: manylinux_x86_64
+        - os: ubuntu-latest
+          python: 39
+          platform: manylinux_x86_64
+        - os: ubuntu-latest
+          python: 310
+          platform: manylinux_x86_64
+        - os: ubuntu-latest
+          python: 311
+          platform: manylinux_x86_64
+
+    steps:
+    - uses: actions/checkout@v3
+    - name: Set up Python
+      uses: actions/setup-python@v4
+      with:
+        python-version: '3.x'
+    - name: Install dependencies
+      run: python -m pip install --upgrade pip setuptools build
+    - name: Build sdist and wheels
+      run: python -m build
+    - name: Store wheels
+      uses: actions/upload-artifact@v2
+      with:
+        path: dist
+
+  publish:
+    runs-on: ubuntu-latest
+    needs:
+    - build-wheels
+    if: github.event_name == 'release' && github.event.action == 'published'
+    steps:
+    - name: Download dists
+      uses: actions/download-artifact@v2
+      with:
+        name: artifact
+        path: dist
+    - name: Publish
+      uses: pypa/gh-action-pypi-publish@release/v1
+      with:
+        password: ${{ secrets.PYPI_API_MOGYM }}
diff --git a/MO-Gymnasium/.github/workflows/manual-build-docs-version.yml b/MO-Gymnasium/.github/workflows/manual-build-docs-version.yml
new file mode 100644
index 0000000..c345b04
--- /dev/null
+++ b/MO-Gymnasium/.github/workflows/manual-build-docs-version.yml
@@ -0,0 +1,71 @@
+name: Manual Docs Versioning
+on:
+  workflow_dispatch:
+    inputs:
+      version:
+          description: 'Documentation version to create'
+          required: true
+      commit:
+          description: 'Commit used to build the Documentation version'
+          required: false
+      latest:
+          description: 'Latest version'
+          type: boolean
+
+permissions:
+  contents: write
+jobs:
+  docs:
+    name: Generate Website for new version
+    runs-on: ubuntu-latest
+    env:
+      SPHINX_GITHUB_CHANGELOG_TOKEN: ${{ secrets.GITHUB_TOKEN }}
+    steps:
+      - uses: actions/checkout@v3
+        if: inputs.commit == ''
+
+      - uses: actions/checkout@v3
+        if: inputs.commit != ''
+        with:
+          ref: ${{ inputs.commit }}
+
+      - uses: actions/setup-python@v4
+        with:
+            python-version: '3.9'
+
+      - name: Install docs requirements
+        run: pip install -r docs/requirements.txt
+
+      - name: Install MO-Gymnasium
+        run: pip install -e .[all]
+
+      - name: Build env docs
+        run: python docs/_scripts/gen_env_docs.py
+
+      - name: Build
+        run: sphinx-build -b dirhtml -v docs _build
+
+      - name: Move 404
+        run: mv _build/404/index.html _build/404.html
+
+      - name: Update 404 links
+        run: python docs/_scripts/move_404.py _build/404.html
+
+      - name: Remove .doctrees
+        run: rm -r _build/.doctrees
+
+      - name: Upload to GitHub Pages
+        uses: JamesIves/github-pages-deploy-action@v4
+        with:
+          folder: _build
+          target-folder: ${{ inputs.version }}
+          clean: false
+
+      - name: Upload to GitHub Pages
+        uses: JamesIves/github-pages-deploy-action@v4
+        if: inputs.latest
+        with:
+          folder: _build
+          clean-exclude: |
+            *.*.*/
+            main
diff --git a/MO-Gymnasium/.github/workflows/pre-commit.yml b/MO-Gymnasium/.github/workflows/pre-commit.yml
new file mode 100644
index 0000000..80ce02a
--- /dev/null
+++ b/MO-Gymnasium/.github/workflows/pre-commit.yml
@@ -0,0 +1,21 @@
+# https://pre-commit.com
+# This GitHub Action assumes that the repo contains a valid .pre-commit-config.yaml file.
+name: pre-commit
+on:
+  pull_request:
+  push:
+    branches: [main]
+
+permissions:
+  contents: read # to fetch code (actions/checkout)
+
+jobs:
+  pre-commit:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v3
+      - uses: actions/setup-python@v4
+      - run: python -m pip install pre-commit
+      - run: python -m pre_commit --version
+      - run: python -m pre_commit install
+      - run: python -m pre_commit run --all-files
diff --git a/MO-Gymnasium/.github/workflows/test.yml b/MO-Gymnasium/.github/workflows/test.yml
new file mode 100644
index 0000000..032bb36
--- /dev/null
+++ b/MO-Gymnasium/.github/workflows/test.yml
@@ -0,0 +1,33 @@
+name: Python tests
+
+on:
+  push:
+    branches: [ main ]
+  pull_request:
+    branches: [ main ]
+
+jobs:
+  linux-test:
+    runs-on: ubuntu-20.04
+    strategy:
+      matrix:
+        python-version: ['3.7', '3.8', '3.9', '3.10']
+    steps:
+    - uses: actions/checkout@v2
+    - name: Set up Python ${{ matrix.python-version }}
+      uses: actions/setup-python@v2
+      with:
+        python-version: ${{ matrix.python-version }}
+    - name: Install dependencies
+      run: |
+        pip install pytest
+        pip install mujoco
+        git clone https://github.com/benelot/pybullet-gym.git
+        pip install -e pybullet-gym
+        sudo apt-get install libglu1-mesa-dev libgl1-mesa-dev libosmesa6-dev xvfb patchelf ffmpeg cmake swig
+        pip install gymnasium
+        pip install -e .[all]
+    - name: Full Python tests
+      run: |
+        pytest tests/test_envs.py
+        pytest tests/test_wrappers.py
diff --git a/MO-Gymnasium/.gitignore b/MO-Gymnasium/.gitignore
new file mode 100644
index 0000000..b6e4761
--- /dev/null
+++ b/MO-Gymnasium/.gitignore
@@ -0,0 +1,129 @@
+# Byte-compiled / optimized / DLL files
+__pycache__/
+*.py[cod]
+*$py.class
+
+# C extensions
+*.so
+
+# Distribution / packaging
+.Python
+build/
+develop-eggs/
+dist/
+downloads/
+eggs/
+.eggs/
+lib/
+lib64/
+parts/
+sdist/
+var/
+wheels/
+pip-wheel-metadata/
+share/python-wheels/
+*.egg-info/
+.installed.cfg
+*.egg
+MANIFEST
+
+# PyInstaller
+#  Usually these files are written by a python script from a template
+#  before PyInstaller builds the exe, so as to inject date/other infos into it.
+*.manifest
+*.spec
+
+# Installer logs
+pip-log.txt
+pip-delete-this-directory.txt
+
+# Unit test / coverage reports
+htmlcov/
+.tox/
+.nox/
+.coverage
+.coverage.*
+.cache
+nosetests.xml
+coverage.xml
+*.cover
+*.py,cover
+.hypothesis/
+.pytest_cache/
+
+# Translations
+*.mo
+*.pot
+
+# Django stuff:
+*.log
+local_settings.py
+db.sqlite3
+db.sqlite3-journal
+
+# Flask stuff:
+instance/
+.webassets-cache
+
+# Scrapy stuff:
+.scrapy
+
+# Sphinx documentation
+docs/_build/
+
+# PyBuilder
+target/
+
+# Jupyter Notebook
+.ipynb_checkpoints
+
+# IPython
+profile_default/
+ipython_config.py
+
+# pyenv
+.python-version
+
+# pipenv
+#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.
+#   However, in case of collaboration, if having platform-specific dependencies or dependencies
+#   having no cross-platform support, pipenv may install dependencies that don't work, or not
+#   install all needed dependencies.
+#Pipfile.lock
+
+# PEP 582; used by e.g. github.com/David-OConnor/pyflow
+__pypackages__/
+
+# Celery stuff
+celerybeat-schedule
+celerybeat.pid
+
+# SageMath parsed files
+*.sage.py
+
+# Environments
+.env
+.venv
+env/
+venv/
+ENV/
+env.bak/
+venv.bak/
+
+# Spyder project settings
+.spyderproject
+.spyproject
+
+# Rope project settings
+.ropeproject
+
+# mkdocs documentation
+/site
+
+# mypy
+.mypy_cache/
+.dmypy.json
+dmypy.json
+
+# Pyre type checker
+.pyre/
diff --git a/MO-Gymnasium/.pre-commit-config.yaml b/MO-Gymnasium/.pre-commit-config.yaml
new file mode 100644
index 0000000..8d021d2
--- /dev/null
+++ b/MO-Gymnasium/.pre-commit-config.yaml
@@ -0,0 +1,69 @@
+# See https://pre-commit.com for more information
+# See https://pre-commit.com/hooks.html for more hooks
+repos:
+  - repo: https://github.com/pre-commit/pre-commit-hooks
+    rev: v4.4.0
+    hooks:
+      - id: check-symlinks
+      - id: destroyed-symlinks
+      - id: trailing-whitespace
+      - id: end-of-file-fixer
+      - id: check-yaml
+      - id: check-toml
+      - id: check-ast
+      - id: check-merge-conflict
+      - id: check-executables-have-shebangs
+      - id: check-shebang-scripts-are-executable
+      - id: detect-private-key
+      - id: debug-statements
+  - repo: https://github.com/codespell-project/codespell
+    rev: v2.2.2
+    hooks:
+      - id: codespell
+        args:
+          - --ignore-words-list=reacher,ure,referenc,wile,mor
+  - repo: https://github.com/PyCQA/flake8
+    rev: 5.0.4
+    hooks:
+      - id: flake8
+        args:
+          - '--per-file-ignores=*/__init__.py:F401'
+          - --ignore=E203,W503,E741
+          - --max-complexity=30
+          - --max-line-length=456
+          - --show-source
+          - --statistics
+  - repo: https://github.com/asottile/pyupgrade
+    rev: v3.3.0
+    hooks:
+      - id: pyupgrade
+        args: ["--py37-plus"]
+  - repo: https://github.com/PyCQA/isort
+    rev: 5.12.0
+    hooks:
+      - id: isort
+  - repo: https://github.com/python/black
+    rev: 22.10.0
+    hooks:
+      - id: black
+  - repo: https://github.com/pycqa/pydocstyle
+    rev: 6.1.1
+    hooks:
+      - id: pydocstyle
+        exclude: ^(mo_gymnasium/envs/)|(tests/)|(docs/)|(setup.py)
+        args:
+          - --source
+          - --explain
+          - --convention=google
+        additional_dependencies: ["toml"]
+  - repo: local
+    hooks:
+      - id: pyright
+        name: pyright
+        entry: pyright
+        language: node
+        pass_filenames: false
+        types: [python]
+        additional_dependencies: ["pyright"]
+        args:
+          - --project=pyproject.toml
diff --git a/MO-Gymnasium/CITATION.bib b/MO-Gymnasium/CITATION.bib
new file mode 100644
index 0000000..acdab6f
--- /dev/null
+++ b/MO-Gymnasium/CITATION.bib
@@ -0,0 +1,6 @@
+@inproceedings{Alegre+2022bnaic,
+  author = {Lucas N. Alegre and Florian	Felten and El-Ghazali Talbi and Gr{\'e}goire Danoy and Ann Now{\'e} and Ana L. C. Bazzan and Bruno C. da Silva},
+  title = {{MO-Gym}: A Library of Multi-Objective Reinforcement Learning Environments},
+  booktitle = {Proceedings of the 34th Benelux Conference on Artificial Intelligence BNAIC/Benelearn 2022},
+  year = {2022}
+}
diff --git a/MO-Gymnasium/CODE_OF_CONDUCT.rst b/MO-Gymnasium/CODE_OF_CONDUCT.rst
new file mode 100644
index 0000000..0694226
--- /dev/null
+++ b/MO-Gymnasium/CODE_OF_CONDUCT.rst
@@ -0,0 +1,67 @@
+=================================
+Farama Foundation Code of Conduct
+=================================
+
+The Farama Foundation is dedicated to providing a harassment-free experience for
+everyone, regardless of gender, gender identity and expression, sexual
+orientation, disability, physical appearance, body size, age, race, or
+religion. We do not tolerate harassment of participants in any form.
+
+This code of conduct applies to all Farama Foundation repositories (including Gist
+comments) both online and off. Anyone who violates this code of
+conduct may be sanctioned or expelled from these spaces at the
+discretion of the moderators.
+
+We may add additional rules over time, which will be made clearly
+available to participants. Participants are responsible for knowing
+and abiding by these rules.
+
+-------------
+Our Standards
+-------------
+Members of the Farama Foundation community are **open**, **inclusive**, and **respectful**.
+Examples of behavior that contributes to a positive environment for our community include:
+
+* **Being open**. Members of the community are open to collaboration, whether it's on issues, PRs, problems, or otherwise
+* **Focusing on what is best for the community**. We're respectful of the processes set forth in the community, and we work within them to
+  improve the community.
+* **Being respectful of differing viewpoints and experiences.**  We're receptive to constructive comments and criticism,
+  as the experiences and skill sets of other members contribute to the whole of our efforts.
+* **Showing empathy.** We're attentive in our communications, and we're tactful when approaching differing views.
+* **Being respectful.** We're respectful of differing opinions, viewpoints, experiences, and efforts.
+* **Gracefully accepting constructive criticism.** When we disagree, we are courteous in raising our issues.
+* **Using welcoming and inclusive language.** We're accepting of all who wish to take part in our activities, fostering
+  an environment where anyone can participate and everyone can make a difference.
+
+Examples of unacceptable behavior include:
+
+* Harassment of any participants in any form.
+* The use of sexual language or imagery, and sexual attention or advances of any kind.
+* Insults, put downs, or jokes that are based upon stereotypes, that are exclusionary, or that hold others up for ridicule.
+* Publishing others' private information, such as a physical or email address, without explicit permission.
+* Incitement of violence or harassment towards any individual, including encouraging a person to commit suicide or to engage in self-harm.
+* Sustained disruption of online community discussions, in-person presentations, or other in-person events.
+* Creating additional online accounts in order to harass another person or circumvent a ban
+* Other conduct which could reasonably be considered inappropriate in a professional setting including people of many different backgrounds.
+
+Members asked to stop any inappropriate behavior are expected to comply immediately.
+
+------------
+Consequences
+------------
+If a participant engages in behavior that violates this code of conduct, the Farama Foundation team may take any action they deem
+appropriate, including warning the offender or expulsion from the community.
+
+Thank you for helping make this a welcoming, friendly community for everyone.
+
+-------
+License
+-------
+This Code of Conduct is licensed under the `Creative Commons Attribution-ShareAlike 3.0 Unported License
+<https://creativecommons.org/licenses/by-sa/3.0/>`_.
+
+-----------
+Attribution
+-----------
+This Code of Conduct is adapted from `Python's Code of Conduct <https://www.python.org/psf/conduct/>`_, which is under a `Creative Commons License
+<https://creativecommons.org/licenses/by-sa/3.0/>`_.
diff --git a/MO-Gymnasium/LICENSE b/MO-Gymnasium/LICENSE
new file mode 100644
index 0000000..1c647b1
--- /dev/null
+++ b/MO-Gymnasium/LICENSE
@@ -0,0 +1,21 @@
+MIT License
+
+Copyright (c) 2022 Lucas Alegre
+
+Permission is hereby granted, free of charge, to any person obtaining a copy
+of this software and associated documentation files (the "Software"), to deal
+in the Software without restriction, including without limitation the rights
+to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+copies of the Software, and to permit persons to whom the Software is
+furnished to do so, subject to the following conditions:
+
+The above copyright notice and this permission notice shall be included in all
+copies or substantial portions of the Software.
+
+THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+SOFTWARE.
diff --git a/MO-Gymnasium/README.md b/MO-Gymnasium/README.md
new file mode 100644
index 0000000..90d5f5e
--- /dev/null
+++ b/MO-Gymnasium/README.md
@@ -0,0 +1,83 @@
+![tests](https://github.com/Farama-Foundation/mo-gymnasium/workflows/Python%20tests/badge.svg)
+[![pre-commit](https://img.shields.io/badge/pre--commit-enabled-brightgreen?logo=pre-commit&logoColor=white)](https://pre-commit.com/)
+[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)
+
+
+# MO-Gymnasium: Multi-Objective Reinforcement Learning Environments
+
+<!-- start elevator-pitch -->
+
+MO-Gymnasium is an open source Python library for developing and comparing multi-objective reinforcement learning algorithms by providing a standard API to communicate between learning algorithms and environments, as well as a standard set of environments compliant with that API. Essentially, the environments follow the standard [Gymnasium API](https://github.com/Farama-Foundation/Gymnasium), but return vectorized rewards as numpy arrays.
+
+The documentation website is at [mo-gymnasium.farama.org](https://mo-gymnasium.farama.org), and we have a public discord server (which we also use to coordinate development work) that you can join here: https://discord.gg/bnJ6kubTg6.
+
+<!-- end elevator-pitch -->
+
+## Environments
+
+MO-Gymnasium includes environments taken from the MORL literature, as well as multi-objective version of classical environments, such as MuJoco.
+The full list of environments is available [here](https://mo-gymnasium.farama.org/environments/all-environments/).
+
+## Installation
+<!-- start install -->
+
+To install MO-Gymnasium, use:
+```bash
+pip install mo-gymnasium
+```
+
+This does not include dependencies for all families of environments (some can be problematic to install on certain systems). You can install these dependencies for one family like `pip install "mo-gymnasium[mujoco]"` or use `pip install "mo-gymnasium[all]"` to install all dependencies.
+
+<!-- end install -->
+
+## API
+
+<!-- start snippet-usage -->
+
+As for Gymnasium, the MO-Gymnasium API models environments as simple Python `env` classes. Creating environment instances and interacting with them is very simple - here's an example using the "minecart-v0" environment:
+
+```python
+import gymnasium as gym
+import mo_gymnasium as mo_gym
+import numpy as np
+
+# It follows the original Gymnasium API ...
+env = mo_gym.make('minecart-v0')
+
+obs, info = env.reset()
+# but vector_reward is a numpy array!
+next_obs, vector_reward, terminated, truncated, info = env.step(your_agent.act(obs))
+
+# Optionally, you can scalarize the reward function with the LinearReward wrapper
+env = mo_gym.LinearReward(env, weight=np.array([0.8, 0.2, 0.2]))
+```
+For details on multi-objective MDP's (MOMDP's) and other MORL definitions, see [A practical guide to multi-objective reinforcement learning and planning](https://link.springer.com/article/10.1007/s10458-022-09552-y).
+
+You can also check more examples in this colab notebook! [![MO-Gym Demo in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Farama-Foundation/MO-Gymnasium/blob/main/mo_gymnasium_demo.ipynb)
+
+<!-- end snippet-usage -->
+
+## Notable related libraries
+
+[MORL-Baselines](https://github.com/LucasAlegre/morl-baselines) is a repository containing various implementations of MORL algorithms by the same authors as MO-Gymnasium. It relies on the MO-Gymnasium API and shows various examples of the usage of wrappers and environments.
+
+## Environment Versioning
+
+MO-Gymnasium keeps strict versioning for reproducibility reasons. All environments end in a suffix like "-v0".  When changes are made to environments that might impact learning results, the number is increased by one to prevent potential confusion.
+
+## Citing
+
+<!-- start citation -->
+
+If you use this repository in your work, please cite:
+
+```bibtex
+@inproceedings{Alegre+2022bnaic,
+  author = {Lucas N. Alegre and Florian	Felten and El-Ghazali Talbi and Gr{\'e}goire Danoy and Ann Now{\'e} and Ana L. C. Bazzan and Bruno C. da Silva},
+  title = {{MO-Gym}: A Library of Multi-Objective Reinforcement Learning Environments},
+  booktitle = {Proceedings of the 34th Benelux Conference on Artificial Intelligence BNAIC/Benelearn 2022},
+  year = {2022}
+}
+```
+
+<!-- end citation -->
diff --git a/MO-Gymnasium/docs/404.md b/MO-Gymnasium/docs/404.md
new file mode 100644
index 0000000..d5baf9a
--- /dev/null
+++ b/MO-Gymnasium/docs/404.md
@@ -0,0 +1,3 @@
+# 404 - Page Not Found
+
+## The requested page could not be found.
diff --git a/MO-Gymnasium/docs/Makefile b/MO-Gymnasium/docs/Makefile
new file mode 100644
index 0000000..ed88099
--- /dev/null
+++ b/MO-Gymnasium/docs/Makefile
@@ -0,0 +1,20 @@
+# Minimal makefile for Sphinx documentation
+#
+
+# You can set these variables from the command line, and also
+# from the environment for the first two.
+SPHINXOPTS    ?=
+SPHINXBUILD   ?= sphinx-build
+SOURCEDIR     = .
+BUILDDIR      = build
+
+# Put it first so that "make" without argument is like "make help".
+help:
+	@$(SPHINXBUILD) -M help "$(SOURCEDIR)" "$(BUILDDIR)" $(SPHINXOPTS) $(O)
+
+.PHONY: help Makefile
+
+# Catch-all target: route all unknown targets to Sphinx using the new
+# "make mode" option.  $(O) is meant as a shortcut for $(SPHINXOPTS).
+%: Makefile
+	@$(SPHINXBUILD) -M $@ "$(SOURCEDIR)" "$(BUILDDIR)" $(SPHINXOPTS) $(O)
diff --git a/MO-Gymnasium/docs/README.md b/MO-Gymnasium/docs/README.md
new file mode 100644
index 0000000..923bba2
--- /dev/null
+++ b/MO-Gymnasium/docs/README.md
@@ -0,0 +1,28 @@
+# MO-Gymnasium documentation
+
+This folder contains the documentation for MO-Gymnasium.
+
+For more information about how to contribute to the documentation go to our [CONTRIBUTING.md](https://github.com/Farama-Foundation/Celshast/blob/main/CONTRIBUTING.md)
+
+## Build the Documentation
+
+Install the required packages and Gymnasium (or your fork):
+
+```
+pip install -r docs/requirements.txt
+pip install -e .
+```
+
+To build the documentation once:
+
+```
+cd docs
+make dirhtml
+```
+
+To rebuild the documentation automatically every time a change is made:
+
+```
+cd docs
+sphinx-autobuild -b dirhtml . _build
+```
diff --git a/MO-Gymnasium/docs/_scripts/gen_env_docs.py b/MO-Gymnasium/docs/_scripts/gen_env_docs.py
new file mode 100644
index 0000000..eec5521
--- /dev/null
+++ b/MO-Gymnasium/docs/_scripts/gen_env_docs.py
@@ -0,0 +1,201 @@
+"""
+Adapted from https://github.com/Farama-Foundation/Gymnasium/blob/main/docs/scripts/gen_mds.py
+"""
+
+import os
+import re
+from functools import reduce
+
+import gymnasium as gym
+import numpy as np
+from tqdm import tqdm
+
+import mo_gymnasium as mo_gym
+
+
+def trim(docstring):
+    if not docstring:
+        return ""
+    # Convert tabs to spaces (following the normal Python rules)
+    # and split into a list of lines:
+    lines = docstring.expandtabs().splitlines()
+    # Determine minimum indentation (first line doesn't count):
+    indent = 232323
+    for line in lines[1:]:
+        stripped = line.lstrip()
+        if stripped:
+            indent = min(indent, len(line) - len(stripped))
+    # Remove indentation (first line is special):
+    trimmed = [lines[0].strip()]
+    if indent < 232323:
+        for line in lines[1:]:
+            trimmed.append(line[indent:].rstrip())
+    # Strip off trailing and leading blank lines:
+    while trimmed and not trimmed[-1]:
+        trimmed.pop()
+    while trimmed and not trimmed[0]:
+        trimmed.pop(0)
+    # Return a single string:
+    return "\n".join(trimmed)
+
+
+pattern = re.compile(r"(?<!^)(?=[A-Z])")
+
+gym.logger.set_level(gym.logger.DISABLED)
+
+all_envs = list(gym.envs.registry.values())
+filtered_envs_by_type = {}
+
+# Obtain filtered list
+for env_spec in tqdm(all_envs):
+    if type(env_spec.entry_point) is not str:
+        continue
+
+    split = env_spec.entry_point.split(".")
+    # ignore gymnasium.envs.env_type:Env
+    env_module = split[0]
+    if env_module != "mo_gymnasium":
+        continue
+    env_type = "environments"  # split[2]
+    env_version = env_spec.version
+
+    try:
+        if env_type not in filtered_envs_by_type.keys():
+            filtered_envs_by_type[env_type] = {}
+        # only store new entries and higher versions
+        if env_spec.id not in filtered_envs_by_type[env_type] or (
+            env_spec.id in filtered_envs_by_type[env_type]
+            and env_version > filtered_envs_by_type[env_type][env_spec.id].version
+        ):
+            filtered_envs_by_type[env_type][env_spec.id] = env_spec
+
+    except Exception as e:
+        print(e)
+
+# Sort
+filtered_envs = list(
+    reduce(
+        lambda s, x: s + x,
+        map(
+            lambda arr: sorted(arr, key=lambda x: x.name),
+            map(lambda dic: list(dic.values()), list(filtered_envs_by_type.values())),
+        ),
+        [],
+    )
+)
+
+env_dir = os.path.join(os.path.dirname(__file__), "..", "environments")
+dir_exists = os.path.exists(env_dir)
+if not dir_exists:
+    # Create a new directory because it does not exist
+    os.makedirs(env_dir)
+    print("environments directory has been created!")
+
+
+# Update Docs
+for i, env_spec in tqdm(enumerate(filtered_envs)):
+    print("ID:", env_spec.id)
+    env_type = env_spec.entry_point.split(".")[2]
+    try:
+        env = mo_gym.make(env_spec.id)
+
+        # variants dont get their own pages
+        e_n = str(env_spec).lower()
+
+        docstring = env.unwrapped.__doc__
+        if not docstring:
+            docstring = env.unwrapped.__class__.__doc__
+        docstring = trim(docstring)
+
+        # pascal case
+        pascal_env_name = env_spec.id
+        snake_env_name = pattern.sub("_", pascal_env_name).lower()
+        # remove what is after the last "-" in snake_env_name e.g. "-v0"
+        snake_env_name = snake_env_name[: snake_env_name.rfind("-")]
+        title_env_name = snake_env_name.replace("_", " ").title().replace("Mo-", "MO-")
+        env_type_title = env_type.replace("_", " ").title()
+        related_pages_meta = ""
+        if i == 0 or not env_type == filtered_envs[i - 1].entry_point.split(".")[2]:
+            related_pages_meta = "firstpage:\n"
+        elif i == len(filtered_envs) - 1 or not env_type == filtered_envs[i + 1].entry_point.split(".")[2]:
+            related_pages_meta = "lastpage:\n"
+
+        # path for saving video
+        v_path = os.path.join(
+            os.path.dirname(__file__),
+            "..",
+            "environments",
+            # env_type,
+            snake_env_name + ".md",
+        )
+
+        front_matter = f"""---
+autogenerated:
+title: {title_env_name}
+{related_pages_meta}---
+"""
+        title = f"# {title_env_name}"
+        if "rgb_array" in env.metadata["render_modes"]:
+            gif = (
+                "```{figure}" + f" ../_static/videos/{snake_env_name}.gif" + f" \n:width: 200px\n:name: {snake_env_name}\n```"
+            )
+        else:
+            gif = ""
+        info = (
+            "This environment is part of the "
+            + f"<a href='..'>{env_type_title} environments</a>."
+            + "Please read that page first for general information."
+        )
+        env_table = "|   |   |\n|---|---|\n"
+        env_table += f"| Action Space | {env.action_space} |\n"
+
+        if env.observation_space.shape:
+            env_table += f"| Observation Shape | {env.observation_space.shape} |\n"
+
+            if hasattr(env.observation_space, "high"):
+                high = env.observation_space.high
+
+                if hasattr(high, "shape"):
+                    if len(high.shape) == 3:
+                        high = high[0][0][0]
+                if env_type == "mujoco":
+                    high = high[0]
+                high = np.round(high, 2)
+                high = str(high).replace("\n", " ")
+                env_table += f"| Observation High | {high} |\n"
+
+            if hasattr(env.observation_space, "low"):
+                low = env.observation_space.low
+                if hasattr(low, "shape"):
+                    if len(low.shape) == 3:
+                        low = low[0][0][0]
+                if env_type == "mujoco":
+                    low = low[0]
+                low = np.round(low, 2)
+                low = str(low).replace("\n", " ")
+                env_table += f"| Observation Low | {low} |\n"
+        else:
+            env_table += f"| Observation Space | {env.observation_space} |\n"
+
+        if env.reward_space.shape:
+            env_table += f"| Reward Shape | {env.reward_space.shape} |\n"
+        if hasattr(env.reward_space, "high"):
+            env_table += f"| Reward High | {env.reward_space.high} |\n"
+        if hasattr(env.reward_space, "low"):
+            env_table += f"| Reward Low | {env.reward_space.low} |\n"
+
+        env_table += f'| Import | `mo_gymnasium.make("{env_spec.id}")` | \n'
+
+        if docstring is None:
+            docstring = "No information provided"
+        all_text = f"""{front_matter}
+{title}
+{gif}
+{env_table}
+{docstring}
+"""
+        file = open(v_path, "w+", encoding="utf-8")
+        file.write(all_text)
+        file.close()
+    except Exception as e:
+        print(e)
diff --git a/MO-Gymnasium/docs/_scripts/gen_gifs.py b/MO-Gymnasium/docs/_scripts/gen_gifs.py
new file mode 100644
index 0000000..670356d
--- /dev/null
+++ b/MO-Gymnasium/docs/_scripts/gen_gifs.py
@@ -0,0 +1,90 @@
+__author__ = "Sander Schulhoff"
+__email__ = "sanderschulhoff@gmail.com"
+
+import os
+import re
+
+import gymnasium
+from PIL import Image
+from tqdm import tqdm
+
+import mo_gymnasium as mo_gym
+
+
+# snake to camel case: https://stackoverflow.com/questions/1175208/elegant-python-function-to-convert-camelcase-to-snake-case # noqa: E501
+pattern = re.compile(r"(?<!^)(?=[A-Z])")
+# how many steps to record an env for
+LENGTH = 300
+# iterate through all envspecs
+for env_spec in tqdm(gymnasium.envs.registry.values()):
+    if type(env_spec.entry_point) is not str:
+        continue
+    print(env_spec.id)
+
+    if env_spec.entry_point.split(".")[0] != "mo_gymnasium":
+        continue
+
+    try:
+        env = mo_gym.make(env_spec.id, render_mode="rgb_array")
+        # the gymnasium needs to be rgb renderable
+        if not ("rgb_array" in env.metadata["render_modes"]):
+            continue
+
+        # extract env name/type from class path
+        split = str(type(env.unwrapped)).split(".")
+
+        # get rid of version info
+        env_name = env_spec.id.split("-")[0]
+        # convert NameLikeThis to name_like_this
+        env_name = pattern.sub("_", env_name).lower()
+        # get the env type (e.g. Box2D)
+        env_type = split[2]
+
+        pascal_env_name = env_spec.id
+        snake_env_name = pattern.sub("_", pascal_env_name).lower()
+        # remove what is after the last "-" in snake_env_name e.g. "-v0"
+        snake_env_name = snake_env_name[: snake_env_name.rfind("-")]
+
+        # if its an atari gymnasium
+        # if env_spec.id[0:3] == "ALE":
+        #     continue
+        #     env_name = env_spec.id.split("-")[0][4:]
+        #     env_name = pattern.sub('_', env_name).lower()
+
+        # path for saving video
+        # v_path = os.path.join("..", "pages", "environments", env_type, "videos") # noqa: E501
+        # # create dir if it doesn't exist
+        # if not path.isdir(v_path):
+        #     mkdir(v_path)
+
+        # obtain and save LENGTH frames worth of steps
+        frames = []
+        while True:
+            state, info = env.reset()
+            terminated, truncated = False, False
+            while not (terminated or truncated) and len(frames) <= LENGTH:
+
+                frame = env.render()
+                frames.append(Image.fromarray(frame))
+                action = env.action_space.sample()
+                state_next, reward, terminated, truncated, info = env.step(action)
+
+            if len(frames) > LENGTH:
+                break
+
+        env.close()
+
+        # make sure video doesn't already exist
+        # if not os.path.exists(os.path.join(v_path, env_name + ".gif")):
+        frames[0].save(
+            os.path.join("..", "_static", "videos", snake_env_name + ".gif"),
+            save_all=True,
+            append_images=frames[1:],
+            duration=50,
+            loop=0,
+        )
+        print("Saved: " + snake_env_name)
+
+    except BaseException as e:
+        print("ERROR", e)
+        continue
diff --git a/MO-Gymnasium/docs/_scripts/move_404.py b/MO-Gymnasium/docs/_scripts/move_404.py
new file mode 100644
index 0000000..0536477
--- /dev/null
+++ b/MO-Gymnasium/docs/_scripts/move_404.py
@@ -0,0 +1,15 @@
+import sys
+
+
+if __name__ == "__main__":
+    if len(sys.argv) < 2:
+        print("Provide a path")
+    filePath = sys.argv[1]
+
+    with open(filePath, "r+") as fp:
+        content = fp.read()
+        content = content.replace('href="../', 'href="/').replace('src="../', 'src="/')
+        fp.seek(0)
+        fp.truncate()
+
+        fp.write(content)
diff --git a/MO-Gymnasium/docs/_static/mo_cheetah.gif b/MO-Gymnasium/docs/_static/mo_cheetah.gif
new file mode 100644
index 0000000..88a4f99
Binary files /dev/null and b/MO-Gymnasium/docs/_static/mo_cheetah.gif differ
diff --git a/MO-Gymnasium/docs/_static/mo_cheetah_rect.gif b/MO-Gymnasium/docs/_static/mo_cheetah_rect.gif
new file mode 100644
index 0000000..a2a3f7b
Binary files /dev/null and b/MO-Gymnasium/docs/_static/mo_cheetah_rect.gif differ
diff --git a/MO-Gymnasium/docs/_static/screenshots/breakable-bottles.jpg b/MO-Gymnasium/docs/_static/screenshots/breakable-bottles.jpg
new file mode 100644
index 0000000..851fc8d
Binary files /dev/null and b/MO-Gymnasium/docs/_static/screenshots/breakable-bottles.jpg differ
diff --git a/MO-Gymnasium/docs/_static/screenshots/cheetah.png b/MO-Gymnasium/docs/_static/screenshots/cheetah.png
new file mode 100644
index 0000000..d31b483
Binary files /dev/null and b/MO-Gymnasium/docs/_static/screenshots/cheetah.png differ
diff --git a/MO-Gymnasium/docs/_static/screenshots/concave_dst.png b/MO-Gymnasium/docs/_static/screenshots/concave_dst.png
new file mode 100644
index 0000000..6557c92
Binary files /dev/null and b/MO-Gymnasium/docs/_static/screenshots/concave_dst.png differ
diff --git a/MO-Gymnasium/docs/_static/screenshots/dst.png b/MO-Gymnasium/docs/_static/screenshots/dst.png
new file mode 100644
index 0000000..5ae3b32
Binary files /dev/null and b/MO-Gymnasium/docs/_static/screenshots/dst.png differ
diff --git a/MO-Gymnasium/docs/_static/screenshots/fishwood.png b/MO-Gymnasium/docs/_static/screenshots/fishwood.png
new file mode 100644
index 0000000..209ccac
Binary files /dev/null and b/MO-Gymnasium/docs/_static/screenshots/fishwood.png differ
diff --git a/MO-Gymnasium/docs/_static/screenshots/four-room.png b/MO-Gymnasium/docs/_static/screenshots/four-room.png
new file mode 100644
index 0000000..c626022
Binary files /dev/null and b/MO-Gymnasium/docs/_static/screenshots/four-room.png differ
diff --git a/MO-Gymnasium/docs/_static/screenshots/fruit-tree.png b/MO-Gymnasium/docs/_static/screenshots/fruit-tree.png
new file mode 100644
index 0000000..3ae7660
Binary files /dev/null and b/MO-Gymnasium/docs/_static/screenshots/fruit-tree.png differ
diff --git a/MO-Gymnasium/docs/_static/screenshots/highway.png b/MO-Gymnasium/docs/_static/screenshots/highway.png
new file mode 100644
index 0000000..f4fbc98
Binary files /dev/null and b/MO-Gymnasium/docs/_static/screenshots/highway.png differ
diff --git a/MO-Gymnasium/docs/_static/screenshots/hopper.png b/MO-Gymnasium/docs/_static/screenshots/hopper.png
new file mode 100644
index 0000000..09a4d2b
Binary files /dev/null and b/MO-Gymnasium/docs/_static/screenshots/hopper.png differ
diff --git a/MO-Gymnasium/docs/_static/screenshots/lunarlander.png b/MO-Gymnasium/docs/_static/screenshots/lunarlander.png
new file mode 100644
index 0000000..cf3ae12
Binary files /dev/null and b/MO-Gymnasium/docs/_static/screenshots/lunarlander.png differ
diff --git a/MO-Gymnasium/docs/_static/screenshots/mario.png b/MO-Gymnasium/docs/_static/screenshots/mario.png
new file mode 100644
index 0000000..89d8400
Binary files /dev/null and b/MO-Gymnasium/docs/_static/screenshots/mario.png differ
diff --git a/MO-Gymnasium/docs/_static/screenshots/minecart.png b/MO-Gymnasium/docs/_static/screenshots/minecart.png
new file mode 100644
index 0000000..f5a886b
Binary files /dev/null and b/MO-Gymnasium/docs/_static/screenshots/minecart.png differ
diff --git a/MO-Gymnasium/docs/_static/screenshots/mo-mountaincar.png b/MO-Gymnasium/docs/_static/screenshots/mo-mountaincar.png
new file mode 100644
index 0000000..8356a63
Binary files /dev/null and b/MO-Gymnasium/docs/_static/screenshots/mo-mountaincar.png differ
diff --git a/MO-Gymnasium/docs/_static/screenshots/reacher-mujoco.png b/MO-Gymnasium/docs/_static/screenshots/reacher-mujoco.png
new file mode 100644
index 0000000..a55a139
Binary files /dev/null and b/MO-Gymnasium/docs/_static/screenshots/reacher-mujoco.png differ
diff --git a/MO-Gymnasium/docs/_static/screenshots/reacher.png b/MO-Gymnasium/docs/_static/screenshots/reacher.png
new file mode 100644
index 0000000..6ffb3b4
Binary files /dev/null and b/MO-Gymnasium/docs/_static/screenshots/reacher.png differ
diff --git a/MO-Gymnasium/docs/_static/screenshots/resource-gathering.png b/MO-Gymnasium/docs/_static/screenshots/resource-gathering.png
new file mode 100644
index 0000000..8d17c02
Binary files /dev/null and b/MO-Gymnasium/docs/_static/screenshots/resource-gathering.png differ
diff --git a/MO-Gymnasium/docs/_static/screenshots/water-reservoir.png b/MO-Gymnasium/docs/_static/screenshots/water-reservoir.png
new file mode 100644
index 0000000..16104fc
Binary files /dev/null and b/MO-Gymnasium/docs/_static/screenshots/water-reservoir.png differ
diff --git a/MO-Gymnasium/docs/_static/videos/deep-sea-treasure-concave.gif b/MO-Gymnasium/docs/_static/videos/deep-sea-treasure-concave.gif
new file mode 100644
index 0000000..4bad068
Binary files /dev/null and b/MO-Gymnasium/docs/_static/videos/deep-sea-treasure-concave.gif differ
diff --git a/MO-Gymnasium/docs/_static/videos/deep-sea-treasure.gif b/MO-Gymnasium/docs/_static/videos/deep-sea-treasure.gif
new file mode 100644
index 0000000..6f5b5d8
Binary files /dev/null and b/MO-Gymnasium/docs/_static/videos/deep-sea-treasure.gif differ
diff --git a/MO-Gymnasium/docs/_static/videos/four-room.gif b/MO-Gymnasium/docs/_static/videos/four-room.gif
new file mode 100644
index 0000000..2428292
Binary files /dev/null and b/MO-Gymnasium/docs/_static/videos/four-room.gif differ
diff --git a/MO-Gymnasium/docs/_static/videos/minecart-deterministic.gif b/MO-Gymnasium/docs/_static/videos/minecart-deterministic.gif
new file mode 100644
index 0000000..3559f6b
Binary files /dev/null and b/MO-Gymnasium/docs/_static/videos/minecart-deterministic.gif differ
diff --git a/MO-Gymnasium/docs/_static/videos/minecart.gif b/MO-Gymnasium/docs/_static/videos/minecart.gif
new file mode 100644
index 0000000..6242074
Binary files /dev/null and b/MO-Gymnasium/docs/_static/videos/minecart.gif differ
diff --git a/MO-Gymnasium/docs/_static/videos/mo-halfcheetah.gif b/MO-Gymnasium/docs/_static/videos/mo-halfcheetah.gif
new file mode 100644
index 0000000..bf4cf6b
Binary files /dev/null and b/MO-Gymnasium/docs/_static/videos/mo-halfcheetah.gif differ
diff --git a/MO-Gymnasium/docs/_static/videos/mo-highway-fast.gif b/MO-Gymnasium/docs/_static/videos/mo-highway-fast.gif
new file mode 100644
index 0000000..7393201
Binary files /dev/null and b/MO-Gymnasium/docs/_static/videos/mo-highway-fast.gif differ
diff --git a/MO-Gymnasium/docs/_static/videos/mo-highway.gif b/MO-Gymnasium/docs/_static/videos/mo-highway.gif
new file mode 100644
index 0000000..28cb79a
Binary files /dev/null and b/MO-Gymnasium/docs/_static/videos/mo-highway.gif differ
diff --git a/MO-Gymnasium/docs/_static/videos/mo-hopper.gif b/MO-Gymnasium/docs/_static/videos/mo-hopper.gif
new file mode 100644
index 0000000..402fc20
Binary files /dev/null and b/MO-Gymnasium/docs/_static/videos/mo-hopper.gif differ
diff --git a/MO-Gymnasium/docs/_static/videos/mo-lunar-lander.gif b/MO-Gymnasium/docs/_static/videos/mo-lunar-lander.gif
new file mode 100644
index 0000000..abb2393
Binary files /dev/null and b/MO-Gymnasium/docs/_static/videos/mo-lunar-lander.gif differ
diff --git a/MO-Gymnasium/docs/_static/videos/mo-mountaincar.gif b/MO-Gymnasium/docs/_static/videos/mo-mountaincar.gif
new file mode 100644
index 0000000..d0b7db8
Binary files /dev/null and b/MO-Gymnasium/docs/_static/videos/mo-mountaincar.gif differ
diff --git a/MO-Gymnasium/docs/_static/videos/mo-mountaincarcontinuous.gif b/MO-Gymnasium/docs/_static/videos/mo-mountaincarcontinuous.gif
new file mode 100644
index 0000000..9af3f1e
Binary files /dev/null and b/MO-Gymnasium/docs/_static/videos/mo-mountaincarcontinuous.gif differ
diff --git a/MO-Gymnasium/docs/_static/videos/mo-reacher.gif b/MO-Gymnasium/docs/_static/videos/mo-reacher.gif
new file mode 100644
index 0000000..79a45fe
Binary files /dev/null and b/MO-Gymnasium/docs/_static/videos/mo-reacher.gif differ
diff --git a/MO-Gymnasium/docs/_static/videos/mo-supermario.gif b/MO-Gymnasium/docs/_static/videos/mo-supermario.gif
new file mode 100644
index 0000000..b60f379
Binary files /dev/null and b/MO-Gymnasium/docs/_static/videos/mo-supermario.gif differ
diff --git a/MO-Gymnasium/docs/_static/videos/resource-gathering.gif b/MO-Gymnasium/docs/_static/videos/resource-gathering.gif
new file mode 100644
index 0000000..2187043
Binary files /dev/null and b/MO-Gymnasium/docs/_static/videos/resource-gathering.gif differ
diff --git a/MO-Gymnasium/docs/_static/videos/water-reservoir.gif b/MO-Gymnasium/docs/_static/videos/water-reservoir.gif
new file mode 100644
index 0000000..c0709c4
Binary files /dev/null and b/MO-Gymnasium/docs/_static/videos/water-reservoir.gif differ
diff --git a/MO-Gymnasium/docs/citing/citing.md b/MO-Gymnasium/docs/citing/citing.md
new file mode 100644
index 0000000..b90bd5d
--- /dev/null
+++ b/MO-Gymnasium/docs/citing/citing.md
@@ -0,0 +1,17 @@
+---
+title: "Citing"
+---
+
+```{include} ../README.md
+:start-after: <!-- start citation -->
+:end-before: <!-- end citation -->
+```
+
+```{toctree}
+:hidden:
+:glob:
+:caption: List of Publications
+
+../examples/publications
+
+```
diff --git a/MO-Gymnasium/docs/community/community.md b/MO-Gymnasium/docs/community/community.md
new file mode 100644
index 0000000..e66d454
--- /dev/null
+++ b/MO-Gymnasium/docs/community/community.md
@@ -0,0 +1,13 @@
+# Community
+
+If you want to help us out, reach us, or simply ask questions, you can join the Farama discord server [here](https://discord.gg/bnJ6kubTg6).
+
+## Acknowledgements
+
+Aside from the main contributors, some people have also contributed to the project in various ways. We would like to thank them all for their contributions.
+
+* The `minecart-v0` env is a refactor of https://github.com/axelabels/DynMORL.
+* The `deep-sea-treasure-v0`, `fruit-tree-v0` and `mo-supermario-v0` envs are based on https://github.com/RunzheYang/MORL.
+* The `four-room-v0` env is based on https://github.com/mike-gimelfarb/deep-successor-features-for-transfer.
+* The `fishwood-v0` code was provided by Denis Steckelmacher and Conor F. Hayes.
+* The `water-reservoir-v0` code was provided by Mathieu Reymond.
diff --git a/MO-Gymnasium/docs/conf.py b/MO-Gymnasium/docs/conf.py
new file mode 100644
index 0000000..9d26972
--- /dev/null
+++ b/MO-Gymnasium/docs/conf.py
@@ -0,0 +1,88 @@
+# Configuration file for the Sphinx documentation builder.
+#
+# This file only contains a selection of the most common options. For a full
+# list see the documentation:
+# https://www.sphinx-doc.org/en/master/usage/configuration.html
+
+# -- Path setup --------------------------------------------------------------
+
+# If extensions (or modules to document with autodoc) are in another directory,
+# add these directories to sys.path here. If the directory is relative to the
+# documentation root, use os.path.abspath to make it absolute, like shown here.
+#
+# import os
+# import sys
+# sys.path.insert(0, os.path.abspath('.'))
+
+# -- Project information -----------------------------------------------------
+import os
+from typing import Any, Dict
+
+import mo_gymnasium
+
+
+project = "MO-Gymnasium"
+copyright = "2023"
+author = "Farama Foundation"
+
+# The full version, including alpha/beta/rc tags
+release = mo_gymnasium.__version__
+
+
+# -- General configuration ---------------------------------------------------
+
+# Add any Sphinx extension module names here, as strings. They can be
+# extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
+# ones.
+extensions = [
+    "sphinx.ext.napoleon",
+    "sphinx.ext.doctest",
+    "sphinx.ext.autodoc",
+    "sphinx.ext.githubpages",
+    "myst_parser",
+    "sphinx_github_changelog",
+]
+
+# Add any paths that contain templates here, relative to this directory.
+templates_path = ["_templates"]
+
+# List of patterns, relative to source directory, that match files and
+# directories to ignore when looking for source files.
+# This pattern also affects html_static_path and html_extra_path.
+exclude_patterns = []
+
+# Napoleon settings
+napoleon_use_ivar = True
+napoleon_use_admonition_for_references = True
+# See https://github.com/sphinx-doc/sphinx/issues/9119
+napoleon_custom_sections = [("Returns", "params_style")]
+
+# -- Options for HTML output -------------------------------------------------
+
+# The theme to use for HTML and HTML Help pages.  See the documentation for
+# a list of builtin themes.
+#
+html_theme = "furo"
+html_title = "MO-Gymnasium Documentation"
+html_baseurl = "https://mo-gymnasium.farama.org"
+html_copy_source = False
+html_favicon = "_static/img/favicon.png"
+html_theme_options = {
+    # "light_logo": "img/Minari.svg",
+    # "dark_logo": "img/Minari_White.svg",
+    "versioning": True,
+}
+html_context: Dict[str, Any] = {}
+html_context["conf_py_path"] = "/docs/"
+html_context["display_github"] = True
+html_context["github_user"] = "Farama-Foundation"
+html_context["github_repo"] = "MO-Gymnasium"
+html_context["github_version"] = "main"
+html_context["slug"] = "mo_gymnasium"
+
+html_static_path = ["_static"]
+html_css_files = []
+
+# -- Generate Changelog -------------------------------------------------
+
+sphinx_github_changelog_token = os.environ.get("SPHINX_GITHUB_CHANGELOG_TOKEN")
diff --git a/MO-Gymnasium/docs/environments/all-environments.md b/MO-Gymnasium/docs/environments/all-environments.md
new file mode 100644
index 0000000..b39b3ee
--- /dev/null
+++ b/MO-Gymnasium/docs/environments/all-environments.md
@@ -0,0 +1,38 @@
+---
+title: "Environments"
+---
+
+# Available environments
+
+
+MO-Gymnasium includes environments taken from the MORL literature, as well as multi-objective version of classical environments, such as Mujoco.
+
+| Env                                                                                                                                                                                                                                        | Obs/Action spaces                   | Objectives                                                    | Description                                                                                                                                                                                                                                                                                                                                         |
+|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------|---------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
+| [`deep-sea-treasure-v0`](https://mo-gymnasium.farama.org/environments/deep-sea-treasure/) <br><img src="https://raw.githubusercontent.com/Farama-Foundation/MO-Gymnasium/main/docs/_static/videos/deep-sea-treasure.gif" width="200px">                          | Discrete / Discrete                 | `[treasure, time_penalty]`                                    | Agent is a submarine that must collect a treasure while taking into account a time penalty. Treasures values taken from [Yang et al. 2019](https://arxiv.org/pdf/1908.08342.pdf).                                                                                                                                                                   |
+| [`deep-sea-treasure-concave-v0`](https://mo-gymnasium.farama.org/environments/deep-sea-treasure-concave/) <br><img src="https://raw.githubusercontent.com/Farama-Foundation/MO-Gymnasium/main/docs/_static/videos/deep-sea-treasure-concave.gif" width="200px">  | Discrete / Discrete                 | `[treasure, time_penalty]`                                    | Agent is a submarine that must collect a treasure while taking into account a time penalty. Treasures values taken from [Vamplew et al. 2010](https://link.springer.com/article/10.1007/s10994-010-5232-5).                                                                                                                                         |
+| [`resource-gathering-v0`](https://mo-gymnasium.farama.org/environments/resource-gathering/) <br><img src="https://raw.githubusercontent.com/Farama-Foundation/MO-Gymnasium/main/docs/_static/videos/resource-gathering.gif" width="200px">         | Discrete / Discrete                 | `[enemy, gold, gem]`                                          | Agent must collect gold or gem. Enemies have a 10% chance of killing the agent. From [Barret & Narayanan 2008](https://dl.acm.org/doi/10.1145/1390156.1390162).                                                                                                                                                                                     |
+| [`fishwood-v0`](https://mo-gymnasium.farama.org/environments/fishwood/) <br><img src="https://raw.githubusercontent.com/Farama-Foundation/MO-Gymnasium/main/docs/_static/screenshots/fishwood.png" width="200px">                                       | Discrete / Discrete                 | `[fish_amount, wood_amount]`                                  | ESR environment, the agent must collect fish and wood to light a fire and eat. From [Roijers et al. 2018](https://www.researchgate.net/publication/328718263_Multi-objective_Reinforcement_Learning_for_the_Expected_Utility_of_the_Return).                                                                                                        |
+| [`breakable-bottles-v0`](https://mo-gymnasium.farama.org/environments/breakable-bottles/) <br><img src="https://raw.githubusercontent.com/Farama-Foundation/MO-Gymnasium/main/docs/_static/screenshots/breakable-bottles.jpg" width="200px">            | Discrete (Dictionary) / Discrete    | `[time_penalty, bottles_delivered, potential]`                | Gridworld with 5 cells. The agents must collect bottles from the source location and deliver to the destination. From [Vamplew et al. 2021](https://www.sciencedirect.com/science/article/pii/S0952197621000336).                                                                                                                                   |
+| [`fruit-tree-v0`](https://mo-gymnasium.farama.org/environments/fruit-tree/) <br><img src="https://raw.githubusercontent.com/Farama-Foundation/MO-Gymnasium/main/docs/_static/screenshots/fruit-tree.png" width="200px">                                 | Discrete / Discrete                 | `[nutri1, ..., nutri6]`                                       | Full binary tree of depth d=5,6 or 7. Every leaf contains a fruit with a value for the nutrients Protein, Carbs, Fats, Vitamins, Minerals and Water. From [Yang et al. 2019](https://arxiv.org/pdf/1908.08342.pdf).                                                                                                                                 |
+| [`water-reservoir-v0`](https://mo-gymnasium.farama.org/environments/water-reservoir/)   <br><img src="https://raw.githubusercontent.com/Farama-Foundation/MO-Gymnasium/main/docs/_static/videos/water-reservoir.gif" width="200px">                            | Continuous / Continuous             | `[cost_flooding, deficit_water]`                              | A Water reservoir environment. The agent executes a continuous action, corresponding to the amount of water released by the dam. From [Pianosi et al. 2013](https://iwaponline.com/jh/article/15/2/258/3425/Tree-based-fitted-Q-iteration-for-multi-objective).                                                                                     |
+| [`four-room-v0`](https://mo-gymnasium.farama.org/environments/four-room/) <br><img src="https://raw.githubusercontent.com/Farama-Foundation/MO-Gymnasium/main/docs/_static/videos/four-room.gif" width="200px">                                    | Discrete / Discrete                 | `[item1, item2, item3]`                                       | Agent must collect three different types of items in the map and reach the goal. From [Alegre et al. 2022](https://proceedings.mlr.press/v162/alegre22a.html).                                                                                                                                                                                      |
+| [`mo-mountaincar-v0`](https://mo-gymnasium.farama.org/environments/mo-mountaincar/) <br><img src="https://raw.githubusercontent.com/Farama-Foundation/MO-Gymnasium/main/docs/_static/videos/mo-mountaincar.gif" width="200px">                     | Continuous / Discrete               | `[time_penalty, reverse_penalty, forward_penalty]`            | Classic Mountain Car env, but with extra penalties for the forward and reverse actions. From [Vamplew et al. 2011](https://www.researchgate.net/publication/220343783_Empirical_evaluation_methods_for_multiobjective_reinforcement_learning_algorithms).                                                                                           |
+| [`mo-mountaincarcontinuous-v0`](https://mo-gymnasium.farama.org/environments/mo-mountaincarcontinuous/) <br><img src="https://raw.githubusercontent.com/Farama-Foundation/MO-Gymnasium/main/docs/_static/videos/mo-mountaincarcontinuous.gif" width="200px"> | Continuous / Continuous             | `[time_penalty, fuel_consumption_penalty]`                    | Continuous Mountain Car env, but with penalties for fuel consumption.                                                                                                                                                                                                                                                                               |
+| [`mo-lunar-lander-v2`](https://mo-gymnasium.farama.org/environments/mo-lunar-lander/) <br><img src="https://raw.githubusercontent.com/Farama-Foundation/MO-Gymnasium/main/docs/_static/videos/mo-lunar-lander.gif" width="200px">                      | Continuous / Discrete or Continuous | `[landed, shaped_reward, main_engine_fuel, side_engine_fuel]` | MO version of the `LunarLander-v2` [environment](https://gymnasium.farama.org/environments/box2d/lunar_lander/). Objectives defined similarly as in [Hung et al. 2022](https://openreview.net/forum?id=AwWaBXLIJE).                                                                                                                                 |
+| [`minecart-v0`](https://mo-gymnasium.farama.org/environments/minecart/) <br><img src="https://raw.githubusercontent.com/Farama-Foundation/MO-Gymnasium/main/docs/_static/videos/minecart.gif" width="200px">                                       | Continuous or Image / Discrete      | `[ore1, ore2, fuel]`                                          | Agent must collect two types of ores and minimize fuel consumption. From [Abels et al. 2019](https://arxiv.org/abs/1809.07803v2).                                                                                                                                                                                                                   |
+| [`mo-highway-v0`](https://mo-gymnasium.farama.org/environments/mo-highway/) and `mo-highway-fast-v0` <br><img src="https://raw.githubusercontent.com/Farama-Foundation/MO-Gymnasium/main/docs/_static/videos/mo-highway.gif" width="200px">           | Continuous / Discrete               | `[speed, right_lane, collision]`                              | The agent's objective is to reach a high speed while avoiding collisions with neighbouring vehicles and staying on the rightest lane. From [highway-env](https://github.com/eleurent/highway-env).                                                                                                                                                  |
+| [`mo-supermario-v0`](https://mo-gymnasium.farama.org/environments/mo-supermario/) <br><img src="https://raw.githubusercontent.com/Farama-Foundation/MO-Gymnasium/main/docs/_static/videos/mo-supermario.gif" width="200px">                                | Image / Discrete                    | `[x_pos, time, death, coin, enemy]`                           | [:warning: SuperMarioBrosEnv support is limited.] Multi-objective version of [SuperMarioBrosEnv](https://github.com/Kautenja/gym-super-mario-bros). Objectives are defined similarly as in [Yang et al. 2019](https://arxiv.org/pdf/1908.08342.pdf).                                                                                                |
+| [`mo-reacher-v4`](https://mo-gymnasium.farama.org/environments/mo-reacher/) <br><img src="https://raw.githubusercontent.com/Farama-Foundation/MO-Gymnasium/main/docs/_static/videos/mo-reacher.gif" width="200px">                             | Continuous / Discrete               | `[target_1, target_2, target_3, target_4]`                    | Mujoco version of `mo-reacher-v0`, based on `Reacher-v4` [environment](https://gymnasium.farama.org/environments/mujoco/reacher/).                                                                                                                                                                                                                  |
+| [`mo-hopper-v4`](https://mo-gymnasium.farama.org/environments/mo-hopper/) <br><img src="https://raw.githubusercontent.com/Farama-Foundation/MO-Gymnasium/main/docs/_static/videos/mo-hopper.gif" width="200px">                                       | Continuous / Continuous             | `[velocity, height, energy]`                                  | Multi-objective version of [Hopper-v4](https://gymnasium.farama.org/environments/mujoco/hopper/) env.                                                                                                                                                                                                                                               |
+| [`mo-halfcheetah-v4`](https://mo-gymnasium.farama.org/environments/mo-halfcheetah/) <br><img src="https://raw.githubusercontent.com/Farama-Foundation/MO-Gymnasium/main/docs/_static/videos/mo-halfcheetah.gif" width="200px">                            | Continuous / Continuous             | `[velocity, energy]`                                          | Multi-objective version of [HalfCheetah-v4](https://gymnasium.farama.org/environments/mujoco/half_cheetah/) env. Similar to [Xu et al. 2020](https://github.com/mit-gfx/PGMORL).                                                                                                                                                                    |
+
+
+```{toctree}
+:hidden:
+:glob:
+:caption: MO-Gymnasium Environments
+
+./*
+
+```
diff --git a/MO-Gymnasium/docs/examples/citation.md b/MO-Gymnasium/docs/examples/citation.md
new file mode 100644
index 0000000..ae016b2
--- /dev/null
+++ b/MO-Gymnasium/docs/examples/citation.md
@@ -0,0 +1,11 @@
+---
+title: Citation
+firstpage:
+---
+
+## Citation
+
+```{include} ../../README.md
+:start-after: <!-- start citation -->
+:end-before: <!-- end citation -->
+```
diff --git a/MO-Gymnasium/docs/examples/morl_baselines.md b/MO-Gymnasium/docs/examples/morl_baselines.md
new file mode 100644
index 0000000..6d43346
--- /dev/null
+++ b/MO-Gymnasium/docs/examples/morl_baselines.md
@@ -0,0 +1,7 @@
+---
+title: "Examples"
+---
+
+# MORL Baselines
+
+[MORL-Baselines](https://github.com/LucasAlegre/morl-baselines) is a repository containing multiple implementations of MORL algorithms under the MO-Gymnasium API. It contains various example of how to use the API and wrappers.
diff --git a/MO-Gymnasium/docs/examples/publications.md b/MO-Gymnasium/docs/examples/publications.md
new file mode 100644
index 0000000..37e58d5
--- /dev/null
+++ b/MO-Gymnasium/docs/examples/publications.md
@@ -0,0 +1,13 @@
+---
+title: Publications
+firstpage:
+---
+
+## List of Publications
+
+MO-Gymnasium (formerly MO-Gym) was first published in:
+- [MO-Gym: A Library of Multi-Objective Reinforcement Learning Environments](https://bnaic2022.uantwerpen.be/wp-content/uploads/BNAICBeNeLearn_2022_submission_6485.pdf) (Alegre et al., BNAIC 2022)
+
+
+List of publications & submissions using MO-Gymnasium (please open a pull request to add missing entries):
+- [Sample-Efficient Multi-Objective Learning via Generalized Policy Improvement Prioritization](https://arxiv.org/abs/2301.07784) (Alegre et al., AAMAS 2023)
diff --git a/MO-Gymnasium/docs/index.md b/MO-Gymnasium/docs/index.md
new file mode 100644
index 0000000..b0f1407
--- /dev/null
+++ b/MO-Gymnasium/docs/index.md
@@ -0,0 +1,68 @@
+---
+title: "MO-Gymnasium"
+firstpage:
+lastpage:
+---
+
+```{toctree}
+:hidden:
+:caption: Introduction
+
+introduction/install
+introduction/api
+environments/all-environments
+wrappers/wrappers
+examples/morl_baselines
+```
+
+```{toctree}
+:hidden:
+:caption: Citing
+
+examples/citation
+examples/publications
+```
+
+```{toctree}
+:hidden:
+:caption: Development
+
+community/community
+release_notes
+Github <https://github.com/Farama-Foundation/MO-Gymnasium>
+Donate <https://farama.org/donations>
+
+```
+
+# MO-Gymnasium is a standardized API and a suite of environments for multi-objective reinforcement learning (MORL)
+
+```{figure} _static/mo_cheetah_rect.gif
+   :alt: MO-HalfCheetah gif
+   :width: 500px
+```
+
+```{include} ../README.md
+:start-after: <!-- start elevator-pitch -->
+:end-before: <!-- end elevator-pitch -->
+```
+
+## API
+
+```{include} ../README.md
+:start-after: <!-- start snippet-usage -->
+:end-before: <!-- end snippet-usage -->
+```
+
+## Install
+
+```{include} ../README.md
+:start-after: <!-- start install -->
+:end-before: <!-- end install -->
+```
+
+## Citing
+
+```{include} ../README.md
+:start-after: <!-- start citation -->
+:end-before: <!-- end citation -->
+```
diff --git a/MO-Gymnasium/docs/introduction/api.md b/MO-Gymnasium/docs/introduction/api.md
new file mode 100644
index 0000000..14d2c19
--- /dev/null
+++ b/MO-Gymnasium/docs/introduction/api.md
@@ -0,0 +1,11 @@
+---
+title: API
+firstpage:
+---
+
+## API
+
+```{include} ../../README.md
+:start-after: <!-- start snippet-usage -->
+:end-before: <!-- end snippet-usage -->
+```
diff --git a/MO-Gymnasium/docs/introduction/install.md b/MO-Gymnasium/docs/introduction/install.md
new file mode 100644
index 0000000..52dd0e1
--- /dev/null
+++ b/MO-Gymnasium/docs/introduction/install.md
@@ -0,0 +1,12 @@
+---
+title: Install
+firstpage:
+---
+
+
+## Install
+
+```{include} ../../README.md
+:start-after: <!-- start install -->
+:end-before: <!-- end install -->
+```
diff --git a/MO-Gymnasium/docs/make.bat b/MO-Gymnasium/docs/make.bat
new file mode 100644
index 0000000..307157b
--- /dev/null
+++ b/MO-Gymnasium/docs/make.bat
@@ -0,0 +1,35 @@
+@ECHO OFF
+
+pushd %~dp0
+
+REM Command file for Sphinx documentation
+
+if "%SPHINXBUILD%" == "" (
+	set SPHINXBUILD=sphinx-build
+)
+set SOURCEDIR=.
+set BUILDDIR=build
+
+if "%1" == "" goto help
+
+%SPHINXBUILD% >NUL 2>NUL
+if errorlevel 9009 (
+	echo.
+	echo.The 'sphinx-build' command was not found. Make sure you have Sphinx
+	echo.installed, then set the SPHINXBUILD environment variable to point
+	echo.to the full path of the 'sphinx-build' executable. Alternatively you
+	echo.may add the Sphinx directory to PATH.
+	echo.
+	echo.If you don't have Sphinx installed, grab it from
+	echo.https://www.sphinx-doc.org/
+	exit /b 1
+)
+
+%SPHINXBUILD% -M %1 %SOURCEDIR% %BUILDDIR% %SPHINXOPTS% %O%
+goto end
+
+:help
+%SPHINXBUILD% -M help %SOURCEDIR% %BUILDDIR% %SPHINXOPTS% %O%
+
+:end
+popd
diff --git a/MO-Gymnasium/docs/release_notes.md b/MO-Gymnasium/docs/release_notes.md
new file mode 100644
index 0000000..842c040
--- /dev/null
+++ b/MO-Gymnasium/docs/release_notes.md
@@ -0,0 +1,8 @@
+# Release Notes
+
+```{eval-rst}
+.. changelog::
+    :github: https://github.com/Farama-Foundation/MO-Gymnasium/releases
+    :pypi: https://pypi.org/project/mo-gymnasium/
+    :changelog-url:
+```
diff --git a/MO-Gymnasium/docs/requirements.txt b/MO-Gymnasium/docs/requirements.txt
new file mode 100644
index 0000000..a3e8a34
--- /dev/null
+++ b/MO-Gymnasium/docs/requirements.txt
@@ -0,0 +1,8 @@
+cython
+gymnasium
+mo_gymnasium[all]
+sphinx
+myst-parser
+tqdm
+git+https://github.com/Farama-Foundation/Celshast#egg=furo
+sphinx_github_changelog
diff --git a/MO-Gymnasium/docs/wrappers/wrappers.md b/MO-Gymnasium/docs/wrappers/wrappers.md
new file mode 100644
index 0000000..542e5cc
--- /dev/null
+++ b/MO-Gymnasium/docs/wrappers/wrappers.md
@@ -0,0 +1,39 @@
+---
+title: "Wrappers"
+---
+
+# Wrappers
+
+A few wrappers inspired from Gymnasium's wrappers are available in MO-Gymnasium. They are all available directly from the `mo_gymnasium` module.
+
+
+## `LinearReward`
+
+
+```{eval-rst}
+.. autoclass:: mo_gymnasium.LinearReward
+```
+
+## `MONormalizeReward`
+
+```{eval-rst}
+.. autoclass:: mo_gymnasium.MONormalizeReward
+```
+
+## `MOClipReward`
+
+```{eval-rst}
+.. autoclass:: mo_gymnasium.MOClipReward
+```
+
+## `MOSyncVectorEnv`
+
+```{eval-rst}
+.. autoclass:: mo_gymnasium.MOSyncVectorEnv
+```
+
+## `MORecordEpisodeStatistics`
+
+```{eval-rst}
+.. autoclass:: mo_gymnasium.MORecordEpisodeStatistics
+```
diff --git a/MO-Gymnasium/mo_gymnasium/__init__.py b/MO-Gymnasium/mo_gymnasium/__init__.py
new file mode 100644
index 0000000..ff282df
--- /dev/null
+++ b/MO-Gymnasium/mo_gymnasium/__init__.py
@@ -0,0 +1,17 @@
+"""Exports everything that is relevant in the repo."""
+
+# Envs
+import mo_gymnasium.envs
+
+# Utils
+from mo_gymnasium.utils import (
+    LinearReward,
+    MOClipReward,
+    MONormalizeReward,
+    MORecordEpisodeStatistics,
+    MOSyncVectorEnv,
+    make,
+)
+
+
+__version__ = "0.3.4"
diff --git a/MO-Gymnasium/mo_gymnasium/envs/__init__.py b/MO-Gymnasium/mo_gymnasium/envs/__init__.py
new file mode 100644
index 0000000..7e91739
--- /dev/null
+++ b/MO-Gymnasium/mo_gymnasium/envs/__init__.py
@@ -0,0 +1,15 @@
+import mo_gymnasium.envs.breakable_bottles
+import mo_gymnasium.envs.continuous_mountain_car
+import mo_gymnasium.envs.deep_sea_treasure
+import mo_gymnasium.envs.fishwood
+import mo_gymnasium.envs.four_room
+import mo_gymnasium.envs.fruit_tree
+import mo_gymnasium.envs.highway
+import mo_gymnasium.envs.lunar_lander
+import mo_gymnasium.envs.mario
+import mo_gymnasium.envs.minecart
+import mo_gymnasium.envs.mountain_car
+import mo_gymnasium.envs.mujoco
+import mo_gymnasium.envs.reacher
+import mo_gymnasium.envs.resource_gathering
+import mo_gymnasium.envs.water_reservoir
diff --git a/MO-Gymnasium/mo_gymnasium/envs/breakable_bottles/__init__.py b/MO-Gymnasium/mo_gymnasium/envs/breakable_bottles/__init__.py
new file mode 100644
index 0000000..b11a5e1
--- /dev/null
+++ b/MO-Gymnasium/mo_gymnasium/envs/breakable_bottles/__init__.py
@@ -0,0 +1,8 @@
+from gymnasium.envs.registration import register
+
+
+register(
+    id="breakable-bottles-v0",
+    entry_point="mo_gymnasium.envs.breakable_bottles.breakable_bottles:BreakableBottles",
+    max_episode_steps=100,
+)
diff --git a/MO-Gymnasium/mo_gymnasium/envs/breakable_bottles/breakable_bottles.py b/MO-Gymnasium/mo_gymnasium/envs/breakable_bottles/breakable_bottles.py
new file mode 100644
index 0000000..8c8abcb
--- /dev/null
+++ b/MO-Gymnasium/mo_gymnasium/envs/breakable_bottles/breakable_bottles.py
@@ -0,0 +1,239 @@
+from typing import Optional
+
+import numpy as np
+from gymnasium import Env
+from gymnasium.spaces import Box, Dict, Discrete, MultiBinary
+from gymnasium.utils import EzPickle
+
+
+class BreakableBottles(Env, EzPickle):
+    """
+    ## Description
+    This environment implements the problems UnbreakableBottles and BreakableBottles defined in Section 4.1.2 of the paper
+    [Potential-based multiobjective reinforcement learning approaches to low-impact agents for AI safety](https://www.sciencedirect.com/science/article/pii/S0952197621000336).
+
+    ## Action Space
+    The action space is a discrete space with 3 actions:
+    - 0: move left
+    - 1: move right
+    - 2: pick up a bottle
+
+    ## Observation Space
+    The observation space is a dictionary with 4 keys:
+    - location: the current location of the agent
+    - bottles_carrying: the number of bottles the agent is currently carrying (0, 1 or 2)
+    - bottles_delivered: the number of bottles the agent has delivered (0 or 1)
+    - bottles_dropped: for each location, a boolean flag indicating if that location currently contains a bottle
+
+    ## Reward Space
+    The reward space has 3 dimensions:
+    - time penalty: -1 for each time step
+    - bottle reward: bottle_reward for each bottle delivered
+    - potential: While carrying multiple bottles there is a small probability of dropping them. A potential-based penalty is applied for bottles left on the ground.
+
+    ## Starting State
+    The agent starts at location 0, carrying no bottles, having delivered no bottles and having dropped no bottles.
+
+    ## Episode Termination
+    The episode terminates when the agent has delivered 2 bottles.
+
+    ## Arguments
+    - size: the number of locations in the environment
+    - prob_drop: the probability of dropping a bottle while carrying 2 bottles
+    - time_penalty: the time penalty for each time step
+    - bottle_reward: the reward for delivering a bottle
+    - unbreakable_bottles: if True, a bottle which is dropped in a location can be picked up again (so the outcome of dropping a bottle is reversible),
+    otherwise a dropped bottle cannot be picked up.
+
+    ## Credits
+    This environment was originally a contribution of Robert Klassert
+    """
+
+    metadata = {"render_modes": ["human"]}
+
+    # actions
+    LEFT = 0
+    RIGHT = 1
+    PICKUP = 2
+
+    def __init__(
+        self,
+        render_mode: Optional[str] = None,
+        size=5,
+        prob_drop=0.1,
+        time_penalty=-1,
+        bottle_reward=25,
+        unbreakable_bottles=False,
+    ):
+        EzPickle.__init__(self, render_mode, size, prob_drop, time_penalty, bottle_reward, unbreakable_bottles)
+
+        self.render_mode = render_mode
+
+        # settings
+        self.prob_drop = prob_drop
+        self.time_penalty = time_penalty
+        self.bottle_reward = bottle_reward
+        self.unbreakable_bottles = unbreakable_bottles
+
+        # properties
+        self.num_objectives = 3
+
+        # initialize env state
+        self.size = size
+        self.location = 0
+        self.bottles_carrying = 0
+        self.bottles_delivered = 0
+        self.bottles_dropped = [0] * (self.size - 2)
+
+        # observation and action space
+        self.observation_space = Dict(
+            {
+                "location": Discrete(self.size),
+                "bottles_carrying": Discrete(3),
+                "bottles_delivered": Discrete(2),
+                "bottles_dropped": MultiBinary(self.size - 2),
+            }
+        )
+        self.num_observations = 240
+
+        self.action_space = Discrete(3)  # LEFT, RIGHT, PICKUP
+        self.num_actions = 3
+
+        # reward space
+        self.reward_space = Box(np.array([-np.inf, 0, -1]), np.array([0, self.bottle_reward * 2, 0]))
+        self.reward_dim = 3
+
+    def step(self, action):
+        observation_old = self._get_obs()
+        old_potential = self.potential(observation_old)
+        terminal = False
+        reward = [self.time_penalty, 0, 0]
+
+        if action == self.LEFT and self.location > 0:
+            # execute bottle drop, if agent is carrying at least two and current location is 1, 2 or 3
+            if (
+                self.location in range(1, self.size - 1)
+                and self.bottles_carrying > 1
+                and self.np_random.random() < self.prob_drop
+            ):
+                self.bottles_carrying -= 1
+                self.bottles_dropped[self.location - 1] += 1
+
+            # move to the left
+            self.location -= 1
+
+        elif action == self.RIGHT and self.location < self.size - 1:
+            # execute bottle drop, if agent is carrying at least two and current location is 1, 2 or 3
+            if (
+                self.location in range(1, self.size - 1)
+                and self.bottles_carrying > 1
+                and self.np_random.random() < self.prob_drop
+            ):
+                self.bottles_carrying -= 1
+                self.bottles_dropped[self.location - 1] += 1
+
+            # move to the right
+            self.location += 1
+
+            # if agent enters destination tile, deliver any carried bottles
+            if self.location == self.size - 1 and self.bottles_carrying > 0:
+                num_before = self.bottles_delivered
+                num_after = min(self.bottles_delivered + self.bottles_carrying, 2)
+                num_delivered = num_after - num_before
+                self.bottles_delivered = num_after
+                self.bottles_carrying = 0
+                reward[1] += self.bottle_reward * num_delivered
+                if self.bottles_delivered == 2:
+                    terminal = True
+
+        elif action == self.PICKUP:
+            # agent is at source and has not reached carrying limit
+            if self.location == 0 and self.bottles_carrying < 2:
+                # add bottle to inventory
+                self.bottles_carrying += 1
+            # agent is on a tile where a dropped bottle lies and it has not reached the carrying limit
+            elif (
+                self.location in range(1, self.size - 1)
+                and self.bottles_dropped[self.location - 1] > 0
+                and self.bottles_carrying < 2
+                and self.unbreakable_bottles
+            ):
+                # remove bottle from current tile
+                self.bottles_dropped[self.location - 1] -= 1
+                # add bottle to agent's inventory
+                self.bottles_carrying += 1
+
+        # next observation
+        observation = self._get_obs()
+
+        # calculate potential-based low impact measure
+        # r2_t = phi(S_t) - phi(S_t-1)
+        # sum_t(r2_t) = 0 -> no impact
+        reward[2] = self.potential(observation) - old_potential
+
+        if self.render_mode == "human":
+            self.render()
+        return observation, reward, terminal, False, {}
+
+    def reset(self, *, seed: Optional[int] = None, options: Optional[dict] = None):
+        super().reset(seed=seed)
+        self.r_star = 0
+        self.location = self.size - 1
+        self.bottles_carrying = 0
+        self.bottles_delivered = 0
+        self.bottles_dropped = [0] * (self.size - 2)
+        state = self._get_obs()
+        if self.render_mode == "human":
+            self.render()
+        return state, {}
+
+    def get_obs_idx(self, obs):
+        multi_index = np.array(
+            [
+                [obs["location"]],
+                [obs["bottles_carrying"]],
+                [obs["bottles_delivered"]],
+                *[[bd > 0] for bd in obs["bottles_dropped"]],
+            ]
+        )
+        return np.ravel_multi_index(multi_index, tuple([self.size, 3, 2, *([2] * (self.size - 2))]))
+
+    def _get_obs(self):
+        return {
+            "location": self.location,
+            "bottles_carrying": self.bottles_carrying,
+            "bottles_delivered": self.bottles_delivered,
+            "bottles_dropped": self.bottles_dropped.copy(),
+        }
+
+    def render(self):
+        if self.render_mode == "human":
+            print("-----")
+            print(
+                f"Location: {self.location}\nCarrying {self.bottles_carrying} bottles.\nDelivered {self.bottles_delivered} so far.\nBottles have been dropped at tiles {'1' if self.bottles_dropped[0] > 0 else ''} {'2' if self.bottles_dropped[1] > 0 else ''} {'3' if self.bottles_dropped[2] > 0 else ''}"
+            )
+            print("-----")
+
+    def close(self):
+        pass
+
+    def potential(self, obs):
+        if sum(obs["bottles_dropped"]) > 0:
+            return -1
+        return 0
+
+
+if __name__ == "__main__":
+    from gymnasium.spaces.utils import flatdim
+
+    env = BreakableBottles(size=5, prob_drop=0.1)
+    assert flatdim(env.action_space) == 3
+    assert flatdim(env.observation_space) == 13
+
+    done = False
+    obs = env.reset()
+    while True:
+        env.render()
+        obs, r, done, info = env.step(env.action_space.sample())
+        if done:
+            break
diff --git a/MO-Gymnasium/mo_gymnasium/envs/continuous_mountain_car/__init__.py b/MO-Gymnasium/mo_gymnasium/envs/continuous_mountain_car/__init__.py
new file mode 100644
index 0000000..a64adfc
--- /dev/null
+++ b/MO-Gymnasium/mo_gymnasium/envs/continuous_mountain_car/__init__.py
@@ -0,0 +1,8 @@
+from gymnasium.envs.registration import register
+
+
+register(
+    id="mo-mountaincarcontinuous-v0",
+    entry_point="mo_gymnasium.envs.continuous_mountain_car.continuous_mountain_car:MOContinuousMountainCar",
+    max_episode_steps=999,
+)
diff --git a/MO-Gymnasium/mo_gymnasium/envs/continuous_mountain_car/continuous_mountain_car.py b/MO-Gymnasium/mo_gymnasium/envs/continuous_mountain_car/continuous_mountain_car.py
new file mode 100644
index 0000000..1bddbc5
--- /dev/null
+++ b/MO-Gymnasium/mo_gymnasium/envs/continuous_mountain_car/continuous_mountain_car.py
@@ -0,0 +1,68 @@
+import math
+from typing import Optional
+
+import numpy as np
+from gymnasium import spaces
+from gymnasium.envs.classic_control.continuous_mountain_car import (
+    Continuous_MountainCarEnv,
+)
+from gymnasium.utils import EzPickle
+
+
+class MOContinuousMountainCar(Continuous_MountainCarEnv, EzPickle):
+    """
+    A continuous version of the MountainCar environment, where the goal is to reach the top of the mountain.
+
+    See [source](https://gymnasium.farama.org/environments/classic_control/mountain_car_continuous/) for more information.
+
+    ## Reward space:
+    The reward space is a 2D vector containing the time penalty and the fuel reward.
+    - time penalty: -1.0 for each time step
+    - fuel reward: -||action||^2 , i.e. the negative of the norm of the action vector
+    """
+
+    def __init__(self, render_mode: Optional[str] = None, goal_velocity=0):
+        super().__init__(render_mode, goal_velocity)
+        EzPickle.__init__(self, render_mode, goal_velocity)
+
+        self.reward_space = spaces.Box(low=np.array([-1.0, -1.0]), high=np.array([0.0, 0.0]), shape=(2,), dtype=np.float32)
+        self.reward_dim = 2
+
+    def step(self, action: np.ndarray):
+        # Essentially a copy paste from original env, except the rewards
+
+        position = self.state[0]
+        velocity = self.state[1]
+        force = min(max(action[0], self.min_action), self.max_action)
+
+        velocity += force * self.power - 0.0025 * math.cos(3 * position)
+        if velocity > self.max_speed:
+            velocity = self.max_speed
+        if velocity < -self.max_speed:
+            velocity = -self.max_speed
+        position += velocity
+        if position > self.max_position:
+            position = self.max_position
+        if position < self.min_position:
+            position = self.min_position
+        if position == self.min_position and velocity < 0:
+            velocity = 0
+
+        # Convert a possible numpy bool to a Python bool.
+        terminated = bool(position >= self.goal_position and velocity >= self.goal_velocity)
+
+        reward = np.zeros(2)
+        # Time reward is negative at all timesteps except when reaching the goal
+        if terminated:
+            reward[0] = 0.0
+        else:
+            reward[0] = -1.0
+
+        # Actions cost fuel, which we want to optimize too
+        reward[1] = -math.pow(action[0], 2)
+
+        self.state = np.array([position, velocity], dtype=np.float32)
+
+        if self.render_mode == "human":
+            self.render()
+        return self.state, reward, terminated, False, {}
diff --git a/MO-Gymnasium/mo_gymnasium/envs/deep_sea_treasure/__init__.py b/MO-Gymnasium/mo_gymnasium/envs/deep_sea_treasure/__init__.py
new file mode 100644
index 0000000..65799cd
--- /dev/null
+++ b/MO-Gymnasium/mo_gymnasium/envs/deep_sea_treasure/__init__.py
@@ -0,0 +1,17 @@
+from gymnasium.envs.registration import register
+
+from mo_gymnasium.envs.deep_sea_treasure.deep_sea_treasure import CONCAVE_MAP
+
+
+register(
+    id="deep-sea-treasure-v0",
+    entry_point="mo_gymnasium.envs.deep_sea_treasure.deep_sea_treasure:DeepSeaTreasure",
+    max_episode_steps=100,
+)
+
+register(
+    id="deep-sea-treasure-concave-v0",
+    entry_point="mo_gymnasium.envs.deep_sea_treasure.deep_sea_treasure:DeepSeaTreasure",
+    max_episode_steps=100,
+    kwargs={"dst_map": CONCAVE_MAP},
+)
diff --git a/MO-Gymnasium/mo_gymnasium/envs/deep_sea_treasure/assets/Minecraft.ttf b/MO-Gymnasium/mo_gymnasium/envs/deep_sea_treasure/assets/Minecraft.ttf
new file mode 100644
index 0000000..85c1472
Binary files /dev/null and b/MO-Gymnasium/mo_gymnasium/envs/deep_sea_treasure/assets/Minecraft.ttf differ
diff --git a/MO-Gymnasium/mo_gymnasium/envs/deep_sea_treasure/assets/rock.png b/MO-Gymnasium/mo_gymnasium/envs/deep_sea_treasure/assets/rock.png
new file mode 100644
index 0000000..4422d8c
Binary files /dev/null and b/MO-Gymnasium/mo_gymnasium/envs/deep_sea_treasure/assets/rock.png differ
diff --git a/MO-Gymnasium/mo_gymnasium/envs/deep_sea_treasure/assets/sea_bg.png b/MO-Gymnasium/mo_gymnasium/envs/deep_sea_treasure/assets/sea_bg.png
new file mode 100644
index 0000000..3770816
Binary files /dev/null and b/MO-Gymnasium/mo_gymnasium/envs/deep_sea_treasure/assets/sea_bg.png differ
diff --git a/MO-Gymnasium/mo_gymnasium/envs/deep_sea_treasure/assets/submarine.png b/MO-Gymnasium/mo_gymnasium/envs/deep_sea_treasure/assets/submarine.png
new file mode 100644
index 0000000..04ae423
Binary files /dev/null and b/MO-Gymnasium/mo_gymnasium/envs/deep_sea_treasure/assets/submarine.png differ
diff --git a/MO-Gymnasium/mo_gymnasium/envs/deep_sea_treasure/assets/treasure.png b/MO-Gymnasium/mo_gymnasium/envs/deep_sea_treasure/assets/treasure.png
new file mode 100644
index 0000000..b61b96e
Binary files /dev/null and b/MO-Gymnasium/mo_gymnasium/envs/deep_sea_treasure/assets/treasure.png differ
diff --git a/MO-Gymnasium/mo_gymnasium/envs/deep_sea_treasure/deep_sea_treasure.py b/MO-Gymnasium/mo_gymnasium/envs/deep_sea_treasure/deep_sea_treasure.py
new file mode 100644
index 0000000..2e0abfb
--- /dev/null
+++ b/MO-Gymnasium/mo_gymnasium/envs/deep_sea_treasure/deep_sea_treasure.py
@@ -0,0 +1,303 @@
+from os import path
+from typing import List, Optional
+
+import gymnasium as gym
+import numpy as np
+import pygame
+from gymnasium.spaces import Box, Discrete
+from gymnasium.utils import EzPickle
+
+
+# As in Yang et al. (2019):
+DEFAULT_MAP = np.array(
+    [
+        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
+        [0.7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
+        [-10, 8.2, 0, 0, 0, 0, 0, 0, 0, 0, 0],
+        [-10, -10, 11.5, 0, 0, 0, 0, 0, 0, 0, 0],
+        [-10, -10, -10, 14.0, 15.1, 16.1, 0, 0, 0, 0, 0],
+        [-10, -10, -10, -10, -10, -10, 0, 0, 0, 0, 0],
+        [-10, -10, -10, -10, -10, -10, 0, 0, 0, 0, 0],
+        [-10, -10, -10, -10, -10, -10, 19.6, 20.3, 0, 0, 0],
+        [-10, -10, -10, -10, -10, -10, -10, -10, 0, 0, 0],
+        [-10, -10, -10, -10, -10, -10, -10, -10, 22.4, 0, 0],
+        [-10, -10, -10, -10, -10, -10, -10, -10, -10, 23.7, 0],
+    ]
+)
+
+CONVEX_FRONT = [
+    np.array([0.7, -1]),
+    np.array([8.2, -3]),
+    np.array([11.5, -5]),
+    np.array([14.0, -7]),
+    np.array([15.1, -8]),
+    np.array([16.1, -9]),
+    np.array([19.6, -13]),
+    np.array([20.3, -14]),
+    np.array([22.4, -17]),
+    np.array([23.7, -19]),
+]
+
+# As in Vamplew et al. (2018):
+CONCAVE_MAP = np.array(
+    [
+        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
+        [1.0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
+        [-10, 2.0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
+        [-10, -10, 3.0, 0, 0, 0, 0, 0, 0, 0, 0],
+        [-10, -10, -10, 5.0, 8.0, 16.0, 0, 0, 0, 0, 0],
+        [-10, -10, -10, -10, -10, -10, 0, 0, 0, 0, 0],
+        [-10, -10, -10, -10, -10, -10, 0, 0, 0, 0, 0],
+        [-10, -10, -10, -10, -10, -10, 24.0, 50.0, 0, 0, 0],
+        [-10, -10, -10, -10, -10, -10, -10, -10, 0, 0, 0],
+        [-10, -10, -10, -10, -10, -10, -10, -10, 74.0, 0, 0],
+        [-10, -10, -10, -10, -10, -10, -10, -10, -10, 124.0, 0],
+    ]
+)
+
+CONCAVE_FRONT = [
+    np.array([1.0, -1]),
+    np.array([2.0, -3]),
+    np.array([3.0, -5]),
+    np.array([5.0, -7]),
+    np.array([8.0, -8]),
+    np.array([16.0, -9]),
+    np.array([24.0, -13]),
+    np.array([50.0, -14]),
+    np.array([74.0, -17]),
+    np.array([124.0, -19]),
+]
+
+
+class DeepSeaTreasure(gym.Env, EzPickle):
+    """
+    ## Description
+    The Deep Sea Treasure environment is classic MORL problem in which the agent controls a submarine in a 2D grid world.
+
+    ## Observation Space
+    The observation space is a 2D discrete box with values in [0, 10] for the x and y coordinates of the submarine.
+
+    ## Action Space
+    The actions is a discrete space where:
+    - 0: up
+    - 1: down
+    - 2: left
+    - 3: right
+
+    ## Reward Space
+    The reward is 2-dimensional:
+    - time penalty: -1 at each time step
+    - treasure value: the value of the treasure at the current position
+
+    ## Starting State
+    The starting state is always the same: (0, 0)
+
+    ## Episode Termination
+    The episode terminates when the agent reaches a treasure.
+
+    ## Arguments
+    - dst_map: the map of the deep sea treasure. Default is the convex map from Yang et al. (2019). To change, use `mo_gymnasium.make("DeepSeaTreasure-v0", dst_map=CONCAVE_MAP).`
+    - float_state: if True, the state is a 2D continuous box with values in [0.0, 1.0] for the x and y coordinates of the submarine.
+
+    ## Credits
+    The code was adapted from: [Yang's source](https://github.com/RunzheYang/MORL).
+    The background art is from https://ansimuz.itch.io/underwater-fantasy-pixel-art-environment.
+    The submarine art was created with the assistance of DALL·E 2.
+    """
+
+    metadata = {"render_modes": ["human", "rgb_array"], "render_fps": 4}
+
+    def __init__(self, render_mode: Optional[str] = None, dst_map=DEFAULT_MAP, float_state=False):
+        EzPickle.__init__(self, render_mode, dst_map, float_state)
+
+        self.render_mode = render_mode
+        self.float_state = float_state
+
+        # The map of the deep sea treasure (convex version)
+        self.sea_map = dst_map
+        self._pareto_front = CONVEX_FRONT if np.all(dst_map == DEFAULT_MAP) else CONCAVE_FRONT
+        assert self.sea_map.shape == DEFAULT_MAP.shape, "The map's shape must be 11x11"
+
+        self.dir = {
+            0: np.array([-1, 0], dtype=np.int32),  # up
+            1: np.array([1, 0], dtype=np.int32),  # down
+            2: np.array([0, -1], dtype=np.int32),  # left
+            3: np.array([0, 1], dtype=np.int32),  # right
+        }
+
+        # state space specification: 2-dimensional discrete box
+        obs_type = np.float32 if self.float_state else np.int32
+        if self.float_state:
+            self.observation_space = Box(low=0.0, high=1.0, shape=(2,), dtype=obs_type)
+        else:
+            self.observation_space = Box(low=0, high=10, shape=(2,), dtype=obs_type)
+
+        # action space specification: 1 dimension, 0 up, 1 down, 2 left, 3 right
+        self.action_space = Discrete(4)
+        self.reward_space = Box(
+            low=np.array([0, -1]),
+            high=np.array([np.max(self.sea_map), -1]),
+            dtype=np.float32,
+        )
+        self.reward_dim = 2
+
+        self.current_state = np.array([0, 0], dtype=np.int32)
+
+        # pygame
+        self.window_size = (min(64 * self.sea_map.shape[1], 512), min(64 * self.sea_map.shape[0], 512))
+        # The size of a single grid square in pixels
+        self.pix_square_size = (
+            self.window_size[1] // self.sea_map.shape[1] + 1,
+            self.window_size[0] // self.sea_map.shape[0] + 1,
+        )
+        self.window = None
+        self.clock = None
+        self.submarine_img = None
+        self.treasure_img = None
+        self.sea_img = None
+        self.rock_img = None
+
+    def pareto_front(self, gamma: float) -> List[np.ndarray]:
+        """Return the discounted pareto front of the environment.
+
+        Args:
+            gamma: the discount factor.
+
+        Returns:
+            The discounted pareto front.
+
+        """
+
+        def discount_time(n):
+            """Discounted time for a given number of steps."""
+            return np.sum(np.array([gamma**i for i in range(int(n))]))
+
+        # The first element is discounted based on the number of steps to reach there (which is -p[1])
+        # e.g. if it takes 1 step to reach 0.7, the discounted value is 0.7 * gamma ** 0
+        discounted_front = [np.array([p[0] * gamma ** (-p[1] - 1), -discount_time(-p[1])]) for p in self._pareto_front]
+        return discounted_front
+
+    def _get_map_value(self, pos):
+        return self.sea_map[pos[0]][pos[1]]
+
+    def _is_valid_state(self, state):
+        if state[0] >= 0 and state[0] <= 10 and state[1] >= 0 and state[1] <= 10:
+            if self._get_map_value(state) != -10:
+                return True
+        return False
+
+    def render(self):
+        if self.render_mode is None:
+            assert self.spec is not None
+            gym.logger.warn(
+                "You are calling render method without specifying any render mode. "
+                "You can specify the render_mode at initialization, "
+                f'e.g. mo_gym.make("{self.spec.id}", render_mode="rgb_array")'
+            )
+            return
+
+        if self.window is None:
+            pygame.init()
+
+            if self.render_mode == "human":
+                pygame.display.init()
+                pygame.display.set_caption("Deep Sea Treasure")
+                self.window = pygame.display.set_mode(self.window_size)
+            else:
+                self.window = pygame.Surface(self.window_size)
+
+            if self.clock is None:
+                self.clock = pygame.time.Clock()
+
+            if self.submarine_img is None:
+                filename = path.join(path.dirname(__file__), "assets", "submarine.png")
+                self.submarine_img = pygame.transform.scale(pygame.image.load(filename), self.pix_square_size)
+                self.submarine_img = pygame.transform.flip(self.submarine_img, flip_x=True, flip_y=False)
+            if self.treasure_img is None:
+                filename = path.join(path.dirname(__file__), "assets", "treasure.png")
+                self.treasure_img = pygame.transform.scale(pygame.image.load(filename), self.pix_square_size)
+            if self.sea_img is None:
+                filename = path.join(path.dirname(__file__), "assets", "sea_bg.png")
+                self.sea_img = pygame.image.load(filename)
+                self.sea_img = pygame.transform.scale(self.sea_img, self.window_size)
+            if self.rock_img is None:
+                filename = path.join(path.dirname(__file__), "assets", "rock.png")
+                self.rock_img = pygame.transform.scale(pygame.image.load(filename), self.pix_square_size)
+
+            self.font = pygame.font.Font(path.join(path.dirname(__file__), "assets", "Minecraft.ttf"), 20)
+
+        self.window.blit(self.sea_img, (0, 0))
+
+        for i in range(self.sea_map.shape[0]):
+            for j in range(self.sea_map.shape[1]):
+                if self.sea_map[i, j] == -10:
+                    self.window.blit(self.rock_img, np.array([j, i]) * self.pix_square_size)
+                elif self.sea_map[i, j] != 0:
+                    self.window.blit(self.treasure_img, np.array([j, i]) * self.pix_square_size)
+                    trailing_space = " " if self.sea_map[i, j] < 10 else ""
+                    img = self.font.render(trailing_space + str(self.sea_map[i, j]), True, (255, 255, 255))
+                    self.window.blit(img, np.array([j, i]) * self.pix_square_size + np.array([5, -20]))
+
+        self.window.blit(self.submarine_img, self.current_state[::-1] * self.pix_square_size)
+
+        if self.render_mode == "human":
+            pygame.event.pump()
+            pygame.display.update()
+            self.clock.tick(self.metadata["render_fps"])
+        elif self.render_mode == "rgb_array":
+            return np.transpose(np.array(pygame.surfarray.pixels3d(self.window)), axes=(1, 0, 2))
+
+    def _get_state(self):
+        if self.float_state:
+            state = self.current_state.astype(np.float32) * 0.1
+        else:
+            state = self.current_state.copy()
+        return state
+
+    def reset(self, seed=None, **kwargs):
+        super().reset(seed=seed)
+
+        self.current_state = np.array([0, 0], dtype=np.int32)
+        self.step_count = 0.0
+        state = self._get_state()
+        if self.render_mode == "human":
+            self.render()
+        return state, {}
+
+    def step(self, action):
+        next_state = self.current_state + self.dir[action]
+
+        if self._is_valid_state(next_state):
+            self.current_state = next_state
+
+        treasure_value = self._get_map_value(self.current_state)
+        if treasure_value == 0 or treasure_value == -10:
+            treasure_value = 0.0
+            terminal = False
+        else:
+            terminal = True
+        time_penalty = -1.0
+        vec_reward = np.array([treasure_value, time_penalty], dtype=np.float32)
+
+        state = self._get_state()
+        if self.render_mode == "human":
+            self.render()
+        return state, vec_reward, terminal, False, {}
+
+    def close(self):
+        if self.window is not None:
+            pygame.display.quit()
+            pygame.quit()
+
+
+if __name__ == "__main__":
+    import mo_gymnasium as mo_gym
+
+    env = mo_gym.make("deep-sea-treasure-v0", render_mode="human")
+    terminated = False
+    env.reset()
+    while True:
+        env.render()
+        obs, r, terminated, truncated, info = env.step(env.action_space.sample())
+        if terminated or truncated:
+            env.reset()
diff --git a/MO-Gymnasium/mo_gymnasium/envs/fishwood/__init__.py b/MO-Gymnasium/mo_gymnasium/envs/fishwood/__init__.py
new file mode 100644
index 0000000..3581f1e
--- /dev/null
+++ b/MO-Gymnasium/mo_gymnasium/envs/fishwood/__init__.py
@@ -0,0 +1,7 @@
+from gymnasium.envs.registration import register
+
+
+register(
+    id="fishwood-v0",
+    entry_point="mo_gymnasium.envs.fishwood.fishwood:FishWood",
+)
diff --git a/MO-Gymnasium/mo_gymnasium/envs/fishwood/assets/Kawa.jpg b/MO-Gymnasium/mo_gymnasium/envs/fishwood/assets/Kawa.jpg
new file mode 100644
index 0000000..34a604a
Binary files /dev/null and b/MO-Gymnasium/mo_gymnasium/envs/fishwood/assets/Kawa.jpg differ
diff --git a/MO-Gymnasium/mo_gymnasium/envs/fishwood/assets/Mori.jpg b/MO-Gymnasium/mo_gymnasium/envs/fishwood/assets/Mori.jpg
new file mode 100644
index 0000000..9c202e6
Binary files /dev/null and b/MO-Gymnasium/mo_gymnasium/envs/fishwood/assets/Mori.jpg differ
diff --git a/MO-Gymnasium/mo_gymnasium/envs/fishwood/fishwood.py b/MO-Gymnasium/mo_gymnasium/envs/fishwood/fishwood.py
new file mode 100644
index 0000000..73b2cb1
--- /dev/null
+++ b/MO-Gymnasium/mo_gymnasium/envs/fishwood/fishwood.py
@@ -0,0 +1,109 @@
+from typing import Optional
+
+import gymnasium as gym
+import numpy as np
+from gymnasium import spaces
+from gymnasium.utils import EzPickle
+
+
+class FishWood(gym.Env, EzPickle):
+    """
+    ## Description
+    The FishWood environment is a simple MORL problem in which the agent controls a fisherman which can either fish or go collect wood.
+    From [Multi-objective Reinforcement Learning for the Expected Utility of the Return](https://www.researchgate.net/publication/328718263_Multi-objective_Reinforcement_Learning_for_the_Expected_Utility_of_the_Return).
+
+    ## Observation Space
+    The observation space is a discrete space with two states:
+    - 0: fishing
+    - 1: in the woods
+
+    ## Action Space
+    The actions is a discrete space where:
+    - 0: go fishing
+    - 1: go collect wood
+
+    ## Reward Space
+    The reward is 2-dimensional:
+    - 0: +1 if agent is in the woods, with woodproba probability, and 0 otherwise
+    - 1: +1 if the agent is fishing, with fishproba probability, and 0 otherwise
+
+    ## Starting State
+    Agent starts in the woods
+
+    ## Termination
+    The episode ends after MAX_TS=200 steps
+
+    ## Arguments
+    - fishproba: probability of catching a fish when fishing
+    - woodproba: probability of collecting wood when in the woods
+
+    ## Credits
+    Code provided by Denis Steckelmacher
+    """
+
+    metadata = {"render_modes": ["human"]}
+    FISH = 0
+    WOOD = 1
+    MAX_TS = 200
+
+    def __init__(self, render_mode: Optional[str] = None, fishproba=0.1, woodproba=0.9):
+        EzPickle.__init__(self, render_mode, fishproba, woodproba)
+
+        self.render_mode = render_mode
+        self._fishproba = fishproba
+        self._woodproba = woodproba
+
+        self.action_space = spaces.Discrete(2)  # 2 actions, go fish and go wood
+        # 2 states, fishing and in the woods
+        self.observation_space = spaces.Discrete(2)
+        # 2 objectives, amount of fish and amount of wood
+        self.reward_space = spaces.Box(low=np.array([0, 0]), high=np.array([1.0, 1.0]), dtype=np.float32)
+        self.reward_dim = 2
+
+        self._state = self.WOOD
+
+    def reset(self, seed=None, **kwargs):
+        super().reset(seed=seed)
+
+        self._state = self.WOOD
+        self._timestep = 0
+        if self.render_mode == "human":
+            self.render()
+
+        return self._state, {}
+
+    def render(self):
+        if self.render_mode == "human":
+            if self._state == self.WOOD:
+                return f"t={self._timestep}, in wood."
+            else:
+                return f"t={self._timestep}, fishing"
+
+    def step(self, action):
+        # Obtain a resource from the current state
+        rewards = np.zeros((2,), dtype=np.float32)
+
+        if self._state == self.WOOD and self.np_random.random() < self._woodproba:
+            rewards[self.WOOD] = 1.0
+        elif self._state == self.FISH and self.np_random.random() < self._fishproba:
+            rewards[self.FISH] = 1.0
+
+        # Execute the action
+        self._state = action
+        self._timestep += 1
+
+        if self.render_mode == "human":
+            self.render()
+        return self._state, rewards, self._timestep == self.MAX_TS, self._timestep == self.MAX_TS, {}
+
+
+if __name__ == "__main__":
+
+    env = FishWood()
+    terminated = False
+    env.reset()
+    while True:
+        env.render()
+        obs, r, terminated, truncated, info = env.step(env.action_space.sample())
+        if terminated:
+            env.reset()
diff --git a/MO-Gymnasium/mo_gymnasium/envs/four_room/__init__.py b/MO-Gymnasium/mo_gymnasium/envs/four_room/__init__.py
new file mode 100644
index 0000000..3df8bc6
--- /dev/null
+++ b/MO-Gymnasium/mo_gymnasium/envs/four_room/__init__.py
@@ -0,0 +1,8 @@
+from gymnasium.envs.registration import register
+
+
+register(
+    id="four-room-v0",
+    entry_point="mo_gymnasium.envs.four_room.four_room:FourRoom",
+    max_episode_steps=200,
+)
diff --git a/MO-Gymnasium/mo_gymnasium/envs/four_room/four_room.py b/MO-Gymnasium/mo_gymnasium/envs/four_room/four_room.py
new file mode 100644
index 0000000..e22a56b
--- /dev/null
+++ b/MO-Gymnasium/mo_gymnasium/envs/four_room/four_room.py
@@ -0,0 +1,344 @@
+import random
+from typing import Optional
+
+import gymnasium as gym
+import numpy as np
+import pygame
+from gymnasium.spaces import Box, Discrete
+from gymnasium.utils import EzPickle
+
+
+MAZE = np.array(
+    [
+        ["1", " ", " ", " ", " ", "2", "X", " ", " ", " ", " ", " ", "G"],
+        [" ", " ", " ", " ", " ", " ", "X", " ", " ", " ", " ", " ", " "],
+        [" ", " ", " ", " ", " ", " ", "1", " ", " ", " ", " ", " ", " "],
+        [" ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " "],
+        [" ", " ", " ", " ", " ", " ", "X", " ", " ", " ", " ", " ", " "],
+        ["2", " ", " ", " ", " ", "3", "X", " ", " ", " ", " ", " ", " "],
+        ["X", "X", "3", " ", "X", "X", "X", "X", "X", " ", "1", "X", "X"],
+        [" ", " ", " ", " ", " ", " ", "X", "2", " ", " ", " ", " ", "3"],
+        [" ", " ", " ", " ", " ", " ", "X", " ", " ", " ", " ", " ", " "],
+        [" ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " ", " "],
+        [" ", " ", " ", " ", " ", " ", "2", " ", " ", " ", " ", " ", " "],
+        [" ", " ", " ", " ", " ", " ", "X", " ", " ", " ", " ", " ", " "],
+        ["_", " ", " ", " ", " ", " ", "X", "3", " ", " ", " ", " ", "1"],
+    ]
+)
+BLUE = (0, 0, 255)
+RED = (255, 0, 0)
+GREEN = (0, 128, 0)
+BLACK = (0, 0, 0)
+
+
+class FourRoom(gym.Env, EzPickle):
+    """
+    ## Description
+    A discretized version of the gridworld environment introduced in [1]. Here, an agent learns to
+    collect shapes with positive reward, while avoid those with negative reward, and then travel to a fixed goal.
+    The gridworld is split into four rooms separated by walls with passage-ways.
+
+    References
+    ----------
+    [1] Barreto, André, et al. "Successor Features for Transfer in Reinforcement Learning." NIPS. 2017.
+
+    ## Observation Space
+    The observation contains the 2D position of the agent in the gridworld, plus a binary vector indicating which items were collected.
+
+    ## Action Space
+    The action space is discrete with 4 actions: left, up, right, down.
+
+    ## Reward Space
+    The reward is a 3-dimensional vector with the following components:
+    - +1 if collected a blue square, else 0
+    - +1 if collected a green triangle, else 0
+    - +1 if collected a red circle, else 0
+
+    ## Starting State
+    The agent starts in the lower left of the map.
+
+    ## Episode Termination
+    The episode terminates when the agent reaches the goal state, G.
+
+    ## Arguments
+    - maze: Array containing the gridworld map. See MAZE for an example.
+
+    ## Credits
+    Code adapted from: [Mike Gimelfarb's source](https://github.com/mike-gimelfarb/deep-successor-features-for-transfer/blob/main/source/tasks/gridworld.py).
+    """
+
+    LEFT, UP, RIGHT, DOWN = 0, 1, 2, 3
+
+    metadata = {"render_modes": ["human", "rgb_array"], "render_fps": 4}
+
+    def __init__(self, render_mode: Optional[str] = None, maze=MAZE):
+        """
+        Creates a new instance of the shapes environment.
+
+        Parameters
+        ----------
+        maze : np.ndarray
+            an array of string values representing the type of each cell in the environment:
+                G indicates a goal state (terminal state)
+                _ indicates an initial state (there can be multiple, and one is selected at random
+                    at the start of each episode)
+                X indicates a barrier
+                0, 1, .... 9 indicates the type of shape to be placed in the corresponding cell
+                entries containing other characters are treated as regular empty cells
+        """
+        EzPickle.__init__(self, render_mode, maze)
+
+        self.render_mode = render_mode
+        self.window_size = 512
+        self.window = None
+        self.clock = None
+
+        self.height, self.width = maze.shape
+        self.maze = maze
+        shape_types = ["1", "2", "3"]
+        self.all_shapes = dict(zip(shape_types, range(len(shape_types))))
+
+        self.goal = None
+        self.initial = []
+        self.occupied = set()
+        self.shape_ids = dict()
+        for c in range(self.width):
+            for r in range(self.height):
+                if maze[r, c] == "G":
+                    self.goal = (r, c)
+                elif maze[r, c] == "_":
+                    self.initial.append((r, c))
+                elif maze[r, c] == "X":
+                    self.occupied.add((r, c))
+                elif maze[r, c] in {"0", "1", "2", "3", "4", "5", "6", "7", "8", "9"}:
+                    self.shape_ids[(r, c)] = len(self.shape_ids)
+
+        self.action_space = Discrete(4)
+        self.observation_space = Box(
+            low=np.zeros(2 + len(self.shape_ids)),
+            high=len(self.maze) * np.ones(2 + len(self.shape_ids)),
+            dtype=np.int32,
+        )
+        self.reward_space = Box(low=0, high=1, shape=(3,))
+        self.reward_dim = 3
+
+    def state_to_array(self, state):
+        s = [element for tupl in state for element in tupl]
+        return np.array(s, dtype=np.int32)
+
+    def reset(self, seed=None, **kwargs):
+        super().reset(seed=seed)
+
+        self.state = (
+            random.choice(self.initial),
+            tuple(0 for _ in range(len(self.shape_ids))),
+        )
+        if self.render_mode == "human":
+            self.render()
+        return self.state_to_array(self.state), {}
+
+    def step(self, action):
+        old_state = self.state
+        (row, col), collected = self.state
+
+        # perform the movement
+        if action == FourRoom.LEFT:
+            col -= 1
+        elif action == FourRoom.UP:
+            row -= 1
+        elif action == FourRoom.RIGHT:
+            col += 1
+        elif action == FourRoom.DOWN:
+            row += 1
+        else:
+            raise Exception(f"bad action {action}")
+
+        terminated = False
+
+        # out of bounds, cannot move
+        if col < 0 or col >= self.width or row < 0 or row >= self.height:
+            return (
+                self.state_to_array(self.state),
+                np.zeros(len(self.all_shapes), dtype=np.float32),
+                terminated,
+                False,
+                {},
+            )
+
+        # into a blocked cell, cannot move
+        s1 = (row, col)
+        if s1 in self.occupied:
+            return (
+                self.state_to_array(self.state),
+                np.zeros(len(self.all_shapes), dtype=np.float32),
+                terminated,
+                False,
+                {},
+            )
+
+        # can now move
+        self.state = (s1, collected)
+
+        # into a goal cell
+        if s1 == self.goal:
+            phi = np.ones(len(self.all_shapes), dtype=np.float32)
+            terminated = True
+            return self.state_to_array(self.state), phi, terminated, False, {}
+
+        # into a shape cell
+        if s1 in self.shape_ids:
+            shape_id = self.shape_ids[s1]
+            if collected[shape_id] == 1:
+                # already collected this flag
+                return (
+                    self.state_to_array(self.state),
+                    np.zeros(len(self.all_shapes), dtype=np.float32),
+                    terminated,
+                    False,
+                    {},
+                )
+            else:
+                # collect the new flag
+                collected = list(collected)
+                collected[shape_id] = 1
+                collected = tuple(collected)
+                self.state = (s1, collected)
+                phi = self.features(old_state, action, self.state)
+                return self.state_to_array(self.state), phi, terminated, False, {}
+
+        # into an empty cell
+        return (
+            self.state_to_array(self.state),
+            np.zeros(len(self.all_shapes), dtype=np.float32),
+            terminated,
+            False,
+            {},
+        )
+
+    def features(self, state, action, next_state):
+        s1, _ = next_state
+        _, collected = state
+        nc = len(self.all_shapes)
+        phi = np.zeros(nc, dtype=np.float32)
+        if s1 in self.shape_ids:
+            if collected[self.shape_ids[s1]] != 1:
+                y, x = s1
+                shape_index = self.all_shapes[self.maze[y, x]]
+                phi[shape_index] = 1.0
+        elif s1 == self.goal:
+            phi[nc] = np.ones(nc, dtype=np.float32)
+        return phi
+
+    def render(self):
+        # The size of a single grid square in pixels
+        pix_square_size = self.window_size / 13
+
+        if self.window is None and self.render_mode is not None:
+            pygame.init()
+            if self.render_mode == "human":
+                pygame.display.init()
+                self.window = pygame.display.set_mode((self.window_size, self.window_size))
+        if self.clock is None and self.render_mode == "human":
+            self.clock = pygame.time.Clock()
+
+        canvas = pygame.Surface((self.window_size, self.window_size))
+        canvas.fill((255, 255, 255))
+
+        pygame.font.init()
+        self.font = pygame.font.SysFont(None, 48)
+        img = self.font.render("G", True, BLACK)
+        canvas.blit(img, (np.array(self.goal)[::-1] + 0.15) * pix_square_size)
+
+        for i in range(self.maze.shape[0]):
+            for j in range(self.maze.shape[1]):
+                (row, col), collected = self.state
+                shape_id = self.shape_ids.get((i, j), 0)
+                if collected[shape_id] == 1 and self.maze[i, j] != "X":
+                    continue
+
+                pos = np.array([j, i])
+                if self.maze[i, j] == "1":
+                    pygame.draw.rect(
+                        canvas,
+                        BLUE,
+                        pygame.Rect(
+                            pix_square_size * pos,
+                            (pix_square_size, pix_square_size),
+                        ),
+                    )
+                elif self.maze[i, j] == "X":
+                    pygame.draw.rect(
+                        canvas,
+                        BLACK,
+                        pygame.Rect(
+                            pix_square_size * pos + 1,
+                            (pix_square_size, pix_square_size),
+                        ),
+                    )
+                elif self.maze[i, j] == "2":
+                    pygame.draw.polygon(
+                        canvas,
+                        GREEN,
+                        [
+                            (pos + np.array([0.5, 0.0])) * pix_square_size,
+                            (pos + np.array([0.0, 1.0])) * pix_square_size,
+                            (pos + 1.0) * pix_square_size,
+                        ],
+                    )
+                elif self.maze[i, j] == "3":
+                    pygame.draw.circle(
+                        canvas,
+                        RED,
+                        (pos + 0.5) * pix_square_size,
+                        pix_square_size / 2,
+                    )
+
+        pygame.draw.circle(
+            canvas,
+            (125, 125, 125),
+            (np.array(self.state[0])[::-1] + 0.5) * pix_square_size,
+            pix_square_size / 3,
+        )
+
+        for x in range(13 + 1):
+            pygame.draw.line(
+                canvas,
+                0,
+                (0, pix_square_size * x),
+                (self.window_size, pix_square_size * x),
+                width=1,
+            )
+            pygame.draw.line(
+                canvas,
+                0,
+                (pix_square_size * x, 0),
+                (pix_square_size * x, self.window_size),
+                width=1,
+            )
+
+        if self.render_mode == "human":
+            # The following line copies our drawings from `canvas` to the visible window
+            self.window.blit(canvas, canvas.get_rect())
+            pygame.event.pump()
+            pygame.display.update()
+
+            # We need to ensure that human-rendering occurs at the predefined framerate.
+            # The following line will automatically add a delay to keep the framerate stable.
+            self.clock.tick(self.metadata["render_fps"])
+        elif self.render_mode == "rgb_array":
+            return np.transpose(np.array(pygame.surfarray.pixels3d(canvas)), axes=(1, 0, 2))
+
+    def close(self):
+        if self.window is not None:
+            pygame.display.quit()
+            pygame.quit()
+
+
+if __name__ == "__main__":
+
+    env = FourRoom()
+    terminated = False
+    env.reset()
+    while not terminated:
+        env.render()
+        obs, r, terminated, truncated, info = env.step(env.action_space.sample())
diff --git a/MO-Gymnasium/mo_gymnasium/envs/fruit_tree/__init__.py b/MO-Gymnasium/mo_gymnasium/envs/fruit_tree/__init__.py
new file mode 100644
index 0000000..c1d99d7
--- /dev/null
+++ b/MO-Gymnasium/mo_gymnasium/envs/fruit_tree/__init__.py
@@ -0,0 +1,4 @@
+from gymnasium.envs.registration import register
+
+
+register(id="fruit-tree-v0", entry_point="mo_gymnasium.envs.fruit_tree.fruit_tree:FruitTreeEnv")
diff --git a/MO-Gymnasium/mo_gymnasium/envs/fruit_tree/fruit_tree.py b/MO-Gymnasium/mo_gymnasium/envs/fruit_tree/fruit_tree.py
new file mode 100644
index 0000000..f028a00
--- /dev/null
+++ b/MO-Gymnasium/mo_gymnasium/envs/fruit_tree/fruit_tree.py
@@ -0,0 +1,314 @@
+# Environment from https://github.com/RunzheYang/MORL/blob/master/synthetic/envs/fruit_tree.py
+import gymnasium as gym
+import numpy as np
+from gymnasium import spaces
+from gymnasium.utils import EzPickle
+
+
+FRUITS = {
+    "6": [
+        [0.26745039, 3.54435815, 4.39088762, 0.5898826, 7.7984232, 2.63110921],
+        [0.46075946, 5.29084735, 7.92804145, 2.28448495, 1.01115855, 1.64300963],
+        [0.5844333, 4.28059796, 7.00237899, 2.51448544, 4.32323182, 2.69974756],
+        [4.01332296, 7.17080888, 1.46983043, 3.82182158, 2.20659648, 3.29195217],
+        [3.74601154, 0.91228863, 5.92072559, 4.37056585, 2.73662976, 4.84656035],
+        [2.42167773, 3.34415377, 6.35216354, 0.03806333, 0.66323198, 6.49313525],
+        [5.26768145, 0.23364916, 0.23646111, 1.25030802, 1.41161868, 8.28161149],
+        [0.19537027, 2.3433365, 6.62653841, 2.84247689, 1.71456358, 6.28809908],
+        [5.9254461, 0.35473447, 5.4459742, 3.57702685, 0.95237377, 4.62628146],
+        [2.22158757, 1.01733311, 7.9499714, 3.6379799, 3.77557594, 1.82692783],
+        [4.43311346, 4.91328158, 5.11707495, 3.9065904, 2.22236853, 3.13406169],
+        [6.44612546, 5.14526023, 1.37156642, 1.37449512, 0.62784821, 5.27343712],
+        [2.39054781, 1.97492965, 4.51911017, 0.07046741, 1.74139824, 8.18077893],
+        [3.26794393, 3.28877157, 2.91598351, 0.49403134, 7.86629258, 2.80694464],
+        [3.96600091, 3.6266905, 4.44655634, 6.0366069, 1.58135473, 3.52204257],
+        [6.15119272, 2.82397981, 4.24282686, 1.75378872, 4.80532629, 3.16535161],
+        [2.7196025, 2.17993876, 2.79799651, 7.20950623, 4.70827355, 2.42446381],
+        [0.29748325, 8.22965311, 0.07526586, 1.98395573, 1.77853129, 5.00793316],
+        [6.37849798, 3.80507597, 2.5126212, 0.75632265, 2.49531244, 5.63243171],
+        [0.79285198, 4.00586269, 0.36314749, 8.9344773, 1.82041716, 0.2318847],
+        [0.24871352, 3.25946831, 3.9988045, 6.9335196, 4.81556096, 1.43535682],
+        [5.2736312, 0.59346769, 0.73640014, 7.30730989, 4.09948515, 1.0448773],
+        [1.74241088, 2.32320373, 9.17490044, 2.28211094, 1.47515927, 0.06168781],
+        [1.65116829, 3.72063198, 5.63953167, 0.25461896, 6.35720791, 3.33875729],
+        [2.5078766, 4.59291179, 0.81935207, 8.24752456, 0.33308447, 1.95237595],
+        [1.05128312, 4.85979168, 3.28552824, 6.26921471, 3.39863537, 3.69171469],
+        [6.30499955, 1.82204004, 1.93686289, 3.35062427, 1.83174219, 6.21238686],
+        [4.74718378, 6.36499948, 4.05818821, 4.43996757, 0.42190953, 0.76864591],
+        [1.25720612, 0.74301296, 1.3374366, 8.30597947, 5.08394071, 1.1148452],
+        [0.63888729, 0.28507461, 4.87857435, 6.41971655, 5.85711844, 0.43757381],
+        [0.74870183, 2.51804488, 6.59949427, 2.14794505, 6.05084902, 2.88429005],
+        [3.57753129, 3.67307393, 5.43392619, 2.06131042, 2.63388133, 5.74420686],
+        [3.94583726, 0.62586462, 0.72667245, 9.06686254, 1.13056724, 0.15630224],
+        [2.53054533, 4.2406129, 2.22057705, 7.51774642, 3.47885032, 1.43654771],
+        [1.63510684, 3.25906419, 0.37991887, 7.02694214, 2.53469812, 5.54598751],
+        [7.11491625, 1.26647073, 5.01203819, 4.52740681, 1.16148237, 0.89835304],
+        [2.75824608, 5.28476545, 2.49891273, 0.63079997, 7.07433925, 2.78829399],
+        [4.92392025, 4.74424707, 2.56041791, 4.76935788, 1.43523334, 4.67811073],
+        [2.43924518, 1.00523211, 6.09587506, 1.47285316, 6.69893956, 2.972341],
+        [1.14431283, 4.55594834, 4.12473926, 5.80221944, 1.92147095, 4.85413307],
+        [7.08401121, 1.66591657, 2.90546299, 2.62634248, 3.62934098, 4.30464879],
+        [0.71623214, 3.11241519, 1.7018771, 7.50296641, 5.38823009, 1.25537605],
+        [1.33651336, 4.76969307, 0.64008086, 6.48262472, 5.64538051, 1.07671362],
+        [3.09497945, 1.2275849, 3.84351994, 7.19938601, 3.78799616, 2.82159852],
+        [5.06781785, 3.12557557, 6.88555034, 1.21769126, 2.73086695, 2.86300362],
+        [8.30192712, 0.40973443, 1.69099424, 4.54961192, 2.64473811, 0.59753994],
+        [5.96294481, 6.46817991, 1.35988062, 2.83106174, 0.74946184, 3.48999411],
+        [0.43320751, 1.24640954, 5.6313907, 1.62670791, 4.58871327, 6.54551489],
+        [3.7064827, 7.60850058, 3.73003227, 2.71892257, 1.4363049, 2.23697394],
+        [4.44128859, 1.8202686, 4.22272069, 2.30194565, 0.67272146, 7.30607281],
+        [0.93689572, 0.77924846, 2.83896436, 1.98294555, 8.45958836, 3.86763124],
+        [1.12281975, 2.73059913, 0.32294675, 2.84237021, 1.68312155, 8.95917647],
+        [4.27687318, 2.83055698, 5.27541783, 5.03273808, 0.01475194, 4.53184284],
+        [3.73578206, 6.07088863, 2.17391882, 4.89911933, 0.27124696, 4.51523815],
+        [6.05671623, 0.7444296, 4.30057711, 3.09050824, 1.16194731, 5.77630391],
+        [1.40468169, 5.19102545, 6.72110624, 4.75666122, 0.91486715, 1.56334486],
+        [4.41604152, 0.86551038, 2.05709774, 4.70986355, 3.106477, 6.60944809],
+        [5.95498781, 5.94146861, 4.17018388, 0.93397018, 0.89950814, 3.18829456],
+        [9.59164585, 1.48925818, 0.72278285, 2.04850964, 1.0181982, 0.16402902],
+        [4.4579775, 3.16479945, 1.00362159, 2.24428595, 7.91409455, 1.19729395],
+        [2.12268361, 0.64607954, 6.43093367, 0.73854263, 6.94484318, 2.22341982],
+        [3.08973572, 5.6223787, 0.9737901, 5.75218769, 3.94430958, 3.04119754],
+        [2.5850297, 0.26144699, 2.28343938, 8.50777354, 3.93535625, 0.40734769],
+        [4.72502594, 5.38532887, 5.40386645, 1.57883722, 0.24912224, 4.11288237],
+    ],
+    "5": [
+        [3.67917966, 0.38835143, 8.09989551, 2.86026356, 3.24527031, 1.41124976],
+        [7.49190652, 0.86177565, 0.26446419, 6.40116659, 1.13497678, 0.89198211],
+        [3.14072363, 8.1320309, 3.56036928, 2.95551047, 0.38337821, 1.56450569],
+        [0.03085158, 4.25364725, 3.34139266, 4.67838906, 1.98970378, 6.70032708],
+        [4.02109647, 4.65093134, 5.52044309, 0.41989912, 5.07013412, 2.41697202],
+        [2.96104264, 6.42797292, 4.00884559, 2.28915409, 0.82767172, 5.28368061],
+        [3.95849765, 4.90714693, 3.91729584, 2.69024104, 6.08226306, 0.82077889],
+        [0.74185053, 1.02527749, 5.89640728, 5.80289307, 2.44397849, 4.89737136],
+        [4.2850684, 0.09305206, 7.94851261, 1.77192616, 2.93208106, 2.59111093],
+        [5.93102382, 4.12666154, 0.77446586, 4.31927672, 5.33551751, 0.26443358],
+        [1.05416268, 2.2897475, 1.46517302, 2.79084328, 9.0996314, 0.95234852],
+        [0.79064992, 0.57247091, 4.45310153, 6.54823417, 6.00440567, 0.53364626],
+        [4.35145246, 0.63842407, 0.7493827, 1.11659248, 7.71912589, 4.38907945],
+        [2.44514113, 3.33980397, 2.63961606, 1.86832856, 8.08225336, 2.66194486],
+        [6.66924742, 3.12365865, 5.35457136, 1.0395009, 0.1690967, 3.99791261],
+        [4.45750971, 3.52046079, 3.30278422, 3.47142192, 4.12997745, 5.26508267],
+        [1.70141153, 6.84591866, 0.69367176, 1.01722498, 6.77031075, 1.69869412],
+        [1.61886242, 1.17208561, 0.89685985, 3.80015468, 8.79718107, 1.83563934],
+        [2.36047279, 2.6207669, 7.50292079, 0.53373023, 5.49537505, 0.88425889],
+        [0.52087376, 3.04167888, 0.13469428, 7.33280189, 2.69762123, 5.42324568],
+        [4.85800708, 1.07551152, 0.44983357, 3.87424429, 5.7594417, 5.18263972],
+        [3.51780254, 1.44977209, 2.26423569, 6.76102431, 5.18387135, 2.79508338],
+        [0.92487495, 0.88767274, 2.25049907, 2.94454564, 2.46602119, 8.86229586],
+        [0.91807423, 2.21441185, 3.17003378, 8.38445357, 1.6293076, 3.35420641],
+        [1.00725144, 3.13922254, 5.11333093, 2.46965818, 6.66930986, 3.52216804],
+        [1.89889027, 4.26005076, 3.8055043, 2.601719, 2.73103432, 7.03824055],
+        [4.57737545, 1.06276164, 4.19723485, 6.86155259, 2.25429762, 2.85282836],
+        [1.23346934, 1.23694538, 9.64358056, 0.16691509, 0.29485875, 1.95833379],
+        [1.51947586, 8.43245754, 4.84809716, 0.58788756, 0.37534243, 1.61068716],
+        [3.8159377, 2.93304432, 0.41425422, 5.33636409, 1.0104518, 6.8677849],
+        [5.85626898, 4.80785389, 1.58310642, 3.48908773, 2.29308951, 4.7592474],
+        [4.24912001, 6.97078189, 4.74298955, 1.38027302, 0.67165641, 2.91563942],
+    ],
+    "7": [
+        [9.49729374, 2.98910393, 0.19374418, 0.48817863, 0.75034508, 0.16672279],
+        [1.74327056, 0.46482846, 7.55950543, 1.57177559, 5.29791865, 3.01004973],
+        [7.44433577, 2.27422887, 4.78271726, 3.46265870, 0.50993921, 2.07010153],
+        [4.78968371, 3.36538593, 0.88592964, 6.38949462, 2.00825232, 4.48213313],
+        [5.77532710, 5.31652260, 2.38513502, 2.08315748, 4.17604119, 3.30339978],
+        [6.88088248, 6.02843423, 0.02104655, 4.00481529, 0.48111127, 0.20243633],
+        [0.96132316, 1.91371320, 0.94262497, 3.51414795, 1.61855745, 8.91942003],
+        [2.06562582, 5.11098416, 6.29032188, 4.60098502, 0.38568959, 2.95382160],
+        [0.58513677, 4.00782651, 0.08530382, 8.62832436, 1.93453438, 2.32320047],
+        [1.24387716, 2.36413212, 3.54918392, 7.29005232, 5.20695449, 0.09851170],
+        [4.42188476, 1.93117784, 7.33958506, 4.19677527, 0.94119320, 2.08547622],
+        [2.22509178, 5.69526240, 4.02537895, 5.51223418, 3.90230246, 0.89251721],
+        [5.63262741, 0.25607217, 4.21983956, 5.34274324, 4.06989729, 2.30041742],
+        [3.99576756, 3.36409888, 5.49561503, 3.48562801, 5.14196367, 1.98128815],
+        [3.86901361, 2.35718170, 0.87556104, 0.82003307, 8.78839182, 0.89416779],
+        [0.39341137, 4.84127331, 1.70702545, 1.67922015, 8.35107069, 0.96602394],
+        [3.45410722, 7.40358442, 4.80502365, 1.81341621, 0.66537847, 2.53704984],
+        [5.10549280, 6.36812455, 4.58494717, 1.22495898, 2.39242255, 2.26604992],
+        [2.10660119, 0.49899214, 2.13559315, 0.77146133, 1.82738095, 9.31761807],
+        [2.41451522, 6.11407574, 0.02657181, 3.15297076, 6.55980963, 1.95324369],
+        [5.07301207, 1.04764679, 0.38126799, 1.75203503, 4.83320736, 6.82584056],
+        [5.86496023, 5.29477535, 0.49074425, 5.02714001, 1.04921431, 3.30964927],
+        [6.04985605, 2.15313597, 3.01872538, 2.14930505, 6.31431071, 2.27167613],
+        [3.38761354, 1.73456127, 2.59356744, 4.78587994, 7.35331757, 1.34642252],
+        [4.61573836, 0.10237836, 3.84736614, 7.75293952, 1.19594946, 1.53097533],
+        [6.49637377, 5.04239913, 0.20827311, 3.29038975, 0.33107957, 4.62511436],
+        [2.30298197, 3.29946741, 4.49935456, 5.34601600, 5.21029158, 2.79974498],
+        [0.54574584, 2.42444606, 1.60434868, 3.73649348, 1.26056229, 8.70056821],
+        [3.61195169, 6.65026280, 3.74800770, 2.29512444, 0.09778884, 4.83767393],
+        [7.63538382, 3.88682729, 3.22292045, 0.21632413, 0.07119400, 4.01925448],
+        [2.96169175, 0.69908964, 1.91478181, 3.19293209, 8.58251758, 1.79411341],
+        [2.90919565, 5.51711193, 1.75245139, 2.91477850, 6.98631598, 0.84995649],
+        [6.76482657, 5.05071646, 3.62871840, 2.62715174, 1.66996898, 2.42261528],
+        [8.03143521, 2.95933932, 3.39642327, 3.24934646, 2.10962576, 0.44033503],
+        [1.80722860, 3.77826266, 0.16093320, 0.03592059, 4.83503526, 7.68465356],
+        [7.22319621, 0.66064721, 1.26699526, 5.78540374, 2.55433573, 2.40586313],
+        [4.03459556, 0.68212834, 1.37929110, 8.75546017, 1.76414640, 1.25857076],
+        [5.73690431, 1.64965556, 0.64870873, 5.25548535, 5.84541730, 1.46857511],
+        [2.53210586, 3.05479350, 0.38493697, 7.20837806, 5.66222550, 0.29493761],
+        [3.89471153, 5.32895093, 1.35814442, 4.04568659, 4.12513338, 4.60484989],
+        [2.41451158, 1.50862356, 1.41410605, 0.51982294, 5.57708595, 7.64986205],
+        [5.34003191, 1.89167534, 2.58671856, 0.85251419, 6.32481989, 4.52596770],
+        [0.57978779, 6.09839275, 7.76016689, 0.41757963, 0.93387774, 1.09852695],
+        [0.00888177, 0.18279964, 6.91394786, 2.79976789, 1.50722126, 6.48486038],
+        [1.19433531, 6.29760769, 1.55051483, 3.59191723, 6.60212631, 0.14022512],
+        [4.67099335, 3.31383981, 1.63220871, 2.23502881, 6.12885063, 4.68807186],
+        [7.30408120, 3.80874526, 3.72204577, 0.66679044, 3.76398221, 1.91782718],
+        [5.92805521, 2.31772110, 2.46175229, 4.46357240, 1.63871164, 5.55132881],
+        [1.68150470, 2.41378962, 3.08030571, 2.26997261, 6.68549178, 5.65767640],
+        [5.91934684, 1.69803498, 2.00819247, 0.77198553, 7.45425308, 1.37234202],
+        [5.53172263, 1.58718247, 2.90049338, 3.84536120, 4.93451202, 4.39679691],
+        [1.81587798, 3.88103491, 6.91095741, 2.44678365, 1.42743937, 5.08473102],
+        [2.55234884, 1.33311404, 4.82796793, 6.60348354, 4.37598182, 2.37567598],
+        [5.12272256, 2.03525835, 0.58747523, 7.29581125, 3.77362349, 1.34209305],
+        [0.27055161, 1.32505827, 0.82880694, 5.07516063, 3.65750329, 7.63868547],
+        [1.46014514, 1.17338868, 1.66221135, 3.56456484, 7.12719319, 5.49774348],
+        [3.98703736, 1.50123650, 3.76330075, 8.02615543, 1.75580291, 0.43055159],
+        [0.60157514, 1.24011244, 3.12416622, 3.63995686, 4.51357407, 7.39717359],
+        [3.12439557, 8.26536950, 0.88929913, 4.26851883, 1.38154049, 1.00102909],
+        [7.26979727, 2.40760022, 4.84651024, 3.36817790, 1.75647764, 1.85337834],
+        [5.29377498, 0.84307824, 6.79950126, 3.34597688, 0.88493918, 3.61293088],
+        [4.63626603, 0.39739091, 0.76906016, 4.12608336, 1.14102927, 7.70903059],
+        [7.30424808, 1.14559469, 2.98920677, 3.67674168, 3.92079189, 2.74028780],
+        [5.29547651, 2.85826185, 6.93657406, 1.33672260, 3.70102745, 0.43333184],
+        [3.49313310, 4.54336493, 1.56445202, 4.02157354, 6.96400202, 0.19485449],
+        [2.73742580, 3.81123532, 5.00271032, 1.84885547, 3.60970535, 6.04198936],
+        [1.31467477, 2.17922309, 2.63299757, 0.16194893, 9.27658681, 0.71319734],
+        [2.76492957, 4.10249912, 5.39611053, 5.09161003, 3.89378732, 2.30663915],
+        [1.94582268, 1.87031650, 4.23230029, 8.24909730, 1.84242598, 1.83335339],
+        [3.39276969, 4.78530893, 1.24380350, 7.20460307, 1.71752335, 3.03095584],
+        [7.75709126, 2.86264536, 2.29374145, 2.68872195, 3.44058744, 2.70271701],
+        [1.47805473, 0.56765227, 7.37702891, 0.09271143, 4.36384564, 4.90110451],
+        [1.82149711, 2.26115095, 1.21455515, 6.81753044, 3.70962435, 5.46389662],
+        [2.36459101, 0.12105503, 0.24822624, 3.62858762, 8.70938165, 2.30487785],
+        [0.78787403, 3.72625502, 1.21190521, 8.38489292, 3.70155394, 0.13278932],
+        [0.75428354, 7.27153941, 4.72464149, 3.80635658, 0.68907238, 3.04472702],
+        [5.12459705, 2.87302371, 0.43644150, 2.27226903, 7.07471522, 3.17473726],
+        [0.70178603, 2.88216063, 4.06577522, 6.39365228, 5.80793442, 0.24336517],
+        [6.99582269, 0.77356834, 2.80788891, 0.88674480, 2.35952093, 6.01848497],
+        [3.69693603, 0.85342739, 8.50506298, 1.79107374, 2.34725025, 2.13323710],
+        [3.09834673, 3.77166522, 3.14744542, 8.10858723, 0.17928020, 0.69788831],
+        [3.26246217, 3.77555064, 5.13658315, 5.95335244, 0.50710019, 3.60797944],
+        [5.71260164, 2.65661250, 0.79608313, 0.61669039, 1.68623679, 7.51339754],
+        [1.73776892, 2.79515779, 5.26232705, 6.15519998, 2.91607776, 3.88396316],
+        [5.84288335, 2.11962167, 3.05793046, 0.02686868, 2.23735440, 6.85642056],
+        [3.91342565, 1.03017066, 1.54153396, 9.00377711, 0.12064941, 0.40615586],
+        [3.43153396, 0.09349497, 5.55042223, 0.76858599, 6.17367356, 4.32477477],
+        [2.77747993, 1.37010370, 7.76474417, 0.73141948, 5.35935029, 0.92712408],
+        [0.79943355, 8.13116642, 3.40436523, 0.56073057, 4.53168239, 0.89709087],
+        [1.86322992, 3.54240501, 4.19454516, 2.38460530, 4.28548294, 6.50644492],
+        [2.28255306, 2.45104299, 0.21768383, 1.84082889, 6.48114612, 6.58339180],
+        [3.76567476, 2.57286633, 5.15499273, 6.27845094, 1.49352325, 3.31308685],
+        [0.95794499, 1.73192724, 5.52153846, 3.27936890, 7.20088206, 1.72870459],
+        [5.26466955, 6.16182728, 5.64238349, 0.09414682, 0.83475936, 1.33152576],
+        [6.93893651, 5.88859230, 0.31336892, 2.38592572, 0.64748960, 3.31142816],
+        [3.12497486, 3.76754998, 4.92020336, 3.59103512, 2.40286990, 5.75867877],
+        [1.84234679, 2.60493608, 4.71846310, 0.96653102, 8.14264373, 0.56510457],
+        [4.90541585, 6.68462108, 3.29352326, 1.25075319, 4.14418724, 1.29103422],
+        [0.77144415, 4.40756648, 2.83853600, 3.65761195, 7.28753539, 2.33123310],
+        [6.65232787, 0.61323121, 0.05237419, 3.02035801, 4.15410725, 5.38410334],
+        [5.31726305, 2.11795941, 2.65122017, 1.55577325, 7.31211902, 2.07953799],
+        [6.80965923, 2.23215998, 2.63179708, 6.09494365, 1.63349762, 1.37949068],
+        [2.87351031, 0.55340637, 5.41604829, 3.55340001, 1.91207540, 6.76907800],
+        [4.39372404, 0.06868095, 4.42865756, 5.35634714, 1.48055098, 5.49499576],
+        [1.04344792, 0.31173723, 1.56573663, 5.79359716, 2.79947656, 7.41347881],
+        [6.79414017, 0.09388667, 6.72878775, 2.10117159, 1.75210434, 1.03415104],
+        [1.08440052, 5.95542225, 0.39378945, 2.98381656, 3.56511194, 6.44893532],
+        [0.99064629, 1.35478691, 4.38745525, 2.66407385, 7.56658676, 3.68549645],
+        [7.14046972, 4.96484612, 3.19339033, 1.54451680, 3.42381000, 0.24134276],
+        [8.79248611, 2.96251195, 2.89483138, 1.82850443, 0.12129745, 1.47564250],
+        [2.94607977, 5.56169236, 2.59013018, 4.02632765, 3.30995993, 5.14900658],
+        [0.88402512, 0.35402459, 3.25443105, 6.37560825, 6.50357815, 2.35731528],
+        [1.94480257, 1.35885649, 0.88307848, 0.22492469, 3.33476206, 9.07855690],
+        [2.48584079, 5.07531399, 4.43407763, 4.51885124, 3.70528802, 3.77512426],
+        [7.48552913, 3.84871747, 2.91583698, 3.06351150, 0.02403987, 3.35655202],
+        [3.59720211, 4.94644652, 2.96841414, 4.91597513, 4.88131902, 2.40566715],
+        [1.87916271, 8.77986293, 0.14218332, 2.30457011, 2.26344244, 2.98803000],
+        [2.83343404, 4.99152641, 4.61233030, 6.57058571, 0.84136667, 1.37921369],
+        [0.95627524, 7.31066478, 4.38424188, 3.81744136, 3.40120183, 0.52641131],
+        [2.37882278, 7.66799820, 2.56820049, 3.40968706, 3.97975549, 1.21775717],
+        [3.90209510, 1.06132813, 0.91070059, 3.86200052, 1.17124110, 8.15665416],
+        [4.61696992, 4.62523330, 5.02800260, 4.30399729, 3.24034661, 1.72793421],
+        [2.21521553, 1.61363547, 2.97780427, 7.14111649, 1.51642660, 5.50695815],
+        [5.65137310, 0.06657199, 0.99745488, 4.47419538, 3.55469208, 5.86586515],
+        [0.68558487, 0.30887798, 2.04370625, 9.13806017, 3.13470834, 1.38826961],
+        [2.61242526, 6.59409851, 4.45452192, 3.66950713, 4.03746871, 0.28965048],
+        [7.79480886, 4.68269928, 3.85253341, 0.20850008, 1.55792871, 0.02558407],
+        [1.68967122, 1.11253309, 3.74425011, 3.12606095, 3.20780397, 7.86292624],
+    ],
+}
+
+
+class FruitTreeEnv(gym.Env, EzPickle):
+    """
+    ## Description
+
+    Full binary tree of depth d=5,6 or 7. Every leaf contains a fruit with a value for the nutrients Protein, Carbs, Fats, Vitamins, Minerals and Water.
+    From [Yang et al. 2019](https://arxiv.org/pdf/1908.08342.pdf).
+
+    ## Observation Space
+    Discrete space of size 2^d-1, where d is the depth of the tree.
+
+    ## Action Space
+    The agent can chose to go left or right at every node. The action space is therefore a discrete space of size 2.
+
+    ## Reward Space
+    Each leaf node contains a 6-dimensional vector containing the nutrients of the fruit. The agent receives a reward for each nutrient it collects.
+
+    ## Starting State
+    The agent starts at the root node (0, 0).
+
+    ## Episode Termination
+    The episode terminates when the agent reaches a leaf node.
+    """
+
+    def __init__(self, depth=6):
+        assert depth in [5, 6, 7], "Depth must be 5, 6 or 7."
+        EzPickle.__init__(self, depth)
+
+        self.reward_dim = 6
+        self.tree_depth = depth  # zero based depth
+        branches = np.zeros((int(2**self.tree_depth - 1), self.reward_dim))
+        # fruits = np.random.randn(2**self.tree_depth, self.reward_dim)
+        # fruits = np.abs(fruits) / np.linalg.norm(fruits, 2, 1, True)
+        # print(fruits*10)
+        fruits = np.array(FRUITS[str(depth)])
+        self.tree = np.concatenate([branches, fruits])
+
+        self.max_reward = 10.0
+        self.reward_space = spaces.Box(low=0.0, high=self.max_reward, shape=(self.reward_dim,), dtype=np.float32)
+
+        self.observation_space = spaces.Box(low=0, high=2**self.tree_depth - 1, shape=(2,), dtype=np.int32)
+
+        # action space specification: 0 left, 1 right
+        self.action_space = spaces.Discrete(2)
+
+        self.current_state = np.array([0, 0], dtype=np.int32)
+        self.terminal = False
+
+    def get_ind(self, pos):
+        return int(2 ** pos[0] - 1) + pos[1]
+
+    def get_tree_value(self, pos):
+        return self.tree[self.get_ind(pos)]
+
+    def reset(self, seed=None, **kwargs):
+        super().reset(seed=seed)
+
+        self.current_state = np.array([0, 0], dtype=np.int32)
+        self.terminal = False
+        return self.current_state.copy(), {}
+
+    def step(self, action):
+        direction = {
+            0: np.array([1, self.current_state[1]], dtype=np.int32),  # left
+            1: np.array([1, self.current_state[1] + 1], dtype=np.int32),  # right
+        }[action]
+
+        self.current_state = self.current_state + direction
+
+        reward = self.get_tree_value(self.current_state)
+        if self.current_state[0] == self.tree_depth:
+            self.terminal = True
+
+        return self.current_state.copy(), reward, self.terminal, False, {}
diff --git a/MO-Gymnasium/mo_gymnasium/envs/highway/__init__.py b/MO-Gymnasium/mo_gymnasium/envs/highway/__init__.py
new file mode 100644
index 0000000..00f66d3
--- /dev/null
+++ b/MO-Gymnasium/mo_gymnasium/envs/highway/__init__.py
@@ -0,0 +1,6 @@
+from gymnasium.envs.registration import register
+
+
+register(id="mo-highway-v0", entry_point="mo_gymnasium.envs.highway.highway:MOHighwayEnv", nondeterministic=True)
+
+register(id="mo-highway-fast-v0", entry_point="mo_gymnasium.envs.highway.highway:MOHighwayEnvFast", nondeterministic=True)
diff --git a/MO-Gymnasium/mo_gymnasium/envs/highway/highway.py b/MO-Gymnasium/mo_gymnasium/envs/highway/highway.py
new file mode 100644
index 0000000..f566078
--- /dev/null
+++ b/MO-Gymnasium/mo_gymnasium/envs/highway/highway.py
@@ -0,0 +1,64 @@
+import numpy as np
+from gymnasium.spaces import Box
+from gymnasium.utils import EzPickle
+from highway_env.envs import HighwayEnv, HighwayEnvFast
+
+
+class MOHighwayEnv(HighwayEnv, EzPickle):
+    """
+    ## Description
+    Multi-objective version of the HighwayEnv environment.
+
+    See [highway-env](https://github.com/eleurent/highway-env) for more information.
+
+    ## Reward Space
+    The reward is 3-dimensional:
+    - 0: high speed reward
+    - 1: right lane reward
+    - 2: collision reward
+    """
+
+    def __init__(self, *args, **kwargs):
+        EzPickle.__init__(self, *args, **kwargs)
+
+        super().__init__(*args, **kwargs)
+        self.reward_space = Box(low=-1.0, high=1.0, shape=(3,), dtype=np.float32)
+        self.reward_dim = 3
+
+    def step(self, action):
+        obs, reward, terminated, truncated, info = super().step(action)
+        rewards = info["rewards"]
+        vec_reward = np.array(
+            [
+                rewards["high_speed_reward"],
+                rewards["right_lane_reward"],
+                -rewards["collision_reward"],
+            ],
+            dtype=np.float32,
+        )
+        vec_reward *= rewards["on_road_reward"]
+        info["original_reward"] = reward
+        return obs, vec_reward, terminated, truncated, info
+
+
+class MOHighwayEnvFast(HighwayEnvFast):
+    """A multi-objective version of the HighwayFastEnv environment."""
+
+    def __init__(self, *args, **kwargs):
+        super().__init__(*args, **kwargs)
+        self.reward_space = Box(low=-1.0, high=1.0, shape=(3,), dtype=np.float32)
+
+    def step(self, action):
+        obs, reward, terminated, truncated, info = super().step(action)
+        rewards = info["rewards"]
+        vec_reward = np.array(
+            [
+                rewards["high_speed_reward"],
+                rewards["right_lane_reward"],
+                -rewards["collision_reward"],
+            ],
+            dtype=np.float32,
+        )
+        vec_reward *= rewards["on_road_reward"]
+        info["original_reward"] = reward
+        return obs, vec_reward, terminated, truncated, info
diff --git a/MO-Gymnasium/mo_gymnasium/envs/lunar_lander/__init__.py b/MO-Gymnasium/mo_gymnasium/envs/lunar_lander/__init__.py
new file mode 100644
index 0000000..d443534
--- /dev/null
+++ b/MO-Gymnasium/mo_gymnasium/envs/lunar_lander/__init__.py
@@ -0,0 +1,15 @@
+from gymnasium.envs.registration import register
+
+
+register(
+    id="mo-lunar-lander-v2",
+    entry_point="mo_gymnasium.envs.lunar_lander.lunar_lander:MOLunarLander",
+    max_episode_steps=1000,
+)
+
+register(
+    id="mo-lunar-lander-continuous-v2",
+    entry_point="mo_gymnasium.envs.lunar_lander.lunar_lander:MOLunarLander",
+    max_episode_steps=1000,
+    kwargs={"continuous": True},
+)
diff --git a/MO-Gymnasium/mo_gymnasium/envs/lunar_lander/lunar_lander.py b/MO-Gymnasium/mo_gymnasium/envs/lunar_lander/lunar_lander.py
new file mode 100644
index 0000000..caeab3a
--- /dev/null
+++ b/MO-Gymnasium/mo_gymnasium/envs/lunar_lander/lunar_lander.py
@@ -0,0 +1,183 @@
+import math
+
+import numpy as np
+from gymnasium import spaces
+from gymnasium.envs.box2d.lunar_lander import (
+    FPS,
+    LEG_DOWN,
+    MAIN_ENGINE_POWER,
+    SCALE,
+    SIDE_ENGINE_AWAY,
+    SIDE_ENGINE_HEIGHT,
+    SIDE_ENGINE_POWER,
+    VIEWPORT_H,
+    VIEWPORT_W,
+    LunarLander,
+)
+
+
+class MOLunarLander(LunarLander):  # no need for EzPickle, it's already in LunarLander
+    """
+    ## Description
+    Multi-objective version of the LunarLander environment.
+
+    See [Gymnasium's env](https://gymnasium.farama.org/environments/box2d/lunar_lander/) for more information.
+
+    ## Reward Space
+    The reward is 4-dimensional:
+    - 0: -100 if crash, +100 if lands successfully
+    - 1: Shaping reward
+    - 2: Fuel cost (main engine)
+    - 3: Fuel cost (side engine)
+    """
+
+    def __init__(self, *args, **kwargs):
+        super().__init__(*args, **kwargs)
+
+        # Result reward, shaping reward, main engine cost, side engine cost
+        self.reward_space = spaces.Box(low=-np.inf, high=np.inf, shape=(4,), dtype=np.float32)
+        self.reward_dim = 4
+
+    def step(self, action):
+        assert self.lander is not None
+
+        # Update wind
+        assert self.lander is not None, "You forgot to call reset()"
+        if self.enable_wind and not (self.legs[0].ground_contact or self.legs[1].ground_contact):
+            # the function used for wind is tanh(sin(2 k x) + sin(pi k x)),
+            # which is proven to never be periodic, k = 0.01
+            wind_mag = math.tanh(math.sin(0.02 * self.wind_idx) + (math.sin(math.pi * 0.01 * self.wind_idx))) * self.wind_power
+            self.wind_idx += 1
+            self.lander.ApplyForceToCenter(
+                (wind_mag, 0.0),
+                True,
+            )
+
+            # the function used for torque is tanh(sin(2 k x) + sin(pi k x)),
+            # which is proven to never be periodic, k = 0.01
+            torque_mag = math.tanh(math.sin(0.02 * self.torque_idx) + (math.sin(math.pi * 0.01 * self.torque_idx))) * (
+                self.turbulence_power
+            )
+            self.torque_idx += 1
+            self.lander.ApplyTorque(
+                (torque_mag),
+                True,
+            )
+
+        if self.continuous:
+            action = np.clip(action, -1, +1).astype(np.float32)
+        else:
+            assert self.action_space.contains(action), f"{action!r} ({type(action)}) invalid "
+
+        # Engines
+        tip = (math.sin(self.lander.angle), math.cos(self.lander.angle))
+        side = (-tip[1], tip[0])
+        dispersion = [self.np_random.uniform(-1.0, +1.0) / SCALE for _ in range(2)]
+
+        m_power = 0.0
+        if (self.continuous and action[0] > 0.0) or (not self.continuous and action == 2):
+            # Main engine
+            if self.continuous:
+                m_power = (np.clip(action[0], 0.0, 1.0) + 1.0) * 0.5  # 0.5..1.0
+                assert m_power >= 0.5 and m_power <= 1.0
+            else:
+                m_power = 1.0
+            # 4 is move a bit downwards, +-2 for randomness
+            ox = tip[0] * (4 / SCALE + 2 * dispersion[0]) + side[0] * dispersion[1]
+            oy = -tip[1] * (4 / SCALE + 2 * dispersion[0]) - side[1] * dispersion[1]
+            impulse_pos = (self.lander.position[0] + ox, self.lander.position[1] + oy)
+            p = self._create_particle(
+                3.5,  # 3.5 is here to make particle speed adequate
+                impulse_pos[0],
+                impulse_pos[1],
+                m_power,
+            )  # particles are just a decoration
+            p.ApplyLinearImpulse(
+                (ox * MAIN_ENGINE_POWER * m_power, oy * MAIN_ENGINE_POWER * m_power),
+                impulse_pos,
+                True,
+            )
+            self.lander.ApplyLinearImpulse(
+                (-ox * MAIN_ENGINE_POWER * m_power, -oy * MAIN_ENGINE_POWER * m_power),
+                impulse_pos,
+                True,
+            )
+
+        s_power = 0.0
+        if (self.continuous and np.abs(action[1]) > 0.5) or (not self.continuous and action in [1, 3]):
+            # Orientation engines
+            if self.continuous:
+                direction = np.sign(action[1])
+                s_power = np.clip(np.abs(action[1]), 0.5, 1.0)
+                assert s_power >= 0.5 and s_power <= 1.0
+            else:
+                direction = action - 2
+                s_power = 1.0
+            ox = tip[0] * dispersion[0] + side[0] * (3 * dispersion[1] + direction * SIDE_ENGINE_AWAY / SCALE)
+            oy = -tip[1] * dispersion[0] - side[1] * (3 * dispersion[1] + direction * SIDE_ENGINE_AWAY / SCALE)
+            impulse_pos = (
+                self.lander.position[0] + ox - tip[0] * 17 / SCALE,
+                self.lander.position[1] + oy + tip[1] * SIDE_ENGINE_HEIGHT / SCALE,
+            )
+            p = self._create_particle(0.7, impulse_pos[0], impulse_pos[1], s_power)
+            p.ApplyLinearImpulse(
+                (ox * SIDE_ENGINE_POWER * s_power, oy * SIDE_ENGINE_POWER * s_power),
+                impulse_pos,
+                True,
+            )
+            self.lander.ApplyLinearImpulse(
+                (-ox * SIDE_ENGINE_POWER * s_power, -oy * SIDE_ENGINE_POWER * s_power),
+                impulse_pos,
+                True,
+            )
+
+        self.world.Step(1.0 / FPS, 6 * 30, 2 * 30)
+
+        pos = self.lander.position
+        vel = self.lander.linearVelocity
+        state = [
+            (pos.x - VIEWPORT_W / SCALE / 2) / (VIEWPORT_W / SCALE / 2),
+            (pos.y - (self.helipad_y + LEG_DOWN / SCALE)) / (VIEWPORT_H / SCALE / 2),
+            vel.x * (VIEWPORT_W / SCALE / 2) / FPS,
+            vel.y * (VIEWPORT_H / SCALE / 2) / FPS,
+            self.lander.angle,
+            20.0 * self.lander.angularVelocity / FPS,
+            1.0 if self.legs[0].ground_contact else 0.0,
+            1.0 if self.legs[1].ground_contact else 0.0,
+        ]
+        assert len(state) == 8
+
+        reward = 0
+        vector_reward = np.zeros(4, dtype=np.float32)
+        shaping = (
+            -100 * np.sqrt(state[0] * state[0] + state[1] * state[1])
+            - 100 * np.sqrt(state[2] * state[2] + state[3] * state[3])
+            - 100 * abs(state[4])
+            + 10 * state[6]
+            + 10 * state[7]
+        )  # And ten points for legs contact, the idea is if you
+        # lose contact again after landing, you get negative reward
+        if self.prev_shaping is not None:
+            reward = shaping - self.prev_shaping
+            vector_reward[1] = shaping - self.prev_shaping
+        self.prev_shaping = shaping
+
+        reward -= m_power * 0.30  # less fuel spent is better, about -30 for heuristic landing
+        vector_reward[2] = -m_power
+        reward -= s_power * 0.03
+        vector_reward[3] = -s_power
+
+        terminated = False
+        if self.game_over or abs(state[0]) >= 1.0:
+            terminated = True
+            reward = -100
+            vector_reward[0] = -100
+        if not self.lander.awake:
+            terminated = True
+            reward = +100
+            vector_reward[0] = +100
+
+        if self.render_mode == "human":
+            self.render()
+
+        return np.array(state, dtype=np.float32), vector_reward, terminated, False, {"original_reward": reward}
diff --git a/MO-Gymnasium/mo_gymnasium/envs/mario/__init__.py b/MO-Gymnasium/mo_gymnasium/envs/mario/__init__.py
new file mode 100644
index 0000000..e88cdfa
--- /dev/null
+++ b/MO-Gymnasium/mo_gymnasium/envs/mario/__init__.py
@@ -0,0 +1,8 @@
+from gymnasium.envs.registration import register
+
+
+register(
+    id="mo-supermario-v0",
+    entry_point="mo_gymnasium.envs.mario.mario:MOSuperMarioBros",
+    nondeterministic=True,
+)
diff --git a/MO-Gymnasium/mo_gymnasium/envs/mario/joypad_space.py b/MO-Gymnasium/mo_gymnasium/envs/mario/joypad_space.py
new file mode 100644
index 0000000..32dee7f
--- /dev/null
+++ b/MO-Gymnasium/mo_gymnasium/envs/mario/joypad_space.py
@@ -0,0 +1,103 @@
+"""An environment wrapper to convert binary to discrete action space. This is a modified version of the original code from nes-py."""
+import gymnasium as gym
+from gymnasium import Env, Wrapper
+
+
+class JoypadSpace(Wrapper):
+    """An environment wrapper to convert binary to discrete action space."""
+
+    # a mapping of buttons to binary values
+    _button_map = {
+        "right": 0b10000000,
+        "left": 0b01000000,
+        "down": 0b00100000,
+        "up": 0b00010000,
+        "start": 0b00001000,
+        "select": 0b00000100,
+        "B": 0b00000010,
+        "A": 0b00000001,
+        "NOOP": 0b00000000,
+    }
+
+    @classmethod
+    def buttons(cls) -> list:
+        """Return the buttons that can be used as actions."""
+        return list(cls._button_map.keys())
+
+    def __init__(self, env: Env, actions: list):
+        """
+        Initialize a new binary to discrete action space wrapper.
+
+        Args:
+            env: the environment to wrap
+            actions: an ordered list of actions (as lists of buttons).
+                The index of each button list is its discrete coded value
+
+        Returns:
+            None
+
+        """
+        super().__init__(env)
+        # create the new action space
+        self.action_space = gym.spaces.Discrete(len(actions))
+        # create the action map from the list of discrete actions
+        self._action_map = {}
+        self._action_meanings = {}
+        # iterate over all the actions (as button lists)
+        for action, button_list in enumerate(actions):
+            # the value of this action's bitmap
+            byte_action = 0
+            # iterate over the buttons in this button list
+            for button in button_list:
+                byte_action |= self._button_map[button]
+            # set this action maps value to the byte action value
+            self._action_map[action] = byte_action
+            self._action_meanings[action] = " ".join(button_list)
+
+    def step(self, action):
+        """
+        Take a step using the given action.
+
+        Args:
+            action (int): the discrete action to perform
+
+        Returns:
+            a tuple of:
+            - (numpy.ndarray) the state as a result of the action
+            - (float) the reward achieved by taking the action
+            - (bool) a flag denoting whether the episode has ended
+            - (dict) a dictionary of extra information
+
+        """
+        # take the step and record the output
+        return self.env.step(self._action_map[action])
+
+    def reset(self):
+        """Reset the environment and return the initial observation."""
+        return self.env.reset()
+
+    def get_keys_to_action(self):
+        """Return the dictionary of keyboard keys to actions."""
+        # get the old mapping of keys to actions
+        old_keys_to_action = self.env.unwrapped.get_keys_to_action()
+        # invert the keys to action mapping to lookup key combos by action
+        action_to_keys = {v: k for k, v in old_keys_to_action.items()}
+        # create a new mapping of keys to actions
+        keys_to_action = {}
+        # iterate over the actions and their byte values in this mapper
+        for action, byte in self._action_map.items():
+            # get the keys to press for the action
+            keys = action_to_keys[byte]
+            # set the keys value in the dictionary to the current discrete act
+            keys_to_action[keys] = action
+
+        return keys_to_action
+
+    def get_action_meanings(self):
+        """Return a list of actions meanings."""
+        actions = sorted(self._action_meanings.keys())
+        return [self._action_meanings[action] for action in actions]
+
+
+# explicitly define the outward facing API of this module
+__all__ = [JoypadSpace.__name__]
diff --git a/MO-Gymnasium/mo_gymnasium/envs/mario/mario.py b/MO-Gymnasium/mo_gymnasium/envs/mario/mario.py
new file mode 100644
index 0000000..d013a7a
--- /dev/null
+++ b/MO-Gymnasium/mo_gymnasium/envs/mario/mario.py
@@ -0,0 +1,185 @@
+from typing import Optional
+
+import gymnasium as gym
+import numpy as np
+from gym_super_mario_bros import SuperMarioBrosEnv
+from gym_super_mario_bros.actions import SIMPLE_MOVEMENT
+from gymnasium.utils import EzPickle, seeding
+
+# from stable_baselines3.common.atari_wrappers import MaxAndSkipEnv
+from gymnasium.wrappers import GrayScaleObservation, ResizeObservation
+from nes_py.nes_env import SCREEN_SHAPE_24_BIT
+
+import mo_gymnasium as mo_gym
+
+# from nes_py.wrappers import JoypadSpace
+from mo_gymnasium.envs.mario.joypad_space import JoypadSpace
+
+
+class MOSuperMarioBros(SuperMarioBrosEnv, EzPickle):
+    """
+    ## Description
+    Multi-objective version of the SuperMarioBro environment.
+
+    See [gym-super-mario-bros](https://github.com/Kautenja/gym-super-mario-bros) for more information.
+
+    ## Reward Space
+    The reward is a 5-dimensional vector:
+    - 0: How far Mario moved in the x position
+    - 1: Time penalty for how much time has passed between two time steps
+    - 2: -25 if Mario died, 0 otherwise
+    - 3: +100 if Mario collected coins, else 0
+    - 4: Points for killing an enemy
+
+    ## Episode Termination
+    The episode terminates when Mario dies or reaches the flag.
+    """
+
+    metadata = {"render_modes": ["human", "rgb_array"], "render_fps": 60}
+
+    def __init__(
+        self,
+        rom_mode="pixel",
+        lost_levels=False,
+        target=None,
+        objectives=["x_pos", "time", "death", "coin", "enemy"],
+        render_mode: Optional[str] = None,
+    ):
+        EzPickle.__init__(self, rom_mode, lost_levels, target, objectives, render_mode)
+        super().__init__(rom_mode, lost_levels, target)
+
+        self.render_mode = render_mode
+
+        self.objectives = set(objectives)
+        self.reward_space = gym.spaces.Box(high=np.inf, low=-np.inf, shape=(len(objectives),))
+        self.reward_dim = len(objectives)
+
+        # observation space for the environment is static across all instances
+        self.observation_space = gym.spaces.Box(low=0, high=255, shape=SCREEN_SHAPE_24_BIT, dtype=np.uint8)
+
+        # action space is a bitmap of button press values for the 8 NES buttons
+        self.action_space = gym.spaces.Discrete(256)
+
+        self.single_stage = True
+        self.done_when_dead = True
+
+    def reset(self, seed=None, **kwargs):
+        self._np_random, seed = seeding.np_random(seed)  # this is not used
+        self.coin = 0
+        self.x_pos = 0
+        self.time = 0
+        self.score = 0
+        self.stage_bonus = 0
+        self.lives = 2
+        obs = super().reset()
+        if self.render_mode == "human":
+            self.render()
+        return obs, {}
+
+    def render(self):
+        if self.render_mode is None:
+            assert self.spec is not None
+            gym.logger.warn(
+                "You are calling render method without specifying any render mode. "
+                "You can specify the render_mode at initialization, "
+                f'e.g. gym.make("{self.spec.id}", render_mode="rgb_array")'
+            )
+            return
+
+        if self.render_mode == "human":
+            super().render(mode="human")
+        elif self.render_mode == "rgb_array":
+            return super().render(mode="rgb_array")
+
+    def step(self, action):
+        obs, reward, done, info = super().step(action)
+
+        if self.single_stage and info["flag_get"]:
+            self.stage_bonus = 10000
+            done = True
+
+        """ Construct Multi-Objective Reward"""
+        # [x_pos, time, death, coin, enemy]
+        moreward = []
+
+        # 1. x position
+        if "x_pos" in self.objectives:
+            xpos_r = info["x_pos"] - self.x_pos
+            self.x_pos = info["x_pos"]
+            # resolve an issue where after death the x position resets
+            if xpos_r < -5:
+                xpos_r = 0
+            moreward.append(xpos_r)
+
+        # 2. time penaltiy
+        if "time" in self.objectives:
+            time_r = info["time"] - self.time
+            self.time = info["time"]
+            # time is always decreasing
+            if time_r > 0:
+                time_r = 0
+            moreward.append(time_r)
+
+        # 3. death
+        if "death" in self.objectives:
+            if self.lives > info["life"]:
+                death_r = -25
+            else:
+                death_r = 0
+            moreward.append(death_r)
+
+        # 4. coin
+        coin_r = 0
+        if "coin" in self.objectives:
+            coin_r = (info["coins"] - self.coin) * 100
+            self.coin = info["coins"]
+            moreward.append(coin_r)
+
+        # 5. enemy
+        if "enemy" in self.objectives:
+            enemy_r = info["score"] - self.score
+            if coin_r > 0 or done:
+                enemy_r = 0
+            self.score = info["score"]
+            moreward.append(enemy_r)
+
+        ############################################################################
+
+        if self.done_when_dead:
+            # when Mario loses life, changes the state to the terminal
+            if self.lives > info["life"] and info["life"] > 0:
+                done = True
+
+        self.lives = info["life"]
+
+        mor = np.array(moreward, dtype=np.float32) * self.reward_space.shape[0] / 150
+
+        info["score"] = info["score"] + self.stage_bonus
+
+        if self.render_mode == "human":
+            self.render()
+
+        return obs, mor, bool(done), False, info
+
+
+if __name__ == "__main__":
+
+    env = MOSuperMarioBros()
+    env = JoypadSpace(env, SIMPLE_MOVEMENT)
+    # env = MaxAndSkipEnv(env, 4)
+    env = ResizeObservation(env, (84, 84))
+    env = GrayScaleObservation(env)
+    # env = FrameStack(env, 4)
+    env = mo_gym.LinearReward(env)
+
+    terminated = False
+    env.reset()
+    while True:
+        obs, r, terminated, truncated, info = env.step(env.action_space.sample())
+        print(r, info["vector_reward"], terminated, info["time"])
+        """ plt.figure()
+        plt.imshow(obs, cmap='gray', vmin=0, vmax=255)
+        plt.show() """
+        env.render()
+        if terminated:
+            env.reset()
diff --git a/MO-Gymnasium/mo_gymnasium/envs/minecart/__init__.py b/MO-Gymnasium/mo_gymnasium/envs/minecart/__init__.py
new file mode 100644
index 0000000..349bae5
--- /dev/null
+++ b/MO-Gymnasium/mo_gymnasium/envs/minecart/__init__.py
@@ -0,0 +1,17 @@
+from pathlib import Path
+
+from gymnasium.envs.registration import register
+
+
+register(
+    id="minecart-v0",
+    entry_point="mo_gymnasium.envs.minecart.minecart:Minecart",
+    max_episode_steps=1000,
+)
+
+register(
+    id="minecart-deterministic-v0",
+    entry_point="mo_gymnasium.envs.minecart.minecart:Minecart",
+    kwargs={"config": str(Path(__file__).parent.absolute()) + "/mine_config_det.json"},
+    max_episode_steps=1000,
+)
diff --git a/MO-Gymnasium/mo_gymnasium/envs/minecart/assets/cart.png b/MO-Gymnasium/mo_gymnasium/envs/minecart/assets/cart.png
new file mode 100644
index 0000000..15d0ac9
Binary files /dev/null and b/MO-Gymnasium/mo_gymnasium/envs/minecart/assets/cart.png differ
diff --git a/MO-Gymnasium/mo_gymnasium/envs/minecart/assets/mine.png b/MO-Gymnasium/mo_gymnasium/envs/minecart/assets/mine.png
new file mode 100644
index 0000000..d6fe1fc
Binary files /dev/null and b/MO-Gymnasium/mo_gymnasium/envs/minecart/assets/mine.png differ
diff --git a/MO-Gymnasium/mo_gymnasium/envs/minecart/mine_config.json b/MO-Gymnasium/mo_gymnasium/envs/minecart/mine_config.json
new file mode 100644
index 0000000..59d9060
--- /dev/null
+++ b/MO-Gymnasium/mo_gymnasium/envs/minecart/mine_config.json
@@ -0,0 +1,89 @@
+{
+    "capacity": 1.5,
+    "mine_cnt": 5,
+    "ore_cnt": 2,
+    "ore_colors": [
+        [
+            57,
+            112,
+            252
+        ],
+        [
+            137,
+            48,
+            182
+        ]
+    ],
+    "mines": [
+        {
+            "x": 0.16,
+            "y": 0.84,
+            "distributions": [
+                [
+                    0.2,
+                    0.05
+                ],
+                [
+                    0,
+                    0.05
+                ]
+            ]
+        },
+        {
+            "x": 0.5,
+            "y": 0.84,
+            "distributions": [
+                [
+                    0.15,
+                    0.05
+                ],
+                [
+                    0.1,
+                    0.05
+                ]
+            ]
+        },
+        {
+            "x": 0.84,
+            "y": 0.84,
+            "distributions": [
+                [
+                    0.2,
+                    0.05
+                ],
+                [
+                    0.2,
+                    0.05
+                ]
+            ]
+        },
+        {
+            "x": 0.84,
+            "y": 0.5,
+            "distributions": [
+                [
+                    0.1,
+                    0.05
+                ],
+                [
+                    0.15,
+                    0.05
+                ]
+            ]
+        },
+        {
+            "x": 0.84,
+            "y": 0.16,
+            "distributions": [
+                [
+                    0,
+                    0.05
+                ],
+                [
+                    0.2,
+                    0.05
+                ]
+            ]
+        }
+    ]
+}
diff --git a/MO-Gymnasium/mo_gymnasium/envs/minecart/mine_config_det.json b/MO-Gymnasium/mo_gymnasium/envs/minecart/mine_config_det.json
new file mode 100644
index 0000000..928c5dc
--- /dev/null
+++ b/MO-Gymnasium/mo_gymnasium/envs/minecart/mine_config_det.json
@@ -0,0 +1,89 @@
+{
+    "capacity": 1.5,
+    "mine_cnt": 5,
+    "ore_cnt": 2,
+    "ore_colors": [
+        [
+            57,
+            112,
+            252
+        ],
+        [
+            137,
+            48,
+            182
+        ]
+    ],
+    "mines": [
+        {
+            "x": 0.16,
+            "y": 0.84,
+            "distributions": [
+                [
+                    0.2,
+                    0
+                ],
+                [
+                    0,
+                    0
+                ]
+            ]
+        },
+        {
+            "x": 0.5,
+            "y": 0.84,
+            "distributions": [
+                [
+                    0.15,
+                    0
+                ],
+                [
+                    0.1,
+                    0
+                ]
+            ]
+        },
+        {
+            "x": 0.84,
+            "y": 0.84,
+            "distributions": [
+                [
+                    0.2,
+                    0
+                ],
+                [
+                    0.2,
+                    0
+                ]
+            ]
+        },
+        {
+            "x": 0.84,
+            "y": 0.5,
+            "distributions": [
+                [
+                    0.1,
+                    0
+                ],
+                [
+                    0.15,
+                    0
+                ]
+            ]
+        },
+        {
+            "x": 0.84,
+            "y": 0.16,
+            "distributions": [
+                [
+                    0,
+                    0
+                ],
+                [
+                    0.2,
+                    0
+                ]
+            ]
+        }
+    ]
+}
diff --git a/MO-Gymnasium/mo_gymnasium/envs/minecart/minecart.py b/MO-Gymnasium/mo_gymnasium/envs/minecart/minecart.py
new file mode 100644
index 0000000..b56257a
--- /dev/null
+++ b/MO-Gymnasium/mo_gymnasium/envs/minecart/minecart.py
@@ -0,0 +1,846 @@
+import itertools
+import json
+import math
+from copy import deepcopy
+from math import ceil
+from pathlib import Path
+from typing import List, Optional
+
+import gymnasium as gym
+import numpy as np
+import pygame
+import scipy.stats
+from gymnasium.spaces import Box, Discrete
+from gymnasium.utils import EzPickle
+from scipy.spatial import ConvexHull
+
+
+EPS_SPEED = 0.001  # Minimum speed to be considered in motion
+HOME_X = 0.0
+HOME_Y = 0.0
+HOME_POS = (HOME_X, HOME_Y)
+
+ROTATION = 10
+MAX_SPEED = 1.0
+
+FUEL_MINE = -0.05
+FUEL_ACC = -0.025
+FUEL_IDLE = -0.005
+
+CAPACITY = 1
+
+ACT_MINE = 0
+ACT_LEFT = 1
+ACT_RIGHT = 2
+ACT_ACCEL = 3
+ACT_BRAKE = 4
+ACT_NONE = 5
+FUEL_LIST = [
+    FUEL_MINE + FUEL_IDLE,
+    FUEL_IDLE,
+    FUEL_IDLE,
+    FUEL_IDLE + FUEL_ACC,
+    FUEL_IDLE,
+    FUEL_IDLE,
+]
+FUEL_DICT = {
+    ACT_MINE: FUEL_MINE + FUEL_IDLE,
+    ACT_LEFT: FUEL_IDLE,
+    ACT_RIGHT: FUEL_IDLE,
+    ACT_ACCEL: FUEL_IDLE + FUEL_ACC,
+    ACT_BRAKE: FUEL_IDLE,
+    ACT_NONE: FUEL_IDLE,
+}
+ACTIONS = ["Mine", "Left", "Right", "Accelerate", "Brake", "None"]
+ACTION_COUNT = len(ACTIONS)
+
+
+MINE_RADIUS = 0.14
+BASE_RADIUS = 0.15
+
+WIDTH = 480
+HEIGHT = 480
+
+# Color definitions
+WHITE = (255, 255, 255)
+GRAY = (150, 150, 150)
+C_GRAY = (150 / 255.0, 150 / 255.0, 150 / 255.0)
+DARK_GRAY = (100, 100, 100)
+BLACK = (0, 0, 0)
+RED = (255, 70, 70)
+C_RED = (1.0, 70 / 255.0, 70 / 255.0)
+
+FPS = 180
+
+MINE_LOCATION_TRIES = 100
+
+MINE_SCALE = 1.0
+BASE_SCALE = 1.0
+CART_SCALE = 1.0
+
+MARGIN = 0.16 * CART_SCALE
+
+ACCELERATION = 0.0075 * CART_SCALE
+DECELERATION = 1
+
+CART_IMG = str(Path(__file__).parent.absolute()) + "/assets/cart.png"
+MINE_IMG = str(Path(__file__).parent.absolute()) + "/assets/mine.png"
+
+
+class Minecart(gym.Env, EzPickle):
+    """
+    ## Description
+    Agent must collect two types of ores and minimize fuel consumption.
+    From [Abels et al. 2019](https://arxiv.org/abs/1809.07803v2).
+
+    ## Observation Space
+    The observation is a 7-dimensional vector containing the following information:
+    - 2D position of the cart
+    - Speed of the cart
+    - sin and cos of the cart's orientation
+    - porcentage of the capacity of the cart filled
+    If image_observation is True, the observation is a 3D image of the environment.
+
+    ## Action Space
+    The action space is a discrete space with 6 actions:
+    - 0: Mine
+    - 1: Left
+    - 2: Right
+    - 3: Accelerate
+    - 4: Brake
+    - 5: None
+
+    ## Reward Space
+    The reward is a 3D vector:
+    - 0: Quantity of the first minerium that was retrieved to the base (sparse)
+    - 1: Quantity of the second minerium that was retrieved to the base (sparse)
+    - 2: Fuel consumed (dense)
+
+    ## Starting State
+    The cart starts at the base on the upper left corner of the map.
+
+    ## Episode Termination
+    The episode ends when the cart returns to the base.
+
+    ## Arguments
+    - render_mode: The render mode to use. Can be "rgb_array" or "human".
+    - image_observation: If True, the observation is a RGB image of the environment.
+    - frame_skip: How many times each action is repeated. Default: 4
+    - incremental_frame_skip: Whether actions are repeated incrementally. Default: True
+    - config: Path to the .json configuration file. See the default configuration file for more information: https://github.com/Farama-Foundation/MO-Gymnasium/blob/main/mo_gymnasium/envs/minecart/mine_config.json
+
+    ## Credits
+    The code was refactored from [Axel Abels' source](https://github.com/axelabels/DynMORL).
+    """
+
+    metadata = {"render_modes": ["rgb_array", "human"], "render_fps": FPS}
+
+    def __init__(
+        self,
+        render_mode: Optional[str] = None,
+        image_observation: bool = False,
+        frame_skip: int = 4,
+        incremental_frane_skip: bool = True,
+        config=str(Path(__file__).parent.absolute()) + "/mine_config.json",
+    ):
+        EzPickle.__init__(self, render_mode, image_observation, frame_skip, incremental_frane_skip, config)
+
+        self.render_mode = render_mode
+        self.screen = None
+        self.last_render_mode_used = None
+        self.config = config
+        self.frame_skip = frame_skip
+        assert self.frame_skip > 0, "Frame skip must be greater than 0."
+        self.incremental_frame_skip = incremental_frane_skip
+
+        with open(self.config) as f:
+            data = json.load(f)
+
+        self.ore_cnt = data["ore_cnt"]
+        self.capacity = data["capacity"]
+        self.mine_cnt = data["mine_cnt"]
+        ore_colors = None if "ore_colors" not in data else data["ore_colors"]
+        self.ore_colors = ore_colors or [
+            (
+                np.random.randint(40, 255),
+                np.random.randint(40, 255),
+                np.random.randint(40, 255),
+            )
+            for i in range(self.ore_cnt)
+        ]
+        self.generate_mines(None)
+
+        if "mines" in data:
+            for mine_data, mine in zip(data["mines"], self.mines):
+                mine.pos = np.array([mine_data["x"], mine_data["y"]])
+                if "distributions" in mine_data:
+                    mine.distributions = [scipy.stats.norm(dist[0], dist[1]) for dist in mine_data["distributions"]]
+
+        self.cart = Cart(self.ore_cnt)
+
+        self.end = False
+
+        self.image_observation = image_observation
+        if self.image_observation:
+            shape = (WIDTH, HEIGHT, 3)
+            self.observation_space = Box(
+                low=np.zeros(shape),
+                high=255 * np.ones((WIDTH, HEIGHT, 3)),
+                dtype=np.uint8,
+            )
+        else:
+            self.observation_space = Box(-np.ones(7), np.ones(7), dtype=np.float32)
+
+        self.action_space = Discrete(6)
+        self.reward_space = Box(low=-1, high=self.capacity, shape=(self.ore_cnt + 1,))
+        self.reward_dim = self.ore_cnt + 1
+
+    def convex_coverage_set(self, gamma: float, symmetric: bool = True) -> List[np.ndarray]:
+        """
+        Computes an approximate convex coverage set (CCS).
+
+        Args:
+            gamma (float): Discount factor to apply to rewards.
+            symmetric (bool): If true, we assume the pattern of accelerations from the base to the mine is the same as from the mine to the base. Default: True
+
+        Returns:
+            The convex coverage set
+        """
+        policies = self.pareto_front(gamma, symmetric)
+        origin = np.min(policies, axis=0)
+        extended_policies = [origin] + policies
+        return [policies[idx - 1] for idx in ConvexHull(extended_policies).vertices if idx != 0]
+
+    def pareto_front(self, gamma: float, symmetric: bool = True) -> List[np.ndarray]:
+        """
+        Computes an approximate pareto front.
+
+        Args:
+            gamma (float): Discount factor to apply to rewards
+            symmetric (bool): If true, we assume the pattern of accelerations from the base to the mine is the same as from the mine to the base. Default: True
+
+        Returns:
+            The pareto coverage set
+        """
+        all_rewards = []
+        base_perimeter = BASE_RADIUS * BASE_SCALE
+
+        # Empty mine just outside the base
+        virtual_mine = Mine(
+            self.ore_cnt,
+            (base_perimeter**2 / 2) ** (1 / 2),
+            (base_perimeter**2 / 2) ** (1 / 2),
+        )
+        virtual_mine.distributions = [scipy.stats.norm(0, 0) for _ in range(self.ore_cnt)]
+        for mine in self.mines + [virtual_mine]:
+            mine_distance = mag(mine.pos - HOME_POS) - MINE_RADIUS * MINE_SCALE - BASE_RADIUS * BASE_SCALE / 2
+
+            # Number of rotations required to face the mine
+            angle = compute_angle(mine.pos, HOME_POS, [1, 1])
+            rotations = int(ceil(abs(angle) / (ROTATION * self.frame_skip)))
+
+            # Build pattern of accelerations/nops to reach the mine
+            # initialize with single acceleration
+            queue = [
+                {
+                    "speed": ACCELERATION * self.frame_skip,
+                    "dist": mine_distance - self.frame_skip * (self.frame_skip + 1) / 2 * ACCELERATION
+                    if self.incremental_frame_skip
+                    else mine_distance - ACCELERATION * self.frame_skip * self.frame_skip,
+                    "seq": [ACT_ACCEL],
+                }
+            ]
+            trimmed_sequences = []
+
+            while len(queue) > 0:
+                seq = queue.pop()
+                # accelerate
+                new_speed = seq["speed"] + ACCELERATION * self.frame_skip
+                accelerations = new_speed / ACCELERATION
+                movement = (
+                    accelerations * (accelerations + 1) / 2 * ACCELERATION
+                    - (accelerations - self.frame_skip) * ((accelerations - self.frame_skip) + 1) / 2 * ACCELERATION
+                )
+                dist = seq["dist"] - movement
+                speed = new_speed
+                if dist <= 0:
+                    trimmed_sequences.append(seq["seq"] + [ACT_ACCEL])
+                else:
+                    queue.append({"speed": speed, "dist": dist, "seq": seq["seq"] + [ACT_ACCEL]})
+                # idle
+                dist = seq["dist"] - seq["speed"] * self.frame_skip
+
+                if dist <= 0:
+                    trimmed_sequences.append(seq["seq"] + [ACT_NONE])
+                else:
+                    queue.append(
+                        {
+                            "speed": seq["speed"],
+                            "dist": dist,
+                            "seq": seq["seq"] + [ACT_NONE],
+                        }
+                    )
+
+            # Build rational mining sequences
+            mine_means = mine.distribution_means() * self.frame_skip
+            mn_sum = np.sum(mine_means)
+            # on average it takes up to this many actions to fill cart
+            max_mine_actions = 0 if mn_sum == 0 else int(ceil(self.capacity / mn_sum))
+
+            # all possible mining sequences (i.e. how many times we mine)
+            mine_sequences = [[ACT_MINE] * i for i in range(1, max_mine_actions + 1)]
+
+            # All possible combinations of actions before, during and after mining
+            if len(mine_sequences) > 0:
+                if not symmetric:
+                    all_sequences = map(
+                        lambda sequences: list(sequences[0])
+                        + list(sequences[1])
+                        + list(sequences[2])
+                        + list(sequences[3])
+                        + list(sequences[4]),
+                        itertools.product(
+                            [[ACT_LEFT] * rotations],
+                            trimmed_sequences,
+                            [[ACT_BRAKE] + [ACT_LEFT] * (180 // (ROTATION * self.frame_skip))],
+                            mine_sequences,
+                            trimmed_sequences,
+                        ),
+                    )
+
+                else:
+                    all_sequences = map(
+                        lambda sequences: list(sequences[0])
+                        + list(sequences[1])
+                        + list(sequences[2])
+                        + list(sequences[3])
+                        + list(sequences[1]),
+                        itertools.product(
+                            [[ACT_LEFT] * rotations],
+                            trimmed_sequences,
+                            [[ACT_BRAKE] + [ACT_LEFT] * (180 // (ROTATION * self.frame_skip))],
+                            mine_sequences,
+                        ),
+                    )
+            else:
+                if not symmetric:
+                    print(
+                        [ACT_NONE] + trimmed_sequences[1:],
+                        trimmed_sequences[1:],
+                        trimmed_sequences,
+                    )
+                    all_sequences = map(
+                        lambda sequences: list(sequences[0])
+                        + list(sequences[1])
+                        + list(sequences[2])
+                        + [ACT_NONE]
+                        + list(sequences[3])[1:],
+                        itertools.product(
+                            [[ACT_LEFT] * rotations],
+                            trimmed_sequences,
+                            [[ACT_LEFT] * (180 // (ROTATION * self.frame_skip))],
+                            trimmed_sequences,
+                        ),
+                    )
+
+                else:
+                    all_sequences = map(
+                        lambda sequences: list(sequences[0])
+                        + list(sequences[1])
+                        + list(sequences[2])
+                        + [ACT_NONE]
+                        + list(sequences[1][1:]),
+                        itertools.product(
+                            [[ACT_LEFT] * rotations],
+                            trimmed_sequences,
+                            [[ACT_LEFT] * (180 // (ROTATION * self.frame_skip))],
+                        ),
+                    )
+
+            # Compute rewards for each sequence
+            fuel_costs = np.array([f * self.frame_skip for f in FUEL_LIST])
+
+            def maxlen(l):
+                if len(l) == 0:
+                    return 0
+                return max([len(s) for s in l])
+
+            longest_pattern = maxlen(trimmed_sequences)
+            max_len = (
+                rotations
+                + longest_pattern
+                + 1
+                + (180 // (ROTATION * self.frame_skip))
+                + maxlen(mine_sequences)
+                + longest_pattern
+            )
+            discount_map = gamma ** np.arange(max_len)
+            for s in all_sequences:
+                reward = np.zeros((len(s), self.reward_dim))
+                reward[:, -1] = fuel_costs[s]
+                mine_actions = s.count(ACT_MINE)
+                reward[-1, :-1] = mine_means * mine_actions / max(1, (mn_sum * mine_actions) / self.capacity)
+
+                reward = np.dot(discount_map[: len(s)], reward)
+                all_rewards.append(reward)
+
+            all_rewards = pareto_filter(all_rewards, minimize=False)
+
+        return all_rewards
+
+    def generate_mines(self, mine_distributions=None):
+        """
+        Randomly generate mines that don't overlap the base
+        TODO: propose some default formations
+        """
+        self.mines = []
+        for i in range(self.mine_cnt):
+            pos = np.array((np.random.random(), np.random.random()))
+
+            tries = 0
+            while (mag(pos - HOME_POS) < BASE_RADIUS * BASE_SCALE + MARGIN) and (tries < MINE_LOCATION_TRIES):
+                pos[0] = np.random.random()
+                pos[1] = np.random.random()
+                tries += 1
+            assert tries < MINE_LOCATION_TRIES
+            self.mines.append(Mine(self.ore_cnt, *pos))
+            if mine_distributions:
+                self.mines[i].distributions = mine_distributions[i]
+
+    def initialize_mines(self):
+        """Assign a random rotation to each mine, and initialize the necessary sprites
+        for the Pygame backend
+        """
+
+        for mine in self.mines:
+            mine.rotation = np.random.randint(0, 360)
+
+        self.mine_sprites = pygame.sprite.Group()
+        self.mine_rects = []
+        for mine in self.mines:
+            mine_sprite = pygame.sprite.Sprite()
+            mine_sprite.image = pygame.transform.rotozoom(
+                pygame.image.load(MINE_IMG), mine.rotation, MINE_SCALE
+            ).convert_alpha()
+            self.mine_sprites.add(mine_sprite)
+            mine_sprite.rect = mine_sprite.image.get_rect()
+            mine_sprite.rect.centerx = (mine.pos[0] * (1 - 2 * MARGIN)) * WIDTH + MARGIN * WIDTH
+            mine_sprite.rect.centery = (mine.pos[1] * (1 - 2 * MARGIN)) * HEIGHT + MARGIN * HEIGHT
+            self.mine_rects.append(mine_sprite.rect)
+
+    def step(self, action):
+        change = False  # Keep track of whether the state has changed
+        reward = np.zeros(self.ore_cnt + 1)
+
+        reward[-1] = FUEL_IDLE * self.frame_skip
+
+        if action == ACT_ACCEL:
+            reward[-1] += FUEL_ACC * self.frame_skip
+        elif action == ACT_MINE:
+            reward[-1] += FUEL_MINE * self.frame_skip
+
+        for _ in range(self.frame_skip if self.incremental_frame_skip else 1):
+
+            if action == ACT_LEFT:
+                self.cart.rotate(-ROTATION * (1 if self.incremental_frame_skip else self.frame_skip))
+                change = True
+            elif action == ACT_RIGHT:
+                self.cart.rotate(ROTATION * (1 if self.incremental_frame_skip else self.frame_skip))
+                change = True
+            elif action == ACT_ACCEL:
+                self.cart.accelerate(ACCELERATION * (1 if self.incremental_frame_skip else self.frame_skip))
+            elif action == ACT_BRAKE:
+                self.cart.accelerate(-DECELERATION * (1 if self.incremental_frame_skip else self.frame_skip))
+            elif action == ACT_MINE:
+                for _ in range(1 if self.incremental_frame_skip else self.frame_skip):
+                    change = self.mine() or change
+
+            if self.end:
+                break
+
+            for _ in range(1 if self.incremental_frame_skip else self.frame_skip):
+                change = self.cart.step() or change
+
+            distanceFromBase = mag(self.cart.pos - HOME_POS)
+            if distanceFromBase < BASE_RADIUS * BASE_SCALE:
+                if self.cart.departed:
+                    # Cart left base then came back, ending the episode
+                    self.end = True
+                    # Sell resources
+                    reward[: self.ore_cnt] += self.cart.content
+                    self.cart.content = np.zeros(self.ore_cnt)
+            else:
+                # Cart left base
+                self.cart.departed = True
+
+        if change and self.image_observation:
+            self.render_pygame()
+        if self.render_mode == "human":
+            self.render()
+
+        return self.get_state(change), reward, self.end, False, {}
+
+    def mine(self):
+        """Perform the MINE action
+
+        Returns:
+            bool -- True if something was mined
+        """
+        if self.cart.speed < EPS_SPEED:
+            # Get closest mine
+            mine = min(self.mines, key=lambda mine: mine.distance(self.cart))
+
+            if mine.mineable(self.cart):
+                cart_free = self.capacity - np.sum(self.cart.content)
+                mined = mine.mine()
+                total_mined = np.sum(mined)
+                if total_mined > cart_free:
+                    # Scale mined content to remaining capacity
+                    scale = cart_free / total_mined
+                    mined = np.array(mined) * scale
+
+                self.cart.content += mined
+
+                if np.sum(mined) > 0:
+                    return True
+        return False
+
+    def get_pixels(self, update=True):
+        """Get the environment's image representation
+
+        Keyword Arguments:
+            update {bool} -- Whether to redraw the environment (default: {True})
+
+        Returns:
+            np.array -- array of pixels, with shape (width, height, channels)
+        """
+        if update:
+            self.pixels = pygame.surfarray.array3d(self.screen)
+
+        return self.pixels
+
+    def get_state(self, update=True):
+        """Returns the environment's state
+
+        Keyword Arguments:
+            update {bool} -- Whether to update the representation (default: {True})
+
+        Returns:
+            dict -- dict containing the aforementioned elements
+        """
+        if self.image_observation:
+            state = self.get_pixels(update)
+        else:
+            angle = math.radians(self.cart.angle)
+            sina = math.sin(angle)
+            cosa = math.cos(angle)
+            angle = np.array([sina, cosa], dtype=np.float32)
+            state = np.concatenate(
+                (
+                    self.cart.pos,
+                    np.array([self.cart.speed]),
+                    angle,
+                    self.cart.content / self.capacity,
+                ),
+                dtype=np.float32,
+            )
+        return state
+
+        """ return {
+            "position": self.cart.pos,
+            "speed": self.cart.speed,
+            "orientation": self.cart.angle,
+            "content": self.cart.content,
+            "pixels": self.get_pixels(update)
+        } """
+
+    def reset(self, seed=None, **kwargs):
+        """Resets the environment to the start state
+
+        Returns:
+            [type] -- [description]
+        """
+        super().reset(seed=seed)
+
+        if self.screen is None and self.image_observation:
+            self.render(mode="rgb_array")  # init pygame
+
+        if self.image_observation:
+            self.render_pygame()
+
+        self.cart.content = np.zeros(self.ore_cnt)
+        self.cart.pos = np.array(HOME_POS)
+        self.cart.speed = 0
+        self.cart.angle = 45
+        self.cart.departed = False
+        self.end = False
+        if self.render_mode == "human":
+            self.render()
+        return self.get_state(), {}
+
+    def __str__(self):
+        string = f"Completed: {self.end} "
+        string += f"Departed: {self.cart.departed} "
+        string += f"Content: {self.cart.content} "
+        string += f"Speed: {self.cart.speed} "
+        string += f"Direction: {self.cart.angle} ({self.cart.angle * math.pi / 180}) "
+        string += f"Position: {self.cart.pos} "
+        return string
+
+    def render(self):
+        if self.screen is None or self.last_render_mode_used != self.render_mode:
+            self.last_render_mode_used = self.render_mode
+            pygame.init()
+            self.screen = pygame.display.set_mode(
+                (WIDTH, HEIGHT),
+                flags=pygame.HIDDEN if self.render_mode == "rgb_array" else 0,
+            )
+            self.clock = pygame.time.Clock()
+
+            self.initialize_mines()
+
+            self.cart_sprite = pygame.sprite.Sprite()
+            self.cart_sprites = pygame.sprite.Group()
+            self.cart_sprites.add(self.cart_sprite)
+            self.cart_image = pygame.transform.rotozoom(pygame.image.load(CART_IMG).convert_alpha(), 0, CART_SCALE)
+
+        if not self.image_observation:
+            self.render_pygame()  # if the obs is not an image, then step would not have rendered the screen
+
+        if self.render_mode == "human":
+            self.clock.tick(FPS)
+            pygame.display.update()
+        elif self.render_mode == "rgb_array":
+            string_image = pygame.image.tostring(self.screen, "RGB")
+            temp_surf = pygame.image.fromstring(string_image, (WIDTH, HEIGHT), "RGB")
+            tmp_arr = pygame.surfarray.array3d(temp_surf)
+            return tmp_arr
+
+    def render_pygame(self):
+        pygame.event.get()
+
+        self.mine_sprites.update()
+
+        # Clear canvas
+        self.screen.fill(GRAY)
+
+        # Draw Home
+        pygame.draw.circle(
+            self.screen,
+            RED,
+            (int(WIDTH * HOME_X), int(HEIGHT * HOME_Y)),
+            int(WIDTH / 3 * BASE_SCALE),
+        )
+
+        # Draw Mines
+        self.mine_sprites.draw(self.screen)
+
+        # Draw cart
+        self.cart_sprite.image = rot_center(self.cart_image, -self.cart.angle).copy()
+
+        self.cart_sprite.rect = self.cart_sprite.image.get_rect(center=(200, 200))
+
+        self.cart_sprite.rect.centerx = self.cart.pos[0] * (1 - 2 * MARGIN) * WIDTH + MARGIN * WIDTH
+        self.cart_sprite.rect.centery = self.cart.pos[1] * (1 - 2 * MARGIN) * HEIGHT + MARGIN * HEIGHT
+
+        self.cart_sprites.update()
+
+        self.cart_sprites.draw(self.screen)
+
+        # Draw cart content
+        width = self.cart_sprite.rect.width / (2 * self.ore_cnt)
+        height = self.cart_sprite.rect.height / 3
+        content_width = (width + 1) * self.ore_cnt
+        offset = (self.cart_sprite.rect.width - content_width) / 2
+        for i in range(self.ore_cnt):
+
+            rect_height = height * self.cart.content[i] / self.capacity
+
+            if rect_height >= 1:
+                pygame.draw.rect(
+                    self.screen,
+                    self.ore_colors[i],
+                    (
+                        self.cart_sprite.rect.left + offset + i * (width + 1),
+                        self.cart_sprite.rect.top + offset * 1.5,
+                        width,
+                        rect_height,
+                    ),
+                )
+
+    def close(self):
+        if self.screen is not None:
+            pygame.display.quit()
+            pygame.quit()
+
+    def __deepcopy__(self, memo):
+        this_copy = Minecart(self.image_observation, self.config)
+        this_copy.cart = deepcopy(self.cart)
+        this_copy.mines = deepcopy(self.mines)
+        this_copy.end = self.end
+        return this_copy
+
+
+class Mine:
+    """Class representing an individual Mine"""
+
+    def __init__(self, ore_cnt, x, y):
+        self.distributions = [scipy.stats.norm(np.random.random(), np.random.random()) for _ in range(ore_cnt)]
+        self.pos = np.array((x, y))
+
+    def distance(self, cart):
+        return mag(cart.pos - self.pos)
+
+    def mineable(self, cart):
+        return self.distance(cart) <= MINE_RADIUS * MINE_SCALE * CART_SCALE
+
+    def mine(self):
+        """Generates collected resources according to the mine's random
+        distribution
+
+        Returns:
+            list -- list of collected resources
+        """
+        return [max(0.0, dist.rvs()) for dist in self.distributions]
+
+    def distribution_means(self):
+        """
+        Computes the mean of the truncated normal distributions
+        """
+        means = np.zeros(len(self.distributions))
+
+        for i, dist in enumerate(self.distributions):
+            mean, std = dist.mean(), dist.std()
+            means[i] = truncated_mean(mean, std, 0, float("inf"))
+            if np.isnan(means[i]):
+                means[i] = 0
+        return means
+
+
+class Cart:
+    """Class representing the actual minecart"""
+
+    def __init__(self, ore_cnt):
+        self.ore_cnt = ore_cnt
+        self.pos = np.array([HOME_X, HOME_Y])
+        self.speed = 0
+        self.angle = 45
+        self.content = np.zeros(self.ore_cnt)
+        self.departed = False  # Keep track of whether the agent has left the base
+
+    def accelerate(self, acceleration):
+        self.speed = clip(self.speed + acceleration, 0, MAX_SPEED)
+
+    def rotate(self, rotation):
+        self.angle = (self.angle + rotation) % 360
+
+    def step(self):
+        """
+        Update cart's position, taking the current speed into account
+        Colliding with a border at anything but a straight angle will cause
+        cart to "slide" along the wall.
+        """
+        pre = np.copy(self.pos)
+        if self.speed < EPS_SPEED:
+            return False
+        x_velocity = self.speed * math.cos(self.angle * math.pi / 180)
+        y_velocity = self.speed * math.sin(self.angle * math.pi / 180)
+        x, y = self.pos
+        if y != 0 and y != 1 and (y_velocity > 0 + EPS_SPEED or y_velocity < 0 - EPS_SPEED):
+            if x == 1 and x_velocity > 0:
+                self.angle += math.copysign(ROTATION, y_velocity)
+            if x == 0 and x_velocity < 0:
+                self.angle -= math.copysign(ROTATION, y_velocity)
+        if x != 0 and x != 1 and (x_velocity > 0 + EPS_SPEED or x_velocity < 0 - EPS_SPEED):
+            if y == 1 and y_velocity > 0:
+                self.angle -= math.copysign(ROTATION, x_velocity)
+
+            if y == 0 and y_velocity < 0:
+                self.angle += math.copysign(ROTATION, x_velocity)
+
+        self.pos[0] = clip(x + x_velocity, 0, 1)
+        self.pos[1] = clip(y + y_velocity, 0, 1)
+        self.speed = mag(pre - self.pos)
+
+        return True
+
+
+def compute_angle(p0, p1, p2):
+    v0 = np.array(p0) - np.array(p1)
+    v1 = np.array(p2) - np.array(p1)
+
+    angle = np.math.atan2(np.linalg.det([v0, v1]), np.dot(v0, v1))
+    return np.degrees(angle)
+
+
+def rot_center(image, angle):
+    """Rotate an image while preserving its center and size"""
+    orig_rect = image.get_rect()
+    rot_image = pygame.transform.rotate(image, angle)
+    rot_rect = orig_rect.copy()
+    rot_rect.center = rot_image.get_rect().center
+    rot_image = rot_image.subsurface(rot_rect).copy()
+    return rot_image
+
+
+def mag(vector2d):
+    return np.sqrt(np.dot(vector2d, vector2d))
+
+
+def clip(val, lo, hi):
+    return lo if val <= lo else hi if val >= hi else val
+
+
+def scl(c):
+    return (c[0] / 255.0, c[1] / 255.0, c[2] / 255.0)
+
+
+def truncated_mean(mean, std, a, b):
+    if std == 0:
+        return mean
+    from scipy.stats import norm
+
+    a = (a - mean) / std
+    b = (b - mean) / std
+    PHIB = norm.cdf(b)
+    PHIA = norm.cdf(a)
+    phib = norm.pdf(b)
+    phia = norm.pdf(a)
+
+    trunc_mean = mean + ((phia - phib) / (PHIB - PHIA)) * std
+    return trunc_mean
+
+
+def pareto_filter(costs, minimize=True):
+    """
+    Find the pareto-efficient points
+    :param costs: An (n_points, n_costs) array
+    :param return_mask: True to return a mask
+    :return: An array of indices of pareto-efficient points.
+        If return_mask is True, this will be an (n_points, ) boolean array
+        Otherwise it will be a (n_efficient_points, ) integer array of indices.
+    from https://stackoverflow.com/a/40239615
+    """
+    costs_copy = np.copy(costs) if minimize else -np.copy(costs)
+    is_efficient = np.arange(costs_copy.shape[0])
+    next_point_index = 0  # Next index in the is_efficient array to search for
+    while next_point_index < len(costs_copy):
+        nondominated_point_mask = np.any(costs_copy < costs_copy[next_point_index], axis=1)
+        nondominated_point_mask[next_point_index] = True
+        # Remove dominated points
+        is_efficient = is_efficient[nondominated_point_mask]
+        costs_copy = costs_copy[nondominated_point_mask]
+        next_point_index = np.sum(nondominated_point_mask[:next_point_index]) + 1
+    return [costs[i] for i in is_efficient]
+
+
+if __name__ == "__main__":
+    env = Minecart(render_mode="human", image_observation=True)
+    terminated = False
+    env.reset()
+    while True:
+        env.render()
+        obs, r, terminated, truncated, info = env.step(env.action_space.sample())
+        # print(str(env))
+        if terminated:
+            env.reset()
diff --git a/MO-Gymnasium/mo_gymnasium/envs/mountain_car/__init__.py b/MO-Gymnasium/mo_gymnasium/envs/mountain_car/__init__.py
new file mode 100644
index 0000000..f75fe75
--- /dev/null
+++ b/MO-Gymnasium/mo_gymnasium/envs/mountain_car/__init__.py
@@ -0,0 +1,8 @@
+from gymnasium.envs.registration import register
+
+
+register(
+    id="mo-mountaincar-v0",
+    entry_point="mo_gymnasium.envs.mountain_car.mountain_car:MOMountainCar",
+    max_episode_steps=200,
+)
diff --git a/MO-Gymnasium/mo_gymnasium/envs/mountain_car/mountain_car.py b/MO-Gymnasium/mo_gymnasium/envs/mountain_car/mountain_car.py
new file mode 100644
index 0000000..6aabe81
--- /dev/null
+++ b/MO-Gymnasium/mo_gymnasium/envs/mountain_car/mountain_car.py
@@ -0,0 +1,51 @@
+import math
+from typing import Optional
+
+import numpy as np
+from gymnasium import spaces
+from gymnasium.envs.classic_control.mountain_car import MountainCarEnv
+from gymnasium.utils import EzPickle
+
+
+class MOMountainCar(MountainCarEnv, EzPickle):
+    """
+    A multi-objective version of the MountainCar environment, where the goal is to reach the top of the mountain.
+
+    See [Gymnasium's env](https://gymnasium.farama.org/environments/classic_control/mountain_car_continuous/) for more information.
+
+    ## Reward space:
+    The reward space is a 3D vector containing the time penalty, and penalties for reversing and going forward.
+    - time penalty: -1.0 for each time step
+    - reverse penalty: -1.0 for each time step the actuin is 0 (reverse)
+    - forward penalty: -1.0 for each time step the action is 2 (forward)
+    """
+
+    def __init__(self, render_mode: Optional[str] = None, goal_velocity=0):
+        super().__init__(render_mode, goal_velocity)
+        EzPickle.__init__(self, render_mode, goal_velocity)
+
+        self.reward_space = spaces.Box(low=-1, high=1, shape=(3,), dtype=np.float32)
+        self.reward_dim = 3
+
+    def step(self, action: int):
+        assert self.action_space.contains(action), f"{action!r} ({type(action)}) invalid"
+
+        position, velocity = self.state
+        velocity += (action - 1) * self.force + math.cos(3 * position) * (-self.gravity)
+        velocity = np.clip(velocity, -self.max_speed, self.max_speed)
+        position += velocity
+        position = np.clip(position, self.min_position, self.max_position)
+        if position == self.min_position and velocity < 0:
+            velocity = 0
+
+        terminated = bool(position >= self.goal_position and velocity >= self.goal_velocity)
+        # reward = -1.0
+        reward = np.zeros(3, dtype=np.float32)
+        reward[0] = 0.0 if terminated else -1.0  # time penalty
+        reward[1] = 0.0 if action != 0 else -1.0  # reverse penalty
+        reward[2] = 0.0 if action != 2 else -1.0  # forward penalty
+
+        self.state = (position, velocity)
+        if self.render_mode == "human":
+            self.render()
+        return np.array(self.state, dtype=np.float32), reward, terminated, False, {}
diff --git a/MO-Gymnasium/mo_gymnasium/envs/mujoco/__init__.py b/MO-Gymnasium/mo_gymnasium/envs/mujoco/__init__.py
new file mode 100644
index 0000000..442feae
--- /dev/null
+++ b/MO-Gymnasium/mo_gymnasium/envs/mujoco/__init__.py
@@ -0,0 +1,27 @@
+from gymnasium.envs.registration import register
+
+
+register(
+    id="mo-halfcheetah-v4",
+    entry_point="mo_gymnasium.envs.mujoco.half_cheetah:MOHalfCheehtahEnv",
+    max_episode_steps=1000,
+)
+
+register(
+    id="mo-hopper-v4",
+    entry_point="mo_gymnasium.envs.mujoco.hopper:MOHopperEnv",
+    max_episode_steps=1000,
+)
+
+register(
+    id="mo-hopper-2d-v4",
+    entry_point="mo_gymnasium.envs.mujoco.hopper:MOHopperEnv",
+    max_episode_steps=1000,
+    kwargs={"cost_objective": False},
+)
+
+register(
+    id="mo-reacher-v4",
+    entry_point="mo_gymnasium.envs.mujoco.reacher:MOReacherEnv",
+    max_episode_steps=50,
+)
diff --git a/MO-Gymnasium/mo_gymnasium/envs/mujoco/assets/mo_reacher.xml b/MO-Gymnasium/mo_gymnasium/envs/mujoco/assets/mo_reacher.xml
new file mode 100644
index 0000000..94e8d0f
--- /dev/null
+++ b/MO-Gymnasium/mo_gymnasium/envs/mujoco/assets/mo_reacher.xml
@@ -0,0 +1,54 @@
+<mujoco model="reacher">
+	<compiler angle="radian" inertiafromgeom="true"/>
+	<default>
+		<joint armature="1" damping="1" limited="true"/>
+		<geom contype="0" friction="1 0.1 0.1" rgba="0.7 0.7 0 1"/>
+	</default>
+	<option gravity="0 0 -9.81" integrator="RK4" timestep="0.01"/>
+	<worldbody>
+		<!-- Arena -->
+		<geom conaffinity="0" contype="0" name="ground" pos="0 0 0" rgba="0.9 0.9 0.9 1" size="1 1 10" type="plane"/>
+		<geom conaffinity="0" fromto="-.3 -.3 .01 .3 -.3 .01" name="sideS" rgba="0.9 0.4 0.6 1" size=".02" type="capsule"/>
+		<geom conaffinity="0" fromto=" .3 -.3 .01 .3  .3 .01" name="sideE" rgba="0.9 0.4 0.6 1" size=".02" type="capsule"/>
+		<geom conaffinity="0" fromto="-.3  .3 .01 .3  .3 .01" name="sideN" rgba="0.9 0.4 0.6 1" size=".02" type="capsule"/>
+		<geom conaffinity="0" fromto="-.3 -.3 .01 -.3 .3 .01" name="sideW" rgba="0.9 0.4 0.6 1" size=".02" type="capsule"/>
+		<!-- Arm -->
+		<geom conaffinity="0" contype="0" fromto="0 0 0 0 0 0.02" name="root" rgba="0.9 0.4 0.6 1" size=".011" type="cylinder"/>
+		<body name="body0" pos="0 0 .01">
+			<geom fromto="0 0 0 0.1 0 0" name="link0" rgba="0.0 0.4 0.6 1" size=".01" type="capsule"/>
+			<joint axis="0 0 1" limited="false" name="joint0" pos="0 0 0" type="hinge"/>
+			<body name="body1" pos="0.1 0 0">
+				<joint axis="0 0 1" limited="true" name="joint1" pos="0 0 0" range="-3.0 3.0" type="hinge"/>
+				<geom fromto="0 0 0 0.1 0 0" name="link1" rgba="0.0 0.4 0.6 1" size=".01" type="capsule"/>
+				<body name="fingertip" pos="0.11 0 0">
+					<geom contype="0" name="fingertip" pos="0 0 0" rgba="0.0 0.8 0.6 1" size=".01" type="sphere"/>
+				</body>
+			</body>
+		</body>
+		<!-- Target -->
+		<body name="target1" pos=".1 -.1 .01">
+			<joint armature="0" axis="1 0 0" damping="0" limited="true" name="target1_x" pos="0 0 0" range="-.27 .27" ref=".1" stiffness="0" type="slide"/>
+			<joint armature="0" axis="0 1 0" damping="0" limited="true" name="target1_y" pos="0 0 0" range="-.27 .27" ref="-.1" stiffness="0" type="slide"/>
+			<geom conaffinity="0" contype="0" name="target1" pos="0 0 0" rgba="0.9 0.2 0.2 1" size=".009" type="sphere"/>
+		</body>
+        <body name="target2" pos=".1 -.1 .01">
+			<joint armature="0" axis="1 0 0" damping="0" limited="true" name="target2_x" pos="0 0 0" range="-.27 .27" ref=".1" stiffness="0" type="slide"/>
+			<joint armature="0" axis="0 1 0" damping="0" limited="true" name="target2_y" pos="0 0 0" range="-.27 .27" ref="-.1" stiffness="0" type="slide"/>
+			<geom conaffinity="0" contype="0" name="target2" pos="0 0 0" rgba="0.9 0.2 0.2 1" size=".009" type="sphere"/>
+		</body>
+        <body name="target3" pos=".1 -.1 .01">
+			<joint armature="0" axis="1 0 0" damping="0" limited="true" name="target3_x" pos="0 0 0" range="-.27 .27" ref=".1" stiffness="0" type="slide"/>
+			<joint armature="0" axis="0 1 0" damping="0" limited="true" name="target3_y" pos="0 0 0" range="-.27 .27" ref="-.1" stiffness="0" type="slide"/>
+			<geom conaffinity="0" contype="0" name="target3" pos="0 0 0" rgba="0.9 0.2 0.2 1" size=".009" type="sphere"/>
+		</body>
+        <body name="target4" pos=".1 -.1 .01">
+			<joint armature="0" axis="1 0 0" damping="0" limited="true" name="target4_x" pos="0 0 0" range="-.27 .27" ref=".1" stiffness="0" type="slide"/>
+			<joint armature="0" axis="0 1 0" damping="0" limited="true" name="target4_y" pos="0 0 0" range="-.27 .27" ref="-.1" stiffness="0" type="slide"/>
+			<geom conaffinity="0" contype="0" name="target4" pos="0 0 0" rgba="0.9 0.2 0.2 1" size=".009" type="sphere"/>
+		</body>
+	</worldbody>
+	<actuator>
+		<motor ctrllimited="true" ctrlrange="-1.0 1.0" gear="200.0" joint="joint0"/>
+		<motor ctrllimited="true" ctrlrange="-1.0 1.0" gear="200.0" joint="joint1"/>
+	</actuator>
+</mujoco>
diff --git a/MO-Gymnasium/mo_gymnasium/envs/mujoco/half_cheetah.py b/MO-Gymnasium/mo_gymnasium/envs/mujoco/half_cheetah.py
new file mode 100644
index 0000000..8427cc5
--- /dev/null
+++ b/MO-Gymnasium/mo_gymnasium/envs/mujoco/half_cheetah.py
@@ -0,0 +1,29 @@
+import numpy as np
+from gymnasium.envs.mujoco.half_cheetah_v4 import HalfCheetahEnv
+from gymnasium.spaces import Box
+from gymnasium.utils import EzPickle
+
+
+class MOHalfCheehtahEnv(HalfCheetahEnv, EzPickle):
+    """
+    ## Description
+    Multi-objective version of the HalfCheetahEnv environment.
+
+    See [Gymnasium's env](https://gymnasium.farama.org/environments/mujoco/half_cheetah/) for more information.
+
+    ## Reward Space
+    The reward is 2-dimensional:
+    - 0: Reward for running forward
+    - 1: Control cost of the action
+    """
+
+    def __init__(self, **kwargs):
+        super().__init__(**kwargs)
+        EzPickle.__init__(self, **kwargs)
+        self.reward_space = Box(low=-np.inf, high=np.inf, shape=(2,))
+        self.reward_dim = 2
+
+    def step(self, action):
+        observation, reward, terminated, truncated, info = super().step(action)
+        vec_reward = np.array([info["reward_run"], info["reward_ctrl"]], dtype=np.float32)
+        return observation, vec_reward, terminated, truncated, info
diff --git a/MO-Gymnasium/mo_gymnasium/envs/mujoco/hopper.py b/MO-Gymnasium/mo_gymnasium/envs/mujoco/hopper.py
new file mode 100644
index 0000000..7d35b40
--- /dev/null
+++ b/MO-Gymnasium/mo_gymnasium/envs/mujoco/hopper.py
@@ -0,0 +1,68 @@
+import numpy as np
+from gymnasium.envs.mujoco.hopper_v4 import HopperEnv
+from gymnasium.spaces import Box
+from gymnasium.utils import EzPickle
+
+
+class MOHopperEnv(HopperEnv, EzPickle):
+    """
+    ## Description
+    Multi-objective version of the HopperEnv environment.
+
+    See [Gymnasium's env](https://gymnasium.farama.org/environments/mujoco/hopper/) for more information.
+
+    ## Reward Space
+    The reward is 3-dimensional:
+    - 0: Reward for going forward on the x-axis
+    - 1: Reward for jumping high on the z-axis
+    - 2: Control cost of the action
+    If the cost_objective flag is set to False, the reward is 2-dimensional, and the cost is added to other objectives.
+    """
+
+    def __init__(self, cost_objective=True, **kwargs):
+        super().__init__(**kwargs)
+        EzPickle.__init__(self, cost_objective, **kwargs)
+        self.cost_objetive = cost_objective
+        self.reward_dim = 3 if cost_objective else 2
+        self.reward_space = Box(low=-np.inf, high=np.inf, shape=(self.reward_dim,))
+
+    def step(self, action):
+        x_position_before = self.data.qpos[0]
+        self.do_simulation(action, self.frame_skip)
+        x_position_after = self.data.qpos[0]
+        x_velocity = (x_position_after - x_position_before) / self.dt
+
+        # ctrl_cost = self.control_cost(action)
+
+        # forward_reward = self._forward_reward_weight * x_velocity
+        healthy_reward = self.healthy_reward
+
+        # rewards = forward_reward + healthy_reward
+        # costs = ctrl_cost
+
+        observation = self._get_obs()
+        # reward = rewards - costs
+        terminated = self.terminated
+
+        z = self.data.qpos[1]
+        height = 10 * (z - self.init_qpos[1])
+        energy_cost = np.sum(np.square(action))
+
+        if self.cost_objetive:
+            vec_reward = np.array([x_velocity, height, -energy_cost], dtype=np.float32)
+        else:
+            vec_reward = np.array([x_velocity, height], dtype=np.float32)
+            vec_reward -= self._ctrl_cost_weight * energy_cost
+
+        vec_reward += healthy_reward
+
+        info = {
+            "x_position": x_position_after,
+            "x_velocity": x_velocity,
+            "height_reward": height,
+            "energy_reward": -energy_cost,
+        }
+
+        if self.render_mode == "human":
+            self.render()
+        return observation, vec_reward, terminated, False, info
diff --git a/MO-Gymnasium/mo_gymnasium/envs/mujoco/reacher.py b/MO-Gymnasium/mo_gymnasium/envs/mujoco/reacher.py
new file mode 100644
index 0000000..d1cb929
--- /dev/null
+++ b/MO-Gymnasium/mo_gymnasium/envs/mujoco/reacher.py
@@ -0,0 +1,98 @@
+from os import path
+
+import numpy as np
+from gymnasium import utils
+from gymnasium.envs.mujoco import MujocoEnv
+from gymnasium.envs.mujoco.reacher_v4 import ReacherEnv
+from gymnasium.spaces import Box, Discrete
+
+
+DEFAULT_CAMERA_CONFIG = {"trackbodyid": 0}
+
+
+class MOReacherEnv(ReacherEnv):
+    """
+    ## Description
+    Mujoco version of `mo-reacher-v0`, based on [`Reacher-v4` environment](https://gymnasium.farama.org/environments/mujoco/reacher/).
+
+    ## Observation Space
+    The observation is 6-dimensional and contains:
+    - sin and cos of the angles of the central and elbow joints
+    - angular velocity of the central and elbow joints
+
+    ## Action Space
+    The action space is discrete and contains the 3^2=9 possible actions based on applying positive (+1), negative (-1) or zero (0) torque to each of the two joints.
+
+    ## Reward Space
+    The reward is 4-dimensional and is defined based on the distance of the tip of the arm and the four target locations.
+    For each i={1,2,3,4} it is computed as:
+    ```math
+        r_i = 1  - 4 * || finger_tip_coord - target_i ||^2
+    ```
+    """
+
+    def __init__(self, **kwargs):
+        utils.EzPickle.__init__(self, **kwargs)
+        self.observation_space = Box(low=-np.inf, high=np.inf, shape=(6,), dtype=np.float64)
+        MujocoEnv.__init__(
+            self,
+            path.join(path.dirname(__file__), "assets", "mo_reacher.xml"),
+            2,
+            observation_space=self.observation_space,
+            default_camera_config=DEFAULT_CAMERA_CONFIG,
+            **kwargs,
+        )
+        actions = [-1.0, 0.0, 1.0]
+        self.action_dict = dict()
+        for a1 in actions:
+            for a2 in actions:
+                self.action_dict[len(self.action_dict)] = (a1, a2)
+        self.action_space = Discrete(9)
+        # Target goals: x1, y1, x2, y2, ... x4, y4
+        self.goal = np.array([0.14, 0.0, -0.14, 0.0, 0.0, 0.14, 0.0, -0.14])
+        self.reward_space = Box(low=-np.inf, high=np.inf, shape=(4,))
+        self.reward_dim = 4
+
+    def step(self, a):
+        real_action = self.action_dict[int(a)]
+        vec_reward = np.array(
+            [
+                1 - 4 * np.linalg.norm(self.get_body_com("fingertip")[:2] - self.get_body_com("target1")[:2]),
+                1 - 4 * np.linalg.norm(self.get_body_com("fingertip")[:2] - self.get_body_com("target2")[:2]),
+                1 - 4 * np.linalg.norm(self.get_body_com("fingertip")[:2] - self.get_body_com("target3")[:2]),
+                1 - 4 * np.linalg.norm(self.get_body_com("fingertip")[:2] - self.get_body_com("target4")[:2]),
+            ],
+            dtype=np.float32,
+        )
+
+        self._step_mujoco_simulation(real_action, self.frame_skip)
+        if self.render_mode == "human":
+            self.render()
+
+        ob = self._get_obs()
+        return (
+            ob,
+            vec_reward,
+            False,
+            False,
+            {},
+        )
+
+    def reset_model(self):
+        qpos = self.np_random.uniform(low=-0.1, high=0.1, size=self.model.nq) + self.init_qpos
+        qpos[:2] = np.array([0, 3.1415 / 2])  # init position
+        qpos[-len(self.goal) :] = self.goal
+        qvel = self.init_qvel + self.np_random.uniform(low=-0.005, high=0.005, size=self.model.nv)
+        qvel[-2:] = 0
+        self.set_state(qpos, qvel)
+        return self._get_obs()
+
+    def _get_obs(self):
+        theta = self.data.qpos.flat[:2]
+        return np.concatenate(
+            [
+                np.cos(theta),
+                np.sin(theta),
+                self.data.qvel.flat[:2] * 0.1,
+            ]
+        )
diff --git a/MO-Gymnasium/mo_gymnasium/envs/reacher/__init__.py b/MO-Gymnasium/mo_gymnasium/envs/reacher/__init__.py
new file mode 100644
index 0000000..b752382
--- /dev/null
+++ b/MO-Gymnasium/mo_gymnasium/envs/reacher/__init__.py
@@ -0,0 +1,9 @@
+from gymnasium.envs.registration import register
+
+
+register(
+    id="mo-reacher-v0",
+    entry_point="mo_gymnasium.envs.reacher.reacher:ReacherBulletEnv",
+    max_episode_steps=100,
+    kwargs={"fixed_initial_state": None},
+)
diff --git a/MO-Gymnasium/mo_gymnasium/envs/reacher/reacher.py b/MO-Gymnasium/mo_gymnasium/envs/reacher/reacher.py
new file mode 100644
index 0000000..0abffe2
--- /dev/null
+++ b/MO-Gymnasium/mo_gymnasium/envs/reacher/reacher.py
@@ -0,0 +1,158 @@
+from typing import Optional
+
+import numpy as np
+from gymnasium import spaces
+from gymnasium.utils import EzPickle
+from pybulletgym.envs.roboschool.envs.env_bases import BaseBulletEnv
+from pybulletgym.envs.roboschool.robots.robot_bases import MJCFBasedRobot
+from pybulletgym.envs.roboschool.scenes.scene_bases import SingleRobotEmptyScene
+
+
+target_positions = list(map(lambda l: np.array(l), [(0.14, 0.0), (-0.14, 0.0), (0.0, 0.14), (0.0, -0.14)]))
+
+
+class ReacherBulletEnv(BaseBulletEnv, EzPickle):
+
+    metadata = {"render_modes": ["human", "rgb_array"]}
+
+    def __init__(
+        self,
+        render_mode: Optional[str] = None,
+        target=(0.14, 0.0),
+        fixed_initial_state: Optional[tuple] = (3.14, 0),
+    ):
+        EzPickle.__init__(self, render_mode, target, fixed_initial_state)
+        self.robot = ReacherRobot(target, fixed_initial_state=fixed_initial_state)
+        self.render_mode = render_mode
+        BaseBulletEnv.__init__(self, self.robot, render=render_mode == "human")
+        self._cam_dist = 0.75
+
+        # self.target_positions = list(map(lambda l: np.array(l), [(0.14, 0.0), (-0.14, 0.0), (0.0, 0.14), (0.0, -0.14), (0.22, 0.0), (-0.22, 0.0), (0.0, 0.22), (0.0, -0.22), (0.1, 0.1), (0.1, -0.1), (-0.1, 0.1), (-0.1, -0.1)]))
+        # self.target_positions = list(map(lambda l: np.array(l), [(0.14, 0.0), (-0.14, 0.0), (0.0, 0.14), (0.0, -0.14), (0.1, 0.1), (0.1, -0.1), (-0.1, 0.1), (-0.1, -0.1)]))
+        self.target_positions = list(
+            map(
+                lambda l: np.array(l),
+                [(0.14, 0.0), (-0.14, 0.0), (0.0, 0.14), (0.0, -0.14)],
+            )
+        )
+
+        actions = [-1.0, 0.0, 1.0]
+        self.action_dict = dict()
+        for a1 in actions:
+            for a2 in actions:
+                self.action_dict[len(self.action_dict)] = (a1, a2)
+
+        self.action_space = spaces.Discrete(9)
+        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(5,), dtype=np.float32)
+        self.reward_space = spaces.Box(low=-np.inf, high=np.inf, shape=(4,), dtype=np.float32)
+        self.reward_dim = 4
+
+    def create_single_player_scene(self, bullet_client):
+        return SingleRobotEmptyScene(bullet_client, gravity=0.0, timestep=0.0165, frame_skip=1)
+
+    def step(self, a):
+        real_action = self.action_dict[int(a)]
+
+        assert not self.scene.multiplayer
+        self.robot.apply_action(real_action)
+        self.scene.global_step()
+
+        state = self.robot.calc_state()  # sets self.to_target_vec
+
+        """ delta = np.linalg.norm(np.array(self.robot.fingertip.pose().xyz()) - np.array(self.robot.target.pose().xyz()))
+        reward = 1. - 4. * delta """
+
+        phi = np.zeros(len(self.target_positions))
+        for index, target in enumerate(self.target_positions):
+            delta = np.linalg.norm(np.array(self.robot.fingertip.pose().xyz()[:2]) - target)
+            phi[index] = 1.0 - 4 * delta  # 1 - 4
+
+        self.HUD(state, real_action, False)
+
+        if self.render_mode == "human":
+            self._render(mode="human")
+
+        return state, phi, False, False, {}
+
+    def render(self):
+        if self.render_mode == "human":
+            self._render(mode="human")
+        else:
+            return self._render(mode="rgb_array")
+
+    def camera_adjust(self):
+        x, y, z = self.robot.fingertip.pose().xyz()
+        x *= 0.5
+        y *= 0.5
+        self.camera.move_and_look_at(0.3, 0.3, 0.3, x, y, z)
+
+    def reset(self, seed=None, **kwargs):
+        self._seed(seed)
+        obs = super().reset()
+        if self.render_mode == "human":
+            self._render(mode="human")
+        return obs, {}
+
+
+class ReacherRobot(MJCFBasedRobot):
+    TARG_LIMIT = 0.27
+
+    def __init__(self, target, fixed_initial_state=False):
+        MJCFBasedRobot.__init__(self, "reacher.xml", "body0", action_dim=2, obs_dim=4)
+        self.target_pos = target
+        self.fixed_initial_state = fixed_initial_state
+
+    def robot_specific_reset(self, bullet_client):
+        self.jdict["target_x"].reset_current_position(target_positions[0][0], 0)
+        self.jdict["target_y"].reset_current_position(target_positions[0][1], 0)
+
+        """ self.jdict["target2_x"].reset_current_position(target_positions[1][0], 0)
+        self.jdict["target2_y"].reset_current_position(target_positions[1][1], 0)
+        self.jdict["target3_x"].reset_current_position(target_positions[2][0], 0)
+        self.jdict["target3_y"].reset_current_position(target_positions[2][1], 0)
+        self.jdict["target4_x"].reset_current_position(target_positions[3][0], 0)
+        self.jdict["target4_y"].reset_current_position(target_positions[3][1], 0) """
+
+        self.fingertip = self.parts["fingertip"]
+        self.target = self.parts["target"]
+        self.central_joint = self.jdict["joint0"]
+        self.elbow_joint = self.jdict["joint1"]
+        if self.fixed_initial_state is None:
+            self.central_joint.reset_current_position(self.np_random.uniform(low=-3.14, high=3.14), 0)
+            self.elbow_joint.reset_current_position(self.np_random.uniform(low=-3.14 / 2, high=3.14 / 2), 0)
+        else:
+            self.central_joint.reset_current_position(0, 0)
+            self.elbow_joint.reset_current_position(self.fixed_initial_state[0], self.fixed_initial_state[1])
+
+    def apply_action(self, a):
+        assert np.isfinite(a).all()
+        self.central_joint.set_motor_torque(0.05 * float(np.clip(a[0], -1, +1)))
+        self.elbow_joint.set_motor_torque(0.05 * float(np.clip(a[1], -1, +1)))
+
+    def calc_state(self):
+        theta, self.theta_dot = self.central_joint.current_relative_position()
+        self.gamma, self.gamma_dot = self.elbow_joint.current_relative_position()
+        # target_x, _ = self.jdict["target_x"].current_position()
+        # target_y, _ = self.jdict["target_y"].current_position()
+        self.to_target_vec = np.array(self.fingertip.pose().xyz()) - np.array(self.target.pose().xyz())
+        return np.array(
+            [
+                np.cos(theta),
+                np.sin(theta),
+                self.theta_dot * 0.1,
+                self.gamma,
+                self.gamma_dot * 0.1,
+            ],
+            dtype=np.float32,
+        )
+
+
+if __name__ == "__main__":
+
+    env = ReacherBulletEnv()
+    # env.render(mode='human')
+    obs = env.reset()
+    print(env.observation_space.contains(obs), obs.dtype, env.observation_space)
+    while True:
+        env.step(env.action_space.sample())
+        # env.render(mode='human')
diff --git a/MO-Gymnasium/mo_gymnasium/envs/resource_gathering/__init__.py b/MO-Gymnasium/mo_gymnasium/envs/resource_gathering/__init__.py
new file mode 100644
index 0000000..132db28
--- /dev/null
+++ b/MO-Gymnasium/mo_gymnasium/envs/resource_gathering/__init__.py
@@ -0,0 +1,8 @@
+from gymnasium.envs.registration import register
+
+
+register(
+    id="resource-gathering-v0",
+    entry_point="mo_gymnasium.envs.resource_gathering.resource_gathering:ResourceGathering",
+    max_episode_steps=100,
+)
diff --git a/MO-Gymnasium/mo_gymnasium/envs/resource_gathering/assets/elf_down.png b/MO-Gymnasium/mo_gymnasium/envs/resource_gathering/assets/elf_down.png
new file mode 100644
index 0000000..afa3daf
Binary files /dev/null and b/MO-Gymnasium/mo_gymnasium/envs/resource_gathering/assets/elf_down.png differ
diff --git a/MO-Gymnasium/mo_gymnasium/envs/resource_gathering/assets/elf_left.png b/MO-Gymnasium/mo_gymnasium/envs/resource_gathering/assets/elf_left.png
new file mode 100644
index 0000000..bc9e22e
Binary files /dev/null and b/MO-Gymnasium/mo_gymnasium/envs/resource_gathering/assets/elf_left.png differ
diff --git a/MO-Gymnasium/mo_gymnasium/envs/resource_gathering/assets/elf_right.png b/MO-Gymnasium/mo_gymnasium/envs/resource_gathering/assets/elf_right.png
new file mode 100644
index 0000000..8364031
Binary files /dev/null and b/MO-Gymnasium/mo_gymnasium/envs/resource_gathering/assets/elf_right.png differ
diff --git a/MO-Gymnasium/mo_gymnasium/envs/resource_gathering/assets/elf_up.png b/MO-Gymnasium/mo_gymnasium/envs/resource_gathering/assets/elf_up.png
new file mode 100644
index 0000000..933f1f0
Binary files /dev/null and b/MO-Gymnasium/mo_gymnasium/envs/resource_gathering/assets/elf_up.png differ
diff --git a/MO-Gymnasium/mo_gymnasium/envs/resource_gathering/assets/enemy.png b/MO-Gymnasium/mo_gymnasium/envs/resource_gathering/assets/enemy.png
new file mode 100644
index 0000000..5946bea
Binary files /dev/null and b/MO-Gymnasium/mo_gymnasium/envs/resource_gathering/assets/enemy.png differ
diff --git a/MO-Gymnasium/mo_gymnasium/envs/resource_gathering/assets/gem.png b/MO-Gymnasium/mo_gymnasium/envs/resource_gathering/assets/gem.png
new file mode 100644
index 0000000..ee836d5
Binary files /dev/null and b/MO-Gymnasium/mo_gymnasium/envs/resource_gathering/assets/gem.png differ
diff --git a/MO-Gymnasium/mo_gymnasium/envs/resource_gathering/assets/gold.png b/MO-Gymnasium/mo_gymnasium/envs/resource_gathering/assets/gold.png
new file mode 100644
index 0000000..648ed6e
Binary files /dev/null and b/MO-Gymnasium/mo_gymnasium/envs/resource_gathering/assets/gold.png differ
diff --git a/MO-Gymnasium/mo_gymnasium/envs/resource_gathering/assets/home.png b/MO-Gymnasium/mo_gymnasium/envs/resource_gathering/assets/home.png
new file mode 100644
index 0000000..565d371
Binary files /dev/null and b/MO-Gymnasium/mo_gymnasium/envs/resource_gathering/assets/home.png differ
diff --git a/MO-Gymnasium/mo_gymnasium/envs/resource_gathering/assets/mountain_bg1.png b/MO-Gymnasium/mo_gymnasium/envs/resource_gathering/assets/mountain_bg1.png
new file mode 100644
index 0000000..e5872ce
Binary files /dev/null and b/MO-Gymnasium/mo_gymnasium/envs/resource_gathering/assets/mountain_bg1.png differ
diff --git a/MO-Gymnasium/mo_gymnasium/envs/resource_gathering/assets/mountain_bg2.png b/MO-Gymnasium/mo_gymnasium/envs/resource_gathering/assets/mountain_bg2.png
new file mode 100644
index 0000000..8cadf7d
Binary files /dev/null and b/MO-Gymnasium/mo_gymnasium/envs/resource_gathering/assets/mountain_bg2.png differ
diff --git a/MO-Gymnasium/mo_gymnasium/envs/resource_gathering/resource_gathering.py b/MO-Gymnasium/mo_gymnasium/envs/resource_gathering/resource_gathering.py
new file mode 100644
index 0000000..a8cd2b9
--- /dev/null
+++ b/MO-Gymnasium/mo_gymnasium/envs/resource_gathering/resource_gathering.py
@@ -0,0 +1,319 @@
+from os import path
+from typing import List, Optional
+
+import gymnasium as gym
+import numpy as np
+import pygame
+from gymnasium.spaces import Box, Discrete
+from gymnasium.utils import EzPickle
+
+
+class ResourceGathering(gym.Env, EzPickle):
+    """
+    ## Description
+    From "Barrett, Leon & Narayanan, Srini. (2008). Learning all optimal policies with multiple criteria.
+    Proceedings of the 25th International Conference on Machine Learning. 41-47. 10.1145/1390156.1390162."
+
+    ## Observation Space
+    The observation is discrete and consists of 4 elements:
+    - 0: The x coordinate of the agent
+    - 1: The y coordinate of the agent
+    - 2: Flag indicating if the agent collected the gold
+    - 3: Flag indicating if the agent collected the diamond
+
+    ## Action Space
+    The action is discrete and consists of 4 elements:
+    - 0: Move up
+    - 1: Move down
+    - 2: Move left
+    - 3: Move right
+
+    ## Reward Space
+    The reward is 3-dimensional:
+    - 0: +1 if returned home with gold, else 0
+    - 1: +1 if returned home with diamond, else 0
+    - 2: -1 if killed by an enemy, else 0
+
+    ## Starting State
+    The agent starts at the home position with no gold or diamond.
+
+    ## Episode Termination
+    The episode terminates when the agent returns home, or when the agent is killed by an enemy.
+
+    ## Credits
+    The home asset is from https://limezu.itch.io/serenevillagerevamped
+    The gold, enemy and gem assets are from https://ninjikin.itch.io/treasure
+    """
+
+    metadata = {"render_modes": ["human", "rgb_array"], "render_fps": 4}
+
+    def __init__(self, render_mode: Optional[str] = None):
+        EzPickle.__init__(self, render_mode)
+
+        self.render_mode = render_mode
+
+        # The map of resource gathering env
+        self.map = np.array(
+            [
+                [" ", " ", "R1", "E2", " "],
+                [" ", " ", "E1", " ", "R2"],
+                [" ", " ", " ", " ", " "],
+                [" ", " ", " ", " ", " "],
+                [" ", " ", "H", " ", " "],
+            ]
+        )
+        self.initial_pos = np.array([4, 2], dtype=np.int32)
+
+        self.dir = {
+            0: np.array([-1, 0], dtype=np.int32),  # up
+            1: np.array([1, 0], dtype=np.int32),  # down
+            2: np.array([0, -1], dtype=np.int32),  # left
+            3: np.array([0, 1], dtype=np.int32),  # right
+        }
+
+        self.observation_space = Box(low=0.0, high=5.0, shape=(4,), dtype=np.int32)
+
+        # action space specification: 1 dimension, 0 up, 1 down, 2 left, 3 right
+        self.action_space = Discrete(4)
+        # reward space:
+        self.reward_space = Box(low=-1, high=1, shape=(3,), dtype=np.float32)
+        self.reward_dim = 3
+
+        # pygame
+        self.size = 5
+        self.cell_size = (64, 64)
+        self.window_size = (
+            self.map.shape[1] * self.cell_size[1],
+            self.map.shape[0] * self.cell_size[0],
+        )
+        self.clock = None
+        self.elf_images = []
+        self.gold_img = None
+        self.gem_img = None
+        self.enemy_img = None
+        self.home_img = None
+        self.mountain_bg_img = []
+        self.window = None
+        self.last_action = None
+
+    def pareto_front(self, gamma: float) -> List[np.ndarray]:
+        """This function returns the pareto front of the resource gathering environment.
+
+        Args:
+            gamma (float): The discount factor.
+
+        Returns:
+            The pareto front of the resource gathering environment.
+        """
+
+        def get_non_dominated(candidates: List[np.ndarray]) -> List[np.ndarray]:
+            """This function returns the non-dominated subset of elements.
+
+            Source: https://stackoverflow.com/questions/32791911/fast-calculation-of-pareto-front-in-python
+            The code provided in all the stackoverflow answers is wrong. Important changes have been made in this function.
+
+            Args:
+                candidates: The input set of candidate vectors.
+
+            Returns:
+                The non-dominated subset of this input set.
+            """
+            candidates = np.array(candidates)  # Turn the input set into a numpy array.
+            candidates = candidates[candidates.sum(1).argsort()[::-1]]  # Sort candidates by decreasing sum of coordinates.
+            for i in range(candidates.shape[0]):  # Process each point in turn.
+                n = candidates.shape[0]  # Check current size of the candidates.
+                if i >= n:  # If we've eliminated everything up until this size we stop.
+                    break
+                non_dominated = np.ones(candidates.shape[0], dtype=bool)  # Initialize a boolean mask for undominated points.
+                # find all points not dominated by i
+                # since points are sorted by coordinate sum
+                # i cannot dominate any points in 1,...,i-1
+                non_dominated[i + 1 :] = np.any(candidates[i + 1 :] > candidates[i], axis=1)
+                candidates = candidates[non_dominated]  # Grab only the non-dominated vectors using the generated bitmask.
+
+            non_dominated = set()
+            for candidate in candidates:
+                non_dominated.add(tuple(candidate))  # Add the non dominated vectors to a set again.
+
+            return [np.array(point) for point in non_dominated]
+
+        # Go directly to the diamond (R2) in 10 steps
+        ret1 = np.array([0.0, 0.0, 1.0]) * gamma**10
+
+        # Go to both resources, through both Es
+        ret2 = 0.9 * 0.9 * np.array([0.0, 1.0, 1.0]) * gamma**12  # Didn't die
+        ret2 += 0.1 * np.array([-1.0, 0.0, 0.0]) * gamma**7  # Died to E2
+        ret2 += 0.9 * 0.1 * np.array([-1.0, 0.0, 0.0]) * gamma**9  # Died to E1
+
+        # Go to gold (R1), through E1 both ways
+        ret3 = 0.9 * 0.9 * np.array([0.0, 1.0, 0.0]) * gamma**8  # Didn't die
+        ret3 += 0.1 * np.array([-1.0, 0.0, 0.0]) * gamma**3  # Died to E1
+        ret3 += 0.9 * 0.1 * np.array([-1.0, 0.0, 0.0]) * gamma**5  # Died to E1 in the way back
+
+        # Go to both resources, dodging E1 but through E2
+        ret4 = 0.9 * np.array([0.0, 1.0, 1.0]) * gamma**14  # Didn't die
+        ret4 += 0.1 * np.array([-1.0, 0.0, 0.0]) * gamma**7  # Died to E2
+
+        # Go to gold (R1), doging all E's in 12 steps
+        ret5 = np.array([0.0, 1.0, 0.0]) * gamma**12  # Didn't die
+
+        # Go to gold (R1), going through E1 only once
+        ret6 = 0.9 * np.array([0.0, 1.0, 0.0]) * gamma**10  # Didn't die
+        ret6 += 0.1 * np.array([-1.0, 0.0, 0.0]) * gamma**7  # Died to E1
+
+        return get_non_dominated([ret1, ret2, ret3, ret4, ret5, ret6])
+
+    def get_map_value(self, pos):
+        return self.map[pos[0]][pos[1]]
+
+    def is_valid_state(self, state):
+        return state[0] >= 0 and state[0] < self.size and state[1] >= 0 and state[1] < self.size
+
+    def render(self):
+        if self.render_mode is None:
+            assert self.spec is not None
+            gym.logger.warn(
+                "You are calling render method without specifying any render mode. "
+                "You can specify the render_mode at initialization, "
+                f'e.g. mo_gym.make("{self.spec.id}", render_mode="rgb_array")'
+            )
+            return
+
+        if self.window is None:
+            pygame.init()
+
+            if self.render_mode == "human":
+                pygame.display.init()
+                pygame.display.set_caption("Resource Gathering")
+                self.window = pygame.display.set_mode(self.window_size)
+            else:
+                self.window = pygame.Surface(self.window_size)
+
+            if self.clock is None:
+                self.clock = pygame.time.Clock()
+
+            if not self.elf_images:
+                hikers = [
+                    path.join(path.dirname(__file__), "assets/elf_up.png"),
+                    path.join(path.dirname(__file__), "assets/elf_down.png"),
+                    path.join(path.dirname(__file__), "assets/elf_left.png"),
+                    path.join(path.dirname(__file__), "assets/elf_right.png"),
+                ]
+                self.elf_images = [pygame.transform.scale(pygame.image.load(f_name), self.cell_size) for f_name in hikers]
+            if not self.mountain_bg_img:
+                bg_imgs = [
+                    path.join(path.dirname(__file__), "assets/mountain_bg1.png"),
+                    path.join(path.dirname(__file__), "assets/mountain_bg2.png"),
+                ]
+                self.mountain_bg_img = [
+                    pygame.transform.scale(pygame.image.load(f_name), self.cell_size) for f_name in bg_imgs
+                ]
+            if self.gold_img is None:
+                self.gold_img = pygame.transform.scale(
+                    pygame.image.load(path.join(path.dirname(__file__), "assets/gold.png")),
+                    (0.6 * self.cell_size[0], 0.6 * self.cell_size[1]),
+                )
+            if self.gem_img is None:
+                self.gem_img = pygame.transform.scale(
+                    pygame.image.load(path.join(path.dirname(__file__), "assets/gem.png")),
+                    (0.6 * self.cell_size[0], 0.6 * self.cell_size[1]),
+                )
+            if self.enemy_img is None:
+                self.enemy_img = pygame.transform.scale(
+                    pygame.image.load(path.join(path.dirname(__file__), "assets/enemy.png")),
+                    (0.8 * self.cell_size[0], 0.8 * self.cell_size[1]),
+                )
+            if self.home_img is None:
+                self.home_img = pygame.transform.scale(
+                    pygame.image.load(path.join(path.dirname(__file__), "assets/home.png")),
+                    self.cell_size,
+                )
+
+        for i in range(self.map.shape[0]):
+            for j in range(self.map.shape[1]):
+                check_board_mask = i % 2 ^ j % 2
+                self.window.blit(
+                    self.mountain_bg_img[check_board_mask],
+                    np.array([j, i]) * self.cell_size[0],
+                )
+                if self.map[i, j] == "R1" and not self.has_gold:
+                    self.window.blit(self.gold_img, np.array([j + 0.22, i + 0.25]) * self.cell_size[0])
+                elif self.map[i, j] == "R2" and not self.has_gem:
+                    self.window.blit(self.gem_img, np.array([j + 0.22, i + 0.25]) * self.cell_size[0])
+                elif self.map[i, j] == "E1" or self.map[i, j] == "E2":
+                    self.window.blit(self.enemy_img, np.array([j + 0.1, i + 0.1]) * self.cell_size[0])
+                elif self.map[i, j] == "H":
+                    self.window.blit(self.home_img, np.array([j, i]) * self.cell_size[0])
+        last_action = self.last_action if self.last_action is not None else 2
+        self.window.blit(self.elf_images[last_action], self.current_pos[::-1] * self.cell_size[0])
+
+        if self.render_mode == "human":
+            pygame.event.pump()
+            pygame.display.update()
+            self.clock.tick(self.metadata["render_fps"])
+        elif self.render_mode == "rgb_array":  # rgb_array
+            return np.transpose(np.array(pygame.surfarray.pixels3d(self.window)), axes=(1, 0, 2))
+
+    def get_state(self):
+        pos = self.current_pos.copy()
+        state = np.concatenate((pos, np.array([self.has_gold, self.has_gem], dtype=np.int32)))
+        return state
+
+    def reset(self, seed=None, **kwargs):
+        super().reset(seed=seed)
+
+        self.current_pos = self.initial_pos
+        self.has_gem = 0
+        self.has_gold = 0
+        self.step_count = 0.0
+        state = self.get_state()
+        if self.render_mode == "human":
+            self.render()
+        return state, {}
+
+    def step(self, action):
+        next_pos = self.current_pos + self.dir[action]
+        self.last_action = action
+
+        if self.is_valid_state(next_pos):
+            self.current_pos = next_pos
+
+        vec_reward = np.zeros(3, dtype=np.float32)
+        done = False
+
+        cell = self.get_map_value(self.current_pos)
+        if cell == "R1":
+            self.has_gold = 1
+        elif cell == "R2":
+            self.has_gem = 1
+        elif cell == "E1" or cell == "E2":
+            if self.np_random.random() < 0.1:
+                vec_reward[0] = -1.0
+                done = True
+        elif cell == "H":
+            done = True
+            vec_reward[1] = self.has_gold
+            vec_reward[2] = self.has_gem
+
+        state = self.get_state()
+        if self.render_mode == "human":
+            self.render()
+        return state, vec_reward, done, False, {}
+
+    def close(self):
+        if self.window is not None:
+            pygame.display.quit()
+            pygame.quit()
+
+
+if __name__ == "__main__":
+    import mo_gymnasium as mo_gym
+
+    env = mo_gym.make("resource-gathering-v0", render_mode="human")
+    terminated = False
+    env.reset()
+    while True:
+        env.render()
+        obs, r, terminated, truncated, info = env.step(env.action_space.sample())
+        if terminated or truncated:
+            env.reset()
diff --git a/MO-Gymnasium/mo_gymnasium/envs/water_reservoir/__init__.py b/MO-Gymnasium/mo_gymnasium/envs/water_reservoir/__init__.py
new file mode 100644
index 0000000..a2d7c5d
--- /dev/null
+++ b/MO-Gymnasium/mo_gymnasium/envs/water_reservoir/__init__.py
@@ -0,0 +1,4 @@
+from gymnasium.envs.registration import register
+
+
+register(id="water-reservoir-v0", entry_point="mo_gymnasium.envs.water_reservoir.dam_env:DamEnv")
diff --git a/MO-Gymnasium/mo_gymnasium/envs/water_reservoir/assets/Minecraft.ttf b/MO-Gymnasium/mo_gymnasium/envs/water_reservoir/assets/Minecraft.ttf
new file mode 100644
index 0000000..85c1472
Binary files /dev/null and b/MO-Gymnasium/mo_gymnasium/envs/water_reservoir/assets/Minecraft.ttf differ
diff --git a/MO-Gymnasium/mo_gymnasium/envs/water_reservoir/assets/sky.png b/MO-Gymnasium/mo_gymnasium/envs/water_reservoir/assets/sky.png
new file mode 100644
index 0000000..97379fd
Binary files /dev/null and b/MO-Gymnasium/mo_gymnasium/envs/water_reservoir/assets/sky.png differ
diff --git a/MO-Gymnasium/mo_gymnasium/envs/water_reservoir/assets/wall.png b/MO-Gymnasium/mo_gymnasium/envs/water_reservoir/assets/wall.png
new file mode 100644
index 0000000..933f5df
Binary files /dev/null and b/MO-Gymnasium/mo_gymnasium/envs/water_reservoir/assets/wall.png differ
diff --git a/MO-Gymnasium/mo_gymnasium/envs/water_reservoir/assets/water.png b/MO-Gymnasium/mo_gymnasium/envs/water_reservoir/assets/water.png
new file mode 100644
index 0000000..ea7e9f9
Binary files /dev/null and b/MO-Gymnasium/mo_gymnasium/envs/water_reservoir/assets/water.png differ
diff --git a/MO-Gymnasium/mo_gymnasium/envs/water_reservoir/dam_env.py b/MO-Gymnasium/mo_gymnasium/envs/water_reservoir/dam_env.py
new file mode 100644
index 0000000..db97525
--- /dev/null
+++ b/MO-Gymnasium/mo_gymnasium/envs/water_reservoir/dam_env.py
@@ -0,0 +1,302 @@
+from contextlib import closing
+from io import StringIO
+from os import path
+from typing import Optional
+
+import gymnasium as gym
+import numpy as np
+import pygame
+from gymnasium.spaces.box import Box
+from gymnasium.utils import EzPickle
+
+
+class DamEnv(gym.Env, EzPickle):
+    """
+    ## Description
+    A Water reservoir environment.
+    The agent executes a continuous action, corresponding to the amount of water released by the dam.
+
+    A. Castelletti, F. Pianosi and M. Restelli, "Tree-based Fitted Q-iteration for Multi-Objective Markov Decision problems,"
+    The 2012 International Joint Conference on Neural Networks (IJCNN),
+    Brisbane, QLD, Australia, 2012, pp. 1-8, doi: 10.1109/IJCNN.2012.6252759.
+
+    ## Observation Space
+    The observation is a float corresponding to the current level of the reservoir.
+
+    ## Action Space
+    The action is a float corresponding to the amount of water released by the dam.
+    If normalized_action is True, the action is a float between 0 and 1 corresponding to the percentage of water released by the dam.
+
+    ## Reward Space
+    There are up to 4 rewards:
+     - cost due to excess level wrt a flooding threshold (upstream)
+     - deficit in the water supply wrt the water demand
+     - deficit in hydroelectric supply wrt hydroelectric demand
+     - cost due to excess level wrt a flooding threshold (downstream)
+     By default, only the first two are used.
+
+     ## Starting State
+     The reservoir is initialized with a random level between 0 and 160.
+
+     ## Arguments
+        - render_mode: The render mode to use. Can be 'human', 'rgb_array' or 'ansi'.
+        - time_limit: The maximum number of steps until the episode is truncated.
+        - nO: The number of objectives to use. Can be 2, 3 or 4.
+        - penalize: Whether to penalize the agent for selecting an action out of bounds.
+        - normalized_action: Whether to normalize the action space as a percentage [0, 1].
+
+     ## Credits
+     Code from:
+     [Mathieu Reymond](https://gitlab.ai.vub.ac.be/mreymond/dam).
+     Ported from:
+     [Simone Parisi](https://github.com/sparisi/mips).
+
+     Sky background image from: Paulina Riva (https://opengameart.org/content/sky-background)
+    """
+
+    S = 1.0  # Reservoir surface
+    W_IRR = 50.0  # Water demand
+    H_FLO_U = 50.0  # Flooding threshold (upstream, i.e. height of dam)
+    S_MIN_REL = 100.0  # Release threshold (i.e. max capacity)
+    DAM_INFLOW_MEAN = 40.0  # Random inflow (e.g. rain)
+    DAM_INFLOW_STD = 10.0
+    Q_MEF = 0.0
+    GAMMA_H2O = 1000.0  # water density
+    W_HYD = 4.36  # Hydroelectric demand
+    Q_FLO_D = 30.0  # Flooding threshold (downstream, i.e. releasing too much water)
+    ETA = 1.0  # Turbine efficiency
+    G = 9.81  # Gravity
+
+    utopia = {2: [-0.5, -9], 3: [-0.5, -9, -0.0001], 4: [-0.5, -9, -0.001, -9]}
+    antiutopia = {2: [-2.5, -11], 3: [-65, -12, -0.7], 4: [-65, -12, -0.7, -12]}
+
+    # Create colors.
+    BLACK = (0, 0, 0)
+    WHITE = (255, 255, 255)
+
+    s_init = np.array(
+        [
+            9.6855361e01,
+            5.8046026e01,
+            1.1615767e02,
+            2.0164311e01,
+            7.9191000e01,
+            1.4013098e02,
+            1.3101816e02,
+            4.4351321e01,
+            1.3185943e01,
+            7.3508622e01,
+        ],
+        dtype=np.float32,
+    )
+
+    metadata = {"render_modes": ["human", "rgb_array", "ansi"], "render_fps": 2}
+
+    def __init__(
+        self,
+        render_mode: Optional[str] = None,
+        time_limit: int = 100,
+        nO=2,
+        penalize: bool = False,
+        normalized_action: bool = False,
+    ):
+        EzPickle.__init__(self, render_mode, time_limit, nO, penalize, normalized_action)
+        self.render_mode = render_mode
+
+        self.observation_space = Box(low=0.0, high=np.inf, shape=(1,), dtype=np.float32)
+        self.normalized_action = normalized_action
+        if self.normalized_action:
+            self.action_space = Box(low=0.0, high=1.0, shape=(1,), dtype=np.float32)
+        else:
+            self.action_space = Box(low=0.0, high=np.inf, shape=(1,), dtype=np.float32)
+
+        self.nO = nO
+        self.penalize = penalize
+        self.time_limit = time_limit
+        self.time_step = 0
+        self.last_action = None
+        self.dam_inflow = None
+        self.excess = None
+        self.defict = None
+
+        low = -np.ones(nO) * np.inf  # DamEnv.antiutopia[nO]
+        high = np.zeros(nO)  # DamEnv.utopia[nO]
+        self.reward_space = Box(low=np.array(low), high=np.array(high), dtype=np.float32)
+        self.reward_dim = nO
+
+        self.window = None
+        self.window_size = (300, 200)  # width x height
+        self.clock = None
+        self.water_img = None
+        self.wall_img = None
+        self.sky_img = None
+
+    def reset(self, *, seed: Optional[int] = None, options: Optional[dict] = None):
+        super().reset(seed=seed)
+        self.time_step = 0
+        if not self.penalize:
+            state = self.np_random.choice(DamEnv.s_init, size=1)
+        else:
+            state = self.np_random.randint(0, 160, size=1)
+
+        self.state = np.array(state, dtype=np.float32)
+
+        if self.render_mode == "human":
+            self.render()
+
+        return self.state, {}
+
+    def render(self):
+        if self.render_mode is None:
+            assert self.spec is not None
+            gym.logger.warn(
+                "You are calling render method without specifying any render mode. "
+                "You can specify the render_mode at initialization, "
+                f'e.g. mo_gym.make("{self.spec.id}", render_mode="rgb_array")'
+            )
+            return
+
+        if self.render_mode == "ansi":
+            return self._render_text()
+        else:  # self.render_mode in {"human", "rgb_array"}:
+            return self._render_gui(self.render_mode)
+
+    def _render_gui(self, render_mode: str):
+        if self.window is None:
+            pygame.init()
+
+            if render_mode == "human":
+                pygame.display.init()
+                pygame.display.set_caption("Water Reservoir")
+                self.window = pygame.display.set_mode(self.window_size)
+            else:
+                self.window = pygame.Surface(self.window_size)
+
+            if self.clock is None:
+                self.clock = pygame.time.Clock()
+
+            if self.water_img is None:
+                self.water_img = pygame.image.load(path.join(path.dirname(__file__), "assets/water.png"))
+            if self.wall_img is None:
+                self.wall_img = pygame.image.load(path.join(path.dirname(__file__), "assets/wall.png"))
+            if self.sky_img is None:
+                self.sky_img = pygame.image.load(path.join(path.dirname(__file__), "assets/sky.png"))
+                self.sky_img = pygame.transform.flip(self.sky_img, False, True)
+                self.sky_img = pygame.transform.scale(self.sky_img, self.window_size)
+
+            self.font = pygame.font.Font(path.join(path.dirname(__file__), "assets", "Minecraft.ttf"), 15)
+
+        self.window.blit(self.sky_img, (0, 0))
+
+        # Draw the dam
+        for x in range(self.wall_img.get_width(), self.window_size[0] - self.wall_img.get_width(), self.water_img.get_width()):
+            for y in range(self.window_size[1] - int(self.state[0]), self.window_size[1], self.water_img.get_height()):
+                self.window.blit(self.water_img, (x, y))
+
+        # Draw the wall
+        for y in range(0, int(DamEnv.H_FLO_U), self.wall_img.get_width()):
+            self.window.blit(self.wall_img, (0, self.window_size[1] - y - self.wall_img.get_height()))
+            self.window.blit(
+                self.wall_img,
+                (self.window_size[0] - self.wall_img.get_width(), self.window_size[1] - y - self.wall_img.get_height()),
+            )
+
+        if self.last_action is not None:
+            img = self.font.render(f"Water Released: {self.last_action:.2f}", True, (0, 0, 0))
+            self.window.blit(img, (20, 10))
+            img = self.font.render(f"Dam Inflow: {self.dam_inflow:.2f}", True, (0, 0, 0))
+            self.window.blit(img, (20, 25))
+            img = self.font.render(f"Water Level: {self.state[0]:.2f}", True, (0, 0, 0))
+            self.window.blit(img, (20, 40))
+            img = self.font.render(f"Demand Deficit: {self.defict:.2f}", True, (0, 0, 0))
+            self.window.blit(img, (20, 55))
+            img = self.font.render(f"Flooding Excess: {self.excess:.2f}", True, (0, 0, 0))
+            self.window.blit(img, (20, 70))
+
+        img = self.font.render("Flooding threshold", True, (255, 0, 0))
+        self.window.blit(img, (20, self.window_size[1] - DamEnv.H_FLO_U))
+        pygame.draw.line(
+            self.window,
+            (255, 0, 0),
+            (0, self.window_size[1] - DamEnv.H_FLO_U),
+            (self.window_size[0], self.window_size[1] - DamEnv.H_FLO_U),
+        )
+
+        if self.render_mode == "human":
+            pygame.event.pump()
+            pygame.display.update()
+            self.clock.tick(self.metadata["render_fps"])
+        elif self.render_mode == "rgb_array":  # rgb_array
+            return np.transpose(np.array(pygame.surfarray.pixels3d(self.window)), axes=(1, 0, 2))
+
+    def _render_text(self):
+        outfile = StringIO()
+        outfile.write(f"Water level: {self.state[0]:.2f}\n")
+        if self.last_action is not None:
+            outfile.write(f"Water released: {self.last_action:.2f}\n")
+            outfile.write(f"Dam inflow: {self.dam_inflow:.2f}\n")
+            outfile.write(f"Demand deficit: {self.defict:.2f}\n")
+            outfile.write(f"Flooding excess: {self.excess:.2f}\n")
+
+        with closing(outfile):
+            return outfile.getvalue()
+
+    def step(self, action):
+        # bound the action
+        actionLB = np.clip(self.state - DamEnv.S_MIN_REL, 0, None)
+        actionUB = self.state
+
+        if self.normalized_action:
+            action = action * (actionUB - actionLB) + actionLB
+            penalty = 0.0
+        else:
+            # Penalty proportional to the violation
+            bounded_action = np.clip(action, actionLB, actionUB)
+            penalty = -self.penalize * np.abs(bounded_action - action)
+            action = bounded_action
+
+        # transition dynamic
+        self.last_action = action[0]
+        self.dam_inflow = self.np_random.normal(DamEnv.DAM_INFLOW_MEAN, DamEnv.DAM_INFLOW_STD, len(self.state))[0]
+        # small chance dam_inflow < 0
+        n_state = np.clip(self.state + self.dam_inflow - action, 0, None).astype(np.float32)
+
+        # cost due to excess level wrt a flooding threshold (upstream)
+        self.excess = np.clip(n_state / DamEnv.S - DamEnv.H_FLO_U, 0, None)[0]
+        r0 = -self.excess + penalty
+        # deficit in the water supply wrt the water demand
+        self.defict = -np.clip(DamEnv.W_IRR - action, 0, None)[0]
+        r1 = self.defict + penalty
+
+        q = np.clip(action - DamEnv.Q_MEF, 0, None)
+        p_hyd = DamEnv.ETA * DamEnv.G * DamEnv.GAMMA_H2O * n_state / DamEnv.S * q / 3.6e6
+
+        # deficit in hydroelectric supply wrt hydroelectric demand
+        r2 = -np.clip(DamEnv.W_HYD - p_hyd, 0, None) + penalty
+        # cost due to excess level wrt a flooding threshold (downstream)
+        r3 = -np.clip(action - DamEnv.Q_FLO_D, 0, None) + penalty
+
+        reward = np.array([r0, r1, r2, r3], dtype=np.float32)[: self.nO].flatten()
+
+        self.state = n_state
+
+        self.time_step += 1
+        truncated = self.time_step >= self.time_limit
+        terminated = False
+
+        if self.render_mode == "human":
+            self.render()
+
+        return n_state, reward, terminated, truncated, {}
+
+
+if __name__ == "__main__":
+    import mo_gymnasium as mo_gym
+
+    env = mo_gym.make("water-reservoir-v0", render_mode="human")
+    obs, info = env.reset()
+    while True:
+        action = env.state
+        obs, reward, terminated, truncated, info = env.step(action)
+        if terminated or truncated:
+            env.reset()
diff --git a/MO-Gymnasium/mo_gymnasium/utils.py b/MO-Gymnasium/mo_gymnasium/utils.py
new file mode 100644
index 0000000..804aefd
--- /dev/null
+++ b/MO-Gymnasium/mo_gymnasium/utils.py
@@ -0,0 +1,310 @@
+"""Utilities function such as wrappers."""
+
+import time
+from copy import deepcopy
+from typing import Iterator, Tuple, TypeVar
+
+import gymnasium as gym
+import numpy as np
+from gymnasium.utils import EzPickle
+from gymnasium.vector import SyncVectorEnv
+from gymnasium.wrappers import RecordEpisodeStatistics
+from gymnasium.wrappers.normalize import RunningMeanStd
+
+
+ObsType = TypeVar("ObsType")
+ActType = TypeVar("ActType")
+
+
+def make(env_name: str, disable_env_checker: bool = True, **kwargs) -> gym.Env:
+    """Overrides Gymnasium's make method to disable env_checker by default.
+
+    Args:
+        env_name: name of the environment to create
+        disable_env_checker: disables environment checker
+        **kwargs: forwards arguments to the environment constructor
+    Returns: a newly created environment.
+    """
+    """Disable env checker, as it requires the reward to be a scalar."""
+    return gym.make(env_name, disable_env_checker=disable_env_checker, **kwargs)
+
+
+class LinearReward(gym.Wrapper, EzPickle):
+    """Makes the env return a scalar reward, which is the dot-product between the reward vector and the weight vector."""
+
+    def __init__(self, env: gym.Env, weight: np.ndarray = None):
+        """Makes the env return a scalar reward, which is the dot-product between the reward vector and the weight vector.
+
+        Args:
+            env: env to wrap
+            weight: weight vector to use in the dot product
+        """
+        super().__init__(env)
+        EzPickle.__init__(self, env, weight)
+        if weight is None:
+            weight = np.ones(shape=env.reward_space.shape)
+        self.set_weight(weight)
+
+    def set_weight(self, weight: np.ndarray):
+        """Changes weights for the scalarization.
+
+        Args:
+            weight: new weights to set
+        Returns: nothing
+        """
+        assert weight.shape == self.env.reward_space.shape, "Reward weight has different shape than reward vector."
+        self.w = weight
+
+    def step(self, action: ActType) -> Tuple[ObsType, float, bool, bool, dict]:
+        """Steps in the environment.
+
+        Args:
+            action: action to perform
+        Returns: obs, scalarized_reward, terminated, truncated, info
+        """
+        observation, reward, terminated, truncated, info = self.env.step(action)
+        scalar_reward = np.dot(reward, self.w)
+        info["vector_reward"] = reward
+
+        return observation, scalar_reward, terminated, truncated, info
+
+
+class MONormalizeReward(gym.Wrapper, EzPickle):
+    """Wrapper to normalize the reward component at index idx. Does not touch other reward components."""
+
+    def __init__(self, env: gym.Env, idx: int, gamma: float = 0.99, epsilon: float = 1e-8):
+        """This wrapper will normalize immediate rewards s.t. their exponential moving average has a fixed variance.
+
+        Args:
+            env (env): The environment to apply the wrapper
+            idx (int): the index of the reward to normalize
+            epsilon (float): A stability parameter
+            gamma (float): The discount factor that is used in the exponential moving average.
+        """
+        super().__init__(env)
+        EzPickle.__init__(self, env, idx, gamma, epsilon)
+        self.idx = idx
+        self.num_envs = getattr(env, "num_envs", 1)
+        self.is_vector_env = getattr(env, "is_vector_env", False)
+        self.return_rms = RunningMeanStd(shape=())
+        self.returns = np.zeros(self.num_envs)
+        self.gamma = gamma
+        self.epsilon = epsilon
+
+    def step(self, action: ActType):
+        """Steps through the environment, normalizing the rewards returned.
+
+        Args:
+            action: action to perform
+        Returns: obs, normalized_rewards, terminated, truncated, infos
+        """
+        obs, rews, terminated, truncated, infos = self.env.step(action)
+        # Extracts the objective value to normalize
+        to_normalize = rews[self.idx]
+        if not self.is_vector_env:
+            to_normalize = np.array([to_normalize])
+        self.returns = self.returns * self.gamma + to_normalize
+        # Defer normalization to gym implementation
+        to_normalize = self.normalize(to_normalize)
+        self.returns[terminated] = 0.0
+        if not self.is_vector_env:
+            to_normalize = to_normalize[0]
+        # Injecting the normalized objective value back into the reward vector
+        rews[self.idx] = to_normalize
+        return obs, rews, terminated, truncated, infos
+
+    def normalize(self, rews):
+        """Normalizes the rewards with the running mean rewards and their variance.
+
+        Args:
+            rews: rewards
+        Returns: the normalized reward
+        """
+        self.return_rms.update(self.returns)
+        return rews / np.sqrt(self.return_rms.var + self.epsilon)
+
+
+class MOClipReward(gym.RewardWrapper, EzPickle):
+    """Clip reward[idx] to [min, max]."""
+
+    def __init__(self, env: gym.Env, idx: int, min_r, max_r):
+        """Clip reward[idx] to [min, max].
+
+        Args:
+            env: environment to wrap
+            idx: index of the MO reward to clip
+            min_r: min reward
+            max_r: max reward
+        """
+        super().__init__(env)
+        EzPickle.__init__(self, env, idx, min_r, max_r)
+        self.idx = idx
+        self.min_r = min_r
+        self.max_r = max_r
+
+    def reward(self, reward):
+        """Clips the reward at the given index.
+
+        Args:
+            reward: reward to clip.
+        Returns: the clipped reward.
+        """
+        reward[self.idx] = np.clip(reward[self.idx], self.min_r, self.max_r)
+        return reward
+
+
+class MOSyncVectorEnv(SyncVectorEnv, EzPickle):
+    """Vectorized environment that serially runs multiple environments."""
+
+    def __init__(
+        self,
+        env_fns: Iterator[callable],
+        copy: bool = True,
+    ):
+        """Vectorized environment that serially runs multiple environments.
+
+        Args:
+            env_fns: env constructors
+            copy: If ``True``, then the :meth:`reset` and :meth:`step` methods return a copy of the observations.
+        """
+        super().__init__(env_fns, copy=copy)
+        EzPickle.__init__(self, env_fns, copy=copy)
+        # Just overrides the rewards memory to add the number of objectives
+        self.reward_space = self.envs[0].reward_space
+        self._rewards = np.zeros(
+            (
+                self.num_envs,
+                self.reward_space.shape[0],
+            ),
+            dtype=np.float64,
+        )
+
+
+class MORecordEpisodeStatistics(RecordEpisodeStatistics, EzPickle):
+    """This wrapper will keep track of cumulative rewards and episode lengths.
+
+    After the completion of an episode, ``info`` will look like this::
+
+        >>> info = {
+        ...     "episode": {
+        ...         "r": "<cumulative reward (array)>",
+        ...         "dr": "<discounted reward (array)>",
+        ...         "l": "<episode length (scalar)>", # contrary to Gymnasium, these are not a numpy array
+        ...         "t": "<elapsed time since beginning of episode (scalar)>"
+        ...     },
+        ... }
+
+    For a vectorized environments the output will be in the form of::
+
+        >>> infos = {
+        ...     "final_observation": "<array of length num-envs>",
+        ...     "_final_observation": "<boolean array of length num-envs>",
+        ...     "final_info": "<array of length num-envs>",
+        ...     "_final_info": "<boolean array of length num-envs>",
+        ...     "episode": {
+        ...         "r": "<array of cumulative reward (2d array, shape (num_envs, dim_reward))>",
+        ...         "dr": "<array of discounted reward (2d array, shape (num_envs, dim_reward))>",
+        ...         "l": "<array of episode length (array)>",
+        ...         "t": "<array of elapsed time since beginning of episode (array)>"
+        ...     },
+        ...     "_episode": "<boolean array of length num-envs>"
+        ... }
+    """
+
+    def __init__(self, env: gym.Env, gamma: float = 1.0, deque_size: int = 100):
+        """This wrapper will keep track of cumulative rewards and episode lengths.
+
+        Args:
+            env (Env): The environment to apply the wrapper
+            gamma (float): Discounting factor
+            deque_size: The size of the buffers :attr:`return_queue` and :attr:`length_queue`
+        """
+        super().__init__(env, deque_size)
+        EzPickle.__init__(self, env, gamma, deque_size)
+        # CHANGE: Here we just override the standard implementation to extend to MO
+        # We also take care of the case where the env is vectorized
+        self.reward_dim = self.env.reward_space.shape[0]
+        if self.is_vector_env:
+            self.rewards_shape = (self.num_envs, self.reward_dim)
+        else:
+            self.rewards_shape = (self.reward_dim,)
+        self.gamma = gamma
+
+    def reset(self, **kwargs):
+        """Resets the environment using kwargs and resets the episode returns and lengths."""
+        obs, info = super().reset(**kwargs)
+
+        # CHANGE: Here we just override the standard implementation to extend to MO
+        self.episode_returns = np.zeros(self.rewards_shape, dtype=np.float32)
+        self.disc_episode_returns = np.zeros(self.rewards_shape, dtype=np.float32)
+
+        return obs, info
+
+    def step(self, action):
+        """Steps through the environment, recording the episode statistics."""
+        # This is very close the code from the RecordEpisodeStatistics wrapper from gym.
+        (
+            observations,
+            rewards,
+            terminations,
+            truncations,
+            infos,
+        ) = self.env.step(action)
+        assert isinstance(
+            infos, dict
+        ), f"`info` dtype is {type(infos)} while supported dtype is `dict`. This may be due to usage of other wrappers in the wrong order."
+        self.episode_returns += rewards
+        self.episode_lengths += 1
+
+        # CHANGE: The discounted returns are also computed here
+        self.disc_episode_returns += rewards * np.repeat(self.gamma**self.episode_lengths, self.reward_dim).reshape(
+            self.episode_returns.shape
+        )
+
+        dones = np.logical_or(terminations, truncations)
+        num_dones = np.sum(dones)
+        if num_dones:
+            if "episode" in infos or "_episode" in infos:
+                raise ValueError("Attempted to add episode stats when they already exist")
+            else:
+                episode_return = np.zeros(self.rewards_shape, dtype=np.float32)
+                disc_episode_return = np.zeros(self.rewards_shape, dtype=np.float32)
+                if self.is_vector_env:
+                    for i in range(self.num_envs):
+                        if dones[i]:
+                            # CHANGE: Makes a deepcopy to avoid subsequent mutations
+                            episode_return[i] = deepcopy(self.episode_returns[i])
+                            disc_episode_return[i] = deepcopy(self.disc_episode_returns[i])
+                else:
+                    episode_return = deepcopy(self.episode_returns)
+                    disc_episode_return = deepcopy(self.disc_episode_returns)
+
+                length_eps = np.where(dones, self.episode_lengths, 0)
+                time_eps = np.where(
+                    dones,
+                    np.round(time.perf_counter() - self.episode_start_times, 6),
+                    0.0,
+                )
+
+                infos["episode"] = {
+                    "r": episode_return,
+                    "dr": disc_episode_return,
+                    "l": length_eps[0] if not self.is_vector_env else length_eps,
+                    "t": time_eps[0] if not self.is_vector_env else time_eps,
+                }
+                if self.is_vector_env:
+                    infos["_episode"] = np.where(dones, True, False)
+            self.return_queue.extend(self.episode_returns[dones])
+            self.length_queue.extend(self.episode_lengths[dones])
+            self.episode_count += num_dones
+            self.episode_lengths[dones] = 0
+            self.episode_returns[dones] = np.zeros(self.reward_dim, dtype=np.float32)
+            self.disc_episode_returns[dones] = np.zeros(self.reward_dim, dtype=np.float32)
+            self.episode_start_times[dones] = time.perf_counter()
+        return (
+            observations,
+            rewards,
+            terminations,
+            truncations,
+            infos,
+        )
diff --git a/MO-Gymnasium/mo_gymnasium_demo.ipynb b/MO-Gymnasium/mo_gymnasium_demo.ipynb
new file mode 100644
index 0000000..6cd44ac
--- /dev/null
+++ b/MO-Gymnasium/mo_gymnasium_demo.ipynb
@@ -0,0 +1,447 @@
+{
+ "cells": [
+  {
+   "cell_type": "markdown",
+   "id": "ed73b6a4",
+   "metadata": {
+    "colab_type": "text",
+    "id": "view-in-github"
+   },
+   "source": [
+    "<a href=\"https://colab.research.google.com/github/LucasAlegre/mo-gym/blob/main/mo_gym_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "id": "ab7f2ffc",
+   "metadata": {
+    "id": "ab7f2ffc"
+   },
+   "source": [
+    "## Step 1: install and import the libraries"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 1,
+   "id": "o8lMdzzrRf0O",
+   "metadata": {
+    "colab": {
+     "base_uri": "https://localhost:8080/"
+    },
+    "id": "o8lMdzzrRf0O",
+    "outputId": "b8bb653e-a81f-42b7-e251-40179a2d0d76"
+   },
+   "outputs": [
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "Cloning into 'MO-Gymnasium'...\n",
+      "remote: Enumerating objects: 1343, done.\u001b[K\n",
+      "remote: Counting objects: 100% (645/645), done.\u001b[K\n",
+      "remote: Compressing objects: 100% (395/395), done.\u001b[K\n",
+      "remote: Total 1343 (delta 321), reused 491 (delta 211), pack-reused 698\u001b[K\n",
+      "Receiving objects: 100% (1343/1343), 2.34 MiB | 1.79 MiB/s, done.\n",
+      "Resolving deltas: 100% (663/663), done.\n",
+      "Obtaining file:///Users/florian.felten/Documents/MO-Gymnasium/mo-gymnasium\n",
+      "  Installing build dependencies ... \u001b[?25ldone\n",
+      "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
+      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
+      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
+      "\u001b[?25hRequirement already satisfied: pygame>=2.1.0 in /usr/local/lib/python3.10/site-packages (from mo-gym==0.3.0) (2.1.0)\n",
+      "Requirement already satisfied: scipy>=1.7.3 in /usr/local/lib/python3.10/site-packages (from mo-gym==0.3.0) (1.9.1)\n",
+      "Requirement already satisfied: pymoo>=0.6.0 in /usr/local/lib/python3.10/site-packages (from mo-gym==0.3.0) (0.6.0)\n",
+      "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/site-packages (from mo-gym==0.3.0) (1.23.3)\n",
+      "Requirement already satisfied: gym>=0.26 in /usr/local/lib/python3.10/site-packages (from mo-gym==0.3.0) (0.26.2)\n",
+      "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/site-packages (from gym>=0.26->mo-gym==0.3.0) (0.0.8)\n",
+      "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/site-packages (from gym>=0.26->mo-gym==0.3.0) (2.2.0)\n",
+      "Requirement already satisfied: autograd>=1.4 in /usr/local/lib/python3.10/site-packages (from pymoo>=0.6.0->mo-gym==0.3.0) (1.5)\n",
+      "Requirement already satisfied: alive-progress in /usr/local/lib/python3.10/site-packages (from pymoo>=0.6.0->mo-gym==0.3.0) (2.4.1)\n",
+      "Requirement already satisfied: matplotlib>=3 in /usr/local/lib/python3.10/site-packages (from pymoo>=0.6.0->mo-gym==0.3.0) (3.6.0)\n",
+      "Requirement already satisfied: Deprecated in /usr/local/lib/python3.10/site-packages (from pymoo>=0.6.0->mo-gym==0.3.0) (1.2.13)\n",
+      "Requirement already satisfied: cma==3.2.2 in /usr/local/lib/python3.10/site-packages (from pymoo>=0.6.0->mo-gym==0.3.0) (3.2.2)\n",
+      "Requirement already satisfied: dill in /usr/local/lib/python3.10/site-packages (from pymoo>=0.6.0->mo-gym==0.3.0) (0.3.5.1)\n",
+      "Requirement already satisfied: future>=0.15.2 in /usr/local/lib/python3.10/site-packages (from autograd>=1.4->pymoo>=0.6.0->mo-gym==0.3.0) (0.18.2)\n",
+      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/site-packages (from matplotlib>=3->pymoo>=0.6.0->mo-gym==0.3.0) (1.0.5)\n",
+      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/site-packages (from matplotlib>=3->pymoo>=0.6.0->mo-gym==0.3.0) (0.11.0)\n",
+      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/site-packages (from matplotlib>=3->pymoo>=0.6.0->mo-gym==0.3.0) (9.2.0)\n",
+      "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.10/site-packages (from matplotlib>=3->pymoo>=0.6.0->mo-gym==0.3.0) (3.0.9)\n",
+      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/site-packages (from matplotlib>=3->pymoo>=0.6.0->mo-gym==0.3.0) (2.8.2)\n",
+      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/site-packages (from matplotlib>=3->pymoo>=0.6.0->mo-gym==0.3.0) (1.4.4)\n",
+      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/site-packages (from matplotlib>=3->pymoo>=0.6.0->mo-gym==0.3.0) (21.3)\n",
+      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/site-packages (from matplotlib>=3->pymoo>=0.6.0->mo-gym==0.3.0) (4.37.3)\n",
+      "Requirement already satisfied: grapheme==0.6.0 in /usr/local/lib/python3.10/site-packages (from alive-progress->pymoo>=0.6.0->mo-gym==0.3.0) (0.6.0)\n",
+      "Requirement already satisfied: about-time==3.1.1 in /usr/local/lib/python3.10/site-packages (from alive-progress->pymoo>=0.6.0->mo-gym==0.3.0) (3.1.1)\n",
+      "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/site-packages (from Deprecated->pymoo>=0.6.0->mo-gym==0.3.0) (1.14.1)\n",
+      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=3->pymoo>=0.6.0->mo-gym==0.3.0) (1.16.0)\n",
+      "Installing collected packages: mo-gym\n",
+      "  Attempting uninstall: mo-gym\n",
+      "    Found existing installation: mo-gym 0.3.0\n",
+      "    Uninstalling mo-gym-0.3.0:\n",
+      "      Successfully uninstalled mo-gym-0.3.0\n",
+      "  Running setup.py develop for mo-gym\n",
+      "Successfully installed mo-gym-0.3.0\n"
+     ]
+    }
+   ],
+   "source": [
+    "!git clone https://github.com/Farama-Foundation/MO-Gymnasium\n",
+    "!pip install -e mo-gymnasium"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 2,
+   "id": "43bddbf7",
+   "metadata": {},
+   "outputs": [
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
+      "\u001b[0mRequirement already satisfied: moviepy in /usr/local/lib/python3.9/site-packages (1.0.3)\n",
+      "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.9/site-packages (from moviepy) (1.21.0)\n",
+      "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.9/site-packages (from moviepy) (4.64.1)\n",
+      "Requirement already satisfied: requests<3.0,>=2.8.1 in /usr/local/lib/python3.9/site-packages (from moviepy) (2.27.1)\n",
+      "Requirement already satisfied: imageio<3.0,>=2.5 in /usr/local/lib/python3.9/site-packages (from moviepy) (2.20.0)\n",
+      "Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.9/site-packages (from moviepy) (0.1.10)\n",
+      "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.9/site-packages (from moviepy) (4.4.2)\n",
+      "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /usr/local/lib/python3.9/site-packages (from moviepy) (0.4.7)\n",
+      "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.9/site-packages (from imageio<3.0,>=2.5->moviepy) (9.2.0)\n",
+      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/site-packages (from requests<3.0,>=2.8.1->moviepy) (2021.10.8)\n",
+      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/site-packages (from requests<3.0,>=2.8.1->moviepy) (3.3)\n",
+      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/site-packages (from requests<3.0,>=2.8.1->moviepy) (2.0.12)\n",
+      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/site-packages (from requests<3.0,>=2.8.1->moviepy) (1.26.8)\n",
+      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
+      "\u001b[0m\n",
+      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.3.1\u001b[0m\n",
+      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.9 -m pip install --upgrade pip\u001b[0m\n"
+     ]
+    }
+   ],
+   "source": [
+    "import sys\n",
+    "!{sys.executable} -m pip install moviepy"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 3,
+   "id": "846b9364",
+   "metadata": {
+    "id": "846b9364"
+   },
+   "outputs": [],
+   "source": [
+    "import gymnasium as gym\n",
+    "import mo_gymnasium as mo_gym"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "id": "f4fef046",
+   "metadata": {
+    "id": "f4fef046"
+   },
+   "source": [
+    "## Step 2: create an environment"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 6,
+   "id": "d5415397",
+   "metadata": {
+    "id": "d5415397"
+   },
+   "outputs": [],
+   "source": [
+    "env = mo_gym.make(\"mo-mountaincar-v0\", render_mode=\"human\")"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "id": "870ae1a4",
+   "metadata": {
+    "id": "870ae1a4"
+   },
+   "source": [
+    "## Step 3: extract environment information"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 7,
+   "id": "8db784fa",
+   "metadata": {
+    "colab": {
+     "base_uri": "https://localhost:8080/"
+    },
+    "id": "8db784fa",
+    "outputId": "dd097381-1f1b-48c3-8be2-f284aa9f493e"
+   },
+   "outputs": [
+    {
+     "data": {
+      "text/plain": [
+       "Box([-1.2  -0.07], [0.6  0.07], (2,), float32)"
+      ]
+     },
+     "execution_count": 7,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
+   "source": [
+    "env.observation_space"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 8,
+   "id": "8e8d91c7",
+   "metadata": {
+    "colab": {
+     "base_uri": "https://localhost:8080/"
+    },
+    "id": "8e8d91c7",
+    "outputId": "3c51452a-8713-4599-9064-72fff4faca85"
+   },
+   "outputs": [
+    {
+     "data": {
+      "text/plain": [
+       "Discrete(3)"
+      ]
+     },
+     "execution_count": 8,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
+   "source": [
+    "env.action_space"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 9,
+   "id": "5bff02b6",
+   "metadata": {
+    "colab": {
+     "base_uri": "https://localhost:8080/"
+    },
+    "id": "5bff02b6",
+    "outputId": "fe8ee38e-ff94-4ad2-9d9c-e2cc58d89159"
+   },
+   "outputs": [
+    {
+     "data": {
+      "text/plain": [
+       "Box(-1.0, 1.0, (3,), float32)"
+      ]
+     },
+     "execution_count": 9,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
+   "source": [
+    "env.reward_space"
+   ]
+  },
+  {
+   "attachments": {},
+   "cell_type": "markdown",
+   "id": "f01faaa2",
+   "metadata": {
+    "id": "f01faaa2"
+   },
+   "source": [
+    "## Step 4: use Gymnasium features in MO-Gymnasium"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 6,
+   "id": "c64e5417",
+   "metadata": {
+    "id": "c64e5417"
+   },
+   "outputs": [],
+   "source": [
+    "from gym.wrappers.record_video import RecordVideo"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 7,
+   "id": "a4190496",
+   "metadata": {
+    "colab": {
+     "base_uri": "https://localhost:8080/"
+    },
+    "id": "a4190496",
+    "outputId": "7739e628-5ef1-4086-98df-33ff30af0a29"
+   },
+   "outputs": [
+    {
+     "name": "stderr",
+     "output_type": "stream",
+     "text": [
+      "/usr/local/lib/python3.9/site-packages/gym/wrappers/record_video.py:75: UserWarning: \u001b[33mWARN: Overwriting existing videos at /Users/florian.felten/Documents/mo-gym/videos/demo folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
+      "  logger.warn(\n"
+     ]
+    }
+   ],
+   "source": [
+    "env = RecordVideo(env, \"videos/demo\", episode_trigger=lambda e: True)"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "id": "dbde25c4",
+   "metadata": {
+    "id": "dbde25c4"
+   },
+   "source": [
+    "## Step 5: go through the environment with a random agent"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 10,
+   "id": "066db701",
+   "metadata": {
+    "id": "066db701"
+   },
+   "outputs": [],
+   "source": [
+    "env.reset()\n",
+    "done = False\n",
+    "\n",
+    "while not done:\n",
+    "    obs, vec_reward, terminated, truncated, info = env.step(env.action_space.sample())\n",
+    "    done = terminated or truncated\n",
+    "    \n",
+    "env.close()"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "id": "IG7S7nx3cces",
+   "metadata": {
+    "id": "IG7S7nx3cces"
+   },
+   "source": [
+    "## Step 6: scalarize env and run it with stable-baselines3\n",
+    "\n"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "id": "DHkO3R7uccOy",
+   "metadata": {
+    "colab": {
+     "base_uri": "https://localhost:8080/"
+    },
+    "id": "DHkO3R7uccOy",
+    "outputId": "7dcaa4a2-f566-40e8-f62a-12d4320c3df4"
+   },
+   "outputs": [],
+   "source": [
+    "!pip install stable-baselines3"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "id": "lgPC1qU8c-0A",
+   "metadata": {
+    "colab": {
+     "base_uri": "https://localhost:8080/"
+    },
+    "id": "lgPC1qU8c-0A",
+    "outputId": "f3b8d2a4-0a91-4005-c0e9-64d9f34f19d3"
+   },
+   "outputs": [],
+   "source": [
+    "import stable_baselines3 as sb3\n",
+    "import numpy as np\n",
+    "import mo_gymnasium as mo_gym"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "id": "SdkUeJHudZ17",
+   "metadata": {
+    "colab": {
+     "base_uri": "https://localhost:8080/",
+     "height": 592
+    },
+    "id": "SdkUeJHudZ17",
+    "outputId": "9557593d-4893-4c4d-dccb-76fefd73dddd"
+   },
+   "outputs": [],
+   "source": [
+    "!pip install gym==0.24.0"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "id": "mn7dMweScz1U",
+   "metadata": {
+    "colab": {
+     "base_uri": "https://localhost:8080/"
+    },
+    "id": "mn7dMweScz1U",
+    "outputId": "fe80d623-a46d-4d9e-bd27-75188c12aa19"
+   },
+   "outputs": [],
+   "source": [
+    "# Linear scalarizes the environment\n",
+    "env = mo_gym.LinearReward(mo_gym.make(\"mo-mountaincar-v0\"), weight=np.array([0.9, 0.1, 0.0]))\n",
+    "\n",
+    "# Run DQN agent!\n",
+    "agent = sb3.DQN(\"MlpPolicy\", env)\n",
+    "agent.learn(10000)"
+   ]
+  }
+ ],
+ "metadata": {
+  "colab": {
+   "include_colab_link": true,
+   "name": "mo-gym-demo.ipynb",
+   "provenance": []
+  },
+  "kernelspec": {
+   "display_name": "Python 3 (ipykernel)",
+   "language": "python",
+   "name": "python3"
+  },
+  "language_info": {
+   "codemirror_mode": {
+    "name": "ipython",
+    "version": 3
+   },
+   "file_extension": ".py",
+   "mimetype": "text/x-python",
+   "name": "python",
+   "nbconvert_exporter": "python",
+   "pygments_lexer": "ipython3",
+   "version": "3.9.14"
+  },
+  "vscode": {
+   "interpreter": {
+    "hash": "ac2eaa0ea0ebeafcc7822e65e46aa9d4f966f30b695406963e145ea4a91cd4fc"
+   }
+  }
+ },
+ "nbformat": 4,
+ "nbformat_minor": 5
+}
diff --git a/MO-Gymnasium/pyproject.toml b/MO-Gymnasium/pyproject.toml
new file mode 100644
index 0000000..dc1d864
--- /dev/null
+++ b/MO-Gymnasium/pyproject.toml
@@ -0,0 +1,130 @@
+[build-system]
+requires = ["setuptools >= 61.0.0"]
+build-backend = "setuptools.build_meta"
+
+[project]
+name = "mo-gymnasium"
+description="A standard API for MORL and a diverse set of reference environments."
+readme = "README.md"
+requires-python = ">= 3.7"
+authors = [{ name = "Farama Foundation", email = "contact@farama.org" }]
+license = { text = "MIT License" }
+keywords = ["Reinforcement Learning", "Multi-Objective", "RL", "AI", "gymnasium"]
+classifiers = [
+    "Development Status :: 4 - Beta",  # change to `5 - Production/Stable` when ready
+    "License :: OSI Approved :: MIT License",
+    "Programming Language :: Python :: 3",
+    "Programming Language :: Python :: 3.7",
+    "Programming Language :: Python :: 3.8",
+    "Programming Language :: Python :: 3.9",
+    "Programming Language :: Python :: 3.10",
+    "Programming Language :: Python :: 3.11",
+    'Intended Audience :: Science/Research',
+    'Topic :: Scientific/Engineering :: Artificial Intelligence',
+]
+dependencies = [
+    "gymnasium >=0.27",
+    "numpy >=1.21.0",
+    "pygame >=2.1.0",
+    "scipy >=1.7.3",
+    "pymoo >=0.6.0",
+]
+dynamic = ["version"]
+
+[project.optional-dependencies]
+# Update dependencies in `all` if any are added or removed
+mario = ["nes-py", "gym-super-mario-bros"]
+minecart = ["scipy >=1.7.3  "]
+mujoco = ["mujoco >=2.3.0", "imageio >=2.14.1"]
+highway = ["highway-env >=1.8"]
+box2d = ["box2d-py ==2.3.5", "pygame ==2.1.3.dev8", "swig ==4.*"]
+all = [
+    # All dependencies above except accept-rom-license
+    # NOTE: No need to manually remove the duplicates, setuptools automatically does that.
+    # Mario
+    "nes-py",
+    "gym-super-mario-bros",
+    # minecart
+    "scipy >=1.7.3",
+    # mujoco
+    "imageio >=2.14.1",
+    "mujoco >=2.3.0",
+    # highway
+    "highway-env >= 1.8",
+    # box2d
+    "box2d-py ==2.3.5",
+    "pygame ==2.1.3.dev8",
+    "swig ==4.*",
+]
+testing = ["pytest ==7.1.3"]
+
+[project.urls]
+Homepage = "https://mo-gymnasium.farama.org"
+Repository = "https://github.com/Farama-Foundation/MO-Gymnasium"
+Documentation = "https://mo-gymnasium.farama.org"
+"Bug Report" = "https://github.com/Farama-Foundation/MO-Gymnasium/issues"
+
+[tool.setuptools]
+include-package-data = true
+
+[tool.setuptools.packages.find]
+include = ["mo_gymnasium", "mo_gymnasium.*"]
+
+[tool.setuptools.package-data]
+mo_gymnasium = [
+    "**/*.json",
+    "**/assets/*",
+]
+
+# Linters and Test tools #######################################################
+
+[tool.black]
+safe = true
+line-length = 127
+target-version = ['py37', 'py38', 'py39', 'py310']
+include = '\.pyi?$'
+
+[tool.isort]
+atomic = true
+profile = "black"
+src_paths = ["mo_gymnasium", "tests", "docs/scripts"]
+extra_standard_library = ["typing_extensions"]
+indent = 4
+lines_after_imports = 2
+multi_line_output = 3
+
+[tool.pyright]
+include = ["mo_gymnasium/**", "tests/**"]
+exclude = ["**/node_modules", "**/__pycache__"]
+strict = []
+
+typeCheckingMode = "basic"
+pythonVersion = "3.7"
+pythonPlatform = "All"
+typeshedPath = "typeshed"
+enableTypeIgnoreComments = true
+
+# This is required as the CI pre-commit does not download the module (i.e. numpy, pygame)
+#   Therefore, we have to ignore missing imports
+reportMissingImports = "none"
+# Some modules are missing type stubs, which is an issue when running pyright locally
+reportMissingTypeStubs = false
+# For warning and error, will raise an error when
+reportInvalidTypeVarUse = "none"
+
+# reportUnknownMemberType = "warning"  # -> raises warnings
+# reportUnknownParameterType = "warning"  # -> raises warnings
+# reportUnknownVariableType = "warning"  # -> raises warnings
+# reportUnknownArgumentType = "warning"  # -> raises warnings
+reportGeneralTypeIssues = "none"  # -> commented out raises 489 errors
+reportUntypedFunctionDecorator = "none"  # -> pytest.mark.parameterize issues
+
+reportOptionalMemberAccess = "none" # -> commented out raises warnings
+reportPrivateImportUsage = "warning" # -> this raises warnings because we use not exported modules from gym (wrappers)
+
+reportPrivateUsage = "warning"
+reportUnboundVariable = "warning"
+
+[tool.pytest.ini_options]
+filterwarnings = ['ignore:.*The environment .* is out of date.*']
+# filterwarnings = ['ignore:.*step API.*:DeprecationWarning']
diff --git a/MO-Gymnasium/setup.py b/MO-Gymnasium/setup.py
new file mode 100644
index 0000000..8f6b0fe
--- /dev/null
+++ b/MO-Gymnasium/setup.py
@@ -0,0 +1,27 @@
+import pathlib
+
+from setuptools import setup
+
+
+CWD = pathlib.Path(__file__).absolute().parent
+
+
+def get_version():
+    """Gets the mo-gymnasium version."""
+    path = CWD / "mo_gymnasium" / "__init__.py"
+    content = path.read_text()
+    for line in content.splitlines():
+        if line.startswith("__version__"):
+            return line.strip().split()[-1].strip().strip('"')
+    raise RuntimeError("bad version data in __init__.py")
+
+
+setup(name="mo-gymnasium", version=get_version(), long_description=open("README.md").read())
+
+# python setup.py sdist
+# python setup.py bdist_wheel
+# twine upload --repository-url https://upload.pypi.org/legacy/ dist/*
+# twine upload --repository-url https://test.pypi.org/legacy/ dist/*
+# twine upload dist/*
+
+# https://towardsdatascience.com/create-your-own-python-package-and-publish-it-into-pypi-9306a29bc116
diff --git a/MO-Gymnasium/tests/test_envs.py b/MO-Gymnasium/tests/test_envs.py
new file mode 100644
index 0000000..027ec1f
--- /dev/null
+++ b/MO-Gymnasium/tests/test_envs.py
@@ -0,0 +1,197 @@
+import pickle
+
+import gymnasium as gym
+import numpy as np
+import pytest
+from gymnasium.envs.registration import EnvSpec
+from gymnasium.utils.env_checker import check_env, data_equivalence
+
+import mo_gymnasium as mo_gym
+
+
+all_testing_env_specs = []
+for env_spec in gym.envs.registry.values():
+    if type(env_spec.entry_point) is not str:
+        continue
+    # collect MO Gymnasium envs
+    if env_spec.entry_point.split(".")[0] == "mo_gymnasium":
+        all_testing_env_specs.append(env_spec)
+
+
+@pytest.mark.parametrize(
+    "spec",
+    all_testing_env_specs,
+    ids=[spec.id for spec in all_testing_env_specs],
+)
+def test_all_env_api(spec):
+    """Check that all environments pass the environment checker."""
+    env = mo_gym.make(spec.id)
+    env = mo_gym.LinearReward(env)
+    check_env(env, skip_render_check=True)
+    _test_pickle_env(env)
+
+
+@pytest.mark.parametrize("spec", all_testing_env_specs, ids=[spec.id for spec in all_testing_env_specs])
+def test_all_env_passive_env_checker(spec):
+    env = mo_gym.make(spec.id)
+    env.reset()
+    env.step(env.action_space.sample())
+    env.close()
+
+
+# Note that this precludes running this test in multiple threads.
+# However, we probably already can't do multithreading due to some environments.
+SEED = 0
+NUM_STEPS = 50
+
+
+@pytest.mark.parametrize(
+    "env_spec",
+    all_testing_env_specs,
+    ids=[env.id for env in all_testing_env_specs],
+)
+def test_env_determinism_rollout(env_spec: EnvSpec):
+    """Run a rollout with two environments and assert equality.
+    This test run a rollout of NUM_STEPS steps with two environments
+    initialized with the same seed and assert that:
+    - observation after first reset are the same
+    - same actions are sampled by the two envs
+    - observations are contained in the observation space
+    - obs, rew, done and info are equals between the two envs
+    """
+    # Don't check rollout equality if it's a nondeterministic environment.
+    if env_spec.nondeterministic is True:
+        return
+
+    env_1 = env_spec.make(disable_env_checker=True)
+    env_2 = env_spec.make(disable_env_checker=True)
+    env_1 = mo_gym.LinearReward(env_1)
+    env_2 = mo_gym.LinearReward(env_2)
+
+    initial_obs_1, initial_info_1 = env_1.reset(seed=SEED)
+    initial_obs_2, initial_info_2 = env_2.reset(seed=SEED)
+    assert_equals(initial_obs_1, initial_obs_2)
+
+    env_1.action_space.seed(SEED)
+
+    for time_step in range(NUM_STEPS):
+        # We don't evaluate the determinism of actions
+        action = env_1.action_space.sample()
+
+        obs_1, rew_1, terminated_1, truncated_1, info_1 = env_1.step(action)
+        obs_2, rew_2, terminated_2, truncated_2, info_2 = env_2.step(action)
+
+        assert_equals(obs_1, obs_2, f"[{time_step}] ")
+        assert env_1.observation_space.contains(obs_1)  # obs_2 verified by previous assertion
+
+        assert rew_1 == rew_2, f"[{time_step}] reward 1={rew_1}, reward 2={rew_2}"
+        assert terminated_1 == terminated_2, f"[{time_step}] done 1={terminated_1}, done 2={terminated_2}"
+        assert truncated_1 == truncated_2, f"[{time_step}] done 1={truncated_1}, done 2={truncated_2}"
+        assert_equals(info_1, info_2, f"[{time_step}] ")
+
+        if terminated_1 or truncated_1:  # terminated_2, truncated_2 verified by previous assertion
+            env_1.reset(seed=SEED)
+            env_2.reset(seed=SEED)
+
+    env_1.close()
+    env_2.close()
+
+
+def _test_pickle_env(env: gym.Env):
+    pickled_env = pickle.loads(pickle.dumps(env))
+
+    data_equivalence(env.reset(), pickled_env.reset())
+
+    action = env.action_space.sample()
+    data_equivalence(env.step(action), pickled_env.step(action))
+    env.close()
+    pickled_env.close()
+
+
+def assert_equals(a, b, prefix=None):
+    """Assert equality of data structures `a` and `b`.
+    Args:
+        a: first data structure
+        b: second data structure
+        prefix: prefix for failed assertion message for types and dicts
+    """
+    assert type(a) == type(b), f"{prefix}Differing types: {a} and {b}"
+    if isinstance(a, dict):
+        assert list(a.keys()) == list(b.keys()), f"{prefix}Key sets differ: {a} and {b}"
+
+        for k in a.keys():
+            v_a = a[k]
+            v_b = b[k]
+            assert_equals(v_a, v_b)
+    elif isinstance(a, np.ndarray):
+        np.testing.assert_array_equal(a, b)
+    elif isinstance(a, tuple):
+        for elem_from_a, elem_from_b in zip(a, b):
+            assert_equals(elem_from_a, elem_from_b)
+    else:
+        assert a == b
+
+
+def test_ccs_dst():
+    env = mo_gym.make("deep-sea-treasure-v0")
+
+    # Known for gamma=0.99
+    known_ccs = [
+        np.array([0.7, -1.0]),
+        np.array([8.037, -2.97]),
+        np.array([11.0469, -4.901]),
+        np.array([13.181, -6.793]),
+        np.array([14.074, -7.726]),
+        np.array([14.856, -8.648]),
+        np.array([17.3731, -12.2479]),
+        np.array([17.814, -13.125]),
+        np.array([19.073, -15.706]),
+        np.array([19.778, -17.383]),
+    ]
+
+    discounted_front = env.pareto_front(gamma=0.99)
+    for desired, actual in zip(known_ccs, discounted_front):
+        np.testing.assert_array_almost_equal(desired, actual, decimal=2)
+
+
+def test_ccs_dst_no_discount():
+    env = mo_gym.make("deep-sea-treasure-v0")
+
+    known_ccs = mo_gym.envs.deep_sea_treasure.deep_sea_treasure.CONVEX_FRONT
+
+    discounted_front = env.pareto_front(gamma=1.0)
+    for desired, actual in zip(known_ccs, discounted_front):
+        np.testing.assert_array_almost_equal(desired, actual, decimal=2)
+
+
+def test_concave_pf_dst():
+    env = mo_gym.make("deep-sea-treasure-concave-v0")
+
+    # Known for gamma=0.99
+    gamma = 0.99
+    known_pf = [
+        np.array([1.0, -1.0]),
+        np.array([2.0 * gamma**2, -2.97]),
+        np.array([3.0 * gamma**4, -4.901]),
+        np.array([5.0 * gamma**6, -6.793]),
+        np.array([8.0 * gamma**7, -7.726]),
+        np.array([16.0 * gamma**8, -8.648]),
+        np.array([24.0 * gamma**12, -12.2479]),
+        np.array([50.0 * gamma**13, -13.125]),
+        np.array([74.0 * gamma**16, -15.706]),
+        np.array([124.0 * gamma**18, -17.383]),
+    ]
+
+    discounted_front = env.pareto_front(gamma=0.99)
+    for desired, actual in zip(known_pf, discounted_front):
+        np.testing.assert_array_almost_equal(desired, actual, decimal=2)
+
+
+def test_concave_pf_dst_no_discount():
+    env = mo_gym.make("deep-sea-treasure-concave-v0")
+
+    known_pf = mo_gym.envs.deep_sea_treasure.deep_sea_treasure.CONCAVE_FRONT
+
+    discounted_front = env.pareto_front(gamma=1.0)
+    for desired, actual in zip(known_pf, discounted_front):
+        np.testing.assert_array_almost_equal(desired, actual, decimal=2)
diff --git a/MO-Gymnasium/tests/test_wrappers.py b/MO-Gymnasium/tests/test_wrappers.py
new file mode 100644
index 0000000..9cf4235
--- /dev/null
+++ b/MO-Gymnasium/tests/test_wrappers.py
@@ -0,0 +1,134 @@
+import numpy as np
+
+import mo_gymnasium as mo_gym
+from mo_gymnasium import (
+    MOClipReward,
+    MONormalizeReward,
+    MORecordEpisodeStatistics,
+    MOSyncVectorEnv,
+)
+
+
+def go_to_8_3(env):
+    """
+    Goes to (8.2, -3) treasure, returns the rewards
+    """
+    env.reset()
+    env.step(3)  # right
+    env.step(1)  # down
+    _, rewards, _, _, infos = env.step(1)
+    return rewards, infos
+
+
+def test_normalization_wrapper():
+    env = mo_gym.make("deep-sea-treasure-v0")
+    norm_treasure_env = MONormalizeReward(env, idx=0)
+    both_norm_env = MONormalizeReward(norm_treasure_env, idx=1)
+
+    # Tests for both rewards normalized
+    for i in range(30):
+        go_to_8_3(both_norm_env)
+    both_norm_env.reset()
+    _, rewards, _, _, _ = both_norm_env.step(1)  # down
+    np.testing.assert_allclose(rewards, [0.18, -1.24], rtol=0, atol=1e-2)
+    rewards, _ = go_to_8_3(both_norm_env)
+    np.testing.assert_allclose(rewards, [2.13, -1.24], rtol=0, atol=1e-2)
+
+    # Tests for only treasure normalized
+    for i in range(30):
+        go_to_8_3(norm_treasure_env)
+    norm_treasure_env.reset()
+    _, rewards, _, _, _ = norm_treasure_env.step(1)  # down
+    # Time rewards are not normalized (-1)
+    np.testing.assert_allclose(rewards, [0.18, -1.0], rtol=0, atol=1e-2)
+    rewards, _ = go_to_8_3(norm_treasure_env)
+    np.testing.assert_allclose(rewards, [2.13, -1.0], rtol=0, atol=1e-2)
+
+
+def test_clip_wrapper():
+    env = mo_gym.make("deep-sea-treasure-v0")
+    clip_treasure_env = MOClipReward(env, idx=0, min_r=0, max_r=0.5)
+    both_clipped_env = MOClipReward(clip_treasure_env, idx=1, min_r=-0.5, max_r=0)
+
+    # Tests for both rewards clipped
+    both_clipped_env.reset()
+    _, rewards, _, _, _ = both_clipped_env.step(1)  # down
+    np.testing.assert_allclose(rewards, [0.5, -0.5], rtol=0, atol=1e-2)
+    rewards, _ = go_to_8_3(both_clipped_env)
+    np.testing.assert_allclose(rewards, [0.5, -0.5], rtol=0, atol=1e-2)
+
+    # Tests for only treasure clipped
+    clip_treasure_env.reset()
+    _, rewards, _, _, _ = clip_treasure_env.step(1)  # down
+    # Time rewards are not clipped (-1)
+    np.testing.assert_allclose(rewards, [0.5, -1.0], rtol=0, atol=1e-2)
+    rewards, _ = go_to_8_3(clip_treasure_env)
+    np.testing.assert_allclose(rewards, [0.5, -1.0], rtol=0, atol=1e-2)
+
+
+def test_mo_sync_wrapper():
+    def make_env(env_id):
+        def thunk():
+            env = mo_gym.make(env_id)
+            env = MORecordEpisodeStatistics(env, gamma=0.97)
+            return env
+
+        return thunk
+
+    num_envs = 3
+    envs = MOSyncVectorEnv([make_env("deep-sea-treasure-v0") for _ in range(num_envs)])
+
+    envs.reset()
+    obs, rewards, terminateds, truncateds, infos = envs.step(envs.action_space.sample())
+    assert len(obs) == num_envs, "Number of observations do not match the number of envs"
+    assert len(rewards) == num_envs, "Number of rewards do not match the number of envs"
+    assert len(terminateds) == num_envs, "Number of terminateds do not match the number of envs"
+    assert len(truncateds) == num_envs, "Number of truncateds do not match the number of envs"
+
+
+def test_mo_record_ep_statistic():
+    env = mo_gym.make("deep-sea-treasure-v0")
+    env = MORecordEpisodeStatistics(env, gamma=0.97)
+
+    env.reset()
+    _, info = go_to_8_3(env)
+
+    assert isinstance(info["episode"]["r"], np.ndarray)
+    assert isinstance(info["episode"]["dr"], np.ndarray)
+    assert info["episode"]["r"].shape == (2,)
+    assert info["episode"]["dr"].shape == (2,)
+    assert tuple(info["episode"]["r"]) == (np.float32(8.2), np.float32(-3.0))
+    assert tuple(np.round(info["episode"]["dr"], 2)) == (
+        np.float32(7.48),
+        np.float32(-2.82),
+    )
+    assert isinstance(info["episode"]["l"], np.int32)
+    assert info["episode"]["l"] == 3
+    assert isinstance(info["episode"]["t"], np.float32)
+
+
+def test_mo_record_ep_statistic_vector_env():
+    def make_env(env_id):
+        def thunk():
+            env = mo_gym.make(env_id)
+            return env
+
+        return thunk
+
+    num_envs = 3
+    envs = MOSyncVectorEnv([make_env("deep-sea-treasure-v0") for _ in range(num_envs)])
+    envs = MORecordEpisodeStatistics(envs)
+
+    envs.reset()
+    terminateds = np.array([False] * num_envs)
+    info = {}
+    while not np.any(terminateds):
+        obs, rewards, terminateds, _, info = envs.step(envs.action_space.sample())
+
+    assert isinstance(info["episode"]["r"], np.ndarray)
+    assert isinstance(info["episode"]["dr"], np.ndarray)
+    # Episode records are vectorized because multiple environments
+    assert info["episode"]["r"].shape == (num_envs, 2)
+    assert info["episode"]["dr"].shape == (num_envs, 2)
+    assert isinstance(info["episode"]["l"], np.ndarray)
+    assert isinstance(info["episode"]["t"], np.ndarray)
Submodule morl-baselines contains modified content
Submodule morl-baselines 0000000...9f683af (new submodule)
diff --git a/morl-baselines/.github/workflows/deploy-docs.yml b/morl-baselines/.github/workflows/deploy-docs.yml
new file mode 100644
index 0000000..45e7c8a
--- /dev/null
+++ b/morl-baselines/.github/workflows/deploy-docs.yml
@@ -0,0 +1,37 @@
+name: Documentation
+on:
+  push:
+    branches: [main]
+permissions:
+  contents: write
+
+jobs:
+  doc:
+    name: Generate and publish website
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v3
+
+      - uses: actions/setup-python@v4
+        with:
+          python-version: '3.9'
+
+      - name: Install doc dependencies
+        run: pip install -r docs/requirements.txt
+
+      - name: Install MORL-Baselines
+        run: |
+          sudo apt-get install libgmp-dev
+          pip install -e .[all]
+
+      - name: Build
+        run: sphinx-build -b dirhtml -v docs _build
+
+      - name: Remove .doctrees
+        run: rm -r _build/.doctrees
+
+      - name: Upload to GitHub Pages
+        uses: JamesIves/github-pages-deploy-action@v4
+        with:
+          folder: _build
+          branch: gh-pages
diff --git a/morl-baselines/.github/workflows/pre-commit.yml b/morl-baselines/.github/workflows/pre-commit.yml
new file mode 100644
index 0000000..80ce02a
--- /dev/null
+++ b/morl-baselines/.github/workflows/pre-commit.yml
@@ -0,0 +1,21 @@
+# https://pre-commit.com
+# This GitHub Action assumes that the repo contains a valid .pre-commit-config.yaml file.
+name: pre-commit
+on:
+  pull_request:
+  push:
+    branches: [main]
+
+permissions:
+  contents: read # to fetch code (actions/checkout)
+
+jobs:
+  pre-commit:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v3
+      - uses: actions/setup-python@v4
+      - run: python -m pip install pre-commit
+      - run: python -m pre_commit --version
+      - run: python -m pre_commit install
+      - run: python -m pre_commit run --all-files
diff --git a/morl-baselines/.github/workflows/test.yml b/morl-baselines/.github/workflows/test.yml
new file mode 100644
index 0000000..c5cc8be
--- /dev/null
+++ b/morl-baselines/.github/workflows/test.yml
@@ -0,0 +1,30 @@
+name: Python tests
+
+on:
+  push:
+    branches: [ main ]
+  pull_request:
+    branches: [ main ]
+
+jobs:
+  linux-test:
+    runs-on: ubuntu-20.04
+    strategy:
+      matrix:
+        python-version: ['3.10']
+    steps:
+    - uses: actions/checkout@v2
+    - name: Set up Python ${{ matrix.python-version }}
+      uses: actions/setup-python@v2
+      with:
+        python-version: ${{ matrix.python-version }}
+    - name: Install dependencies
+      run: |
+        pip install pytest
+        pip install mujoco
+        sudo apt-get install libgmp-dev
+        pip install imageio
+        pip install -e .[all]
+    - name: Full Python tests
+      run: |
+        pytest tests/test_algos.py
diff --git a/morl-baselines/.gitignore b/morl-baselines/.gitignore
new file mode 100644
index 0000000..e96ea84
--- /dev/null
+++ b/morl-baselines/.gitignore
@@ -0,0 +1,139 @@
+# Byte-compiled / optimized / DLL files
+__pycache__/
+*.py[cod]
+*$py.class
+
+# C extensions
+*.so
+
+# Distribution / packaging
+.Python
+build/
+develop-eggs/
+dist/
+downloads/
+eggs/
+.eggs/
+lib/
+lib64/
+parts/
+sdist/
+var/
+wheels/
+pip-wheel-metadata/
+share/python-wheels/
+*.egg-info/
+.installed.cfg
+*.egg
+MANIFEST
+
+# PyInstaller
+#  Usually these files are written by a python script from a template
+#  before PyInstaller builds the exe, so as to inject date/other infos into it.
+*.manifest
+*.spec
+
+# Installer logs
+pip-log.txt
+pip-delete-this-directory.txt
+
+# Unit test / coverage reports
+htmlcov/
+.tox/
+.nox/
+.coverage
+.coverage.*
+.cache
+nosetests.xml
+coverage.xml
+*.cover
+*.py,cover
+.hypothesis/
+.pytest_cache/
+
+# Translations
+*.mo
+*.pot
+
+# Django stuff:
+*.log
+local_settings.py
+db.sqlite3
+db.sqlite3-journal
+
+# Flask stuff:
+instance/
+.webassets-cache
+
+# Scrapy stuff:
+.scrapy
+
+# Sphinx documentation
+docs/_build/
+
+# PyBuilder
+target/
+
+# Jupyter Notebook
+.ipynb_checkpoints
+
+# IPython
+profile_default/
+ipython_config.py
+
+# pyenv
+.python-version
+
+# pipenv
+#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.
+#   However, in case of collaboration, if having platform-specific dependencies or dependencies
+#   having no cross-platform support, pipenv may install dependencies that don't work, or not
+#   install all needed dependencies.
+#Pipfile.lock
+
+# PEP 582; used by e.g. github.com/David-OConnor/pyflow
+__pypackages__/
+
+# Celery stuff
+celerybeat-schedule
+celerybeat.pid
+
+# SageMath parsed files
+*.sage.py
+
+# Environments
+.env
+.venv
+env/
+venv/
+ENV/
+env.bak/
+venv.bak/
+
+# Spyder project settings
+.spyderproject
+.spyproject
+
+# Rope project settings
+.ropeproject
+
+# mkdocs documentation
+/site
+
+# mypy
+.mypy_cache/
+.dmypy.json
+dmypy.json
+
+# Pyre type checker
+.pyre/
+
+# wandb
+wandb/
+
+# Pycharm
+.idea/
+.DS_Store
+
+# Saved weights
+weights/
diff --git a/morl-baselines/.pre-commit-config.yaml b/morl-baselines/.pre-commit-config.yaml
new file mode 100644
index 0000000..a3ee3bc
--- /dev/null
+++ b/morl-baselines/.pre-commit-config.yaml
@@ -0,0 +1,60 @@
+# See https://pre-commit.com for more information
+# See https://pre-commit.com/hooks.html for more hooks
+repos:
+  - repo: https://github.com/pre-commit/pre-commit-hooks
+    rev: v4.4.0
+    hooks:
+      - id: check-symlinks
+      - id: destroyed-symlinks
+      - id: trailing-whitespace
+      - id: end-of-file-fixer
+      - id: check-yaml
+      - id: check-toml
+      - id: check-ast
+      - id: check-added-large-files
+      - id: check-merge-conflict
+      - id: check-executables-have-shebangs
+      - id: check-shebang-scripts-are-executable
+      - id: detect-private-key
+      - id: debug-statements
+  - repo: https://github.com/codespell-project/codespell
+    rev: v2.2.2
+    hooks:
+      - id: codespell
+        args:
+          - --ignore-words-list=reacher,ure,referenc,wile,mor,ser,esr,nowe
+  - repo: https://github.com/PyCQA/flake8
+    rev: 5.0.4
+    hooks:
+      - id: flake8
+        args:
+          - '--per-file-ignores=*/__init__.py:F401'
+          - --ignore=E203,W503,E741
+          - --max-complexity=30
+          - --max-line-length=456
+          - --show-source
+          - --statistics
+  - repo: https://github.com/asottile/pyupgrade
+    rev: v3.3.0
+    hooks:
+      - id: pyupgrade
+        args: ["--py37-plus"]
+  - repo: https://github.com/PyCQA/isort
+    rev: 5.12.0
+    hooks:
+      - id: isort
+  - repo: https://github.com/python/black
+    rev: 22.10.0
+    hooks:
+      - id: black
+  - repo: https://github.com/pycqa/pydocstyle
+    rev: 6.1.1
+    hooks:
+      - id: pydocstyle
+        exclude: ^(tests/)|(docs/)|(setup.py)|(examples/)
+        args:
+          - --source
+          - --explain
+          - --ignore-decorators=override
+          - --convention=google
+        additional_dependencies: ["toml"]
diff --git a/morl-baselines/LICENSE b/morl-baselines/LICENSE
new file mode 100644
index 0000000..1c647b1
--- /dev/null
+++ b/morl-baselines/LICENSE
@@ -0,0 +1,21 @@
+MIT License
+
+Copyright (c) 2022 Lucas Alegre
+
+Permission is hereby granted, free of charge, to any person obtaining a copy
+of this software and associated documentation files (the "Software"), to deal
+in the Software without restriction, including without limitation the rights
+to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+copies of the Software, and to permit persons to whom the Software is
+furnished to do so, subject to the following conditions:
+
+The above copyright notice and this permission notice shall be included in all
+copies or substantial portions of the Software.
+
+THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+SOFTWARE.
diff --git a/morl-baselines/README.md b/morl-baselines/README.md
new file mode 100644
index 0000000..aba61e2
--- /dev/null
+++ b/morl-baselines/README.md
@@ -0,0 +1,113 @@
+<img src="docs/_static/_images/mo_cheetah.gif" alt="Multiple policies" align="right" width="50%"/>
+
+
+[![Project Status: Active – The project has reached a stable, usable state and is being actively developed.](https://www.repostatus.org/badges/latest/active.svg)](https://www.repostatus.org/#active)
+![tests](https://github.com/LucasAlegre/morl-baselines/workflows/Python%20tests/badge.svg)
+[![License](http://img.shields.io/badge/license-MIT-brightgreen.svg?style=flat)](https://github.com/LucasAlegre/morl-baselines/blob/main/LICENSE)
+[![Discord](https://img.shields.io/discord/999693014618362036?label=discord)](https://discord.gg/ygmkfnBvKA)
+[![pre-commit](https://img.shields.io/badge/pre--commit-enabled-brightgreen?logo=pre-commit&logoColor=white)](https://pre-commit.com/)
+[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)
+[![Imports: isort](https://img.shields.io/badge/%20imports-isort-%231674b1?style=flat&labelColor=ef8336)](https://pycqa.github.io/isort/)
+
+# MORL-Baselines
+
+<!-- start elevator-pitch -->
+
+MORL-Baselines is a library of Multi-Objective Reinforcement Learning (MORL) algorithms.
+This repository aims at containing reliable MORL algorithms implementations in PyTorch.
+
+It strictly follows [MO-Gymnasium](https://github.com/Farama-Foundation/mo-gymnasium) API, which differs from the standard [Gymnasium](https://github.com/Farama-Foundation/Gymnasium) API only in that the environment returns a numpy array as the reward.
+
+For details on multi-objective MDP's (MOMDP's) and other MORL definitions, we suggest reading [A practical guide to multi-objective reinforcement learning and planning](https://link.springer.com/article/10.1007/s10458-022-09552-y).
+
+<!-- end elevator-pitch -->
+
+## Features
+
+<!-- start features -->
+
+* Single and multi-policy algorithms under both SER and ESR criteria are implemented.
+* All algorithms follow the [MO-Gymnasium](https://www.github.com/Farama-Foundation/mo-gymnasium) API.
+* Performances are automatically reported in [Weights and Biases](https://wandb.ai/) dashboards.
+* Linting and formatting are enforced by pre-commit hooks.
+* Code is well documented.
+* All algorithms are automatically tested.
+* Utility functions are provided e.g. pareto pruning, experience buffers, etc.
+* 🔜 Performances have been tested against the ones reported in the original papers.
+* 🔜 Hyper-parameter optimization available.
+
+<!-- end features -->
+
+
+## Implemented Algorithms
+
+<!-- start algos-list -->
+
+| **Name**                                                                                                                                                                 | Single/Multi-policy | ESR/SER | Observation space | Action space | Paper                                                                                                                       |
+|----------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------|---------|------------------|--------------|-----------------------------------------------------------------------------------------------------------------------------|
+| [GPI-LS + GPI-PD](https://github.com/LucasAlegre/morl-baselines/blob/main/morl_baselines/multi_policy/gpi_pd/gpi_pd.py)                                      | Multi               | SER     | Continuous       | Discrete / Continuous     | [Paper and Supplementary Materials](https://arxiv.org/abs/2301.07784)
+| [Envelope Q-Learning](https://github.com/LucasAlegre/morl-baselines/blob/main/morl_baselines/multi_policy/envelope/envelope.py)                                      | Multi               | SER     | Continuous       | Discrete     | [Paper](https://arxiv.org/pdf/1908.08342.pdf)                                                                                        |
+| [PGMORL](https://github.com/LucasAlegre/morl-baselines/blob/main/morl_baselines/multi_policy/pgmorl/pgmorl.py) <sup>[1](#f1)</sup>                                                     | Multi               | SER     | Continuous       | Continuous   | [Paper](https://people.csail.mit.edu/jiex/papers/PGMORL/paper.pdf) / [Supplementary Materials](https://people.csail.mit.edu/jiex/papers/PGMORL/supp.pdf)        |
+| [Pareto Conditioned Networks (PCN)](https://github.com/LucasAlegre/morl-baselines/blob/main/morl_baselines/multi_policy/pcn/pcn.py)                                      | Multi               | SER/ESR <sup>[2](#f2)</sup>      | Continuous       | Discrete     | [Paper](https://www.ifaamas.org/Proceedings/aamas2022/pdfs/p1110.pdf)                                                          |
+| [Pareto Q-Learning](https://github.com/LucasAlegre/morl-baselines/blob/main/morl_baselines/multi_policy/pareto_q_learning/pql.py)                                    | Multi               | SER     | Discrete         | Discrete     | [Paper](https://jmlr.org/papers/volume15/vanmoffaert14a/vanmoffaert14a.pdf)                                                          |
+| [MO Q learning](https://github.com/LucasAlegre/morl-baselines/blob/main/morl_baselines/single_policy/ser/mo_q_learning.py)                                           | Single              | SER     | Discrete         | Discrete     | [Paper](https://www.researchgate.net/publication/235698665_Scalarized_Multi-Objective_Reinforcement_Learning_Novel_Design_Techniques)                                                                                                                             |
+| [MPMOQLearning](https://github.com/LucasAlegre/morl-baselines/blob/main/morl_baselines/multi_policy/multi_policy_moqlearning/mp_mo_q_learning.py)  (outer loop MOQL) | Multi               | SER     | Discrete         | Discrete     | [Paper](https://www.researchgate.net/publication/235698665_Scalarized_Multi-Objective_Reinforcement_Learning_Novel_Design_Techniques) |
+| [Optimistic Linear Support (OLS)](https://github.com/LucasAlegre/morl-baselines/blob/main/morl_baselines/multi_policy/ols/ols.py)                                    | Multi               | SER     | /                | /            | Section 3.3 of the [thesis](http://roijers.info/pub/thesis.pdf)     |
+| [Expected Utility Policy Gradient (EUPG)](https://github.com/LucasAlegre/morl-baselines/blob/main/morl_baselines/single_policy/esr/eupg.py)                          | Single              | ESR     | Discrete         | Discrete     |   [Paper](https://www.researchgate.net/publication/328718263_Multi-objective_Reinforcement_Learning_for_the_Expected_Utility_of_the_Return)                                                   |
+
+:warning: The algorithms have not been benchmarked yet, and some of them have limited features.
+
+<b id="f1">1</b>: Currently, PGMORL is limited to environments with 2 objectives.
+
+<b id="f2">2</b>: PCN assumes environments with deterministic transitions.
+
+<!-- end algos-list -->
+
+## Structure
+
+<!-- start structure -->
+As much as possible, this repo tries to follow the single-file implementation rule for all algorithms. The repo's structure is as follows:
+
+* `examples/` contains a set of examples to use MORL Baselines with [MO-Gymnasium](https://www.github.com/Farama-Foundation/mo-gymnasium) environments.
+* `common/` contains the implementation recurring concepts: replay buffers, neural nets, etc. See the [documentation](https://lucasalegre.github.io/morl-baselines/) for more details.
+* `multi_policy/` contains the implementations of multi-policy algorithms.
+* `single_policy/` contains the implementations of single-policy algorithms (ESR and SER).
+
+<!-- end structure -->
+
+
+## Citing the Project
+
+<!-- start citing -->
+
+```bibtex
+@misc{morl_baselines,
+    author = {Florian Felten and Lucas N. Alegre},
+    title = {MORL-Baselines: Multi-Objective Reinforcement Learning algorithms implementations},
+    year = {2022},
+    publisher = {GitHub},
+    journal = {GitHub repository},
+    howpublished = {\url{https://github.com/LucasAlegre/morl-baselines}},
+}
+```
+
+<!-- end citing -->
+
+## Maintainers
+
+<!-- start maintainers -->
+MORL-Baselines is currently maintained by [Florian Felten](https://ffelten.github.io/) (@ffelten) and [Lucas N. Alegre](https://www.inf.ufrgs.br/~lnalegre/) (@LucasAlegre).
+<!-- end maintainers -->
+
+## Contributing
+
+<!-- start contributing -->
+This repository is open to contributions and we are always happy to receive new algorithms, bug fixes, or features. If you want to contribute, you can join our [Discord server](https://discord.gg/ygmkfnBvKA) and discuss your ideas with us. You can also open an issue or a pull request directly.
+
+<!-- end contributing -->
+
+## Acknowledgements
+<!-- start acknowledgements -->
+* Willem Röpke, for his implementation of Pareto Q-Learning (@wilrop)
+* Denis Steckelmacher and Conor F. Hayes, for providing us with the original implementation of EUPG.
+<!-- end acknowledgements -->
diff --git a/morl-baselines/docs/Makefile b/morl-baselines/docs/Makefile
new file mode 100644
index 0000000..d0c3cbf
--- /dev/null
+++ b/morl-baselines/docs/Makefile
@@ -0,0 +1,20 @@
+# Minimal makefile for Sphinx documentation
+#
+
+# You can set these variables from the command line, and also
+# from the environment for the first two.
+SPHINXOPTS    ?=
+SPHINXBUILD   ?= sphinx-build
+SOURCEDIR     = source
+BUILDDIR      = build
+
+# Put it first so that "make" without argument is like "make help".
+help:
+	@$(SPHINXBUILD) -M help "$(SOURCEDIR)" "$(BUILDDIR)" $(SPHINXOPTS) $(O)
+
+.PHONY: help Makefile
+
+# Catch-all target: route all unknown targets to Sphinx using the new
+# "make mode" option.  $(O) is meant as a shortcut for $(SPHINXOPTS).
+%: Makefile
+	@$(SPHINXBUILD) -M $@ "$(SOURCEDIR)" "$(BUILDDIR)" $(SPHINXOPTS) $(O)
diff --git a/morl-baselines/docs/_static/_images/mo_cheetah.gif b/morl-baselines/docs/_static/_images/mo_cheetah.gif
new file mode 100644
index 0000000..88a4f99
Binary files /dev/null and b/morl-baselines/docs/_static/_images/mo_cheetah.gif differ
diff --git a/morl-baselines/docs/algos/algorithms.md b/morl-baselines/docs/algos/algorithms.md
new file mode 100644
index 0000000..956297d
--- /dev/null
+++ b/morl-baselines/docs/algos/algorithms.md
@@ -0,0 +1,7 @@
+# Overview
+MORL-Baselines contains multiple implementations of multi-objective reinforcement learning algorithms. The following table lists the algorithms that are currently implemented in MORL-Baselines.
+
+```{include} ../../README.md
+:start-after: <!-- start algos-list -->
+:end-before: <!-- end algos-list -->
+```
diff --git a/morl-baselines/docs/algos/multi_policy.md b/morl-baselines/docs/algos/multi_policy.md
new file mode 100644
index 0000000..d1c8ed1
--- /dev/null
+++ b/morl-baselines/docs/algos/multi_policy.md
@@ -0,0 +1,12 @@
+# Multi-Policy Algorithms
+
+```{toctree}
+:hidden:
+multi_policy/gpi_pd
+multi_policy/envelope
+multi_policy/pgmorl
+multi_policy/pcn
+multi_policy/pareto_q_learning
+multi_policy/mp_mo_q_learning
+multi_policy/ols
+```
diff --git a/morl-baselines/docs/algos/multi_policy/envelope.md b/morl-baselines/docs/algos/multi_policy/envelope.md
new file mode 100644
index 0000000..af7026b
--- /dev/null
+++ b/morl-baselines/docs/algos/multi_policy/envelope.md
@@ -0,0 +1,6 @@
+# Envelope Q-Learning
+
+```{eval-rst}
+.. autoclass:: morl_baselines.multi_policy.envelope.envelope.Envelope
+    :members:
+```
diff --git a/morl-baselines/docs/algos/multi_policy/gpi_pd.md b/morl-baselines/docs/algos/multi_policy/gpi_pd.md
new file mode 100644
index 0000000..ad265b6
--- /dev/null
+++ b/morl-baselines/docs/algos/multi_policy/gpi_pd.md
@@ -0,0 +1,6 @@
+# GPI-Prioritized Dyna
+
+```{eval-rst}
+.. autoclass:: morl_baselines.multi_policy.gpi_pd.gpi_pd.GPIPD
+    :members:
+```
diff --git a/morl-baselines/docs/algos/multi_policy/linear_support.md b/morl-baselines/docs/algos/multi_policy/linear_support.md
new file mode 100644
index 0000000..b105ce2
--- /dev/null
+++ b/morl-baselines/docs/algos/multi_policy/linear_support.md
@@ -0,0 +1,6 @@
+# Linear Support
+
+```{eval-rst}
+.. autoclass:: morl_baselines.multi_policy.linear_support.linear_support.LinearSupport
+    :members:
+```
diff --git a/morl-baselines/docs/algos/multi_policy/mp_mo_q_learning.md b/morl-baselines/docs/algos/multi_policy/mp_mo_q_learning.md
new file mode 100644
index 0000000..c7b5ecd
--- /dev/null
+++ b/morl-baselines/docs/algos/multi_policy/mp_mo_q_learning.md
@@ -0,0 +1,6 @@
+# MPMOQ Learning
+
+```{eval-rst}
+.. autoclass:: morl_baselines.multi_policy.multi_policy_moqlearning.mp_mo_q_learning.MPMOQLearning
+    :members:
+```
diff --git a/morl-baselines/docs/algos/multi_policy/pareto_q_learning.md b/morl-baselines/docs/algos/multi_policy/pareto_q_learning.md
new file mode 100644
index 0000000..ae155f8
--- /dev/null
+++ b/morl-baselines/docs/algos/multi_policy/pareto_q_learning.md
@@ -0,0 +1,7 @@
+# Pareto Q-Learning
+
+
+```{eval-rst}
+.. autoclass:: morl_baselines.multi_policy.pareto_q_learning.pql.PQL
+    :members:
+```
diff --git a/morl-baselines/docs/algos/multi_policy/pcn.md b/morl-baselines/docs/algos/multi_policy/pcn.md
new file mode 100644
index 0000000..69ffb4a
--- /dev/null
+++ b/morl-baselines/docs/algos/multi_policy/pcn.md
@@ -0,0 +1,6 @@
+# Pareto Conditioned Networks
+
+```{eval-rst}
+.. autoclass:: morl_baselines.multi_policy.pcn.pcn.PCN
+    :members:
+```
diff --git a/morl-baselines/docs/algos/multi_policy/pgmorl.md b/morl-baselines/docs/algos/multi_policy/pgmorl.md
new file mode 100644
index 0000000..8ddb79a
--- /dev/null
+++ b/morl-baselines/docs/algos/multi_policy/pgmorl.md
@@ -0,0 +1,6 @@
+# PGMORL
+
+```{eval-rst}
+.. autoclass:: morl_baselines.multi_policy.pgmorl.pgmorl.PGMORL
+    :members:
+```
diff --git a/morl-baselines/docs/algos/performances.md b/morl-baselines/docs/algos/performances.md
new file mode 100644
index 0000000..d81cc41
--- /dev/null
+++ b/morl-baselines/docs/algos/performances.md
@@ -0,0 +1,57 @@
+# Performance assessments
+
+:warning: This document is a work in progress.
+
+## Introduction
+To ensure the implementation of the algorithms are correct, we want to test them on various environments. For the sake of reproducibility, we want to run for 10 seeds on various environments. For maintenance purposes and long-term support, these tests will be conducted on environments available in [MO-Gymnasium](www.github.com/farama-foundation/mo-gymnasium). Hence, we will not be able to test on some environments that were presented in original papers.
+
+
+## Metrics
+### Single-policy algorithms
+For single-policy algorithms, the metric used will be the scalarized return of the policy on the evaluation env (utility). Keywords: `eval/scalarized_return` and `eval/scalarized_discounted_return`.
+
+### Multi-policy algorithms
+For multi-policy algorithms, we propose to rely on various metrics to assess the quality of the **discounted** Pareto Fronts (PF) or Convex Coverage Set (CCS). In general, we want to have a metric that is able to assess the convergence of the PF, a metric that is able to assess the diversity of the PF, and a hybrid metric assessing both. The metrics are implemented in `common/performance_indicators`. We propose to use the following metrics:
+* (Diversity) Sparsity: average distance between each consecutive point in the PF. From the PGMORL paper [1]. Keyword: `eval/sparsity`.
+* (Convergence) IGD: a SOTA metric from Multi-Objective Optimization (MOO) literature. It requires a reference PF that we can compute a posteriori. That is, we do a merge of all the PFs found by the method and compute the IGD with respect to this reference PF. Keyword: `eval/igd`.
+* (Hybrid) Hypervolume: a SOTA metric from MOO and MORL literature. Keyword: `eval/hypervolume`.
+
+Moreover, some metrics relying on assumptions on the utility function of the user are proposed in the literature. These metric allow to have an idea on the true value on the user utility, whereas others such as hypervolume do not [2]. We propose to use the following metrics:
+* EUM: Expected Utility Metric. From [3]. Keyword: `eval/eum`.
+* MUL: Maximum Utility Loss for the problems we know the true CCS/PF. From [3]. Keyword: `eval/mul`.
+For both these metrics, we propose to generate a number of equally spaced weights on the objective simplex. The number of weights is 50 by default, can be changed.
+
+Finally, the PF can also be logged as a wandb table for a posteriori analysis. Keyword: `eval/front`.
+
+Here is the function that logs all the metrics:
+```{eval-rst}
+.. autofunction:: morl_baselines.common.utils.log_all_multi_policy_metrics
+```
+
+## Storage
+
+All the metrics will be reported to wandb to allow for easy manipulation and exporting of the data.
+
+## Algorithms
+
+Below are the algorithms that we want to test along with the environments from their original papers which are supported in MO-Gymnasium.
+
+### Single-policy algorithms
+* MOQ-Learning (discrete/discrete): DST, MO-MountainCar;
+* EUPG (discrete/discrete): Fishwood;
+
+### Multi-policy algorithms
+* Envelope Q-Learning (continuous/discrete): DST, Fruit-tree, Mario;
+* Pareto Q-Learning (discrete/discrete): DST;
+* PGMORL (continuous/continuous): Half-Cheetah, 2-obj Hopper. Note: the original paper tweaked the environments to have positive rewards only. We feel that such a modification is no good practice and we will use the official environments from MO-Gymnasium instead. Hence, our version of PGMORL cannot be compared to the original paper.
+* MPMOQLearning (discrete/discrete): DST, MO-MountainCar;
+* GPI-LS, GPI-PD (*/*): DST, Minecart, MO-Hopper;
+* OLS - it is a weight generation method, not a full algorithm.
+* PCN (continuous/discrete): DST, MineCart;
+
+## References
+[1]  J. Xu, Y. Tian, P. Ma, D. Rus, S. Sueda, and W. Matusik, “Prediction-Guided Multi-Objective Reinforcement Learning for Continuous Robot Control,” in Proceedings of the 37th International Conference on Machine Learning, Nov. 2020, pp. 10607–10616. Available: https://proceedings.mlr.press/v119/xu20h.html
+
+[2] C. Hayes et al., “A practical guide to multi-objective reinforcement learning and planning,” Autonomous Agents and Multi-Agent Systems, vol. 36, Apr. 2022, doi: 10.1007/s10458-022-09552-y.
+
+[3] L. M. Zintgraf, T. V. Kanters, D. M. Roijers, F. A. Oliehoek, and P. Beau, “Quality Assessment of MORL Algorithms: A Utility-Based Approach,” 2015.
diff --git a/morl-baselines/docs/algos/single_policy.md b/morl-baselines/docs/algos/single_policy.md
new file mode 100644
index 0000000..936cd4c
--- /dev/null
+++ b/morl-baselines/docs/algos/single_policy.md
@@ -0,0 +1,10 @@
+# Single-policy Algorithms
+
+```{toctree}
+:caption: Single-policy Algorithms
+:hidden:
+
+single_policy/moq_learning
+
+single_policy/eupg
+```
diff --git a/morl-baselines/docs/algos/single_policy/eupg.md b/morl-baselines/docs/algos/single_policy/eupg.md
new file mode 100644
index 0000000..c27fe81
--- /dev/null
+++ b/morl-baselines/docs/algos/single_policy/eupg.md
@@ -0,0 +1,6 @@
+# EUPG
+
+```{eval-rst}
+.. autoclass:: morl_baselines.single_policy.esr.eupg.EUPG
+    :members:
+```
diff --git a/morl-baselines/docs/algos/single_policy/moq_learning.md b/morl-baselines/docs/algos/single_policy/moq_learning.md
new file mode 100644
index 0000000..accd013
--- /dev/null
+++ b/morl-baselines/docs/algos/single_policy/moq_learning.md
@@ -0,0 +1,6 @@
+# MOQ-Learning
+
+```{eval-rst}
+.. autoclass:: morl_baselines.single_policy.ser.mo_q_learning.MOQLearning
+    :members:
+```
diff --git a/morl-baselines/docs/community/community.md b/morl-baselines/docs/community/community.md
new file mode 100644
index 0000000..7248fe6
--- /dev/null
+++ b/morl-baselines/docs/community/community.md
@@ -0,0 +1,26 @@
+# Community
+
+We have a discord server where you can ask questions and get help with the repository. You can join the server [here](https://discord.gg/ygmkfnBvKA).
+
+## Maintainers
+
+```{include} ../../README.md
+:start-after: <!-- start maintainers -->
+:end-before: <!-- end maintainers -->
+```
+
+## Contributing
+
+```{include} ../../README.md
+:start-after: <!-- start contributing -->
+:end-before: <!-- end contributing -->
+```
+
+## Acknowledgements
+
+Aside from the main contributors, there are many people who have contributed to the project in various ways. We would like to thank them all for their contributions.
+
+```{include} ../../README.md
+:start-after: <!-- start acknowledgements -->
+:end-before: <!-- end acknowledgements -->
+```
diff --git a/morl-baselines/docs/conf.py b/morl-baselines/docs/conf.py
new file mode 100644
index 0000000..aedb740
--- /dev/null
+++ b/morl-baselines/docs/conf.py
@@ -0,0 +1,44 @@
+# Configuration file for the Sphinx documentation builder.
+#
+# For the full list of built-in configuration values, see the documentation:
+# https://www.sphinx-doc.org/en/master/usage/configuration.html
+
+# -- Project information -----------------------------------------------------
+# https://www.sphinx-doc.org/en/master/usage/configuration.html#project-information
+
+project = "MORL-Baselines"
+copyright = "2023, Florian Felten & Lucas Alegre"
+author = "Florian Felten & Lucas Alegre"
+release = "0.0.1"
+
+# -- General configuration ---------------------------------------------------
+# https://www.sphinx-doc.org/en/master/usage/configuration.html#general-configuration
+
+extensions = [
+    "sphinx.ext.napoleon",
+    "sphinx.ext.doctest",
+    "sphinx.ext.autodoc",
+    "sphinx.ext.githubpages",
+    "myst_parser",
+]
+
+templates_path = ["_templates"]
+exclude_patterns = []
+
+# Napoleon settings
+napoleon_use_ivar = True
+napoleon_use_admonition_for_references = True
+# See https://github.com/sphinx-doc/sphinx/issues/9119
+napoleon_custom_sections = [("Returns", "params_style")]
+
+
+# -- Options for HTML output -------------------------------------------------
+# https://www.sphinx-doc.org/en/master/usage/configuration.html#options-for-html-output
+
+html_theme = "furo"
+html_theme_options = {
+    "source_repository": "https://github.com/LucasAlegre/morl-baselines/",
+    "source_branch": "main",
+    "source_directory": "docs/",
+}
+html_static_path = ["_static"]
diff --git a/morl-baselines/docs/features/buffers.md b/morl-baselines/docs/features/buffers.md
new file mode 100644
index 0000000..353bac8
--- /dev/null
+++ b/morl-baselines/docs/features/buffers.md
@@ -0,0 +1,31 @@
+# Replay Buffers
+
+Multiple implementations of replay buffers are available in the library. These are listed below:
+
+## Multi-Objective Replay Buffer
+
+```{eval-rst}
+.. autoclass:: morl_baselines.common.buffer.ReplayBuffer
+    :members:
+```
+
+## Diverse Replay Buffer
+
+```{eval-rst}
+.. autoclass:: morl_baselines.common.diverse_buffer.DiverseMemory
+    :members:
+```
+
+## Prioritized Replay Buffer
+
+```{eval-rst}
+.. autoclass:: morl_baselines.common.prioritized_buffer.PrioritizedReplayBuffer
+    :members:
+```
+
+## Accrued Reward Replay Buffer
+
+```{eval-rst}
+.. autoclass:: morl_baselines.common.accrued_reward_buffer.AccruedRewardReplayBuffer
+    :members:
+```
diff --git a/morl-baselines/docs/features/evaluations.md b/morl-baselines/docs/features/evaluations.md
new file mode 100644
index 0000000..f4cb431
--- /dev/null
+++ b/morl-baselines/docs/features/evaluations.md
@@ -0,0 +1,6 @@
+# Evaluations
+
+```{eval-rst}
+.. automodule:: morl_baselines.common.evaluation
+    :members:
+```
diff --git a/morl-baselines/docs/features/misc.md b/morl-baselines/docs/features/misc.md
new file mode 100644
index 0000000..1fecd53
--- /dev/null
+++ b/morl-baselines/docs/features/misc.md
@@ -0,0 +1,6 @@
+# Miscellaneous
+
+```{eval-rst}
+.. automodule:: morl_baselines.common.utils
+    :members:
+```
diff --git a/morl-baselines/docs/features/networks.md b/morl-baselines/docs/features/networks.md
new file mode 100644
index 0000000..22adcee
--- /dev/null
+++ b/morl-baselines/docs/features/networks.md
@@ -0,0 +1,6 @@
+# Neural Networks helpers
+
+```{eval-rst}
+.. automodule:: morl_baselines.common.networks
+    :members:
+```
diff --git a/morl-baselines/docs/features/pareto.md b/morl-baselines/docs/features/pareto.md
new file mode 100644
index 0000000..5298448
--- /dev/null
+++ b/morl-baselines/docs/features/pareto.md
@@ -0,0 +1,6 @@
+# Pareto utils
+
+```{eval-rst}
+.. automodule:: morl_baselines.common.pareto
+    :members:
+```
diff --git a/morl-baselines/docs/features/performance_indicators.md b/morl-baselines/docs/features/performance_indicators.md
new file mode 100644
index 0000000..11ff6ca
--- /dev/null
+++ b/morl-baselines/docs/features/performance_indicators.md
@@ -0,0 +1,6 @@
+# Performance indicators
+
+```{eval-rst}
+.. automodule:: morl_baselines.common.performance_indicators
+    :members:
+```
diff --git a/morl-baselines/docs/features/scalarization.md b/morl-baselines/docs/features/scalarization.md
new file mode 100644
index 0000000..91081d0
--- /dev/null
+++ b/morl-baselines/docs/features/scalarization.md
@@ -0,0 +1,6 @@
+# Scalarization functions
+
+```{eval-rst}
+.. automodule:: morl_baselines.common.scalarization
+    :members:
+```
diff --git a/morl-baselines/docs/index.md b/morl-baselines/docs/index.md
new file mode 100644
index 0000000..c74e6e7
--- /dev/null
+++ b/morl-baselines/docs/index.md
@@ -0,0 +1,68 @@
+---
+hide-toc: true
+firstpage:
+lastpage:
+---
+
+# MORL-Baselines: A collection of multi-objective reinforcement learning algorithms.
+
+```{figure} _static/_images/mo_cheetah.gif
+   :alt: Multiple policies
+   :width: 500
+```
+
+```{include} ../README.md
+:start-after: <!-- start elevator-pitch -->
+:end-before: <!-- end elevator-pitch -->
+```
+
+## Features of MORL-Baselines
+
+```{include} ../README.md
+:start-after: <!-- start features -->
+:end-before: <!-- end features -->
+```
+
+## Citing MORL-Baselines
+```{include} ../README.md
+:start-after: <!-- start citing -->
+:end-before: <!-- end citing -->
+```
+
+```{toctree}
+:hidden:
+:caption: Quickstart
+
+quickstart/overview
+```
+
+```{toctree}
+:hidden:
+:caption: MORL Algorithms
+
+algos/algorithms
+algos/multi_policy
+algos/single_policy
+algos/performances
+```
+
+```{toctree}
+:hidden:
+:caption: Features
+
+features/pareto
+features/evaluations
+features/performance_indicators
+features/scalarization
+features/buffers
+features/networks
+features/misc
+```
+
+```{toctree}
+:hidden:
+:caption: Development
+
+community/community
+Github <https://github.com/LucasAlegre/MORL-baselines>
+```
diff --git a/morl-baselines/docs/make.bat b/morl-baselines/docs/make.bat
new file mode 100644
index 0000000..747ffb7
--- /dev/null
+++ b/morl-baselines/docs/make.bat
@@ -0,0 +1,35 @@
+@ECHO OFF
+
+pushd %~dp0
+
+REM Command file for Sphinx documentation
+
+if "%SPHINXBUILD%" == "" (
+	set SPHINXBUILD=sphinx-build
+)
+set SOURCEDIR=source
+set BUILDDIR=build
+
+%SPHINXBUILD% >NUL 2>NUL
+if errorlevel 9009 (
+	echo.
+	echo.The 'sphinx-build' command was not found. Make sure you have Sphinx
+	echo.installed, then set the SPHINXBUILD environment variable to point
+	echo.to the full path of the 'sphinx-build' executable. Alternatively you
+	echo.may add the Sphinx directory to PATH.
+	echo.
+	echo.If you don't have Sphinx installed, grab it from
+	echo.https://www.sphinx-doc.org/
+	exit /b 1
+)
+
+if "%1" == "" goto help
+
+%SPHINXBUILD% -M %1 %SOURCEDIR% %BUILDDIR% %SPHINXOPTS% %O%
+goto end
+
+:help
+%SPHINXBUILD% -M help %SOURCEDIR% %BUILDDIR% %SPHINXOPTS% %O%
+
+:end
+popd
diff --git a/morl-baselines/docs/quickstart/overview.md b/morl-baselines/docs/quickstart/overview.md
new file mode 100644
index 0000000..dde5087
--- /dev/null
+++ b/morl-baselines/docs/quickstart/overview.md
@@ -0,0 +1,6 @@
+# Overview
+
+```{include} ../../README.md
+:start-after: <!-- start structure -->
+:end-before: <!-- end structure -->
+```
diff --git a/morl-baselines/docs/requirements.txt b/morl-baselines/docs/requirements.txt
new file mode 100644
index 0000000..600e05f
--- /dev/null
+++ b/morl-baselines/docs/requirements.txt
@@ -0,0 +1,3 @@
+sphinx
+furo
+myst-parser
diff --git a/morl-baselines/examples/.DS_Store b/morl-baselines/examples/.DS_Store
new file mode 100644
index 0000000..f7d190a
Binary files /dev/null and b/morl-baselines/examples/.DS_Store differ
diff --git a/morl-baselines/examples/envelope_minecart.py b/morl-baselines/examples/envelope_minecart.py
new file mode 100644
index 0000000..d926fd8
--- /dev/null
+++ b/morl-baselines/examples/envelope_minecart.py
@@ -0,0 +1,58 @@
+import mo_gymnasium as mo_gym
+import numpy as np
+from mo_gymnasium.utils import MORecordEpisodeStatistics
+
+from morl_baselines.multi_policy.envelope.envelope import Envelope
+
+
+def main():
+    def make_env():
+        env = mo_gym.make("minecart-v0")
+        env = MORecordEpisodeStatistics(env, gamma=0.98)
+        # env = mo_gym.LinearReward(env)
+        return env
+
+    env = make_env()
+    eval_env = make_env()
+    # RecordVideo(make_env(), "videos/minecart/", episode_trigger=lambda e: e % 1000 == 0)
+
+    agent = Envelope(
+        env,
+        max_grad_norm=0.1,
+        learning_rate=3e-4,
+        gamma=0.98,
+        batch_size=64,
+        net_arch=[256, 256, 256, 256],
+        buffer_size=int(2e6),
+        initial_epsilon=1.0,
+        final_epsilon=0.05,
+        epsilon_decay_steps=50000,
+        initial_homotopy_lambda=0.0,
+        final_homotopy_lambda=1.0,
+        homotopy_decay_steps=10000,
+        learning_starts=100,
+        envelope=True,
+        gradient_updates=1,
+        target_net_update_freq=1000,  # 1000,  # 500 reduce by gradient updates
+        tau=1,
+        log=True,
+        project_name="MORL-Baselines",
+        experiment_name="Envelope",
+    )
+
+    agent.train(
+        total_timesteps=100000,
+        total_episodes=None,
+        weight=None,
+        eval_env=eval_env,
+        ref_point=np.array([0, 0, -200.0]),
+        known_pareto_front=env.unwrapped.pareto_front(gamma=0.98),
+        eval_weights_number_for_front=100,
+        eval_freq=1000,
+        reset_num_timesteps=False,
+        reset_learning_starts=False,
+    )
+
+
+if __name__ == "__main__":
+    main()
diff --git a/morl-baselines/examples/eupg_fishwood.py b/morl-baselines/examples/eupg_fishwood.py
new file mode 100644
index 0000000..dd7dba8
--- /dev/null
+++ b/morl-baselines/examples/eupg_fishwood.py
@@ -0,0 +1,19 @@
+import mo_gymnasium as mo_gym
+import numpy as np
+from mo_gymnasium.utils import MORecordEpisodeStatistics
+
+from morl_baselines.common.evaluation import eval_mo_reward_conditioned
+from morl_baselines.single_policy.esr.eupg import EUPG
+
+
+if __name__ == "__main__":
+    env = MORecordEpisodeStatistics(mo_gym.make("fishwood-v0"), gamma=0.99)
+    eval_env = mo_gym.make("fishwood-v0")
+
+    def scalarization(reward: np.ndarray):
+        return min(reward[0], reward[1] // 2)
+
+    agent = EUPG(env, scalarization=scalarization, gamma=0.99, log=True, learning_rate=0.001)
+    agent.train(total_timesteps=int(4e6), eval_env=eval_env, eval_freq=1000)
+
+    print(eval_mo_reward_conditioned(agent, env=eval_env, scalarization=scalarization))
diff --git a/morl-baselines/examples/gpi_pd_hopper.py b/morl-baselines/examples/gpi_pd_hopper.py
new file mode 100644
index 0000000..4b46a21
--- /dev/null
+++ b/morl-baselines/examples/gpi_pd_hopper.py
@@ -0,0 +1,55 @@
+import fire
+import mo_gymnasium as mo_gym
+import numpy as np
+
+from morl_baselines.multi_policy.gpi_pd.gpi_pd_continuous_action import (
+    GPIPDContinuousAction,
+)
+
+
+# from gymnasium.wrappers.record_video import RecordVideo
+
+
+def main(algo: str, gpi_pd: bool, g: int, timesteps_per_iter: int = 15000):
+    def make_env(record_episode_statistics: bool = False):
+        env = mo_gym.make("mo-hopper-v4", cost_objective=False, max_episode_steps=500)
+        if record_episode_statistics:
+            env = mo_gym.MORecordEpisodeStatistics(env, gamma=0.99)
+        return env
+
+    env = make_env(record_episode_statistics=True)
+    eval_env = make_env()  # RecordVideo(make_env(), "videos/minecart/", episode_trigger=lambda e: e % 1000 == 0)
+
+    agent = GPIPDContinuousAction(
+        env,
+        gradient_updates=g,
+        min_priority=0.1,
+        batch_size=128,
+        buffer_size=int(4e5),
+        dynamics_rollout_starts=1000,
+        dynamics_rollout_len=5,
+        dynamics_rollout_freq=250,
+        dynamics_rollout_batch_size=50000,
+        dynamics_train_freq=250,
+        dynamics_buffer_size=200000,
+        dynamics_real_ratio=0.1,
+        dynamics_min_uncertainty=2.0,
+        dyna=gpi_pd,
+        per=gpi_pd,
+        project_name="MORL-Baselines",
+        experiment_name="GPI-PD",
+        log=True,
+    )
+
+    agent.train(
+        eval_env=eval_env,
+        ref_point=np.array([-100.0, -100.0]),
+        known_pareto_front=None,
+        weight_selection_algo=algo,
+        max_iter=10,
+        timesteps_per_iter=timesteps_per_iter,
+    )
+
+
+if __name__ == "__main__":
+    fire.Fire(main)
diff --git a/morl-baselines/examples/gpi_pd_minecart.py b/morl-baselines/examples/gpi_pd_minecart.py
new file mode 100644
index 0000000..356cdf4
--- /dev/null
+++ b/morl-baselines/examples/gpi_pd_minecart.py
@@ -0,0 +1,67 @@
+import fire
+import mo_gymnasium as mo_gym
+import numpy as np
+
+from morl_baselines.multi_policy.gpi_pd.gpi_pd import GPIPD
+
+
+# from gymnasium.wrappers.record_video import RecordVideo
+
+
+def main(algo: str, gpi_pd: bool, g: int, timesteps_per_iter: int = 10000, seed: int = 0):
+    def make_env():
+        env = mo_gym.make("minecart-v0")
+        env = mo_gym.MORecordEpisodeStatistics(env, gamma=0.98)
+        return env
+
+    env = make_env()
+    eval_env = make_env()  # RecordVideo(make_env(), "videos/minecart/", episode_trigger=lambda e: e % 1000 == 0)
+
+    agent = GPIPD(
+        env,
+        num_nets=2,
+        max_grad_norm=None,
+        learning_rate=3e-4,
+        gamma=0.98,
+        batch_size=128,
+        net_arch=[256, 256, 256, 256],
+        buffer_size=int(2e5),
+        initial_epsilon=1.0,
+        final_epsilon=0.05,
+        epsilon_decay_steps=50000,
+        learning_starts=100,
+        alpha_per=0.6,
+        min_priority=0.01,
+        per=gpi_pd,
+        gpi_pd=gpi_pd,
+        use_gpi=True,
+        gradient_updates=g,
+        target_net_update_freq=200,
+        tau=1,
+        dyna=gpi_pd,
+        dynamics_uncertainty_threshold=1.5,
+        dynamics_net_arch=[256, 256, 256],
+        dynamics_buffer_size=int(1e5),
+        dynamics_rollout_batch_size=25000,
+        dynamics_train_freq=lambda t: 250,
+        dynamics_rollout_freq=250,
+        dynamics_rollout_starts=5000,
+        dynamics_rollout_len=1,
+        real_ratio=0.5,
+        log=True,
+        project_name="MORL-Baselines",
+        experiment_name="GPI-PD",
+    )
+
+    agent.train(
+        eval_env=eval_env,
+        ref_point=np.array([0.0, 0.0, -200.0]),
+        known_pareto_front=env.unwrapped.pareto_front(gamma=0.98),
+        weight_selection_algo=algo,
+        max_iter=15,
+        timesteps_per_iter=timesteps_per_iter,
+    )
+
+
+if __name__ == "__main__":
+    fire.Fire(main)
diff --git a/morl-baselines/examples/mo_q_learning_DST.py b/morl-baselines/examples/mo_q_learning_DST.py
new file mode 100644
index 0000000..ab23408
--- /dev/null
+++ b/morl-baselines/examples/mo_q_learning_DST.py
@@ -0,0 +1,26 @@
+import time
+
+import mo_gymnasium as mo_gym
+import numpy as np
+from mo_gymnasium.utils import MORecordEpisodeStatistics
+
+from morl_baselines.common.evaluation import eval_mo
+from morl_baselines.common.scalarization import tchebicheff
+from morl_baselines.single_policy.ser.mo_q_learning import MOQLearning
+
+
+if __name__ == "__main__":
+    env = MORecordEpisodeStatistics(mo_gym.make("deep-sea-treasure-concave-v0"), gamma=0.99)
+    eval_env = mo_gym.make("deep-sea-treasure-concave-v0")
+    scalarization = tchebicheff(tau=4.0, reward_dim=2)
+    weights = np.array([0.3, 0.7])
+
+    agent = MOQLearning(env, scalarization=scalarization, weights=weights, log=True)
+    agent.train(
+        total_timesteps=int(1e5),
+        start_time=time.time(),
+        eval_freq=100,
+        eval_env=eval_env,
+    )
+
+    print(eval_mo(agent, env=eval_env, w=weights))
diff --git a/morl-baselines/examples/mp_mo_q_learning_DST.py b/morl-baselines/examples/mp_mo_q_learning_DST.py
new file mode 100644
index 0000000..6d75c39
--- /dev/null
+++ b/morl-baselines/examples/mp_mo_q_learning_DST.py
@@ -0,0 +1,36 @@
+import mo_gymnasium as mo_gym
+import numpy as np
+from mo_gymnasium import MORecordEpisodeStatistics
+
+from morl_baselines.common.scalarization import tchebicheff
+from morl_baselines.multi_policy.multi_policy_moqlearning.mp_mo_q_learning import (
+    MPMOQLearning,
+)
+
+
+if __name__ == "__main__":
+    env = MORecordEpisodeStatistics(mo_gym.make("deep-sea-treasure-concave-v0"), gamma=0.99)
+    eval_env = mo_gym.make("deep-sea-treasure-concave-v0")
+    scalarization = tchebicheff(tau=4.0, reward_dim=2)
+
+    mp_moql = MPMOQLearning(
+        env,
+        learning_rate=0.3,
+        scalarization=scalarization,
+        use_gpi_policy=False,
+        dyna=False,
+        initial_epsilon=1,
+        final_epsilon=0.01,
+        epsilon_decay_steps=int(2e5),
+        weight_selection_algo="random",
+        epsilon_ols=0.0,
+    )
+    mp_moql.train(
+        num_iterations=15,
+        timesteps_per_iteration=int(2e5),
+        eval_freq=100,
+        num_episodes_eval=1,
+        eval_env=eval_env,
+        ref_point=np.array([0.0, -25.0]),
+        known_pareto_front=env.unwrapped.pareto_front(gamma=0.99),
+    )
diff --git a/morl-baselines/examples/ols_dst.py b/morl-baselines/examples/ols_dst.py
new file mode 100644
index 0000000..418731d
--- /dev/null
+++ b/morl-baselines/examples/ols_dst.py
@@ -0,0 +1,41 @@
+import mo_gymnasium as mo_gym
+import numpy as np
+
+from morl_baselines.multi_policy.multi_policy_moqlearning.mp_mo_q_learning import (
+    MPMOQLearning,
+)
+
+
+def main():
+
+    GAMMA = 0.99
+    env = mo_gym.MORecordEpisodeStatistics(mo_gym.make("deep-sea-treasure-v0"), gamma=GAMMA)
+
+    eval_env = mo_gym.make("deep-sea-treasure-v0")
+
+    mp_moql = MPMOQLearning(
+        env,
+        learning_rate=0.3,
+        gamma=GAMMA,
+        use_gpi_policy=True,
+        dyna=True,
+        dyna_updates=5,
+        initial_epsilon=1,
+        final_epsilon=0.01,
+        epsilon_decay_steps=int(2e5),
+        weight_selection_algo="ols",
+        epsilon_ols=0.0,
+    )
+    mp_moql.train(
+        num_iterations=15,
+        timesteps_per_iteration=int(2e5),
+        eval_freq=100,
+        num_episodes_eval=1,
+        eval_env=eval_env,
+        ref_point=np.array([0.0, -25.0]),
+        known_pareto_front=env.unwrapped.pareto_front(gamma=GAMMA),
+    )
+
+
+if __name__ == "__main__":
+    main()
diff --git a/morl-baselines/examples/pcn_minecart.py b/morl-baselines/examples/pcn_minecart.py
new file mode 100644
index 0000000..d89ce06
--- /dev/null
+++ b/morl-baselines/examples/pcn_minecart.py
@@ -0,0 +1,39 @@
+import mo_gymnasium as mo_gym
+import numpy as np
+from mo_gymnasium.utils import MORecordEpisodeStatistics
+
+from morl_baselines.multi_policy.pcn.pcn import PCN
+
+
+def main():
+    def make_env():
+        env = mo_gym.make("minecart-deterministic-v0")
+        env = MORecordEpisodeStatistics(env, gamma=1.0)
+        return env
+
+    env = make_env()
+
+    agent = PCN(
+        env,
+        scaling_factor=np.array([1, 1, 0.1, 0.1]),
+        learning_rate=1e-3,
+        batch_size=256,
+        project_name="MORL-Baselines",
+        experiment_name="PCN",
+        log=True,
+    )
+
+    agent.train(
+        env,
+        num_er_episodes=20,
+        max_buffer_size=50,
+        num_model_updates=50,
+        total_time_steps=int(1e7),
+        max_return=np.array([1.5, 1.5, -0.0]),
+        ref_point=np.array([0, 0, -200.0]),
+        known_pareto_front=env.unwrapped.pareto_front(gamma=1.0),
+    )
+
+
+if __name__ == "__main__":
+    main()
diff --git a/morl-baselines/examples/pgmorl_halfcheetah.py b/morl-baselines/examples/pgmorl_halfcheetah.py
new file mode 100644
index 0000000..15b7831
--- /dev/null
+++ b/morl-baselines/examples/pgmorl_halfcheetah.py
@@ -0,0 +1,33 @@
+import numpy as np
+
+from morl_baselines.common.evaluation import eval_mo
+from morl_baselines.multi_policy.pgmorl.pgmorl import PGMORL
+from morl_baselines.single_policy.ser.mo_ppo import make_env
+
+
+if __name__ == "__main__":
+    env_id = "mo-halfcheetah-v4"
+    algo = PGMORL(
+        env_id=env_id,
+        ref_point=np.array([0.0, -5.0]),
+        known_pareto_front=None,
+        num_envs=4,
+        pop_size=6,
+        warmup_iterations=80,
+        evolutionary_iterations=20,
+        num_weight_candidates=7,
+        limit_env_steps=int(5e6),
+    )
+    algo.train()
+    env = make_env(env_id, 422, 1, "PGMORL_test", gamma=0.995)()  # idx != 0 to avoid taking videos
+
+    # Execution of trained policies
+    for a in algo.archive.individuals:
+        scalarized, discounted_scalarized, reward, discounted_reward = eval_mo(
+            agent=a, env=env, w=np.array([1.0, 1.0]), render=True
+        )
+        print(f"Agent #{a.id}")
+        print(f"Scalarized: {scalarized}")
+        print(f"Discounted scalarized: {discounted_scalarized}")
+        print(f"Vectorial: {reward}")
+        print(f"Discounted vectorial: {discounted_reward}")
diff --git a/morl-baselines/examples/pql_dst.py b/morl-baselines/examples/pql_dst.py
new file mode 100644
index 0000000..d9482b1
--- /dev/null
+++ b/morl-baselines/examples/pql_dst.py
@@ -0,0 +1,38 @@
+import mo_gymnasium as mo_gym
+import numpy as np
+
+from morl_baselines.multi_policy.pareto_q_learning.pql import PQL
+
+
+if __name__ == "__main__":
+    env = mo_gym.make("deep-sea-treasure-concave-v0")
+    ref_point = np.array([0, -25])
+
+    agent = PQL(
+        env,
+        ref_point,
+        gamma=0.99,
+        initial_epsilon=1.0,
+        epsilon_decay=0.997,
+        final_epsilon=0.2,
+        seed=1,
+        project_name="MORL-Baselines",
+        experiment_name="Pareto Q-Learning",
+        log=True,
+    )
+
+    num_episodes = 10000
+    pf = agent.train(
+        num_episodes=10000,
+        log_every=100,
+        action_eval="hypervolume",
+        known_pareto_front=env.pareto_front(gamma=0.99),
+        eval_ref_point=ref_point,
+    )
+    print(pf)
+
+    # Execute a policy
+    target = np.array(pf.pop())
+    print(f"Tracking {target}")
+    reward = agent.track_policy(target)
+    print(f"Obtained {reward}")
diff --git a/morl-baselines/morl_baselines/__init__.py b/morl-baselines/morl_baselines/__init__.py
new file mode 100644
index 0000000..2936dcf
--- /dev/null
+++ b/morl-baselines/morl_baselines/__init__.py
@@ -0,0 +1 @@
+"""MORL-Baselines contains various MORL algorithms and utility functions."""
diff --git a/morl-baselines/morl_baselines/common/__init__.py b/morl-baselines/morl_baselines/common/__init__.py
new file mode 100644
index 0000000..607a085
--- /dev/null
+++ b/morl-baselines/morl_baselines/common/__init__.py
@@ -0,0 +1 @@
+"""Utilities for the development of MORL Algorithms."""
diff --git a/morl-baselines/morl_baselines/common/accrued_reward_buffer.py b/morl-baselines/morl_baselines/common/accrued_reward_buffer.py
new file mode 100644
index 0000000..7f4a5b0
--- /dev/null
+++ b/morl-baselines/morl_baselines/common/accrued_reward_buffer.py
@@ -0,0 +1,121 @@
+"""Accrued reward buffer for ESR algorithms."""
+
+import numpy as np
+import torch as th
+
+
+class AccruedRewardReplayBuffer:
+    """Replay buffer with accrued rewards stored (for ESR algorithms)."""
+
+    def __init__(
+        self,
+        obs_shape,
+        action_shape,
+        rew_dim=1,
+        max_size=100000,
+        obs_dtype=np.float32,
+        action_dtype=np.float32,
+    ):
+        """Initialize the Replay Buffer.
+
+        Args:
+            obs_shape: Shape of the observations
+            action_shape:  Shape of the actions
+            rew_dim: Dimension of the rewards
+            max_size: Maximum size of the buffer
+            obs_dtype: Data type of the observations
+            action_dtype: Data type of the actions
+        """
+        self.max_size = max_size
+        self.ptr, self.size = 0, 0
+        self.obs = np.zeros((max_size,) + obs_shape, dtype=obs_dtype)
+        self.next_obs = np.zeros((max_size,) + obs_shape, dtype=obs_dtype)
+        self.actions = np.zeros((max_size,) + action_shape, dtype=action_dtype)
+        self.rewards = np.zeros((max_size, rew_dim), dtype=np.float32)
+        self.accrued_rewards = np.zeros((max_size, rew_dim), dtype=np.float32)
+        self.dones = np.zeros((max_size, 1), dtype=np.float32)
+
+    def add(self, obs, accrued_reward, action, reward, next_obs, done):
+        """Add a new experience to memory.
+
+        Args:
+            obs: Observation
+            accrued_reward: Accrued reward
+            action: Action
+            reward: Reward
+            next_obs: Next observation
+            done: Done
+        """
+        self.obs[self.ptr] = np.array(obs).copy()
+        self.next_obs[self.ptr] = np.array(next_obs).copy()
+        self.actions[self.ptr] = np.array(action).copy()
+        self.rewards[self.ptr] = np.array(reward).copy()
+        self.accrued_rewards[self.ptr] = np.array(accrued_reward).copy()
+        self.dones[self.ptr] = np.array(done).copy()
+        self.ptr = (self.ptr + 1) % self.max_size
+        self.size = min(self.size + 1, self.max_size)
+
+    def sample(self, batch_size, replace=True, use_cer=False, to_tensor=False, device=None):
+        """Sample a batch of experiences.
+
+        Args:
+            batch_size: Number of elements to sample
+            replace: Whether to sample with replacement or not
+            use_cer: Whether to use CER or not
+            to_tensor: Whether to convert the data to tensors or not
+            device: Device to use for the tensors
+
+        Returns:
+            Tuple of (obs, accrued_rewards, actions, rewards, next_obs, dones)
+        """
+        inds = np.random.choice(self.size, batch_size, replace=replace)
+        if use_cer:
+            inds[0] = self.ptr - 1  # always use last experience
+        experience_tuples = (
+            self.obs[inds],
+            self.accrued_rewards[inds],
+            self.actions[inds],
+            self.rewards[inds],
+            self.next_obs[inds],
+            self.dones[inds],
+        )
+        if to_tensor:
+            return tuple(map(lambda x: th.tensor(x).to(device), experience_tuples))
+        else:
+            return experience_tuples
+
+    def cleanup(self):
+        """Cleanup the buffer."""
+        self.size, self.ptr = 0, 0
+
+    def get_all_data(self, max_samples=None, to_tensor=False, device=None):
+        """Returns the whole buffer (with a specified maximum number of samples).
+
+        Args:
+            max_samples: the number of samples to return, if not specified, returns the full buffer (ordered!)
+            to_tensor: Whether to convert the data to tensors or not
+            device: Device to use for the tensors
+
+        Returns:
+            Tuple of (obs, accrued_rewards, actions, rewards, next_obs, dones)
+        """
+        if max_samples is not None:
+            inds = np.random.choice(self.size, min(max_samples, self.size), replace=False)
+        else:
+            inds = np.arange(self.size)
+        experience_tuples = (
+            self.obs[inds],
+            self.accrued_rewards[inds],
+            self.actions[inds],
+            self.rewards[inds],
+            self.next_obs[inds],
+            self.dones[inds],
+        )
+        if to_tensor:
+            return tuple(map(lambda x: th.tensor(x).to(device), experience_tuples))
+        else:
+            return experience_tuples
+
+    def __len__(self):
+        """Return the current size of internal memory."""
+        return self.size
diff --git a/morl-baselines/morl_baselines/common/buffer.py b/morl-baselines/morl_baselines/common/buffer.py
new file mode 100644
index 0000000..c20558c
--- /dev/null
+++ b/morl-baselines/morl_baselines/common/buffer.py
@@ -0,0 +1,124 @@
+"""Replay buffer for multi-objective reinforcement learning."""
+import numpy as np
+import torch as th
+
+
+class ReplayBuffer:
+    """Multi-objective replay buffer for multi-objective reinforcement learning."""
+
+    def __init__(
+        self,
+        obs_shape,
+        action_dim,
+        rew_dim=1,
+        max_size=100000,
+        obs_dtype=np.float32,
+        action_dtype=np.float32,
+    ):
+        """Initialize the replay buffer.
+
+        Args:
+            obs_shape: Shape of the observations
+            action_dim: Dimension of the actions
+            rew_dim: Dimension of the rewards
+            max_size: Maximum size of the buffer
+            obs_dtype: Data type of the observations
+            action_dtype: Data type of the actions
+        """
+        self.max_size = max_size
+        self.ptr, self.size = 0, 0
+        self.obs = np.zeros((max_size,) + obs_shape, dtype=obs_dtype)
+        self.next_obs = np.zeros((max_size,) + obs_shape, dtype=obs_dtype)
+        self.actions = np.zeros((max_size, action_dim), dtype=action_dtype)
+        self.rewards = np.zeros((max_size, rew_dim), dtype=np.float32)
+        self.dones = np.zeros((max_size, 1), dtype=np.float32)
+
+    def add(self, obs, action, reward, next_obs, done):
+        """Add a new experience to the buffer.
+
+        Args:
+            obs: Observation
+            action: Action
+            reward: Reward
+            next_obs: Next observation
+            done: Done
+        """
+        self.obs[self.ptr] = np.array(obs).copy()
+        self.next_obs[self.ptr] = np.array(next_obs).copy()
+        self.actions[self.ptr] = np.array(action).copy()
+        self.rewards[self.ptr] = np.array(reward).copy()
+        self.dones[self.ptr] = np.array(done).copy()
+        self.ptr = (self.ptr + 1) % self.max_size
+        self.size = min(self.size + 1, self.max_size)
+
+    def sample(self, batch_size, replace=True, use_cer=False, to_tensor=False, device=None):
+        """Sample a batch of experiences from the buffer.
+
+        Args:
+            batch_size: Batch size
+            replace: Whether to sample with replacement
+            use_cer: Whether to use CER
+            to_tensor: Whether to convert the data to PyTorch tensors
+            device: Device to use
+
+        Returns:
+            A tuple of (observations, actions, rewards, next observations, dones)
+
+        """
+        inds = np.random.choice(self.size, batch_size, replace=replace)
+        if use_cer:
+            inds[0] = self.ptr - 1  # always use last experience
+        experience_tuples = (
+            self.obs[inds],
+            self.actions[inds],
+            self.rewards[inds],
+            self.next_obs[inds],
+            self.dones[inds],
+        )
+        if to_tensor:
+            return tuple(map(lambda x: th.tensor(x).to(device), experience_tuples))
+        else:
+            return experience_tuples
+
+    def sample_obs(self, batch_size, replace=True, to_tensor=False, device=None):
+        """Sample a batch of observations from the buffer.
+
+        Args:
+            batch_size: Batch size
+            replace: Whether to sample with replacement
+            to_tensor: Whether to convert the data to PyTorch tensors
+            device: Device to use
+
+        Returns:
+            A batch of observations
+        """
+        inds = np.random.choice(self.size, batch_size, replace=replace)
+        if to_tensor:
+            return th.tensor(self.obs[inds]).to(device)
+        else:
+            return self.obs[inds]
+
+    def get_all_data(self, max_samples=None):
+        """Get all the data in the buffer (with a maximum specified).
+
+        Args:
+            max_samples: Maximum number of samples to return
+
+        Returns:
+            A tuple of (observations, actions, rewards, next observations, dones)
+        """
+        if max_samples is not None:
+            inds = np.random.choice(self.size, min(max_samples, self.size), replace=False)
+        else:
+            inds = np.arange(self.size)
+        return (
+            self.obs[inds],
+            self.actions[inds],
+            self.rewards[inds],
+            self.next_obs[inds],
+            self.dones[inds],
+        )
+
+    def __len__(self):
+        """Get the size of the buffer."""
+        return self.size
diff --git a/morl-baselines/morl_baselines/common/diverse_buffer.py b/morl-baselines/morl_baselines/common/diverse_buffer.py
new file mode 100644
index 0000000..9eaa4ed
--- /dev/null
+++ b/morl-baselines/morl_baselines/common/diverse_buffer.py
@@ -0,0 +1,602 @@
+"""Diverse Experience Replay Buffer. Code extracted from https://github.com/axelabels/DynMORL."""
+from dataclasses import dataclass
+
+import numpy as np
+
+
+MAIN_TREE = 0
+
+
+class SumTree:
+    """Implementation of a SumTree with multiple trees covering the same data array.
+
+    Adapted from: https://github.com/jaara/AI-blog/blob/master/SumTree.py.
+    Each transition is stored along with its trace_id.
+    """
+
+    def __init__(self, capacity):
+        """Initializes the SumTree.
+
+        Args:
+            capacity: The maximum number of transitions to store in the SumTree
+        """
+        self.capacity = capacity
+        self.write = 0
+        self.trees = {}
+        self.main_tree = MAIN_TREE
+        self.data = np.repeat(((None, None, None),), capacity, axis=0)
+        self.updates = {}
+
+    def copy_tree(self, trg_i, src_i=MAIN_TREE):
+        """Copies src_i's priorities into a new tree trg_i.
+
+        Args:
+            trg_i:Target tree identifier
+            src_i: Source tree identifier (default: {MAIN_TREE})
+        """
+        if trg_i not in self.trees:
+            self.trees[trg_i] = np.copy(self.trees[src_i])
+            self.updates[trg_i] = 0
+
+    def create(self, i):
+        """Create tree i, either by copying the main tree if it exists, or by creating a new tree from scratch.
+
+        Args:
+            i: The new tree's identifier
+        """
+        if i is None:
+            i = self.main_tree
+        if i not in self.trees and self.main_tree in self.trees:
+            self.copy_tree(i, self.main_tree)
+        elif i not in self.trees:
+            self.trees[i] = np.zeros(2 * self.capacity - 1)
+            self.updates[i] = 0
+
+    def _propagate(self, idx: int, change: float, tree_id=None):
+        """Propagate priority changes to root.
+
+        Args:
+            idx: Node to propagate from
+            change: Priority difference to propagate
+            tree_id: Which tree the change applies to)
+        """
+        tree_id = tree_id if tree_id is not None else self.main_tree
+
+        parent = (idx - 1) // 2
+
+        self.trees[tree_id][parent] += change
+
+        if parent != 0:
+            self._propagate(parent, change, tree_id)
+
+    def _retrieve(self, idx: int, s: float, tree_id=None):
+        """Retrieve the node covering offset s starting from node idx.
+
+        Args:
+            idx: Note to start from
+            s: offset
+            tree_id: Which tree the priorities relate to
+
+        Returns:
+            node covering the offset
+        """
+        tree_id = tree_id if tree_id is not None else self.main_tree
+
+        left = 2 * idx + 1
+        right = left + 1
+        if left >= len(self.trees[tree_id]):
+            return idx
+
+        if s <= self.trees[tree_id][left]:
+            return self._retrieve(left, s, tree_id)
+        else:
+            return self._retrieve(right, s - self.trees[tree_id][left], tree_id)
+
+    def total(self, tree_id=None):
+        """Returns the tree's total priority.
+
+        Args:
+            tree_id: Tree identifier
+
+        Returns:
+            Total priority
+        """
+        tree_id = tree_id if tree_id is not None else self.main_tree
+
+        return self.trees[tree_id][0]
+
+    def average(self, tree_id=None):
+        """Return the tree's average priority, assumes the tree is full.
+
+        Args:
+            tree_id: Tree identifier
+
+        Returns:
+            Average priority
+        """
+        return self.total(tree_id) / self.capacity
+
+    def add(self, priorities: dict, data: tuple, write=None):
+        """Adds a data sample to the SumTree at position write.
+
+        Args:
+            priorities: Dictionary of priorities, one key per tree
+            data: Transition to be added
+            write: Position to write to
+
+        Returns:
+            Tuple containing the replaced data as well as the node's index in the tree.
+        """
+        write = write if write is not None else self.write
+        idx = write + self.capacity - 1
+
+        # Save replaced data to eventually save in secondary memory
+        replaced_data = np.copy(self.data[write])
+        replaced_priorities = {tree: np.copy(self.trees[tree][idx]) for tree in self.trees}
+        replaced = (replaced_data, replaced_priorities)
+
+        # Set new priorities
+        for i, p in priorities.items():
+            self.update(idx, p, i)
+
+        # Set new data
+        self.data[write] = data
+        return replaced, idx
+
+    def update(self, idx: int, p, tree_id=None):
+        """For a given index, update priorities for the given trees.
+
+        Args:
+            idx: Node's position in the tree
+            p: Dictionary of priorities or priority for the given tree_id
+            tree_id: Tree to be updated
+
+        Keyword Arguments:
+            tree_id {object} -- Tree to be updated (default: {None})
+        """
+        if type(p) == dict:
+            for k in p:
+                self.update(idx, p[k], k)
+            return
+        tree_id = tree_id if tree_id is not None else self.main_tree
+
+        change = p - self.trees[tree_id][idx]
+
+        self.trees[tree_id][idx] = p
+        self._propagate(idx, change, tree_id)
+
+    def get(self, s: float, tree_id=None):
+        """Get the node covering the given offset.
+
+        Args:
+            s: Offset to retrieve
+            tree_id: Tree to retrieve from
+
+        Returns:
+            Containing the index, the priority and the transition
+        """
+        tree_id = tree_id if tree_id is not None else self.main_tree
+
+        idx = self._retrieve(0, s, tree_id)
+
+        return self.get_by_id(idx, tree_id)
+
+    def get_by_id(self, idx: int, tree_id=None):
+        """Get the node at the given index.
+
+        Args:
+            idx: Index to retrieve
+            tree_id: Tree to retrieve from
+
+        Returns:
+            A tuple containing the index, the priority and the transition
+        """
+        tree_id = tree_id if tree_id is not None else self.main_tree
+        dataIdx = idx - self.capacity + 1
+
+        return idx, self.trees[tree_id][idx], self.data[dataIdx]
+
+
+class DiverseMemory:
+    """Prioritized Replay Buffer with integrated secondary Diverse Replay Buffer. Code extracted from https://github.com/axelabels/DynMORL."""
+
+    def __init__(
+        self,
+        main_capacity: int,
+        sec_capacity: int = 0,
+        trace_diversity: bool = True,
+        crowding_diversity: bool = True,
+        value_function=lambda trace, trace_id, memory_indices: np.random.random(1),
+        e: float = 0.01,
+        a: float = 2,
+    ):
+        """Initializes the DiverseMemory.
+
+        Args:
+            main_capacity: Normal prioritized replay capacity
+            sec_capacity: Size of the secondary diverse replay buffer, if 0, the buffer functions as a normal prioritized Replay Buffer (default: {0})
+            trace_diversity: Whether diversity should be enforced at trace-level (True) or at transition-level (False)
+            crowding_diversity: Whether a crowding distance is applied to compute diversity
+            value_function: When applied to a trace, this function should return the trace's value to be used in the crowding distance computation
+            e: epsilon to be added to errors (default: {0.01})
+            a: Power to which the error will be raised, if a==0, functionality is reduced to a replay buffer without prioritization (default: {2})
+        """
+        self.len = 0
+        self.trace_diversity = trace_diversity
+        self.value_function = value_function
+        self.crowding_diversity = crowding_diversity
+        self.e = e
+        self.a = a
+        self.capacity = main_capacity + sec_capacity
+        self.tree = SumTree(self.capacity)
+        self.main_capacity = main_capacity
+        self.sec_capacity = sec_capacity
+        self.secondary_traces = []
+
+    def _getPriority(self, error):
+        """Compute priority from error.
+
+        Args:
+            error {float} -- error
+
+        Returns:
+            float -- priority
+        """
+        return (error + self.e) ** self.a
+
+    def _getError(self, priority):
+        """Given a priority, computes the corresponding error.
+
+        Args:
+            priority {float} -- priority
+
+        Returns:
+            float -- error
+        """
+        return priority ** (1 / self.a) - self.e
+
+    def dupe(self, trg_i, src_i):
+        """Copies the tree src_i into a new tree trg_i.
+
+        Args:
+            trg_i: target tree identifier
+            src_i: source tree identifier
+        """
+        self.tree.copy_tree(trg_i, src_i)
+
+    def main_mem_is_full(self):
+        """Because of the circular way in which we fill the memory, checking whether the current write position is free is sufficient to know if the memory is full."""
+        return self.tree.data[self.tree.write][1] is not None
+
+    def extract_trace(self, start: int):
+        """Determines the end of the trace starting at position start.
+
+        Args:
+            start: Trace's starting position
+
+        Returns:
+            The trace's end position
+        """
+        trace_id = self.tree.data[start][0]
+
+        end = (start + 1) % self.main_capacity
+
+        if not self.trace_diversity:
+            return end
+        if trace_id is not None:
+            while self.tree.data[end][0] == trace_id:
+                end = (end + 1) % self.main_capacity
+                if end == start:
+                    break
+
+        return end
+
+    def add(self, error, sample, trace_id=None, pred_idx=None, tree_id=None):
+        """Add the sample to the replay buffer, with a priority proportional to its error. If trace_id is provided, the sample and the other samples with the same id will be treated as a trace when determining diversity.
+
+        Args:
+            error: Error
+            sample: The transition to be stored
+            trace_id: The trace's identifier (default: {None})
+            tree_id: The tree for which the error is relevant (default: {None})
+
+        Returns:
+            The index of the node in which the sample was stored
+        """
+        self.len = min(self.len + 1, self.capacity)
+        self.tree.create(tree_id)
+
+        sample = (
+            trace_id,
+            sample,
+            None if pred_idx is None else (pred_idx - self.capacity + 1),
+        )
+
+        # Free up space in main memory if necessary
+        if self.main_mem_is_full():
+            end = self.extract_trace(self.tree.write)
+            self.move_to_sec(self.tree.write, end)
+
+        # Save sample into main memory
+        if type(error) is not dict:
+            error = {tree_id: error}
+        idx = self.add_sample(sample, error, self.tree.write)
+        self.tree.write = (self.tree.write + 1) % self.main_capacity
+
+        return idx
+
+    def remove_trace(self, trace):
+        """Removes the trace from the main memory.
+
+        Args:
+            trace: List of indices for the trace
+        """
+        _, trace_idx = trace
+        for i in trace_idx:
+            self.tree.data[i] = (None, None, None)
+
+            idx = i + self.tree.capacity - 1
+            for tree in self.tree.trees:
+                self.tree.update(idx, 0, tree)
+
+    def get_trace_value(self, trace_tuple):
+        """Applies the value_function to the trace's data to compute its value.
+
+        Args:
+            trace_tuple: Tuple containing the trace and the trace's indices
+
+        Returns:
+            The trace's value
+        """
+        trace, write_indices = trace_tuple
+        if not self.trace_diversity:
+            assert len(trace) == 1
+        trace_id = trace[0][0]
+        trace_data = [t[1] for t in trace]
+
+        return self.value_function(trace_data, trace_id, write_indices)
+
+    def sec_distances(self, traces):
+        """Give a set of traces, this method computes each trace's crowding distance.
+
+        Args:
+            traces: List of trace tuples
+
+        Returns:
+            List of distances
+        """
+        values = [self.get_trace_value(tr) for tr in traces]
+        if self.crowding_diversity:
+            distances = crowd_dist(values)
+        else:
+            distances = values
+        return [(i, d) for i, d in enumerate(distances)], values
+
+    def get_sec_write(self, secondary_traces, trace, reserved_idx=None):
+        """Given a trace, find free spots in the secondary memory to store it by recursively removing past traces with a low crowding distance."""
+        if reserved_idx is None:
+            reserved_idx = []
+
+        if len(trace) > self.sec_capacity:
+            return None
+
+        if len(reserved_idx) >= len(trace):
+            return reserved_idx[: len(trace)]
+
+        # Find free spots in the secondary memory
+        # TODO: keep track of free spots so recomputation isn't necessary
+        free_spots = [
+            i + self.main_capacity for i in range(self.sec_capacity) if (self.tree.data[self.main_capacity + i][1]) is None
+        ]
+
+        if len(free_spots) > len(reserved_idx):
+            return self.get_sec_write(secondary_traces, trace, free_spots[: len(trace)])
+
+        # Get crowding distance of traces stored in the secondary buffer
+        idx_dist, _ = self.sec_distances(secondary_traces)
+
+        # Highest density = lowest distance
+        i, _ = min(idx_dist, key=lambda d: d[1])
+
+        _, trace_idx = secondary_traces[i]
+        reserved_idx += trace_idx
+
+        self.remove_trace(secondary_traces[i])
+
+        del secondary_traces[i]
+        return self.get_sec_write(secondary_traces, trace, reserved_idx)
+
+    def move_to_sec(self, start: int, end: int):
+        """Move the trace spanning from start to end to the secondary replay buffer.
+
+        Args:
+            start: Start position of the trace
+            end: End position of the trace
+        """
+        # Recover trace that needs to be moved
+        if end <= start:
+            indices = np.r_[start : self.main_capacity, 0:end]
+        else:
+            indices = np.r_[start:end]
+        if not self.trace_diversity:
+            assert len(indices) == 1
+        trace = np.copy(self.tree.data[indices])
+        priorities = {tree_id: self.tree.trees[tree_id][indices + self.tree.capacity - 1] for tree_id in self.tree.trees}
+
+        # Get destination indices in secondary memory
+        write_indices = self.get_sec_write(self.secondary_traces, trace)
+
+        # Move trace to secondary memory if enough space was freed
+        if write_indices is not None and len(write_indices) >= len(trace):
+            for i, (w, t) in enumerate(zip(write_indices, trace)):
+
+                self.tree.data[w] = t
+
+                idx = w + self.tree.capacity - 1
+                for tree_id in priorities:
+                    p = priorities[tree_id][i]
+                    self.tree.update(idx, p, tree_id)
+
+                if i > 0:
+                    self.tree.data[w][2] = write_indices[i - 1]
+            if not self.trace_diversity:
+                assert len(trace) == 1
+            self.secondary_traces.append((trace, write_indices))
+        # elif self.sec_capacity>0:
+        #     print("No space found for trace", trace[0][0],", discarding...",file=sys.stderr)
+
+        # Remove trace from main memory
+        self.remove_trace((None, indices))
+
+    def add_sample(self, transition, error, write=None):
+        """Stores the transition into the priority tree.
+
+        Args:
+            transition: Tuple containing the trace id, the sample and the previous sample's index
+            error: Dictionary containing the error for each tree
+            write: Index to write the transition to
+        """
+        p = {k: self._getPriority(error[k]) for k in error}
+        _, idx = self.tree.add(p, transition, write=write)
+        return idx
+
+    def add_tree(self, tree_id):
+        """Adds a secondary priority tree.
+
+        Args:
+            tree_id: The secondary tree's id
+        """
+        self.tree.create(tree_id)
+
+    def get_data(self, include_indices: bool = False):
+        """Get all the data stored in the replay buffer.
+
+        Args:
+            include_indices: Whether to include each sample's position in the replay buffer (default: {False})
+
+        Returns:
+            The data
+        """
+        all_data = list(np.arange(self.capacity) + self.capacity - 1), list(self.tree.data)
+        indices = []
+        data = []
+        for i, d in zip(all_data[0], all_data[1]):
+            if (d[1]) is not None:
+                indices.append(i)
+                data.append(d[1])
+        if include_indices:
+            return indices, data
+        else:
+            return data
+
+    def sample(self, n: int, tree_id=None):
+        """Sample n transitions from the replay buffer, following the priorities of the tree identified by tree_id.
+
+        Args:
+            n: Number of transitions to sample
+            tree_id: identifier of the tree whose priorities should be followed (default: {None})
+
+        Returns:
+            pair of (indices, transitions)
+        """
+        if n < 1:
+            return None, None, None
+        batch = np.zeros((n,), dtype=np.ndarray)
+        ids = np.zeros(n, dtype=int)
+        priorities = np.zeros(n, dtype=float)
+        segment = self.tree.total(tree_id) / n
+        for i in range(n):
+            a = segment * i
+            b = segment * (i + 1)
+
+            s = np.random.uniform(a, b)
+            (idx, p, data) = self.tree.get(s, tree_id)
+            while (data[1]) is None or (idx - self.capacity + 1 >= self.capacity):
+                s = np.random.uniform(0, self.tree.total(tree_id))
+                (idx, p, data) = self.tree.get(s, tree_id)
+            ids[i] = idx
+            batch[i] = data[1]
+            priorities[i] = p
+        return ids, batch, priorities
+
+    def update(self, idx: int, error: float, tree_id=None):
+        """Given a node's idx, this method updates the corresponding priority in the tree identified by tree_id.
+
+        Args:
+            idx: Node's index
+            error: New error
+            tree_id: Identifies the tree to update (default: {None})
+        """
+        if tree_id is None:
+            tree_id = self.tree.main_tree
+        if type(error) is not dict:
+            error = {tree_id: error}
+        p = {k: self._getPriority(error[k]) for k in error}
+        self.tree.update(idx, p, tree_id)
+
+    def get(self, indices: list):
+        """Given a list of node indices, this method returns the data stored at those indices.
+
+        Args:
+            indices: List of indices
+
+        Returns:
+            array of transitions
+        """
+        indices = np.array(indices, dtype=int) - self.capacity + 1
+        return self.tree.data[indices][:, 1]
+
+    def get_error(self, idx, tree_id=None):
+        """Given a node's idx, this method returns the corresponding error in the tree identified by tree_id.
+
+        Args:
+            idx: Node's index
+            tree_id: Identifies the tree to update (default: {None})
+
+        Returns:
+            Error
+        """
+        tree_id = self.tree.main_tree if tree_id is None else tree_id
+        priority = self.tree.trees[tree_id][idx]
+        return self._getError(priority)
+
+
+def crowd_dist(evals: list):
+    """Given a list of vectors, this method computes the crowding distance of each vector, i.e. the sum of distances between neighbors for each dimension.
+
+    Args:
+        evals: list of vectors
+
+    Returns:
+        list of crowding distances
+    """
+
+    @dataclass
+    class Point:
+        data: np.ndarray
+        distance: float
+        i: int
+
+    points = np.array([Point() for _ in evals])
+    dimensions = len(evals[0])
+    for i, d in enumerate(evals):
+        points[i].data = d
+        points[i].i = i
+        points[i].distance = 0.0
+
+    # Compute the distance between neighbors for each dimension and add it to
+    # each point's global distance
+    for d in range(dimensions):
+        points = sorted(points, key=lambda p: p.data[d])
+        spread = points[-1].data[d] - points[0].data[d]
+        for i, p in enumerate(points):
+            if i == 0 or i == len(points) - 1:
+                p.distance += float("inf")
+            else:
+                p.distance += (points[i + 1].data[d] - points[i - 1].data[d]) / spread
+
+    # Sort points back to their original order
+    points = sorted(points, key=lambda p: p.i)
+    distances = np.array([p.distance for p in points])
+
+    return distances
diff --git a/morl-baselines/morl_baselines/common/evaluation.py b/morl-baselines/morl_baselines/common/evaluation.py
new file mode 100644
index 0000000..4a7f4ca
--- /dev/null
+++ b/morl-baselines/morl_baselines/common/evaluation.py
@@ -0,0 +1,124 @@
+"""Utilities related to evaluation."""
+
+from typing import Optional, Tuple
+
+import numpy as np
+
+
+def eval_mo(
+    agent,
+    env,
+    w: Optional[np.ndarray] = None,
+    scalarization=np.dot,
+    render: bool = False,
+) -> Tuple[float, float, np.ndarray, np.ndarray]:
+    """Evaluates one episode of the agent in the environment.
+
+    Args:
+        agent: Agent
+        env: MO-Gymnasium environment with LinearReward wrapper
+        scalarization: scalarization function, taking weights and reward as parameters
+        w (np.ndarray): Weight vector
+        render (bool, optional): Whether to render the environment. Defaults to False.
+
+    Returns:
+        (float, float, np.ndarray, np.ndarray): Scalarized return, scalarized discounted return, vectorized return, vectorized discounted return
+    """
+    obs, _ = env.reset()
+    done = False
+    vec_return, disc_vec_return = np.zeros_like(w), np.zeros_like(w)
+    gamma = 1.0
+    while not done:
+        if render:
+            env.render(mode="human")
+        obs, r, terminated, truncated, info = env.step(agent.eval(obs, w))
+        done = terminated or truncated
+        vec_return += r
+        disc_vec_return += gamma * r
+        gamma *= agent.gamma
+
+    if w is None:
+        scalarized_return = scalarization(vec_return)
+        scalarized_discounted_return = scalarization(disc_vec_return)
+    else:
+        scalarized_return = scalarization(w, vec_return)
+        scalarized_discounted_return = scalarization(w, disc_vec_return)
+
+    return (
+        scalarized_return,
+        scalarized_discounted_return,
+        vec_return,
+        disc_vec_return,
+    )
+
+
+def eval_mo_reward_conditioned(
+    agent,
+    env,
+    scalarization=np.dot,
+    w: Optional[np.ndarray] = None,
+    render: bool = False,
+) -> Tuple[float, float, np.ndarray, np.ndarray]:
+    """Evaluates one episode of the agent in the environment. This makes the assumption that the agent is conditioned on the accrued reward i.e. for ESR agent.
+
+    Args:
+        agent: Agent
+        env: MO-Gymnasium environment
+        scalarization: scalarization function, taking weights and reward as parameters
+        w: weight vector
+        render (bool, optional): Whether to render the environment. Defaults to False.
+
+    Returns:
+        (float, float, np.ndarray, np.ndarray): Scalarized return, scalarized discounted return, vectorized return, vectorized discounted return
+    """
+    obs, _ = env.reset()
+    done = False
+    vec_return, disc_vec_return = np.zeros(env.reward_space.shape[0]), np.zeros(env.reward_space.shape[0])
+    gamma = 1.0
+    while not done:
+        if render:
+            env.render(mode="human")
+        obs, r, terminated, truncated, info = env.step(agent.eval(obs, disc_vec_return))
+        done = terminated or truncated
+        vec_return += r
+        disc_vec_return += gamma * r
+        gamma *= agent.gamma
+    if w is None:
+        scalarized_return = scalarization(vec_return)
+        scalarized_discounted_return = scalarization(disc_vec_return)
+    else:
+        scalarized_return = scalarization(w, vec_return)
+        scalarized_discounted_return = scalarization(w, disc_vec_return)
+
+    return (
+        scalarized_return,
+        scalarized_discounted_return,
+        vec_return,
+        disc_vec_return,
+    )
+
+
+def policy_evaluation_mo(agent, env, w: np.ndarray, rep: int = 5) -> Tuple[float, float, np.ndarray, np.ndarray]:
+    """Evaluates the value of a policy by running the policy for multiple episodes. Returns the average returns.
+
+    Args:
+        agent: Agent
+        env: MO-Gymnasium environment
+        w (np.ndarray): Weight vector
+        rep (int, optional): Number of episodes for averaging. Defaults to 5.
+
+    Returns:
+        (float, float, np.ndarray, np.ndarray): Avg scalarized return, Avg scalarized discounted return, Avg vectorized return, Avg vectorized discounted return
+    """
+    evals = [eval_mo(agent, env, w) for _ in range(rep)]
+    avg_scalarized_return = np.mean([eval[0] for eval in evals])
+    avg_scalarized_discounted_return = np.mean([eval[1] for eval in evals])
+    avg_vec_return = np.mean([eval[2] for eval in evals], axis=0)
+    avg_disc_vec_return = np.mean([eval[3] for eval in evals], axis=0)
+
+    return (
+        avg_scalarized_return,
+        avg_scalarized_discounted_return,
+        avg_vec_return,
+        avg_disc_vec_return,
+    )
diff --git a/morl-baselines/morl_baselines/common/model_based/__init__.py b/morl-baselines/morl_baselines/common/model_based/__init__.py
new file mode 100644
index 0000000..ae01522
--- /dev/null
+++ b/morl-baselines/morl_baselines/common/model_based/__init__.py
@@ -0,0 +1 @@
+"""Model-Based utils."""
diff --git a/morl-baselines/morl_baselines/common/model_based/probabilistic_ensemble.py b/morl-baselines/morl_baselines/common/model_based/probabilistic_ensemble.py
new file mode 100644
index 0000000..db40b5f
--- /dev/null
+++ b/morl-baselines/morl_baselines/common/model_based/probabilistic_ensemble.py
@@ -0,0 +1,289 @@
+"""Probabilistic ensemble of neural networks."""
+import os
+
+import numpy as np
+import torch as th
+from torch import nn as nn
+from torch.nn import functional as F
+
+
+class EnsembleLayer(nn.Module):
+    """Ensemble layer."""
+
+    def __init__(self, ensemble_size, input_dim, output_dim):
+        """Initialize the ensemble layer."""
+        super().__init__()
+        self.W = nn.Parameter(th.empty((ensemble_size, input_dim, output_dim)), requires_grad=True).float()
+        nn.init.orthogonal_(self.W, gain=nn.init.calculate_gain("relu"))
+        self.b = nn.Parameter(th.zeros((ensemble_size, 1, output_dim)), requires_grad=True).float()
+
+    def forward(self, x):
+        """Forward pass of the ensemble layer."""
+        # assumes x is 3D: (ensemble_size, batch_size, dimension)
+        return x @ self.W + self.b
+
+
+class ProbabilisticEnsemble(nn.Module):
+    """Probababilistic ensemble of neural networks."""
+
+    def __init__(
+        self,
+        input_dim,
+        output_dim,
+        ensemble_size=5,
+        arch=[200, 200, 200, 200],
+        activation=F.relu,
+        learning_rate=0.001,
+        num_elites=2,
+        normalize_inputs=True,
+        device="auto",
+    ):
+        """Initialize the ensemble.
+
+        Args:
+            input_dim (int): dimension of the input
+            output_dim (int): dimension of the output
+            ensemble_size (int): number of networks in the ensemble
+            arch (list): list of hidden layer sizes
+            activation (function): activation function
+            learning_rate (float): learning rate
+            num_elites (int): number of elite networks
+            normalize_inputs (bool): whether to normalize inputs
+            device (str): device to use for training
+        """
+        super().__init__()
+
+        self.ensemble_size = ensemble_size
+        self.input_dim = input_dim
+        self.output_dim = output_dim * 2  # mean and std
+        self.activation = activation
+        self.arch = arch
+        self.num_elites = num_elites
+        self.elites = [i for i in range(self.ensemble_size)]
+        self.normalize_inputs = normalize_inputs
+        self.learning_rate = learning_rate
+
+        self.layers = nn.ModuleList()
+        in_size = input_dim
+        for hidden_size in self.arch:
+            self.layers.append(EnsembleLayer(ensemble_size, in_size, hidden_size))
+            in_size = hidden_size
+        self.layers.append(EnsembleLayer(ensemble_size, self.arch[-1], self.output_dim))
+
+        if self.normalize_inputs:
+            self.inputs_mu = nn.Parameter(th.zeros((1, input_dim)), requires_grad=False)
+            self.inputs_sigma = nn.Parameter(th.zeros((1, input_dim)), requires_grad=False)
+
+        self.max_logvar = nn.Parameter(th.ones(1, output_dim, dtype=th.float32) / 2.0)
+        self.min_logvar = nn.Parameter(-th.ones(1, output_dim, dtype=th.float32) * 10.0)
+
+        if device == "auto":
+            self.device = th.device("cuda") if th.cuda.is_available() else th.device("cpu")
+        else:
+            self.device = device
+        self.to(self.device)
+
+    def forward(self, input, deterministic=False, return_dist=False):
+        """Forward pass through the ensemble."""
+        dim = len(input.shape)
+        # input normalization
+        if self.normalize_inputs:
+            h = (input - self.inputs_mu) / self.inputs_sigma
+        else:
+            h = input
+        # repeat h to make amenable to parallelization
+        # if dim = 3, then we probably already did this somewhere else (e.g. bootstrapping in training optimization)
+        if dim < 3:
+            h = h.unsqueeze(0)
+            if dim == 1:
+                h = h.unsqueeze(0)
+            h = h.repeat(self.ensemble_size, 1, 1)
+
+        for layer in self.layers[:-1]:
+            h = layer(h)
+            h = self.activation(h)
+        output = self.layers[-1](h)
+
+        # if original dim was 1D, squeeze the extra created layer
+        if dim == 1:
+            output = output.squeeze(1)  # output is (ensemble_size, output_size)
+
+        mean, logvar = th.chunk(output, 2, dim=-1)
+
+        # Variance clamping to prevent poor numerical predictions
+        logvar = self.max_logvar - F.softplus(self.max_logvar - logvar)
+        logvar = self.min_logvar + F.softplus(logvar - self.min_logvar)
+
+        if deterministic:
+            if return_dist:
+                return mean, logvar
+            else:
+                return mean
+        else:
+            std = th.exp(0.5 * logvar)  # exp(0.5*logvar) = sqrt(exp(logvar))
+            samples = mean + std * th.randn(std.shape, device=std.device)
+            if return_dist:
+                return samples, mean, logvar
+            else:
+                return samples
+
+    def sample(self, input, deterministic=False):
+        """Sample from the ensemble."""
+        if not deterministic:
+            samples, means, logvar = self.forward(input, deterministic=False, return_dist=True)
+            samples = samples.detach().cpu().numpy()
+        else:
+            means, logvar = self.forward(input, deterministic=True, return_dist=True)
+        means = means.detach().cpu().numpy()
+        logvar = logvar.detach().cpu().numpy()
+        vars = np.exp(logvar)
+        num_models, batch_size, _ = means.shape
+        batch_inds = np.arange(0, batch_size)
+        model_inds = np.random.choice(self.elites, size=batch_size)
+
+        # Ensemble Standard Deviation/Variance (Lakshminarayanan et al., 2017)
+        mean_ensemble = means.mean(axis=0)
+        var_ensemble = (means**2 + vars).mean(axis=0) - mean_ensemble**2
+        std_ensemble = np.sqrt(var_ensemble + 1e-12)
+        uncertainties = std_ensemble.sum(-1)
+
+        if deterministic:
+            return means[model_inds, batch_inds], vars[model_inds, batch_inds], uncertainties
+        else:
+            return samples[model_inds, batch_inds], vars[model_inds, batch_inds], uncertainties
+
+    def _compute_loss(self, x, y):
+        mean, logvar = self.forward(x, deterministic=True, return_dist=True)
+
+        if len(y.shape) < 3:
+            y = y.unsqueeze(0).repeat(self.ensemble_size, 1, 1)
+
+        var = th.exp(logvar)
+        total_losses = F.gaussian_nll_loss(mean, y, var, reduction="none")
+        total_losses = total_losses.mean()
+
+        total_losses += 0.01 * self.max_logvar.sum() - 0.01 * self.min_logvar.sum()
+
+        return total_losses
+
+    def _compute_mse_losses(self, x, y):
+        mean = self.forward(x, deterministic=True, return_dist=False)
+        if len(y.shape) < 3:
+            y = y.unsqueeze(0).repeat(self.ensemble_size, 1, 1)
+        mse_losses = (mean - y) ** 2
+        return mse_losses.mean(-1).mean(-1)
+
+    def save(self, path):
+        """Saves the model to a file."""
+        save_dir = "weights/"
+        if not os.path.isdir(save_dir):
+            os.makedirs(save_dir)
+        th.save({"ensemble_state_dict": self.state_dict()}, path + ".tar")
+
+    def load(self, path):
+        """Loads the model from a file."""
+        params = th.load(path)
+        self.load_state_dict(params["ensemble_state_dict"])
+
+    def _fit_input_stats(self, data):
+        mu = np.mean(data, axis=0, keepdims=True)
+        sigma = np.std(data, axis=0, keepdims=True)
+        sigma[sigma < 1e-12] = 1.0
+        self.inputs_mu.data = th.tensor(mu).to(self.device).float()
+        self.inputs_sigma.data = th.tensor(sigma).to(self.device).float()
+
+    def fit(
+        self,
+        X,
+        Y,
+        batch_size=256,
+        holdout_ratio=0.1,
+        max_holdout_size=5000,
+        max_epochs_no_improvement=5,
+        max_epochs=200,
+    ):
+        """Trains the model on the given data.
+
+        Args:
+            X: Input data
+            Y: Output data
+            batch_size: Batch size
+            holdout_ratio: Ratio of data to use for early stopping
+            max_holdout_size: Maximum number of samples to use for early stopping
+            max_epochs_no_improvement: Maximum number of epochs without improvement before early stopping
+            max_epochs: Maximum number of epochs to train for
+
+        Returns:
+            _type_: _description_
+        """
+        if self.normalize_inputs:
+            self._fit_input_stats(X)
+
+        self.decays = [0.000025, 0.00005, 0.000075, 0.000075, 0.0001]
+        self.optim = th.optim.Adam(
+            [{"params": self.layers[i].parameters(), "weight_decay": self.decays[i]} for i in range(len(self.layers))]
+            + [{"params": self.max_logvar}, {"params": self.min_logvar}],
+            lr=self.learning_rate,
+        )
+
+        num_holdout = min(int(X.shape[0] * holdout_ratio), max_holdout_size)
+        permutation = np.random.permutation(X.shape[0])
+        inputs, holdout_inputs = (
+            X[permutation[num_holdout:]],
+            X[permutation[:num_holdout]],
+        )
+        targets, holdout_targets = (
+            Y[permutation[num_holdout:]],
+            Y[permutation[:num_holdout]],
+        )
+        holdout_inputs = th.from_numpy(holdout_inputs).to(self.device).float()
+        holdout_targets = th.from_numpy(holdout_targets).to(self.device).float()
+
+        idxs = np.random.randint(inputs.shape[0], size=[self.ensemble_size, inputs.shape[0]])
+        num_batches = int(np.ceil(idxs.shape[-1] / batch_size))
+
+        def shuffle_rows(arr):
+            idxs = np.argsort(np.random.uniform(size=arr.shape), axis=-1)
+            return arr[np.arange(arr.shape[0])[:, None], idxs]
+
+        num_epochs_no_improvement = 0
+        epoch = 0
+        best_holdout_losses = [float("inf") for _ in range(self.ensemble_size)]
+        while num_epochs_no_improvement < max_epochs_no_improvement and epoch < max_epochs:
+            self.train()
+            for batch_num in range(num_batches):
+                batch_idxs = idxs[:, batch_num * batch_size : (batch_num + 1) * batch_size]
+                batch_x, batch_y = inputs[batch_idxs], targets[batch_idxs]
+                batch_x, batch_y = (
+                    th.from_numpy(batch_x).to(self.device).float(),
+                    th.from_numpy(batch_y).to(self.device).float(),
+                )
+
+                loss = self._compute_loss(batch_x, batch_y)
+                self.optim.zero_grad()
+                loss.backward()
+                self.optim.step()
+
+            idxs = shuffle_rows(idxs)
+
+            self.eval()
+            with th.no_grad():
+                holdout_losses = self._compute_mse_losses(holdout_inputs, holdout_targets)
+            holdout_losses = [l.item() for l in holdout_losses]
+            # print('Epoch:', epoch, 'Holdout losses:', [l.item() for l in holdout_losses])
+
+            self.elites = np.argsort(holdout_losses)[: self.num_elites]
+
+            improved = False
+            for i in range(self.ensemble_size):
+                if epoch == 0 or (best_holdout_losses[i] - holdout_losses[i]) / (best_holdout_losses[i]) > 0.01:
+                    best_holdout_losses[i] = holdout_losses[i]
+                    num_epochs_no_improvement = 0
+                    improved = True
+            if not improved:
+                num_epochs_no_improvement += 1
+
+            epoch += 1
+
+        print("Epoch:", epoch, "Holdout losses:", ", ".join(["%.4f" % hl for hl in holdout_losses]))
+        return np.mean(holdout_losses)
diff --git a/morl-baselines/morl_baselines/common/model_based/tabular_model.py b/morl-baselines/morl_baselines/common/model_based/tabular_model.py
new file mode 100644
index 0000000..dc3a571
--- /dev/null
+++ b/morl-baselines/morl_baselines/common/model_based/tabular_model.py
@@ -0,0 +1,123 @@
+"""Tabular dynamics model S_{t+1}, R_t ~ m(.,.|s,a) ."""
+import random
+
+import numpy as np
+
+from morl_baselines.common.prioritized_buffer import SumTree
+
+
+class TabularModel:
+    """Tabular dynamics model S_{t+1}, R_t ~ m(.,.|s,a) ."""
+
+    def __init__(self, deterministic: bool = False, prioritize=False, max_size=int(1e5)) -> None:
+        """Initialize the model.
+
+        Args:
+            deterministic: If True, the model is deterministic and the next state and reward are stored directly.
+            prioritize: If True, the transitions are stored in a prioritized buffer.
+            max_size: The maximum size of the prioritized buffer.
+        """
+        self.deterministic = deterministic
+        self.model = dict()
+        self.state_actions_pairs = list()
+        self.prioritize = prioritize
+        if self.prioritize:
+            self.priorities = SumTree(max_size=max_size)
+            self.sa_to_ind = dict()
+
+    def update(self, state, action, reward, next_state, terminal, priority=None):
+        """Update the model with the given transition."""
+        sa = (tuple(state), int(action))
+        srt = (tuple(next_state), tuple(reward) if isinstance(reward, np.ndarray) else reward, terminal)
+
+        if sa not in self.model:
+            self.state_actions_pairs.append(sa)
+            if priority is not None:
+                self.priorities.set(len(self.state_actions_pairs) - 1, priority)
+                self.sa_to_ind[sa] = len(self.state_actions_pairs) - 1
+
+            if self.deterministic:
+                self.model[sa] = srt
+            else:
+                self.model[sa] = {srt: 1}
+        else:
+            if priority is not None:
+                self.priorities.set(self.sa_to_ind[sa], priority)
+
+            if not self.deterministic:
+                self.model[sa][srt] = self.model[sa].get(srt, 0) + 1
+
+    def predict(self, state, action):
+        """Return the next state, reward, and terminal flag for the given state and action."""
+        sa = (tuple(state), int(action))
+        if sa not in self.model:
+            return None, None, None
+
+        if self.deterministic:
+            next_state, reward, terminal = self.model[sa]
+        else:
+            next = list(self.model[sa].keys())
+            probs = np.array(list(self.model[sa].values()), dtype=np.float32)
+            next_state, reward, terminal = random.choices(next, weights=probs / probs.sum(), k=1)[0]
+        if isinstance(reward, tuple):
+            reward = np.array(reward)
+        return next_state, reward, terminal
+
+    def transitions(self, state, action):
+        """Return the transitions for the given state and action."""
+        sa = (tuple(state), int(action))
+        if sa not in self.model:
+            return [((None, None, None), None)]
+
+        if self.deterministic:
+            next_state, reward, terminal = self.model[sa]
+            return [((next_state, reward, terminal), 1.0)]
+        else:
+            next = list(self.model[sa].keys())
+            probs = np.array(list(self.model[sa].values()), dtype=np.float32)
+            probs /= probs.sum()
+            return list(zip(next, probs))
+
+    def probs(self, state, action):
+        """Return the probabilities of the transitions for the given state and action."""
+        sa = (tuple(state), int(action))
+        if self.deterministic or sa not in self.model:
+            return [1.0]
+        probs = np.array(list(self.model[sa].values()), dtype=np.float32)
+        probs /= probs.sum()
+        return probs
+
+    def random_transition(self):
+        """Sample a random transition from the model."""
+        if self.prioritize:
+            ind = self.priorities.sample(1)[0]
+        else:
+            ind = random.randint(0, len(self.state_actions_pairs) - 1)
+        sa = self.state_actions_pairs[ind]
+        if self.deterministic:
+            srt = self.model[sa]
+            if self.prioritize:
+                return sa[0], sa[1], np.array(srt[1]), srt[0], srt[2], ind
+            else:
+                return sa[0], sa[1], np.array(srt[1]), srt[0], srt[2]  # S A R S T
+        else:
+            next = list(self.model[sa].keys())
+            probs = np.array(list(self.model[sa].values()))
+            next_state, reward, terminal = random.choices(next, weights=probs / probs.sum(), k=1)[0]
+            if isinstance(reward, tuple):
+                reward = np.array(reward)
+            if self.prioritize:
+                return sa[0], sa[1], reward, next_state, terminal, ind
+            else:
+                return sa[0], sa[1], reward, next_state, terminal
+
+    def update_priority(self, ind, priority):
+        """Update priority of the transition at index ind.
+
+        Args:
+            ind (int): index of the transition
+            priority (float): new priority
+        """
+        self.priorities.set(ind, priority)
+        if ind > 0:
+            self.priorities.set(ind - 1, max(priority, self.priorities.get_priority(ind - 1)))
diff --git a/morl-baselines/morl_baselines/common/model_based/utils.py b/morl-baselines/morl_baselines/common/model_based/utils.py
new file mode 100644
index 0000000..0586a0a
--- /dev/null
+++ b/morl-baselines/morl_baselines/common/model_based/utils.py
@@ -0,0 +1,273 @@
+"""Utility functions for the model."""
+from typing import Tuple
+
+import matplotlib.pyplot as plt
+import numpy as np
+import seaborn as sns
+import torch as th
+import torch.nn.functional as F
+from gymnasium.spaces import Discrete
+
+
+def termination_fn_false(obs, act, next_obs):
+    """Returns a vector of False values of the same length as the batch size."""
+    assert len(obs.shape) == len(next_obs.shape) == len(act.shape) == 2
+    done = np.array([False]).repeat(len(obs))
+    done = done[:, np.newaxis]
+    return done
+
+
+def termination_fn_dst(obs, act, next_obs):
+    """Termination function of DST."""
+    from mo_gymnasium.deep_sea_treasure.deep_sea_treasure import CONCAVE_MAP
+
+    assert len(obs.shape) == len(next_obs.shape) == len(act.shape) == 2
+    done = np.array([False]).repeat(len(obs))
+    next_obs_int = (next_obs * 10).astype(int)
+    for i in range(len(done)):
+        if next_obs_int[i, 0] < 0 or next_obs_int[i, 0] > 10 or next_obs_int[i, 1] < 0 or next_obs_int[i, 1] > 10:
+            done[i] = False
+        else:
+            done[i] = CONCAVE_MAP[next_obs_int[i, 0]][next_obs_int[i, 1]] > 0.1
+    done = done[:, np.newaxis]
+    return done
+
+
+def termination_fn_mountaincar(obs, act, next_obs):
+    """Termination function of mountin car."""
+    assert len(obs.shape) == len(next_obs.shape) == len(act.shape) == 2
+    position = next_obs[:, 0]
+    velocity = next_obs[:, 1]
+    done = (position >= 0.45) * (velocity >= 0.0)
+    done = done[:, np.newaxis]
+    return done
+
+
+def termination_fn_minecart(obs, act, next_obs):
+    """Termination function of minecart."""
+    assert len(obs.shape) == len(next_obs.shape) == len(act.shape) == 2
+    old_pos = obs[:, 0:2]
+    pos = next_obs[:, 0:2]
+    # had_ore = (obs[:,-2] > 0) + (obs[:,-1] > 0)
+    in_base = np.sqrt(np.einsum("ij,ij->i", pos, pos)) < 0.15
+    was_out_base = np.sqrt(np.einsum("ij,ij->i", old_pos, old_pos)) >= 0.15
+    done = was_out_base * in_base
+    done = done[:, np.newaxis]
+    return done
+
+
+def termination_fn_hopper(obs, act, next_obs):
+    """Termination function of hopper."""
+    assert len(obs.shape) == len(next_obs.shape) == len(act.shape) == 2
+    height = next_obs[:, 0]
+    angle = next_obs[:, 1]
+    not_done = (
+        np.isfinite(next_obs).all(axis=-1)
+        * np.abs(next_obs[:, 1:] < 100).all(axis=-1)
+        * (height > 0.7)
+        * (np.abs(angle) < 0.2)
+    )
+    done = ~not_done
+    done = done[:, np.newaxis]
+    return done
+
+
+class ModelEnv:
+    """Wrapper for the model to be used as an environment."""
+
+    def __init__(self, model, env_id=None, rew_dim=1):
+        """Initialize the environment.
+
+        Args:
+            model: model to be used as an environment.
+            env_id: environment id.
+            rew_dim: reward dimension.
+        """
+        self.model = model
+        self.rew_dim = rew_dim
+        if env_id == "Hopper-v2" or env_id == "Hopper-v4" or env_id == "mo-hopper-v4":
+            self.termination_func = termination_fn_hopper
+        elif env_id == "HalfCheetah-v2" or env_id == "mo-halfcheetah-v4":
+            self.termination_func = termination_fn_false
+        elif env_id == "LunarLanderContinuous-v2":
+            self.termination_func = termination_fn_false
+        elif env_id == "ReacherMultiTask-v0" or env_id.startswith("mo-reacher-v"):
+            self.termination_func = termination_fn_false
+        elif env_id == "MountainCarContinuous-v0":
+            self.termination_func = termination_fn_mountaincar
+        elif env_id == "minecart-v0":
+            self.termination_func = termination_fn_minecart
+        elif env_id == "SEIRsingle-v0":
+            self.termination_func = termination_fn_false
+        elif env_id == "mo-highway-fast-v0" or env_id == "mo-highway-v0":
+            self.termination_func = termination_fn_false
+        elif env_id == "deep-sea-treasure-v0":
+            self.termination_func = termination_fn_dst
+        else:
+            raise NotImplementedError
+
+    def step(
+        self, obs: th.Tensor, act: th.Tensor, deterministic: bool = False
+    ) -> Tuple[np.ndarray, np.ndarray, np.ndarray, dict]:
+        """Step the environment.
+
+        Args:
+            obs (th.Tensor): current bservation.
+            act (th.Tensor): current action.
+            deterministic (bool): whether to use deterministic model prediction.
+
+        Returns:
+            Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]: next observation, reward, terminals, info.
+        """
+        assert len(obs.shape) == len(act.shape)
+        if len(obs.shape) == 1:
+            obs = obs.unsqueeze(0)
+            act = act.unsqueeze(0)
+            return_single = True
+        else:
+            return_single = False
+
+        inputs = th.cat((obs, act), dim=-1).float().to(self.model.device)
+        with th.no_grad():
+            samples, vars, uncertainties = self.model.sample(inputs, deterministic=deterministic)
+
+        obs = obs.detach().cpu().numpy()
+
+        samples[:, self.rew_dim :] += obs
+
+        rewards, next_obs = samples[:, : self.rew_dim], samples[:, self.rew_dim :]
+        terminals = self.termination_func(obs, act, next_obs)
+        var_rewards, var_obs = vars[:, : self.rew_dim], vars[:, self.rew_dim :]
+
+        if return_single:
+            next_obs = next_obs[0]
+            rewards = rewards[0]
+            terminals = terminals[0]
+            uncertainties = uncertainties[0]
+            var_obs = var_obs[0]
+            var_rewards = var_rewards[0]
+
+        info = {"uncertainty": uncertainties, "var_obs": var_obs, "var_rewards": var_rewards}
+
+        # info = {'mean': return_means, 'std': return_stds, 'log_prob': log_prob, 'dev': dev}
+        return next_obs, rewards, terminals, info
+
+
+def visualize_eval(
+    agent, env, model=None, w=None, horizon=10, init_obs=None, compound=True, deterministic=False, show=False, filename=None
+):
+    """Generates a plot of the evolution of the state, reward and model predicitions ove time.
+
+    Args:
+        agent: agent to be evaluated
+        env: environment to be evaluated
+        model: model to be evaluated
+        w: weights to be used for the evaluation
+        horizon: number of time steps
+        init_obs: initial observation
+        compound: whether to use compound model predictions
+        deterministic: whether to use deterministic model predictions
+        show: whether to show the plot
+        filename: filename to save the plot
+
+    Returns:
+        plt: plt object with the figure
+    """
+    if init_obs is None:
+        init_obs, _ = env.reset()
+    obs_dim = env.observation_space.shape[0]
+    actions = []
+    real_obs = []
+    real_rewards = []
+    real_vec_rewards = []
+    obs = init_obs.copy()
+    for step in range(horizon):
+        if w is not None:
+            act = agent.eval(obs, w)
+        else:
+            act = agent.eval(obs)
+        actions.append(act)
+        obs, r, terminated, truncated, info = env.step(act)
+        done = terminated or truncated
+        real_obs.append(obs.copy())
+        if type(r) is float:
+            real_rewards.append(r)
+        else:
+            real_rewards.append(np.dot(r, w))
+        if "vector_reward" in info:
+            real_vec_rewards.append(info["vector_reward"])
+        elif type(r) is np.ndarray:
+            real_vec_rewards.append(r)
+        if done:
+            break
+
+    model_obs = []
+    model_obs_stds = []
+    model_rewards_stds = []
+    model_rewards = []
+    if model is not None:
+        obs = init_obs.copy()
+        model_env = ModelEnv(model, env_id=env.unwrapped.spec.id, rew_dim=1 if w is None else len(w))
+        acts = th.tensor(actions).to(agent.device)
+        if isinstance(env.action_space, Discrete):
+            acts = F.one_hot(acts, num_classes=env.action_space.n).squeeze(1)
+        for step in range(len(real_obs)):
+            if compound or step == 0:
+                obs, r, done, info = model_env.step(th.tensor(obs).to(agent.device), acts[step], deterministic=deterministic)
+            else:
+                obs, r, done, info = model_env.step(
+                    th.tensor(real_obs[step - 1]).to(agent.device), acts[step], deterministic=deterministic
+                )
+            model_obs.append(obs.copy())
+            model_obs_stds.append(np.sqrt(info["var_obs"].copy()))
+            model_rewards_stds.append(np.sqrt(info["var_rewards"].copy()))
+            model_rewards.append(r)
+            # if done:
+            #    break
+
+    num_plots = obs_dim + (1 if w is None else len(w)) + 1
+    num_cols = int(np.ceil(np.sqrt(num_plots)))
+    num_rows = int(np.ceil(num_plots / num_cols))
+    x = np.arange(0, len(real_obs))
+    fig, axs = plt.subplots(num_rows, num_cols, figsize=(20, 15))
+    axs = np.array(axs).reshape(-1)
+    for i in range(num_plots):
+        if i == num_plots - 1:
+            axs[i].set_ylabel("Action")
+            axs[i].grid(alpha=0.25)
+            axs[i].plot(x, [actions[step] for step in x], label="Action", color="orange")
+        elif i >= obs_dim:
+            axs[i].set_ylabel(f"Reward {i - obs_dim}")
+            axs[i].grid(alpha=0.25)
+            if w is not None:
+                axs[i].plot(x, [real_vec_rewards[step][i - obs_dim] for step in x], label="Environment", color="black")
+            else:
+                axs[i].plot(x, [real_rewards[step] for step in x], label="Environment", color="black")
+            if model is not None:
+                axs[i].plot(x, [model_rewards[step][i - obs_dim] for step in x], label="Model", color="blue")
+                axs[i].fill_between(
+                    x,
+                    [model_rewards[step][i - obs_dim] + model_rewards_stds[step][i - obs_dim] for step in x],
+                    [model_rewards[step][i - obs_dim] - model_rewards_stds[step][i - obs_dim] for step in x],
+                    alpha=0.2,
+                    facecolor="blue",
+                )
+        else:
+            axs[i].set_ylabel(f"State {i}")
+            axs[i].grid(alpha=0.25)
+            axs[i].plot(x, [real_obs[step][i] for step in x], label="Environment", color="black")
+            if model is not None:
+                axs[i].plot(x, [model_obs[step][i] for step in x], label="Model", color="blue")
+                axs[i].fill_between(
+                    x,
+                    [model_obs[step][i] + model_obs_stds[step][i] for step in x],
+                    [model_obs[step][i] - model_obs_stds[step][i] for step in x],
+                    alpha=0.2,
+                    facecolor="blue",
+                )
+    sns.despine()
+    if filename is not None:
+        plt.savefig(filename + ".pdf", format="pdf", bbox_inches="tight")
+    if show:
+        plt.show()
+    return plt
diff --git a/morl-baselines/morl_baselines/common/morl_algorithm.py b/morl-baselines/morl_baselines/common/morl_algorithm.py
new file mode 100644
index 0000000..5203b42
--- /dev/null
+++ b/morl-baselines/morl_baselines/common/morl_algorithm.py
@@ -0,0 +1,238 @@
+"""MORL algorithm base classes."""
+from abc import ABC, abstractmethod
+from typing import Optional, Union
+
+import gymnasium as gym
+import numpy as np
+import torch as th
+from gymnasium import spaces
+from torch.utils.tensorboard import SummaryWriter
+
+from morl_baselines.common.evaluation import (
+    eval_mo_reward_conditioned,
+    policy_evaluation_mo,
+)
+
+
+class MOPolicy(ABC):
+    """An MORL policy.
+
+    It has an underlying learning structure which can be:
+    - used to get a greedy action via eval()
+    - updated using some experiences via update()
+
+    Note that the learning structure can embed multiple policies (for example using a Conditioned Network).
+    In this case, eval() requires a weight vector as input.
+    """
+
+    def __init__(self, id: Optional[int] = None, device: Union[th.device, str] = "auto") -> None:
+        """Initializes the policy.
+
+        Args:
+            id: The id of the policy
+            device: The device to use for the tensors
+        """
+        self.id = id
+        self.device = th.device("cuda" if th.cuda.is_available() else "cpu") if device == "auto" else device
+        self.global_step = 0
+
+    @abstractmethod
+    def eval(self, obs: np.ndarray, w: Optional[np.ndarray]) -> Union[int, np.ndarray]:
+        """Gives the best action for the given observation.
+
+        Args:
+            obs (np.array): Observation
+            w (optional np.array): weight for scalarization
+
+        Returns:
+            np.array or int: Action
+        """
+
+    def __report(
+        self,
+        scalarized_return,
+        scalarized_discounted_return,
+        vec_return,
+        discounted_vec_return,
+        writer: SummaryWriter,
+    ):
+        """Writes the data to wandb summary."""
+        if self.id is None:
+            idstr = ""
+        else:
+            idstr = f"_{self.id}"
+
+        writer.add_scalar(f"eval{idstr}/scalarized_return", scalarized_return, self.global_step)
+        writer.add_scalar(
+            f"eval{idstr}/scalarized_discounted_return",
+            scalarized_discounted_return,
+            self.global_step,
+        )
+        for i in range(vec_return.shape[0]):
+            writer.add_scalar(f"eval{idstr}/vec_{i}", vec_return[i], self.global_step)
+            writer.add_scalar(
+                f"eval{idstr}/discounted_vec_{i}",
+                discounted_vec_return[i],
+                self.global_step,
+            )
+
+    def policy_eval(
+        self,
+        eval_env,
+        num_episodes: int = 5,
+        scalarization=np.dot,
+        weights: Optional[np.ndarray] = None,
+        writer: Optional[SummaryWriter] = None,
+    ):
+        """Runs a policy evaluation (typically over a few episodes) on eval_env and logs some metrics using writer.
+
+        Args:
+            eval_env: evaluation environment
+            num_episodes: number of episodes to evaluate
+            scalarization: scalarization function
+            weights: weights to use in the evaluation
+            writer: wandb writer
+
+        Returns:
+             a tuple containing the average evaluations
+        """
+        (
+            scalarized_return,
+            scalarized_discounted_return,
+            vec_return,
+            discounted_vec_return,
+        ) = policy_evaluation_mo(self, eval_env, w=weights, rep=num_episodes)
+
+        if writer is not None:
+            self.__report(
+                scalarized_return,
+                scalarized_discounted_return,
+                vec_return,
+                discounted_vec_return,
+                writer,
+            )
+
+        return scalarized_return, scalarized_discounted_return, vec_return, discounted_vec_return
+
+    def policy_eval_esr(
+        self,
+        eval_env,
+        scalarization,
+        weights: Optional[np.ndarray] = None,
+        writer: Optional[SummaryWriter] = None,
+    ):
+        """Runs a policy evaluation (typically on one episode) on eval_env and logs some metrics using writer.
+
+        Args:
+            eval_env: evaluation environment
+            scalarization: scalarization function
+            weights: weights to use in the evaluation
+            writer: wandb writer
+
+        Returns:
+             a tuple containing the evaluations
+        """
+        (
+            scalarized_reward,
+            scalarized_discounted_reward,
+            vec_reward,
+            discounted_vec_reward,
+        ) = eval_mo_reward_conditioned(self, eval_env, scalarization, weights)
+
+        if writer is not None:
+            self.__report(
+                scalarized_reward,
+                scalarized_discounted_reward,
+                vec_reward,
+                discounted_vec_reward,
+                writer,
+            )
+
+        return scalarized_reward, scalarized_discounted_reward, vec_reward, discounted_vec_reward
+
+    @abstractmethod
+    def update(self) -> None:
+        """Update algorithm's parameters (e.g. using experiences from the buffer)."""
+
+
+class MOAgent(ABC):
+    """An MORL Agent, can contain one or multiple MOPolicies. Contains helpers to extract features from the environment, setup logging etc."""
+
+    def __init__(self, env: Optional[gym.Env], device: Union[th.device, str] = "auto") -> None:
+        """Initializes the agent.
+
+        Args:
+            env: (gym.Env): The environment
+            device: (str): The device to use for training. Can be "auto", "cpu" or "cuda".
+        """
+        self.extract_env_info(env)
+        self.device = th.device("cuda" if th.cuda.is_available() else "cpu") if device == "auto" else device
+
+        self.global_step = 0
+        self.num_episodes = 0
+
+    def extract_env_info(self, env: Optional[gym.Env]) -> None:
+        """Extracts all the features of the environment: observation space, action space, ...
+
+        Args:
+            env (gym.Env): The environment
+        """
+        # Sometimes, the environment is not instantiated at the moment the MORL algorithms is being instantiated.
+        # So env can be None. It is the responsibility of the implemented MORLAlgorithm to call this method in those cases
+        if env is not None:
+            self.env = env
+            if isinstance(self.env.observation_space, spaces.Discrete):
+                self.observation_shape = (1,)
+                self.observation_dim = self.env.observation_space.n
+            else:
+                self.observation_shape = self.env.observation_space.shape
+                self.observation_dim = self.env.observation_space.shape[0]
+
+            self.action_space = env.action_space
+            if isinstance(self.env.action_space, (spaces.Discrete, spaces.MultiBinary)):
+                self.action_shape = (1,)
+                self.action_dim = self.env.action_space.n
+            else:
+                self.action_shape = self.env.action_space.shape
+                self.action_dim = self.env.action_space.shape[0]
+            self.reward_dim = self.env.reward_space.shape[0]
+
+    @abstractmethod
+    def get_config(self) -> dict:
+        """Generates dictionary of the algorithm parameters configuration.
+
+        Returns:
+            dict: Config
+        """
+
+    def setup_wandb(self, project_name: str, experiment_name: str) -> None:
+        """Initializes the wandb writer.
+
+        Args:
+            project_name: name of the wandb project. Usually MORL-Baselines.
+            experiment_name: name of the wandb experiment. Usually the algorithm name.
+
+        Returns:
+            None
+        """
+        self.experiment_name = experiment_name
+        import wandb
+
+        wandb.init(
+            project=project_name,
+            sync_tensorboard=True,
+            config=self.get_config(),
+            name=self.experiment_name,
+            monitor_gym=False,
+            save_code=True,
+        )
+        self.writer = SummaryWriter(f"/tmp/{self.experiment_name}")
+        # The default "step" of wandb is not the actual time step (gloabl_step) of the MDP
+        wandb.define_metric("*", step_metric="global_step")
+
+    def close_wandb(self) -> None:
+        """Closes the wandb writer and finishes the run."""
+        import wandb
+
+        self.writer.close()
+        wandb.finish()
diff --git a/morl-baselines/morl_baselines/common/networks.py b/morl-baselines/morl_baselines/common/networks.py
new file mode 100644
index 0000000..f12a987
--- /dev/null
+++ b/morl-baselines/morl_baselines/common/networks.py
@@ -0,0 +1,87 @@
+"""Utilities for Neural Networks."""
+
+from typing import List, Type
+
+import numpy as np
+import torch as th
+from torch import nn
+
+
+def mlp(
+    input_dim: int,
+    output_dim: int,
+    net_arch: List[int],
+    activation_fn: Type[nn.Module] = nn.ReLU,
+    drop_rate: float = 0.0,
+    layer_norm: bool = False,
+) -> nn.Sequential:
+    """Create a multi layer perceptron (MLP), which is a collection of fully-connected layers each followed by an activation function.
+
+    Args:
+        input_dim: Dimension of the input vector
+        output_dim: Dimension of the output vector
+        net_arch: Architecture of the neural net. It represents the number of units per layer. The length of this list is the number of layers.
+        activation_fn: The activation function to use after each layer.
+        drop_rate: Dropout rate
+        layer_norm: Whether to use layer normalization
+    """
+    assert len(net_arch) > 0
+    modules = [nn.Linear(input_dim, net_arch[0])]
+    if drop_rate > 0.0:
+        modules.append(nn.Dropout(p=drop_rate))
+    if layer_norm:
+        modules.append(nn.LayerNorm(net_arch[0]))
+    modules.append(activation_fn())
+
+    for idx in range(len(net_arch) - 1):
+        modules.append(nn.Linear(net_arch[idx], net_arch[idx + 1]))
+        if drop_rate > 0.0:
+            modules.append(nn.Dropout(p=drop_rate))
+        if layer_norm:
+            modules.append(nn.LayerNorm(net_arch[idx + 1]))
+        modules.append(activation_fn())
+
+    if output_dim > 0:
+        last_layer_dim = net_arch[-1]
+        modules.append(nn.Linear(last_layer_dim, output_dim))
+
+    return nn.Sequential(*modules)
+
+
+class NatureCNN(nn.Module):
+    """CNN from DQN nature paper: Mnih, Volodymyr, et al. "Human-level control through deep reinforcement learning." Nature 518.7540 (2015): 529-533."""
+
+    def __init__(self, observation_shape: np.ndarray, features_dim: int = 512):
+        """CNN from DQN Nature.
+
+        Args:
+            observation_shape: Shape of the observation.
+            features_dim: Number of features extracted. This corresponds to the number of unit for the last layer.
+        """
+        super().__init__()
+        self.features_dim = features_dim
+        n_input_channels = 1 if len(observation_shape) == 2 else observation_shape[0]
+        self.cnn = nn.Sequential(
+            nn.Conv2d(n_input_channels, 32, kernel_size=8, stride=4, padding=0),
+            nn.ReLU(),
+            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=0),
+            nn.ReLU(),
+            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=0),
+            nn.ReLU(),
+            nn.Flatten(),
+        )
+        # Compute shape by doing one forward pass
+        with th.no_grad():
+            n_flatten = self.cnn(th.as_tensor(np.zeros(observation_shape)[np.newaxis]).float()).shape[1]
+
+        self.linear = nn.Sequential(nn.Linear(n_flatten, features_dim), nn.ReLU())
+
+    def forward(self, observations: th.Tensor) -> th.Tensor:
+        """Predicts the features from the observations.
+
+        Args:
+            observations: current observations
+        """
+        if observations.dim() == 3:
+            observations = observations.unsqueeze(0)
+        return self.linear(self.cnn(observations / 255.0))
diff --git a/morl-baselines/morl_baselines/common/pareto.py b/morl-baselines/morl_baselines/common/pareto.py
new file mode 100644
index 0000000..78d3047
--- /dev/null
+++ b/morl-baselines/morl_baselines/common/pareto.py
@@ -0,0 +1,80 @@
+"""Pareto utilities."""
+from copy import deepcopy
+from typing import List
+
+import numpy as np
+
+
+def get_non_dominated(candidates: set) -> set:
+    """This function returns the non-dominated subset of elements.
+
+    Source: https://stackoverflow.com/questions/32791911/fast-calculation-of-pareto-front-in-python
+    The code provided in all the stackoverflow answers is wrong. Important changes have been made in this function.
+
+    Args:
+        candidates: The input set of candidate vectors.
+
+    Returns:
+        The non-dominated subset of this input set.
+    """
+    candidates = np.array(list(candidates))  # Turn the input set into a numpy array.
+    candidates = candidates[candidates.sum(1).argsort()[::-1]]  # Sort candidates by decreasing sum of coordinates.
+    for i in range(candidates.shape[0]):  # Process each point in turn.
+        n = candidates.shape[0]  # Check current size of the candidates.
+        if i >= n:  # If we've eliminated everything up until this size we stop.
+            break
+        non_dominated = np.ones(candidates.shape[0], dtype=bool)  # Initialize a boolean mask for undominated points.
+        # find all points not dominated by i
+        # since points are sorted by coordinate sum
+        # i cannot dominate any points in 1,...,i-1
+        non_dominated[i + 1 :] = np.any(candidates[i + 1 :] > candidates[i], axis=1)
+        candidates = candidates[non_dominated]  # Grab only the non-dominated vectors using the generated bitmask.
+
+    non_dominated = set()
+    for candidate in candidates:
+        non_dominated.add(tuple(candidate))  # Add the non dominated vectors to a set again.
+
+    return non_dominated
+
+
+def get_non_dominated_inds(solutions: np.ndarray) -> np.ndarray:
+    """Returns a boolean array indicating which points are non-dominated."""
+    is_efficient = np.ones(solutions.shape[0], dtype=bool)
+    for i, c in enumerate(solutions):
+        if is_efficient[i]:
+            # Remove dominated points, will also remove itself
+            is_efficient[is_efficient] = np.any(solutions[is_efficient] > c, axis=1)
+            # keep this solution as non-dominated
+            is_efficient[i] = 1
+    return is_efficient
+
+
+class ParetoArchive:
+    """Pareto archive."""
+
+    def __init__(self):
+        """Initializes the Pareto archive."""
+        self.individuals: list = []
+        self.evaluations: List[np.ndarray] = []
+
+    def add(self, candidate, evaluation: np.ndarray):
+        """Adds the candidate to the memory and removes Pareto inefficient points.
+
+        Args:
+            candidate: The candidate to add.
+            evaluation: The evaluation of the candidate.
+        """
+        self.evaluations.append(evaluation)
+        self.individuals.append(deepcopy(candidate))
+        # Non-dominated sorting
+        nd_candidates = get_non_dominated({tuple(e) for e in self.evaluations})
+
+        # Reconstruct the pareto archive (because Non-Dominated sorting might change the order of candidates)
+        non_dominated_evals = []
+        non_dominated_individuals = []
+        for e, i in zip(self.evaluations, self.individuals):
+            if tuple(e) in nd_candidates:
+                non_dominated_evals.append(e)
+                non_dominated_individuals.append(i)
+        self.evaluations = non_dominated_evals
+        self.individuals = non_dominated_individuals
diff --git a/morl-baselines/morl_baselines/common/performance_indicators.py b/morl-baselines/morl_baselines/common/performance_indicators.py
new file mode 100644
index 0000000..8bdf729
--- /dev/null
+++ b/morl-baselines/morl_baselines/common/performance_indicators.py
@@ -0,0 +1,110 @@
+"""Performance indicators for multi-objective RL algorithms.
+
+We mostly rely on pymoo for the computation of axiomatic indicators (HV and IGD), but some are customly made.
+"""
+from copy import deepcopy
+from typing import Callable, List
+
+import numpy as np
+import numpy.typing as npt
+from pymoo.indicators.hv import HV
+from pymoo.indicators.igd import IGD
+
+
+def hypervolume(ref_point: np.ndarray, points: List[npt.ArrayLike]) -> float:
+    """Computes the hypervolume metric for a set of points (value vectors) and a reference point (from Pymoo).
+
+    Args:
+        ref_point (np.ndarray): Reference point
+        points (List[np.ndarray]): List of value vectors
+
+    Returns:
+        float: Hypervolume metric
+    """
+    return HV(ref_point=ref_point * -1)(np.array(points) * -1)
+
+
+def igd(known_front: List[np.ndarray], current_estimate: List[np.ndarray]) -> float:
+    """Inverted generational distance metric. Requires to know the optimal front.
+
+    Args:
+        known_front: known pareto front for the problem
+        current_estimate: current pareto front
+
+    Return:
+        a float stating the average distance between a point in current_estimate and its nearest point in known_front
+    """
+    ind = IGD(np.array(known_front))
+    return ind(np.array(current_estimate))
+
+
+def sparsity(front: List[np.ndarray]) -> float:
+    """Sparsity metric from PGMORL.
+
+    Basically, the sparsity is the average distance between each point in the front.
+
+    Args:
+        front: current pareto front to compute the sparsity on
+
+    Returns:
+        float: sparsity metric
+    """
+    if len(front) < 2:
+        return 0.0
+
+    sparsity_value = 0.0
+    m = len(front[0])
+    front = np.array(front)
+    for dim in range(m):
+        objs_i = np.sort(deepcopy(front.T[dim]))
+        for i in range(1, len(objs_i)):
+            sparsity_value += np.square(objs_i[i] - objs_i[i - 1])
+    sparsity_value /= len(front) - 1
+
+    return sparsity_value
+
+
+def expected_utility(front: List[np.ndarray], weights_set: List[np.ndarray], utility: Callable = np.dot) -> float:
+    """Expected Utility Metric.
+
+    Expected utility of the policies on the PF for various weights.
+    Similar to R-Metrics in MOO. But only needs one PF approximation.
+    Paper: L. M. Zintgraf, T. V. Kanters, D. M. Roijers, F. A. Oliehoek, and P. Beau, “Quality Assessment of MORL Algorithms: A Utility-Based Approach,” 2015.
+
+    Args:
+        front: current pareto front to compute the eum on
+        weights_set: weights to use for the utility computation
+        utility: utility function to use (default: dot product)
+
+    Returns:
+        float: eum metric
+    """
+    maxs = []
+    for weights in weights_set:
+        scalarized_front = np.array([utility(weights, point) for point in front])
+        maxs.append(np.max(scalarized_front))
+
+    return np.mean(np.array(maxs), axis=0)
+
+
+def maximum_utility_loss(
+    front: List[np.ndarray], reference_set: List[np.ndarray], weights_set: np.ndarray, utility: Callable = np.dot
+) -> float:
+    """Maximum Utility Loss Metric.
+
+    Maximum utility loss of the policies on the PF for various weights.
+    Paper: L. M. Zintgraf, T. V. Kanters, D. M. Roijers, F. A. Oliehoek, and P. Beau, “Quality Assessment of MORL Algorithms: A Utility-Based Approach,” 2015.
+
+    Args:
+        front: current pareto front to compute the mul on
+        reference_set: reference set (e.g. true Pareto front) to compute the mul on
+        weights_set: weights to use for the utility computation
+        utility: utility function to use (default: dot product)
+
+    Returns:
+        float: mul metric
+    """
+    max_scalarized_values_ref = [np.max([utility(weight, point) for point in reference_set]) for weight in weights_set]
+    max_scalarized_values = [np.max([utility(weight, point) for point in front]) for weight in weights_set]
+    utility_losses = [max_scalarized_values_ref[i] - max_scalarized_values[i] for i in range(len(max_scalarized_values))]
+    return np.max(utility_losses)
diff --git a/morl-baselines/morl_baselines/common/prioritized_buffer.py b/morl-baselines/morl_baselines/common/prioritized_buffer.py
new file mode 100644
index 0000000..01277b9
--- /dev/null
+++ b/morl-baselines/morl_baselines/common/prioritized_buffer.py
@@ -0,0 +1,224 @@
+"""Prioritized Replay Buffer.
+
+Code adapted from https://github.com/sfujim/LAP-PAL
+"""
+import numpy as np
+import torch as th
+
+
+class SumTree:
+    """SumTree with fixed size."""
+
+    def __init__(self, max_size):
+        """Initialize the SumTree.
+
+        Args:
+            max_size: Maximum size of the SumTree
+        """
+        self.nodes = []
+        # Tree construction
+        # Double the number of nodes at each level
+        level_size = 1
+        for _ in range(int(np.ceil(np.log2(max_size))) + 1):
+            nodes = np.zeros(level_size)
+            self.nodes.append(nodes)
+            level_size *= 2
+
+    def sample(self, batch_size):
+        """Batch binary search through sum tree. Sample a priority between 0 and the max priority and then search the tree for the corresponding index.
+
+        Args:
+            batch_size: Number of indices to sample
+
+        Returns:
+            indices: Indices of the sampled nodes
+
+        """
+        query_value = np.random.uniform(0, self.nodes[0][0], size=batch_size)
+        node_index = np.zeros(batch_size, dtype=int)
+
+        for nodes in self.nodes[1:]:
+            node_index *= 2
+            left_sum = nodes[node_index]
+
+            is_greater = np.greater(query_value, left_sum)
+            # If query_value > left_sum -> go right (+1), else go left (+0)
+            node_index += is_greater
+            # If we go right, we only need to consider the values in the right tree
+            # so we subtract the sum of values in the left tree
+            query_value -= left_sum * is_greater
+
+        return node_index
+
+    def set(self, node_index, new_priority):
+        """Set the priority of node at node_index to new_priority.
+
+        Args:
+            node_index: Index of the node to update
+            new_priority: New priority of the node
+        """
+        priority_diff = new_priority - self.nodes[-1][node_index]
+
+        for nodes in self.nodes[::-1]:
+            np.add.at(nodes, node_index, priority_diff)
+            node_index //= 2
+
+    def batch_set(self, node_index, new_priority):
+        """Batched version of set.
+
+        Args:
+            node_index: Index of the nodes to update
+            new_priority: New priorities of the nodes
+        """
+        # Confirm we don't increment a node twice
+        node_index, unique_index = np.unique(node_index, return_index=True)
+        priority_diff = new_priority[unique_index] - self.nodes[-1][node_index]
+
+        for nodes in self.nodes[::-1]:
+            np.add.at(nodes, node_index, priority_diff)
+            node_index //= 2
+
+
+class PrioritizedReplayBuffer:
+    """Prioritized Replay Buffer."""
+
+    def __init__(
+        self,
+        obs_shape,
+        action_dim,
+        rew_dim=1,
+        max_size=100000,
+        obs_dtype=np.float32,
+        action_dtype=np.float32,
+        min_priority=1e-5,
+    ):
+        """Initialize the Prioritized Replay Buffer.
+
+        Args:
+            obs_shape: Shape of the observations
+            action_dim: Dimension of the actions
+            rew_dim: Dimension of the rewards
+            max_size: Maximum size of the buffer
+            obs_dtype: Data type of the observations
+            action_dtype: Data type of the actions
+            min_priority: Minimum priority of the buffer
+        """
+        self.max_size = max_size
+        self.ptr, self.size, = (
+            0,
+            0,
+        )
+        self.obs = np.zeros((max_size,) + (obs_shape), dtype=obs_dtype)
+        self.next_obs = np.zeros((max_size,) + (obs_shape), dtype=obs_dtype)
+        self.actions = np.zeros((max_size, action_dim), dtype=action_dtype)
+        self.rewards = np.zeros((max_size, rew_dim), dtype=np.float32)
+        self.dones = np.zeros((max_size, 1), dtype=np.float32)
+
+        self.tree = SumTree(max_size)
+        self.min_priority = min_priority
+
+    def add(self, obs, action, reward, next_obs, done, priority=None):
+        """Add a new experience to the buffer.
+
+        Args:
+            obs: Observation
+            action: Action
+            reward: Reward
+            next_obs: Next observation
+            done: Done
+            priority: Priority of the new experience
+
+        """
+        self.obs[self.ptr] = np.array(obs).copy()
+        self.next_obs[self.ptr] = np.array(next_obs).copy()
+        self.actions[self.ptr] = np.array(action).copy()
+        self.rewards[self.ptr] = np.array(reward).copy()
+        self.dones[self.ptr] = np.array(done).copy()
+
+        self.tree.set(self.ptr, self.min_priority if priority is None else priority)
+
+        self.ptr = (self.ptr + 1) % self.max_size
+        self.size = min(self.size + 1, self.max_size)
+
+    def sample(self, batch_size, to_tensor=False, device=None):
+        """Sample a batch of experience tuples from the buffer.
+
+        Args:
+            batch_size: Number of experiences to sample
+            to_tensor:  Whether to convert the batch to a tensor
+            device: Device to move the tensor to
+
+        Returns:
+            batch: Batch of experiences
+        """
+        idxes = self.tree.sample(batch_size)
+
+        experience_tuples = (
+            self.obs[idxes],
+            self.actions[idxes],
+            self.rewards[idxes],
+            self.next_obs[idxes],
+            self.dones[idxes],
+        )
+        if to_tensor:
+            return tuple(map(lambda x: th.tensor(x).to(device), experience_tuples)) + (idxes,)  # , weights)
+        else:
+            return experience_tuples + (idxes,)
+
+    def sample_obs(self, batch_size, to_tensor=False, device=None):
+        """Sample a batch of observations from the buffer.
+
+        Args:
+            batch_size: Number of observations to sample
+            to_tensor: Whether to convert the batch to a tensor
+            device: Device to move the tensor to
+
+        Returns:
+            batch: Batch of observations
+        """
+        idxes = self.tree.sample(batch_size)
+        if to_tensor:
+            return th.tensor(self.obs[idxes]).to(device)
+        else:
+            return self.obs[idxes]
+
+    def update_priorities(self, idxes, priorities):
+        """Update the priorities of the experiences at idxes.
+
+        Args:
+            idxes: Indexes of the experiences to update
+            priorities: New priorities of the experiences
+        """
+        self.min_priority = max(self.min_priority, priorities.max())
+        self.tree.batch_set(idxes, priorities)
+
+    def get_all_data(self, max_samples=None, to_tensor=False, device=None):
+        """Get all the data in the buffer.
+
+        Args:
+            max_samples: Maximum number of samples to return
+            to_tensor: Whether to convert the batch to a tensor
+            device: Device to move the tensor to
+
+        Returns:
+            batch: Batch of experiences
+        """
+        if max_samples is not None and max_samples < self.size:
+            inds = np.random.choice(self.size, max_samples, replace=False)
+        else:
+            inds = np.arange(self.size)
+        tuples = (
+            self.obs[inds],
+            self.actions[inds],
+            self.rewards[inds],
+            self.next_obs[inds],
+            self.dones[inds],
+        )
+        if to_tensor:
+            return tuple(map(lambda x: th.tensor(x).to(device), tuples))
+        else:
+            return tuples
+
+    def __len__(self):
+        """Return the size of the buffer."""
+        return self.size
diff --git a/morl-baselines/morl_baselines/common/scalarization.py b/morl-baselines/morl_baselines/common/scalarization.py
new file mode 100644
index 0000000..45a57fb
--- /dev/null
+++ b/morl-baselines/morl_baselines/common/scalarization.py
@@ -0,0 +1,40 @@
+"""Scalarization functions relying on numpy."""
+import numpy as np
+from pymoo.decomposition.tchebicheff import Tchebicheff
+
+
+def weighted_sum(reward: np.ndarray, weights: np.ndarray) -> float:
+    """Weighted sum scalarization (numpy dot product).
+
+    Args:
+        reward: Reward vector
+        weights: Weight vector
+
+    Returns:
+        float: Weighted sum
+    """
+    return np.dot(reward, weights)
+
+
+def tchebicheff(tau: float, reward_dim: int):
+    """Tchebicheff scalarization function.
+
+    This function requires a reference point. It is automatically adapted to the best value seen so far for each component of the reward.
+
+    Args:
+        tau: Parameter to be sure the reference point is always dominating (automatically adapted).
+        reward_dim: Dimension of the reward vector
+
+    Returns:
+        Callable: Tchebicheff scalarization function
+    """
+    best_so_far = [float("-inf") for _ in range(reward_dim)]
+
+    def thunk(reward: np.ndarray, weights: np.ndarray):
+        for i, r in enumerate(reward):
+            if best_so_far[i] < r + tau:
+                best_so_far[i] = r + tau
+        tch = Tchebicheff()
+        return -tch.do(F=reward, weights=weights, utopian_point=np.array(best_so_far))[0][0]
+
+    return thunk
diff --git a/morl-baselines/morl_baselines/common/utils.py b/morl-baselines/morl_baselines/common/utils.py
new file mode 100644
index 0000000..e232e66
--- /dev/null
+++ b/morl-baselines/morl_baselines/common/utils.py
@@ -0,0 +1,299 @@
+"""General utils for the MORL baselines."""
+from typing import Iterable, List, Optional
+
+import numpy as np
+import torch as th
+import wandb
+from pymoo.util.ref_dirs import get_reference_directions
+from torch import nn
+from torch.utils.tensorboard import SummaryWriter
+
+from morl_baselines.common.performance_indicators import (
+    expected_utility,
+    hypervolume,
+    igd,
+    maximum_utility_loss,
+    sparsity,
+)
+
+
+@th.no_grad()
+def layer_init(layer, method="orthogonal", weight_gain: float = 1, bias_const: float = 0) -> None:
+    """Initialize a layer with the given method.
+
+    Args:
+        layer: The layer to initialize.
+        method: The initialization method to use.
+        weight_gain: The gain for the weights.
+        bias_const: The constant for the bias.
+    """
+    if isinstance(layer, (nn.Linear, nn.Conv2d)):
+        if method == "xavier":
+            th.nn.init.xavier_uniform_(layer.weight, gain=weight_gain)
+        elif method == "orthogonal":
+            th.nn.init.orthogonal_(layer.weight, gain=weight_gain)
+        th.nn.init.constant_(layer.bias, bias_const)
+
+
+@th.no_grad()
+def polyak_update(
+    params: Iterable[th.nn.Parameter],
+    target_params: Iterable[th.nn.Parameter],
+    tau: float,
+) -> None:
+    """Polyak averaging for target network parameters.
+
+    Args:
+        params: The parameters to update.
+        target_params: The target parameters.
+        tau: The polyak averaging coefficient (usually small).
+
+    """
+    for param, target_param in zip(params, target_params):
+        if tau == 1:
+            target_param.data.copy_(param.data)
+        else:
+            target_param.data.mul_(1.0 - tau)
+            th.add(target_param.data, param.data, alpha=tau, out=target_param.data)
+
+
+def get_grad_norm(params: Iterable[th.nn.Parameter]) -> th.Tensor:
+    """This is how the grad norm is computed inside torch.nn.clip_grad_norm_().
+
+    Args:
+        params: The parameters to compute the grad norm for.
+
+    Returns:
+        The grad norm.
+    """
+    parameters = [p for p in params if p.grad is not None]
+    if len(parameters) == 0:
+        return th.tensor(0.0)
+    device = parameters[0].grad.device
+    total_norm = th.norm(th.stack([th.norm(p.grad.detach(), 2.0).to(device) for p in parameters]), 2.0)
+    return total_norm
+
+
+def huber(x, min_priority=0.01):
+    """Huber loss function.
+
+    Args:
+        x: The input tensor.
+        min_priority: The minimum priority.
+
+    Returns:
+        The huber loss.
+    """
+    return th.where(x < min_priority, 0.5 * x.pow(2), min_priority * x).mean()
+
+
+def linearly_decaying_value(initial_value, decay_period, step, warmup_steps, final_value):
+    """Returns the current value for a linearly decaying parameter.
+
+    This follows the Nature DQN schedule of a linearly decaying epsilon (Mnih et
+    al., 2015). The schedule is as follows:
+    Begin at 1. until warmup_steps steps have been taken; then
+    Linearly decay epsilon from 1. to epsilon in decay_period steps; and then
+    Use epsilon from there on.
+
+    Args:
+        decay_period: float, the period over which the value is decayed.
+        step: int, the number of training steps completed so far.
+        warmup_steps: int, the number of steps taken before the value is decayed.
+        final value: float, the final value to which to decay the value parameter.
+
+    Returns:
+        A float, the current value computed according to the schedule.
+    """
+    steps_left = decay_period + warmup_steps - step
+    bonus = (initial_value - final_value) * steps_left / decay_period
+    value = final_value + bonus
+    value = np.clip(value, min(initial_value, final_value), max(initial_value, final_value))
+    return value
+
+
+def extrema_weights(dim: int) -> List[np.ndarray]:
+    """Generate weight vectors in the extrema of the weight simplex. That is, one element is 1 and the rest are 0.
+
+    Args:
+        dim: size of the weight vector
+    """
+    return list(np.eye(dim, dtype=np.float32))
+
+
+def equally_spaced_weights(dim: int, n: int, seed: int = 42) -> List[np.ndarray]:
+    """Generate weight vectors that are equally spaced in the weight simplex.
+
+    It uses the Riesz s-Energy method from pymoo: https://pymoo.org/misc/reference_directions.html
+
+    Args:
+        dim: size of the weight vector
+        n: number of weight vectors to generate
+        seed: random seed
+    """
+    return list(get_reference_directions("energy", dim, n, seed=seed))
+
+
+def random_weights(dim: int, seed: Optional[int] = None, n: int = 1, dist: str = "dirichlet") -> np.ndarray:
+    """Generate random normalized weight vectors from a Gaussian or Dirichlet distribution alpha=1.
+
+    Args:
+        dim: size of the weight vector
+        seed: random seed
+        n : number of weight vectors to generate
+        dist: distribution to use, either 'gaussian' or 'dirichlet'. Default is 'dirichlet' as it is equivalent to sampling uniformly from the weight simplex.
+    """
+    if seed is not None:
+        rng = np.random.default_rng(seed)
+    else:
+        rng = np.random
+
+    if dist == "gaussian":
+        w = np.random.randn(n, dim)
+        w = np.abs(w) / np.linalg.norm(w, ord=1, axis=1, keepdims=True)
+    elif dist == "dirichlet":
+        w = rng.dirichlet(np.ones(dim), n)
+    else:
+        raise ValueError(f"Unknown distribution {dist}")
+
+    if n == 1:
+        return w[0]
+    return w
+
+
+def log_episode_info(
+    info: dict,
+    scalarization,
+    weights: Optional[np.ndarray],
+    global_timestep: int,
+    id: Optional[int] = None,
+    writer: Optional[SummaryWriter] = None,
+    verbose: bool = True,
+):
+    """Logs information of the last episode from the info dict (automatically filled by the RecordStatisticsWrapper).
+
+    Args:
+        info: info dictionary containing the episode statistics
+        scalarization: scalarization function
+        weights: weights to be used in the scalarization
+        global_timestep: global timestep
+        id: agent's id
+        writer: wandb writer
+        verbose: whether to print the episode info
+    """
+    episode_ts = info["l"]
+    episode_time = info["t"]
+    episode_return = info["r"]
+    disc_episode_return = info["dr"]
+    if weights is None:
+        scal_return = scalarization(episode_return)
+        disc_scal_return = scalarization(disc_episode_return)
+    else:
+        scal_return = scalarization(episode_return, weights)
+        disc_scal_return = scalarization(disc_episode_return, weights)
+
+    if verbose:
+        print("Episode infos:")
+        print(f"Steps: {episode_ts}, Time: {episode_time}")
+        print(f"Total Reward: {episode_return}, Discounted: {disc_episode_return}")
+        print(f"Scalarized Reward: {scal_return}, Discounted: {disc_scal_return}")
+
+    if writer is not None:
+        if id is not None:
+            idstr = "_" + str(id)
+        else:
+            idstr = ""
+        writer.add_scalar(f"charts{idstr}/timesteps_per_episode", episode_ts, global_timestep)
+        writer.add_scalar(f"charts{idstr}/episode_time", episode_time, global_timestep)
+        writer.add_scalar(f"metrics{idstr}/scalarized_episode_return", scal_return, global_timestep)
+        writer.add_scalar(
+            f"metrics{idstr}/discounted_scalarized_episode_return",
+            disc_scal_return,
+            global_timestep,
+        )
+
+        for i in range(episode_return.shape[0]):
+            writer.add_scalar(
+                f"metrics{idstr}/episode_return_obj_{i}",
+                episode_return[i],
+                global_timestep,
+            )
+            writer.add_scalar(
+                f"metrics{idstr}/disc_episode_return_obj_{i}",
+                disc_episode_return[i],
+                global_timestep,
+            )
+
+
+def log_all_multi_policy_metrics(
+    current_front: List[np.ndarray],
+    hv_ref_point: np.ndarray,
+    reward_dim: int,
+    global_step: int,
+    writer: SummaryWriter,
+    n_sample_weights: int = 50,
+    ref_front: Optional[List[np.ndarray]] = None,
+):
+    """Logs all metrics for multi-policy training.
+
+    Logged metrics:
+    - hypervolume
+    - sparsity
+    - expected utility metric (EUM)
+    If a reference front is provided, also logs:
+    - Inverted generational distance (IGD)
+    - Maximum utility loss (MUL)
+
+    Args:
+        current_front (List) : current Pareto front approximation, computed in an evaluation step
+        hv_ref_point: reference point for hypervolume computation
+        reward_dim: number of objectives
+        global_step: global step for logging
+        writer: wandb writer
+        n_sample_weights: number of weights to sample for EUM and MUL computation
+        ref_front: reference front, if known
+    """
+    hv = hypervolume(hv_ref_point, current_front)
+    sp = sparsity(current_front)
+    eum = expected_utility(current_front, weights_set=equally_spaced_weights(reward_dim, n_sample_weights))
+
+    writer.add_scalar("eval/hypervolume", hv, global_step=global_step)
+    writer.add_scalar("eval/sparsity", sp, global_step=global_step)
+    writer.add_scalar("eval/eum", eum, global_step=global_step)
+    front = wandb.Table(
+        columns=[f"objective_{i}" for i in range(1, reward_dim + 1)],
+        data=[p.tolist() for p in current_front],
+    )
+    wandb.log({"eval/front": front}, step=global_step)
+
+    # If PF is known, log the additional metrics
+    if ref_front is not None:
+        generational_distance = igd(known_front=ref_front, current_estimate=current_front)
+        writer.add_scalar("eval/igd", generational_distance, global_step=global_step)
+        mul = maximum_utility_loss(
+            front=current_front,
+            reference_set=ref_front,
+            weights_set=get_reference_directions("energy", reward_dim, n_sample_weights).astype(np.float32),
+        )
+        writer.add_scalar("eval/mul", mul, global_step=global_step)
+
+
+def make_gif(env, agent, weight: np.ndarray, fullpath: str, fps: int = 50, length: int = 300):
+    """Render an episode and save it as a gif."""
+    assert "rgb_array" in env.metadata["render_modes"], "Environment does not have rgb_array rendering."
+
+    frames = []
+    state, info = env.reset()
+    terminated, truncated = False, False
+    while not (terminated or truncated) and len(frames) < length:
+        frame = env.render()
+        frames.append(frame)
+        action = agent.eval(state, weight)
+        state, reward, terminated, truncated, info = env.step(action)
+    env.close()
+
+    from moviepy.editor import ImageSequenceClip
+
+    clip = ImageSequenceClip(list(frames), fps=fps)
+    clip.write_gif(fullpath + ".gif", fps=fps)
+    print("Saved gif at: " + fullpath + ".gif")
diff --git a/morl-baselines/morl_baselines/multi_policy/__init__.py b/morl-baselines/morl_baselines/multi_policy/__init__.py
new file mode 100644
index 0000000..37b07ba
--- /dev/null
+++ b/morl-baselines/morl_baselines/multi_policy/__init__.py
@@ -0,0 +1 @@
+"""Multi-policy algorithms."""
diff --git a/morl-baselines/morl_baselines/multi_policy/envelope/__init__.py b/morl-baselines/morl_baselines/multi_policy/envelope/__init__.py
new file mode 100644
index 0000000..8dbbb1d
--- /dev/null
+++ b/morl-baselines/morl_baselines/multi_policy/envelope/__init__.py
@@ -0,0 +1 @@
+"""Envelope Q-Learning (EQL) implementation."""
diff --git a/morl-baselines/morl_baselines/multi_policy/envelope/envelope.py b/morl-baselines/morl_baselines/multi_policy/envelope/envelope.py
new file mode 100644
index 0000000..132f1ba
--- /dev/null
+++ b/morl-baselines/morl_baselines/multi_policy/envelope/envelope.py
@@ -0,0 +1,533 @@
+"""Envelope Q-Learning implementation."""
+import os
+from typing import List, Optional, Union
+from typing_extensions import override
+
+import gymnasium as gym
+import numpy as np
+import torch as th
+import torch.nn as nn
+import torch.nn.functional as F
+import torch.optim as optim
+
+from morl_baselines.common.buffer import ReplayBuffer
+from morl_baselines.common.morl_algorithm import MOAgent, MOPolicy
+from morl_baselines.common.networks import NatureCNN, mlp
+from morl_baselines.common.prioritized_buffer import PrioritizedReplayBuffer
+from morl_baselines.common.utils import (
+    equally_spaced_weights,
+    get_grad_norm,
+    layer_init,
+    linearly_decaying_value,
+    log_all_multi_policy_metrics,
+    log_episode_info,
+    polyak_update,
+    random_weights,
+)
+
+
+class QNet(nn.Module):
+    """Multi-objective Q-Network conditioned on the weight vector."""
+
+    def __init__(self, obs_shape, action_dim, rew_dim, net_arch):
+        """Initialize the Q network.
+
+        Args:
+            obs_shape: shape of the observation
+            action_dim: number of actions
+            rew_dim: number of objectives
+            net_arch: network architecture (number of units per layer)
+        """
+        super().__init__()
+        self.obs_shape = obs_shape
+        self.action_dim = action_dim
+        self.rew_dim = rew_dim
+        if len(obs_shape) == 1:
+            self.feature_extractor = None
+            input_dim = obs_shape[0] + rew_dim
+        elif len(obs_shape) > 1:  # Image observation
+            self.feature_extractor = NatureCNN(self.obs_shape, features_dim=512)
+            input_dim = self.feature_extractor.features_dim + rew_dim
+        # |S| + |R| -> ... -> |A| * |R|
+        self.net = mlp(input_dim, action_dim * rew_dim, net_arch)
+        self.apply(layer_init)
+
+    def forward(self, obs, w):
+        """Predict Q values for all actions.
+
+        Args:
+            obs: current observation
+            w: weight vector
+
+        Returns: the Q values for all actions
+
+        """
+        if self.feature_extractor is not None:
+            features = self.feature_extractor(obs / 255.0)
+            input = th.cat((features, w), dim=w.dim() - 1)
+        else:
+            input = th.cat((obs, w), dim=w.dim() - 1)
+        q_values = self.net(input)
+        return q_values.view(-1, self.action_dim, self.rew_dim)  # Batch size X Actions X Rewards
+
+
+class Envelope(MOPolicy, MOAgent):
+    """Envelope Q-Leaning Algorithm.
+
+    Envelope uses a conditioned network to embed multiple policies (taking the weight as input).
+    The main change of this algorithm compare to a scalarized CN DQN is the target update.
+    Paper: R. Yang, X. Sun, and K. Narasimhan, “A Generalized Algorithm for Multi-Objective Reinforcement Learning and Policy Adaptation,” arXiv:1908.08342 [cs], Nov. 2019, Accessed: Sep. 06, 2021. [Online]. Available: http://arxiv.org/abs/1908.08342.
+    """
+
+    def __init__(
+        self,
+        env,
+        learning_rate: float = 3e-4,
+        initial_epsilon: float = 0.01,
+        final_epsilon: float = 0.01,
+        epsilon_decay_steps: int = None,  # None == fixed epsilon
+        tau: float = 1.0,
+        target_net_update_freq: int = 1000,  # ignored if tau != 1.0
+        buffer_size: int = int(1e6),
+        net_arch: List = [256, 256],
+        batch_size: int = 256,
+        learning_starts: int = 100,
+        gradient_updates: int = 1,
+        gamma: float = 0.99,
+        max_grad_norm: Optional[float] = None,
+        envelope: bool = True,
+        num_sample_w: int = 4,
+        per: bool = True,
+        per_alpha: float = 0.6,
+        initial_homotopy_lambda: float = 0.0,
+        final_homotopy_lambda: float = 1.0,
+        homotopy_decay_steps: int = None,
+        project_name: str = "MORL-Baselines",
+        experiment_name: str = "Envelope",
+        log: bool = True,
+        device: Union[th.device, str] = "auto",
+    ):
+        """Envelope Q-learning algorithm.
+
+        Args:
+            env: The environment to learn from.
+            learning_rate: The learning rate (alpha).
+            initial_epsilon: The initial epsilon value for epsilon-greedy exploration.
+            final_epsilon: The final epsilon value for epsilon-greedy exploration.
+            epsilon_decay_steps: The number of steps to decay epsilon over.
+            tau: The soft update coefficient (keep in [0, 1]).
+            target_net_update_freq: The frequency with which the target network is updated.
+            buffer_size: The size of the replay buffer.
+            net_arch: The size of the hidden layers of the value net.
+            batch_size: The size of the batch to sample from the replay buffer.
+            learning_starts: The number of steps before learning starts i.e. the agent will be random until learning starts.
+            gradient_updates: The number of gradient updates per step.
+            gamma: The discount factor (gamma).
+            max_grad_norm: The maximum norm for the gradient clipping. If None, no gradient clipping is applied.
+            envelope: Whether to use the envelope method.
+            num_sample_w: The number of weight vectors to sample for the envelope target.
+            per: Whether to use prioritized experience replay.
+            per_alpha: The alpha parameter for prioritized experience replay.
+            initial_homotopy_lambda: The initial value of the homotopy parameter for homotopy optimization.
+            final_homotopy_lambda: The final value of the homotopy parameter.
+            homotopy_decay_steps: The number of steps to decay the homotopy parameter over.
+            project_name: The name of the project, for wandb logging.
+            experiment_name: The name of the experiment, for wandb logging.
+            log: Whether to log to wandb.
+            device: The device to use for training.
+        """
+        MOAgent.__init__(self, env, device=device)
+        MOPolicy.__init__(self, device)
+        self.learning_rate = learning_rate
+        self.initial_epsilon = initial_epsilon
+        self.epsilon = initial_epsilon
+        self.epsilon_decay_steps = epsilon_decay_steps
+        self.final_epsilon = final_epsilon
+        self.tau = tau
+        self.target_net_update_freq = target_net_update_freq
+        self.gamma = gamma
+        self.max_grad_norm = max_grad_norm
+        self.buffer_size = buffer_size
+        self.net_arch = net_arch
+        self.learning_starts = learning_starts
+        self.batch_size = batch_size
+        self.per = per
+        self.per_alpha = per_alpha
+        self.gradient_updates = gradient_updates
+        self.initial_homotopy_lambda = initial_homotopy_lambda
+        self.final_homotopy_lambda = final_homotopy_lambda
+        self.homotopy_decay_steps = homotopy_decay_steps
+
+        self.q_net = QNet(self.observation_shape, self.action_dim, self.reward_dim, net_arch=net_arch).to(self.device)
+        self.target_q_net = QNet(self.observation_shape, self.action_dim, self.reward_dim, net_arch=net_arch).to(self.device)
+        self.target_q_net.load_state_dict(self.q_net.state_dict())
+        for param in self.target_q_net.parameters():
+            param.requires_grad = False
+
+        self.q_optim = optim.Adam(self.q_net.parameters(), lr=self.learning_rate)
+
+        self.envelope = envelope
+        self.num_sample_w = num_sample_w
+        self.homotopy_lambda = self.initial_homotopy_lambda
+        if self.per:
+            self.replay_buffer = PrioritizedReplayBuffer(
+                self.observation_shape,
+                1,
+                rew_dim=self.reward_dim,
+                max_size=buffer_size,
+                action_dtype=np.uint8,
+            )
+        else:
+            self.replay_buffer = ReplayBuffer(
+                self.observation_shape,
+                1,
+                rew_dim=self.reward_dim,
+                max_size=buffer_size,
+                action_dtype=np.uint8,
+            )
+
+        self.log = log
+        if log:
+            self.setup_wandb(project_name, experiment_name)
+
+    @override
+    def get_config(self):
+        return {
+            "env_id": self.env.unwrapped.spec.id,
+            "learning_rate": self.learning_rate,
+            "initial_epsilon": self.initial_epsilon,
+            "epsilon_decay_steps:": self.epsilon_decay_steps,
+            "batch_size": self.batch_size,
+            "tau": self.tau,
+            "clip_grand_norm": self.max_grad_norm,
+            "target_net_update_freq": self.target_net_update_freq,
+            "gamma": self.gamma,
+            "use_envelope": self.envelope,
+            "num_sample_w": self.num_sample_w,
+            "net_arch": self.net_arch,
+            "per": self.per,
+            "gradient_updates": self.gradient_updates,
+            "buffer_size": self.buffer_size,
+            "initial_homotopy_lambda": self.initial_homotopy_lambda,
+            "final_homotopy_lambda": self.final_homotopy_lambda,
+            "homotopy_decay_steps": self.homotopy_decay_steps,
+            "learning_starts": self.learning_starts,
+        }
+
+    def save(self, save_replay_buffer: bool = True, save_dir: str = "weights/", filename: Optional[str] = None):
+        """Save the model and the replay buffer if specified.
+
+        Args:
+            save_replay_buffer: Whether to save the replay buffer too.
+            save_dir: Directory to save the model.
+            filename: filename to save the model.
+        """
+        if not os.path.isdir(save_dir):
+            os.makedirs(save_dir)
+        saved_params = {}
+        saved_params["q_net_state_dict"] = self.q_net.state_dict()
+
+        saved_params["q_net_optimizer_state_dict"] = self.q_optim.state_dict()
+        if save_replay_buffer:
+            saved_params["replay_buffer"] = self.replay_buffer
+        filename = self.experiment_name if filename is None else filename
+        th.save(saved_params, save_dir + "/" + filename + ".tar")
+
+    def load(self, path: str, load_replay_buffer: bool = True):
+        """Load the model and the replay buffer if specified.
+
+        Args:
+            path: Path to the model.
+            load_replay_buffer: Whether to load the replay buffer too.
+        """
+        params = th.load(path)
+        self.q_net.load_state_dict(params["q_net_state_dict"])
+        self.target_q_net.load_state_dict(params["q_net_state_dict"])
+        self.q_optim.load_state_dict(params["q_net_optimizer_state_dict"])
+        if load_replay_buffer and "replay_buffer" in params:
+            self.replay_buffer = params["replay_buffer"]
+
+    def __sample_batch_experiences(self):
+        return self.replay_buffer.sample(self.batch_size, to_tensor=True, device=self.device)
+
+    @override
+    def update(self):
+        critic_losses = []
+        for g in range(self.gradient_updates):
+            if self.per:
+                (
+                    b_obs,
+                    b_actions,
+                    b_rewards,
+                    b_next_obs,
+                    b_dones,
+                    b_inds,
+                ) = self.__sample_batch_experiences()
+            else:
+                (
+                    b_obs,
+                    b_actions,
+                    b_rewards,
+                    b_next_obs,
+                    b_dones,
+                ) = self.__sample_batch_experiences()
+
+            sampled_w = (
+                th.tensor(random_weights(dim=self.reward_dim, n=self.num_sample_w, dist="gaussian")).float().to(self.device)
+            )  # sample num_sample_w random weights
+            w = sampled_w.repeat_interleave(b_obs.size(0), 0)  # repeat the weights for each sample
+            b_obs, b_actions, b_rewards, b_next_obs, b_dones = (
+                b_obs.repeat(self.num_sample_w, 1),
+                b_actions.repeat(self.num_sample_w, 1),
+                b_rewards.repeat(self.num_sample_w, 1),
+                b_next_obs.repeat(self.num_sample_w, 1),
+                b_dones.repeat(self.num_sample_w, 1),
+            )
+
+            with th.no_grad():
+                if self.envelope:
+                    target = self.envelope_target(b_next_obs, w, sampled_w)
+                else:
+                    target = self.ddqn_target(b_next_obs, w)
+                target_q = b_rewards + (1 - b_dones) * self.gamma * target
+
+            q_values = self.q_net(b_obs, w)
+            q_value = q_values.gather(
+                1,
+                b_actions.long().reshape(-1, 1, 1).expand(q_values.size(0), 1, q_values.size(2)),
+            )
+            q_value = q_value.reshape(-1, self.reward_dim)
+
+            critic_loss = F.mse_loss(q_value, target_q)
+
+            if self.homotopy_lambda > 0:
+                wQ = th.einsum("br,br->b", q_value, w)
+                wTQ = th.einsum("br,br->b", target_q, w)
+                auxiliary_loss = F.mse_loss(wQ, wTQ)
+                critic_loss = (1 - self.homotopy_lambda) * critic_loss + self.homotopy_lambda * auxiliary_loss
+
+            self.q_optim.zero_grad()
+            critic_loss.backward()
+            if self.log and self.global_step % 100 == 0:
+                self.writer.add_scalar(
+                    "losses/grad_norm",
+                    get_grad_norm(self.q_net.parameters()).item(),
+                    self.global_step,
+                )
+            if self.max_grad_norm is not None:
+                th.nn.utils.clip_grad_norm_(self.q_net.parameters(), self.max_grad_norm)
+            self.q_optim.step()
+            critic_losses.append(critic_loss.item())
+
+            if self.per:
+                td_err = (q_value[: len(b_inds)] - target_q[: len(b_inds)]).detach()
+                priority = th.einsum("sr,sr->s", td_err, w[: len(b_inds)]).abs()
+                priority = priority.cpu().numpy().flatten()
+                priority = (priority + self.replay_buffer.min_priority) ** self.per_alpha
+                self.replay_buffer.update_priorities(b_inds, priority)
+
+        if self.tau != 1 or self.global_step % self.target_net_update_freq == 0:
+            polyak_update(self.q_net.parameters(), self.target_q_net.parameters(), self.tau)
+
+        if self.epsilon_decay_steps is not None:
+            self.epsilon = linearly_decaying_value(
+                self.initial_epsilon,
+                self.epsilon_decay_steps,
+                self.global_step,
+                self.learning_starts,
+                self.final_epsilon,
+            )
+
+        if self.homotopy_decay_steps is not None:
+            self.homotopy_lambda = linearly_decaying_value(
+                self.initial_homotopy_lambda,
+                self.homotopy_decay_steps,
+                self.global_step,
+                self.learning_starts,
+                self.final_homotopy_lambda,
+            )
+
+        if self.log and self.global_step % 100 == 0:
+            self.writer.add_scalar("losses/critic_loss", np.mean(critic_losses), self.global_step)
+            self.writer.add_scalar("metrics/epsilon", self.epsilon, self.global_step)
+            self.writer.add_scalar("metrics/homotopy_lambda", self.homotopy_lambda, self.global_step)
+
+    @override
+    def eval(self, obs: np.ndarray, w: np.ndarray) -> int:
+        obs = th.as_tensor(obs).float().to(self.device)
+        w = th.as_tensor(w).float().to(self.device)
+        return self.max_action(obs, w)
+
+    def act(self, obs: th.Tensor, w: th.Tensor) -> int:
+        """Epsilon-greedily select an action given an observation and weight.
+
+        Args:
+            obs: observation
+            w: weight vector
+
+        Returns: an integer representing the action to take.
+        """
+        if np.random.random() < self.epsilon:
+            return self.env.action_space.sample()
+        else:
+            return self.max_action(obs, w)
+
+    @th.no_grad()
+    def max_action(self, obs: th.Tensor, w: th.Tensor) -> int:
+        """Select the action with the highest Q-value given an observation and weight.
+
+        Args:
+            obs: observation
+            w: weight vector
+
+        Returns: the action with the highest Q-value.
+        """
+        q_values = self.q_net(obs, w)
+        scalarized_q_values = th.einsum("r,bar->ba", w, q_values)
+        max_act = th.argmax(scalarized_q_values, dim=1)
+        return max_act.detach().item()
+
+    @th.no_grad()
+    def envelope_target(self, obs: th.Tensor, w: th.Tensor, sampled_w: th.Tensor) -> th.Tensor:
+        """Computes the envelope target for the given observation and weight.
+
+        Args:
+            obs: current observation.
+            w: current weight vector.
+            sampled_w: set of sampled weight vectors (>1!).
+
+        Returns: the envelope target.
+        """
+        # Repeat the weights for each sample
+        W = sampled_w.unsqueeze(0).repeat(obs.size(0), 1, 1)
+        # Repeat the observations for each sampled weight
+        next_obs = obs.unsqueeze(1).repeat(1, sampled_w.size(0), 1)
+
+        # Batch size X Num sampled weights X Num actions X Num objectives
+        next_q_values = self.q_net(next_obs, W).view(obs.size(0), sampled_w.size(0), self.action_dim, self.reward_dim)
+        # Scalarized Q values for each sampled weight
+        scalarized_next_q_values = th.einsum("br,bwar->bwa", w, next_q_values)
+        # Max Q values for each sampled weight
+        max_q, ac = th.max(scalarized_next_q_values, dim=2)
+        # Max weights in the envelope
+        pref = th.argmax(max_q, dim=1)
+
+        # MO Q-values evaluated on the target network
+        next_q_values_target = self.target_q_net(next_obs, W).view(
+            obs.size(0), sampled_w.size(0), self.action_dim, self.reward_dim
+        )
+
+        # Index the Q-values for the max actions
+        max_next_q = next_q_values_target.gather(
+            2,
+            ac.unsqueeze(2).unsqueeze(3).expand(next_q_values.size(0), next_q_values.size(1), 1, next_q_values.size(3)),
+        ).squeeze(2)
+        # Index the Q-values for the max sampled weights
+        max_next_q = max_next_q.gather(1, pref.reshape(-1, 1, 1).expand(max_next_q.size(0), 1, max_next_q.size(2))).squeeze(1)
+        return max_next_q
+
+    @th.no_grad()
+    def ddqn_target(self, obs: th.Tensor, w: th.Tensor) -> th.Tensor:
+        """Double DQN target for the given observation and weight.
+
+        Args:
+            obs: observation
+            w: weight vector.
+
+        Returns: the DQN target.
+        """
+        # Max action for each state
+        q_values = self.q_net(obs, w)
+        scalarized_q_values = th.einsum("br,bar->ba", w, q_values)
+        max_acts = th.argmax(scalarized_q_values, dim=1)
+        # Action evaluated with the target network
+        q_values_target = self.target_q_net(obs, w)
+        q_values_target = q_values_target.gather(
+            1,
+            max_acts.long().reshape(-1, 1, 1).expand(q_values_target.size(0), 1, q_values_target.size(2)),
+        )
+        q_values_target = q_values_target.reshape(-1, self.reward_dim)
+        return q_values_target
+
+    def train(
+        self,
+        total_timesteps: int,
+        weight: Optional[np.ndarray] = None,
+        total_episodes: Optional[int] = None,
+        reset_num_timesteps: bool = True,
+        eval_env: Optional[gym.Env] = None,
+        ref_point: Optional[np.ndarray] = None,
+        known_pareto_front: Optional[List[np.ndarray]] = None,
+        eval_freq: int = 10000,
+        eval_weights_number_for_front: int = 100,
+        reset_learning_starts: bool = False,
+    ):
+        """Train the agent.
+
+        Args:
+            total_timesteps: total number of timesteps to train for.
+            weight: weight vector. If None, it is randomly sampled every episode (as done in the paper).
+            total_episodes: total number of episodes to train for. If None, it is ignored.
+            reset_num_timesteps: whether to reset the number of timesteps. Useful when training multiple times.
+            eval_env: environment to use for evaluation. If None, it is ignored.
+            ref_point: reference point for the hypervolume computation.
+            known_pareto_front: known pareto front for the hypervolume computation.
+            eval_freq: policy evaluation frequency (in number of steps).
+            eval_weights_number_for_front: number of weights to sample for creating the pareto front when evaluating.
+            reset_learning_starts: whether to reset the learning starts. Useful when training multiple times.
+        """
+        self.global_step = 0 if reset_num_timesteps else self.global_step
+        self.num_episodes = 0 if reset_num_timesteps else self.num_episodes
+        if reset_learning_starts:  # Resets epsilon-greedy exploration
+            self.learning_starts = self.global_step
+
+        num_episodes = 0
+        eval_weights = equally_spaced_weights(self.reward_dim, n=eval_weights_number_for_front)
+        obs, _ = self.env.reset()
+
+        w = weight if weight is not None else random_weights(self.reward_dim, 1, dist="gaussian")
+        tensor_w = th.tensor(w).float().to(self.device)
+
+        for _ in range(1, total_timesteps + 1):
+            if total_episodes is not None and num_episodes == total_episodes:
+                break
+            self.global_step += 1
+
+            if self.global_step < self.learning_starts:
+                action = self.env.action_space.sample()
+            else:
+                action = self.act(th.as_tensor(obs).float().to(self.device), tensor_w)
+
+            next_obs, vec_reward, terminated, truncated, info = self.env.step(action)
+
+            self.replay_buffer.add(obs, action, vec_reward, next_obs, terminated)
+
+            if self.global_step >= self.learning_starts:
+                self.update()
+
+            if eval_env is not None and self.log and self.global_step % eval_freq == 0:
+                assert ref_point is not None, "Reference point must be provided for the hypervolume computation."
+                current_front = [self.policy_eval(eval_env, weights=ew, writer=None)[3] for ew in eval_weights]
+                log_all_multi_policy_metrics(
+                    current_front=current_front,
+                    hv_ref_point=ref_point,
+                    reward_dim=self.reward_dim,
+                    global_step=self.global_step,
+                    writer=self.writer,
+                    ref_front=known_pareto_front,
+                )
+
+            if terminated or truncated:
+                obs, _ = self.env.reset()
+                num_episodes += 1
+                self.num_episodes += 1
+
+                if weight is None:
+                    w = random_weights(self.reward_dim, 1, dist="gaussian")
+                    tensor_w = th.tensor(w).float().to(self.device)
+
+                if self.log and "episode" in info.keys():
+                    log_episode_info(info["episode"], np.dot, w, self.global_step, self.writer)
+
+            else:
+                obs = next_obs
diff --git a/morl-baselines/morl_baselines/multi_policy/gpi_pd/__init__.py b/morl-baselines/morl_baselines/multi_policy/gpi_pd/__init__.py
new file mode 100644
index 0000000..a78faf7
--- /dev/null
+++ b/morl-baselines/morl_baselines/multi_policy/gpi_pd/__init__.py
@@ -0,0 +1 @@
+"""GPI-Prioritized Dyna."""
diff --git a/morl-baselines/morl_baselines/multi_policy/gpi_pd/gpi_pd.py b/morl-baselines/morl_baselines/multi_policy/gpi_pd/gpi_pd.py
new file mode 100644
index 0000000..0ba5374
--- /dev/null
+++ b/morl-baselines/morl_baselines/multi_policy/gpi_pd/gpi_pd.py
@@ -0,0 +1,820 @@
+"""GPI-PD algorithm."""
+import os
+import random
+from itertools import chain
+from typing import Callable, List, Optional, Union
+
+import gymnasium as gym
+import numpy as np
+import torch as th
+import torch.nn as nn
+import torch.nn.functional as F
+import torch.optim as optim
+import wandb as wb
+
+from morl_baselines.common.buffer import ReplayBuffer
+from morl_baselines.common.evaluation import policy_evaluation_mo
+from morl_baselines.common.model_based.probabilistic_ensemble import (
+    ProbabilisticEnsemble,
+)
+from morl_baselines.common.model_based.utils import ModelEnv, visualize_eval
+from morl_baselines.common.morl_algorithm import MOAgent, MOPolicy
+from morl_baselines.common.networks import NatureCNN, mlp
+from morl_baselines.common.prioritized_buffer import PrioritizedReplayBuffer
+from morl_baselines.common.utils import (
+    equally_spaced_weights,
+    get_grad_norm,
+    huber,
+    layer_init,
+    linearly_decaying_value,
+    log_all_multi_policy_metrics,
+    log_episode_info,
+    polyak_update,
+)
+from morl_baselines.multi_policy.linear_support.linear_support import LinearSupport
+
+
+class QNet(nn.Module):
+    """Conditioned MO Q network."""
+
+    def __init__(self, obs_shape, action_dim, rew_dim, net_arch, drop_rate=0.01, layer_norm=True):
+        """Initialize the net.
+
+        Args:
+            obs_shape: The observation shape.
+            action_dim: The action dimension.
+            rew_dim: The reward dimension.
+            net_arch: The network architecture.
+            drop_rate: The dropout rate.
+            layer_norm: Whether to use layer normalization.
+        """
+        super().__init__()
+        self.obs_shape = obs_shape
+        self.action_dim = action_dim
+        self.phi_dim = rew_dim
+
+        self.weights_features = mlp(rew_dim, -1, net_arch[:1])
+        if len(obs_shape) == 1:
+            self.state_features = mlp(obs_shape[0], -1, net_arch[:1])
+        elif len(obs_shape) > 1:  # Image observation
+            self.state_features = NatureCNN(self.obs_shape, features_dim=net_arch[0])
+        self.net = mlp(
+            net_arch[0], action_dim * rew_dim, net_arch[1:], drop_rate=drop_rate, layer_norm=layer_norm
+        )  # 128/128 256 256 256
+
+        self.apply(layer_init)
+
+    def forward(self, obs, w):
+        """Forward pass."""
+        sf = self.state_features(obs)
+        wf = self.weights_features(w)
+        q_values = self.net(sf * wf)
+        return q_values.view(-1, self.action_dim, self.phi_dim)  # Batch size X Actions X Rewards
+
+
+class GPIPD(MOPolicy, MOAgent):
+    """GPI-PD Algorithm.
+
+    Sample-Efficient Multi-Objective Learning via Generalized Policy Improvement Prioritization
+    Lucas N. Alegre, Ana L. C. Bazzan, Diederik M. Roijers, Ann Nowé, Bruno C. da Silva
+    AAMAS 2023
+    Paper: https://arxiv.org/abs/2301.07784
+    """
+
+    def __init__(
+        self,
+        env,
+        learning_rate: float = 3e-4,
+        initial_epsilon: float = 0.01,
+        final_epsilon: float = 0.01,
+        epsilon_decay_steps: int = None,  # None == fixed epsilon
+        tau: float = 1.0,
+        target_net_update_freq: int = 1000,  # ignored if tau != 1.0
+        buffer_size: int = int(1e6),
+        net_arch: List = [256, 256, 256, 256],
+        num_nets: int = 2,
+        batch_size: int = 256,
+        learning_starts: int = 100,
+        gradient_updates: int = 1,
+        gamma: float = 0.99,
+        max_grad_norm: Optional[float] = None,
+        use_gpi: bool = True,
+        dyna: bool = False,
+        per: bool = True,
+        gpi_pd: bool = True,
+        alpha_per: float = 0.6,
+        min_priority: float = 1.0,
+        drop_rate: float = 0.01,
+        layer_norm: bool = True,
+        dynamics_normalize_inputs: bool = False,
+        dynamics_uncertainty_threshold: float = 0.5,
+        dynamics_train_freq: Callable = lambda timestep: 250,
+        dynamics_rollout_len: int = 1,
+        dynamics_rollout_starts: int = 5000,
+        dynamics_rollout_freq: int = 250,
+        dynamics_rollout_batch_size: int = 10000,
+        dynamics_buffer_size: int = 400000,
+        dynamics_net_arch: List = [200, 200, 200, 200],
+        dynamics_ensemble_size: int = 5,
+        dynamics_num_elites: int = 2,
+        real_ratio: float = 0.05,
+        project_name: str = "MORL-Baselines",
+        experiment_name: str = "GPI-PD",
+        log: bool = True,
+        device: Union[th.device, str] = "auto",
+    ):
+        """Initialize the GPI-PD algorithm.
+
+        Args:
+            env: The environment to learn from.
+            learning_rate: The learning rate.
+            initial_epsilon: The initial epsilon value.
+            final_epsilon: The final epsilon value.
+            epsilon_decay_steps: The number of steps to decay epsilon.
+            tau: The soft update coefficient.
+            target_net_update_freq: The target network update frequency.
+            buffer_size: The size of the replay buffer.
+            net_arch: The network architecture.
+            num_nets: The number of networks.
+            batch_size: The batch size.
+            learning_starts: The number of steps before learning starts.
+            gradient_updates: The number of gradient updates per step.
+            gamma: The discount factor.
+            max_grad_norm: The maximum gradient norm.
+            use_gpi: Whether to use GPI.
+            dyna: Whether to use Dyna.
+            per: Whether to use PER.
+            gpi_pd: Whether to use GPI-PD.
+            alpha_per: The alpha parameter for PER.
+            min_priority: The minimum priority for PER.
+            drop_rate: The dropout rate.
+            layer_norm: Whether to use layer normalization.
+            dynamics_normalize_inputs: Whether to normalize inputs to the dynamics model.
+            dynamics_uncertainty_threshold: The uncertainty threshold for the dynamics model.
+            dynamics_train_freq: The dynamics model training frequency.
+            dynamics_rollout_len: The rollout length for the dynamics model.
+            dynamics_rollout_starts: The number of steps before the first rollout.
+            dynamics_rollout_freq: The rollout frequency.
+            dynamics_rollout_batch_size: The rollout batch size.
+            dynamics_buffer_size: The size of the dynamics model buffer.
+            dynamics_net_arch: The network architecture for the dynamics model.
+            dynamics_ensemble_size: The ensemble size for the dynamics model.
+            dynamics_num_elites: The number of elites for the dynamics model.
+            real_ratio: The ratio of real transitions to sample.
+            project_name: The name of the project.
+            experiment_name: The name of the experiment.
+            log: Whether to log.
+            device: The device to use.
+        """
+        MOAgent.__init__(self, env, device=device)
+        MOPolicy.__init__(self, device)
+        self.learning_rate = learning_rate
+        self.initial_epsilon = initial_epsilon
+        self.epsilon = initial_epsilon
+        self.epsilon_decay_steps = epsilon_decay_steps
+        self.final_epsilon = final_epsilon
+        self.tau = tau
+        self.target_net_update_freq = target_net_update_freq
+        self.gamma = gamma
+        self.max_grad_norm = max_grad_norm
+        self.use_gpi = use_gpi
+        self.buffer_size = buffer_size
+        self.net_arch = net_arch
+        self.learning_starts = learning_starts
+        self.batch_size = batch_size
+        self.gradient_updates = gradient_updates
+        self.num_nets = num_nets
+        self.drop_rate = drop_rate
+        self.layer_norm = layer_norm
+
+        # Q-Networks
+        self.q_nets = [
+            QNet(
+                self.observation_shape,
+                self.action_dim,
+                self.reward_dim,
+                net_arch=net_arch,
+                drop_rate=drop_rate,
+                layer_norm=layer_norm,
+            ).to(self.device)
+            for _ in range(self.num_nets)
+        ]
+        self.target_q_nets = [
+            QNet(
+                self.observation_shape,
+                self.action_dim,
+                self.reward_dim,
+                net_arch=net_arch,
+                drop_rate=drop_rate,
+                layer_norm=layer_norm,
+            ).to(self.device)
+            for _ in range(self.num_nets)
+        ]
+        for q, target_q in zip(self.q_nets, self.target_q_nets):
+            target_q.load_state_dict(q.state_dict())
+            for param in target_q.parameters():
+                param.requires_grad = False
+        self.q_optim = optim.Adam(chain(*[net.parameters() for net in self.q_nets]), lr=self.learning_rate)
+
+        # Prioritized experience replay parameters
+        self.per = per
+        self.gpi_pd = gpi_pd
+        if self.per:
+            self.replay_buffer = PrioritizedReplayBuffer(
+                self.observation_shape, 1, rew_dim=self.reward_dim, max_size=buffer_size, action_dtype=np.uint8
+            )
+        else:
+            self.replay_buffer = ReplayBuffer(
+                self.observation_shape, 1, rew_dim=self.reward_dim, max_size=buffer_size, action_dtype=np.uint8
+            )
+        self.min_priority = min_priority
+        self.alpha = alpha_per
+
+        # model-based parameters
+        self.dyna = dyna
+        self.dynamics_net_arch = dynamics_net_arch
+        if self.dyna:
+            self.dynamics = ProbabilisticEnsemble(
+                input_dim=self.observation_dim + self.action_dim,
+                output_dim=self.observation_dim + self.reward_dim,
+                arch=self.dynamics_net_arch,
+                normalize_inputs=dynamics_normalize_inputs,
+                ensemble_size=dynamics_ensemble_size,
+                num_elites=dynamics_num_elites,
+                device=self.device,
+            )
+            self.dynamics_buffer = ReplayBuffer(
+                self.observation_shape, 1, rew_dim=self.reward_dim, max_size=dynamics_buffer_size, action_dtype=np.uint8
+            )
+        self.dynamics_train_freq = dynamics_train_freq
+        self.dynamics_rollout_len = dynamics_rollout_len
+        self.dynamics_rollout_starts = dynamics_rollout_starts
+        self.dynamics_rollout_freq = dynamics_rollout_freq
+        self.dynamics_rollout_batch_size = dynamics_rollout_batch_size
+        self.dynamics_uncertainty_threshold = dynamics_uncertainty_threshold
+        self.real_ratio = real_ratio
+
+        self.log = log
+        if self.log:
+            self.setup_wandb(project_name, experiment_name)
+
+    def get_config(self):
+        """Return the configuration of the agent."""
+        return {
+            "env_id": self.env.unwrapped.spec.id,
+            "learning_rate": self.learning_rate,
+            "initial_epsilon": self.initial_epsilon,
+            "epsilon_decay_steps:": self.epsilon_decay_steps,
+            "batch_size": self.batch_size,
+            "per": self.per,
+            "gpi_pd": self.gpi_pd,
+            "alpha_per": self.alpha,
+            "min_priority": self.min_priority,
+            "tau": self.tau,
+            "num_nets": self.num_nets,
+            "clip_grand_norm": self.max_grad_norm,
+            "target_net_update_freq": self.target_net_update_freq,
+            "gamma": self.gamma,
+            "net_arch": self.net_arch,
+            "dynamics_model_arch": self.dynamics_net_arch,
+            "gradient_updates": self.gradient_updates,
+            "buffer_size": self.buffer_size,
+            "learning_starts": self.learning_starts,
+            "dyna": self.dyna,
+            "dynamics_rollout_len": self.dynamics_rollout_len,
+            "dynamics_uncertainty_threshold": self.dynamics_uncertainty_threshold,
+            "dynamics_rollout_starts": self.dynamics_rollout_starts,
+            "dynamics_rollout_freq": self.dynamics_rollout_freq,
+            "dynamics_rollout_batch_size": self.dynamics_rollout_batch_size,
+            "dynamics_buffer_size": self.dynamics_buffer.max_size,
+            "dynamics_normalize_inputs": self.dynamics.normalize_inputs,
+            "dynamics_ensemble_size": self.dynamics.ensemble_size,
+            "dynamics_num_elites": self.dynamics.num_elites,
+            "real_ratio": self.real_ratio,
+            "drop_rate": self.drop_rate,
+            "layer_norm": self.layer_norm,
+        }
+
+    def save(self, save_replay_buffer=True, save_dir="weights/", filename=None):
+        """Save the model parameters and the replay buffer."""
+        if not os.path.isdir(save_dir):
+            os.makedirs(save_dir)
+        saved_params = {}
+        for i, psi_net in enumerate(self.q_nets):
+            saved_params[f"psi_net_{i}_state_dict"] = psi_net.state_dict()
+        saved_params["psi_nets_optimizer_state_dict"] = self.q_optim.state_dict()
+        saved_params["M"] = self.weight_support
+        if self.dyna:
+            saved_params["dynamics_state_dict"] = self.dynamics.state_dict()
+        if save_replay_buffer:
+            saved_params["replay_buffer"] = self.replay_buffer
+        filename = self.experiment_name if filename is None else filename
+        th.save(saved_params, save_dir + "/" + filename + ".tar")
+
+    def load(self, path, load_replay_buffer=True):
+        """Load the model parameters and the replay buffer."""
+        params = th.load(path, map_location=self.device)
+        for i, (psi_net, target_psi_net) in enumerate(zip(self.q_nets, self.target_q_nets)):
+            psi_net.load_state_dict(params[f"psi_net_{i}_state_dict"])
+            target_psi_net.load_state_dict(params[f"psi_net_{i}_state_dict"])
+        self.q_optim.load_state_dict(params["psi_nets_optimizer_state_dict"])
+        self.weight_support = params["M"]
+        if self.dyna:
+            self.dynamics.load_state_dict(params["dynamics_state_dict"])
+        if load_replay_buffer and "replay_buffer" in params:
+            self.replay_buffer = params["replay_buffer"]
+
+    def _sample_batch_experiences(self):
+        if not self.dyna or self.global_step < self.dynamics_rollout_starts or len(self.dynamics_buffer) == 0:
+            return self.replay_buffer.sample(self.batch_size, to_tensor=True, device=self.device)
+        else:
+            num_real_samples = int(self.batch_size * self.real_ratio)  # real_ratio% of real world data
+            if self.per:
+                s_obs, s_actions, s_rewards, s_next_obs, s_dones, idxes = self.replay_buffer.sample(
+                    num_real_samples, to_tensor=True, device=self.device
+                )
+            else:
+                s_obs, s_actions, s_rewards, s_next_obs, s_dones = self.replay_buffer.sample(
+                    num_real_samples, to_tensor=True, device=self.device
+                )
+            m_obs, m_actions, m_rewards, m_next_obs, m_dones = self.dynamics_buffer.sample(
+                self.batch_size - num_real_samples, to_tensor=True, device=self.device
+            )
+            experience_tuples = (
+                th.cat([s_obs, m_obs], dim=0),
+                th.cat([s_actions, m_actions], dim=0),
+                th.cat([s_rewards, m_rewards], dim=0),
+                th.cat([s_next_obs, m_next_obs], dim=0),
+                th.cat([s_dones, m_dones], dim=0),
+            )
+            if self.per:
+                return experience_tuples + (idxes,)
+            return experience_tuples
+
+    @th.no_grad()
+    def _rollout_dynamics(self, w: th.Tensor):
+        # Dyna Planning
+        num_times = int(np.ceil(self.dynamics_rollout_batch_size / 10000))
+        batch_size = min(self.dynamics_rollout_batch_size, 10000)
+        num_added_imagined_transitions = 0
+        for iteration in range(num_times):
+            obs = self.replay_buffer.sample_obs(batch_size, to_tensor=False)
+            model_env = ModelEnv(self.dynamics, self.env.unwrapped.spec.id, rew_dim=len(w))
+
+            for h in range(self.dynamics_rollout_len):
+                obs = th.tensor(obs).to(self.device)
+                M = th.stack(self.weight_support)
+                M = M.unsqueeze(0).repeat(len(obs), 1, 1)
+                obs_m = obs.unsqueeze(1).repeat(1, M.size(1), 1)
+
+                psi_values = self.q_nets[0](obs_m, M)
+                q_values = th.einsum("r,bar->ba", w, psi_values).view(obs.size(0), len(self.weight_support), self.action_dim)
+                max_q, ac = th.max(q_values, dim=2)
+                pi = th.argmax(max_q, dim=1)
+                actions = ac.gather(1, pi.unsqueeze(1))
+                actions_one_hot = F.one_hot(actions, num_classes=self.action_dim).squeeze(1)
+
+                next_obs_pred, r_pred, dones, info = model_env.step(obs, actions_one_hot, deterministic=False)
+                uncertainties = info["uncertainty"]
+                obs, actions = obs.cpu().numpy(), actions.cpu().numpy()
+
+                for i in range(len(obs)):
+                    if uncertainties[i] < self.dynamics_uncertainty_threshold:
+                        self.dynamics_buffer.add(obs[i], actions[i], r_pred[i], next_obs_pred[i], dones[i])
+                        num_added_imagined_transitions += 1
+
+                nonterm_mask = ~dones.squeeze(-1)
+                if nonterm_mask.sum() == 0:
+                    break
+                obs = next_obs_pred[nonterm_mask]
+
+        if self.log:
+            self.writer.add_scalar("dynamics/uncertainty_mean", uncertainties.mean(), self.global_step)
+            self.writer.add_scalar("dynamics/uncertainty_max", uncertainties.max(), self.global_step)
+            self.writer.add_scalar("dynamics/uncertainty_min", uncertainties.min(), self.global_step)
+            self.writer.add_scalar("dynamics/model_buffer_size", len(self.dynamics_buffer), self.global_step)
+            self.writer.add_scalar("dynamics/imagined_transitions", num_added_imagined_transitions, self.global_step)
+
+    def update(self, weight: th.Tensor):
+        """Update the parameters of the networks."""
+        critic_losses = []
+        for g in range(self.gradient_updates if self.global_step >= self.dynamics_rollout_starts else 1):
+            if self.per:
+                s_obs, s_actions, s_rewards, s_next_obs, s_dones, idxes = self._sample_batch_experiences()
+            else:
+                s_obs, s_actions, s_rewards, s_next_obs, s_dones = self._sample_batch_experiences()
+
+            if len(self.weight_support) > 1:
+                s_obs, s_actions, s_rewards, s_next_obs, s_dones = (
+                    s_obs.repeat(2, 1),
+                    s_actions.repeat(2, 1),
+                    s_rewards.repeat(2, 1),
+                    s_next_obs.repeat(2, 1),
+                    s_dones.repeat(2, 1),
+                )
+                # Half of the batch uses the given weight vector, the other half uses weights sampled from the support set
+                w = th.vstack(
+                    [weight for _ in range(s_obs.size(0) // 2)] + random.choices(self.weight_support, k=s_obs.size(0) // 2)
+                )
+            else:
+                w = weight.repeat(s_obs.size(0), 1)
+
+            if len(self.weight_support) > 5:
+                sampled_w = th.stack([weight] + random.sample(self.weight_support, k=4))
+            else:
+                sampled_w = th.stack(self.weight_support)
+
+            with th.no_grad():
+                # Compute min_i Q_i(s', a, w) . w
+                next_q_values = th.stack([target_psi_net(s_next_obs, w) for target_psi_net in self.target_q_nets])
+                scalarized_next_q_values = th.einsum("nbar,br->nba", next_q_values, w)  # q_i(s', a, w)
+                min_inds = th.argmin(scalarized_next_q_values, dim=0)
+                min_inds = min_inds.reshape(1, next_q_values.size(1), next_q_values.size(2), 1).expand(
+                    1, next_q_values.size(1), next_q_values.size(2), next_q_values.size(3)
+                )
+                next_q_values = next_q_values.gather(0, min_inds).squeeze(0)
+
+                # Compute max_a Q(s', a, w) . w
+                max_q = th.einsum("br,bar->ba", w, next_q_values)
+                max_acts = th.argmax(max_q, dim=1)
+
+                q_targets = next_q_values.gather(
+                    1, max_acts.long().reshape(-1, 1, 1).expand(next_q_values.size(0), 1, next_q_values.size(2))
+                )
+                target_q = q_targets.reshape(-1, self.reward_dim)
+                target_q = s_rewards + (1 - s_dones) * self.gamma * target_q
+
+                if self.gpi_pd:
+                    target_q_envelope, _ = self._envelope_target(s_next_obs, w, sampled_w)
+                    target_q_envelope = s_rewards + (1 - s_dones) * self.gamma * target_q_envelope
+
+            losses = []
+            td_errors = []
+            gtd_errors = []
+            for psi_net in self.q_nets:
+                psi_value = psi_net(s_obs, w)
+                psi_value = psi_value.gather(
+                    1, s_actions.long().reshape(-1, 1, 1).expand(psi_value.size(0), 1, psi_value.size(2))
+                )
+                psi_value = psi_value.reshape(-1, self.reward_dim)
+
+                if self.gpi_pd:
+                    gtd_error = psi_value - target_q_envelope
+
+                td_error = psi_value - target_q
+                loss = huber(td_error.abs(), min_priority=self.min_priority)
+                losses.append(loss)
+                if self.gpi_pd:
+                    gtd_errors.append(gtd_error.abs())
+                if self.per:
+                    td_errors.append(td_error.abs())
+            critic_loss = (1 / self.num_nets) * sum(losses)
+
+            self.q_optim.zero_grad()
+            critic_loss.backward()
+            if self.log and self.global_step % 100 == 0:
+                self.writer.add_scalar("losses/grad_norm", get_grad_norm(self.q_nets[0].parameters()).item(), self.global_step)
+            if self.max_grad_norm is not None:
+                for psi_net in self.q_nets:
+                    th.nn.utils.clip_grad_norm_(psi_net.parameters(), self.max_grad_norm)
+            self.q_optim.step()
+            critic_losses.append(critic_loss.item())
+
+            if self.per or self.gpi_pd:
+                if self.gpi_pd:
+                    gtd_error = th.max(th.stack(gtd_errors), dim=0)[0]
+                    gtd_error = gtd_error[: len(idxes)].detach()
+                    gper = th.einsum("br,br->b", w[: len(idxes)], gtd_error).abs()
+                    gpriority = gper.cpu().numpy().flatten()
+                    gpriority = gpriority.clip(min=self.min_priority) ** self.alpha
+
+                if self.per:
+                    td_error = th.max(th.stack(td_errors), dim=0)[0]
+                    td_error = td_error[: len(idxes)].detach()
+                    per = th.einsum("br,br->b", w[: len(idxes)], td_error).abs()
+                    priority = per.cpu().numpy().flatten()
+                    priority = priority.clip(min=self.min_priority) ** self.alpha
+
+                if self.gpi_pd:
+                    self.replay_buffer.update_priorities(idxes, gpriority)
+                else:
+                    self.replay_buffer.update_priorities(idxes, priority)
+
+        if self.tau != 1 or self.global_step % self.target_net_update_freq == 0:
+            for psi_net, target_psi_net in zip(self.q_nets, self.target_q_nets):
+                polyak_update(psi_net.parameters(), target_psi_net.parameters(), self.tau)
+
+        if self.epsilon_decay_steps is not None:
+            self.epsilon = linearly_decaying_value(
+                self.initial_epsilon, self.epsilon_decay_steps, self.global_step, self.learning_starts, self.final_epsilon
+            )
+
+        if self.log and self.global_step % 100 == 0:
+            if self.per:
+                self.writer.add_scalar("metrics/mean_priority", np.mean(priority), self.global_step)
+                self.writer.add_scalar("metrics/max_priority", np.max(priority), self.global_step)
+                self.writer.add_scalar("metrics/mean_td_error_w", per.abs().mean().item(), self.global_step)
+            if self.gpi_pd:
+                self.writer.add_scalar("metrics/mean_gpriority", np.mean(gpriority), self.global_step)
+                self.writer.add_scalar("metrics/max_gpriority", np.max(gpriority), self.global_step)
+                self.writer.add_scalar("metrics/mean_gtd_error_w", gper.abs().mean().item(), self.global_step)
+                self.writer.add_scalar("metrics/mean_absolute_diff_gtd_td", (gper - per).abs().mean().item(), self.global_step)
+            self.writer.add_scalar("losses/critic_loss", np.mean(critic_losses), self.global_step)
+            self.writer.add_scalar("metrics/epsilon", self.epsilon, self.global_step)
+
+    @th.no_grad()
+    def gpi_action(self, obs: th.Tensor, w: th.Tensor, return_policy_index=False, include_w=False):
+        """Select an action using GPI."""
+        if include_w:
+            M = th.stack(self.weight_support + [w])
+        else:
+            M = th.stack(self.weight_support)
+
+        obs_m = obs.repeat(M.size(0), *(1 for _ in range(obs.dim())))
+        q_values = self.q_nets[0](obs_m, M)
+
+        scalar_q_values = th.einsum("r,bar->ba", w, q_values)  # q(s,a,w_i) = q(s,a,w_i) . w
+        max_q, a = th.max(scalar_q_values, dim=1)
+        policy_index = th.argmax(max_q)  # max_i max_a q(s,a,w_i)
+        action = a[policy_index].detach().item()
+
+        if return_policy_index:
+            return action, policy_index.item()
+        return action
+
+    def eval(self, obs: np.ndarray, w: np.ndarray) -> int:
+        """Select an action for the given obs and weight vector."""
+        obs = th.as_tensor(obs).float().to(self.device)
+        w = th.as_tensor(w).float().to(self.device)
+        if self.use_gpi:
+            action = self.gpi_action(obs, w, include_w=False)
+        else:
+            action = self.max_action(obs, w)
+        return action
+
+    def _act(self, obs: th.Tensor, w: th.Tensor) -> int:
+        if np.random.random() < self.epsilon:
+            return self.env.action_space.sample()
+        else:
+            if self.use_gpi:
+                action, policy_index = self.gpi_action(obs, w, return_policy_index=True)
+                self.police_indices.append(policy_index)
+                return action
+            else:
+                return self.max_action(obs, w)
+
+    @th.no_grad()
+    def max_action(self, obs: th.Tensor, w: th.Tensor) -> int:
+        """Select the greedy action."""
+        psi = th.min(th.stack([psi_net(obs, w) for psi_net in self.q_nets]), dim=0)[0]
+        # psi = self.psi_nets[0](obs, w)
+        q = th.einsum("r,bar->ba", w, psi)
+        max_act = th.argmax(q, dim=1)
+        return max_act.detach().item()
+
+    @th.no_grad()
+    def _reset_priorities(self, w: th.Tensor):
+        inds = np.arange(self.replay_buffer.size)
+        priorities = np.repeat(0.1, self.replay_buffer.size)
+        (
+            obs_s,
+            actions_s,
+            rewards_s,
+            next_obs_s,
+            dones_s,
+        ) = self.replay_buffer.get_all_data(to_tensor=True, device=self.device)
+        num_batches = int(np.ceil(obs_s.size(0) / 1000))
+        for i in range(num_batches):
+            b = i * 1000
+            e = min((i + 1) * 1000, obs_s.size(0))
+            obs, actions, rewards, next_obs, dones = obs_s[b:e], actions_s[b:e], rewards_s[b:e], next_obs_s[b:e], dones_s[b:e]
+            q_values = self.q_nets[0](obs, w.repeat(obs.size(0), 1))
+            q_a = q_values.gather(1, actions.long().reshape(-1, 1, 1).expand(q_values.size(0), 1, q_values.size(2))).squeeze(1)
+
+            if self.gpi_pd:
+                max_next_q, _ = self._envelope_target(next_obs, w.repeat(next_obs.size(0), 1), th.stack(self.weight_support))
+            else:
+                next_q_values = self.q_nets[0](next_obs, w.repeat(next_obs.size(0), 1))
+                max_q = th.einsum("r,bar->ba", w, next_q_values)
+                max_acts = th.argmax(max_q, dim=1)
+                q_targets = self.target_q_nets[0](next_obs, w.repeat(next_obs.size(0), 1))
+                q_targets = q_targets.gather(
+                    1, max_acts.long().reshape(-1, 1, 1).expand(q_targets.size(0), 1, q_targets.size(2))
+                )
+                max_next_q = q_targets.reshape(-1, self.reward_dim)
+
+            gtderror = th.einsum("r,br->b", w, (rewards + (1 - dones) * self.gamma * max_next_q - q_a)).abs()
+            priorities[b:e] = gtderror.clamp(min=self.min_priority).pow(self.alpha).cpu().detach().numpy().flatten()
+
+        self.replay_buffer.update_priorities(inds, priorities)
+
+    @th.no_grad()
+    def _envelope_target(self, obs: th.Tensor, w: th.Tensor, sampled_w: th.Tensor):
+        W = sampled_w.unsqueeze(0).repeat(obs.size(0), 1, 1)
+        next_obs = obs.unsqueeze(1).repeat(1, sampled_w.size(0), 1)
+
+        next_q_target = th.stack(
+            [
+                target_net(next_obs, W).view(obs.size(0), sampled_w.size(0), self.action_dim, self.reward_dim)
+                for target_net in self.target_q_nets
+            ]
+        )
+
+        q_values = th.einsum("br,nbpar->nbpa", w, next_q_target)
+        min_inds = th.argmin(q_values, dim=0)
+        min_inds = min_inds.reshape(1, next_q_target.size(1), next_q_target.size(2), next_q_target.size(3), 1).expand(
+            1, next_q_target.size(1), next_q_target.size(2), next_q_target.size(3), next_q_target.size(4)
+        )
+        next_q_target = next_q_target.gather(0, min_inds).squeeze(0)
+
+        q_values = th.einsum("br,bpar->bpa", w, next_q_target)
+        max_q, ac = th.max(q_values, dim=2)
+        pi = th.argmax(max_q, dim=1)
+
+        max_next_q = next_q_target.gather(
+            2,
+            ac.unsqueeze(2).unsqueeze(3).expand(next_q_target.size(0), next_q_target.size(1), 1, next_q_target.size(3)),
+        ).squeeze(2)
+        max_next_q = max_next_q.gather(1, pi.reshape(-1, 1, 1).expand(max_next_q.size(0), 1, max_next_q.size(2))).squeeze(1)
+        return max_next_q, next_q_target
+
+    def set_weight_support(self, weight_list: List[np.ndarray]):
+        """Set the weight support set."""
+        self.weight_support = [th.tensor(w).float().to(self.device) for w in weight_list]
+
+    def train_iteration(
+        self,
+        total_timesteps: int,
+        weight: np.ndarray,
+        weight_support: List[np.ndarray],
+        change_w_every_episode: bool = True,
+        reset_num_timesteps: bool = True,
+        eval_env: Optional[gym.Env] = None,
+        eval_freq: int = 1000,
+        reset_learning_starts: bool = False,
+    ):
+        """Train the agent for one iteration.
+
+        Args:
+            total_timesteps (int): Number of timesteps to train for
+            weight (np.ndarray): Weight vector
+            weight_support (List[np.ndarray]): Weight support set
+            change_w_every_episode (bool): Whether to change the weight vector at the end of each episode
+            reset_num_timesteps (bool): Whether to reset the number of timesteps
+            eval_env (Optional[gym.Env]): Environment to evaluate on
+            eval_freq (int): Number of timesteps between evaluations
+            reset_learning_starts (bool): Whether to reset the learning starts
+        """
+        self.weight_support = [th.tensor(z).float().to(self.device) for z in weight_support]
+        tensor_w = th.tensor(weight).float().to(self.device)
+
+        self.police_indices = []
+        self.global_step = 0 if reset_num_timesteps else self.global_step
+        self.num_episodes = 0 if reset_num_timesteps else self.num_episodes
+        if reset_learning_starts:  # Resets epsilon-greedy exploration
+            self.learning_starts = self.global_step
+
+        if self.per and len(self.replay_buffer) > 0:
+            self._reset_priorities(tensor_w)
+
+        obs, info = self.env.reset()
+        for _ in range(1, total_timesteps + 1):
+            self.global_step += 1
+
+            if self.global_step < self.learning_starts:
+                action = self.env.action_space.sample()
+            else:
+                action = self._act(th.as_tensor(obs).float().to(self.device), tensor_w)
+
+            next_obs, vec_reward, terminated, truncated, info = self.env.step(action)
+
+            self.replay_buffer.add(obs, action, vec_reward, next_obs, terminated)
+
+            if self.global_step >= self.learning_starts:
+                if self.dyna:
+                    if self.global_step % self.dynamics_train_freq(self.global_step) == 0:
+                        m_obs, m_actions, m_rewards, m_next_obs, m_dones = self.replay_buffer.get_all_data()
+                        one_hot = np.zeros((len(m_obs), self.action_dim))
+                        one_hot[np.arange(len(m_obs)), m_actions.astype(int).reshape(len(m_obs))] = 1
+                        X = np.hstack((m_obs, one_hot))
+                        Y = np.hstack((m_rewards, m_next_obs - m_obs))
+                        mean_holdout_loss = self.dynamics.fit(X, Y)
+                        if self.log:
+                            self.writer.add_scalar("dynamics/mean_holdout_loss", mean_holdout_loss, self.global_step)
+
+                    if self.global_step >= self.dynamics_rollout_starts and self.global_step % self.dynamics_rollout_freq == 0:
+                        self._rollout_dynamics(tensor_w)
+
+                self.update(tensor_w)
+
+            if eval_env is not None and self.log and self.global_step % eval_freq == 0:
+                self.policy_eval(eval_env, weights=weight, writer=self.writer)
+
+                if self.dyna and self.global_step >= self.dynamics_rollout_starts:
+                    plot = visualize_eval(self, eval_env, self.dynamics, weight, compound=False, horizon=1000)
+                    wb.log({"dynamics/predictions": wb.Image(plot), "global_step": self.global_step})
+                    plot.close()
+
+            if terminated or truncated:
+                obs, _ = self.env.reset()
+                self.num_episodes += 1
+
+                if self.log and "episode" in info.keys():
+                    log_episode_info(info["episode"], np.dot, weight, self.global_step, writer=self.writer)
+                    wb.log({"metrics/policy_index": np.array(self.police_indices), "global_step": self.global_step})
+                    self.police_indices = []
+
+                if change_w_every_episode:
+                    weight = random.choice(weight_support)
+                    tensor_w = th.tensor(weight).float().to(self.device)
+            else:
+                obs = next_obs
+
+    def train(
+        self,
+        eval_env,
+        ref_point: np.ndarray,
+        known_pareto_front: Optional[List[np.ndarray]] = None,
+        eval_weights_number_for_front: int = 100,
+        timesteps_per_iter: int = 10000,
+        max_iter: int = 15,
+        weight_selection_algo: str = "gpi-ls",
+    ):
+        """Train agent.
+
+        Args:
+            eval_env (gym.Env): Environment to evaluate on
+            ref_point (np.ndarray): Reference point for hypervolume calculation
+            known_pareto_front (Optional[List[np.ndarray]]): Optimal Pareto front if known.
+            eval_weights_number_for_front: Number of weights to evaluate for the Pareto front
+            timesteps_per_iter (int): Number of timesteps to train for per iteration
+            max_iter (int): Number of iterations to train for
+            weight_selection_algo (str): Weight selection algorithm to use
+        """
+        linear_support = LinearSupport(num_objectives=self.reward_dim, epsilon=0.0 if weight_selection_algo == "ols" else None)
+
+        weight_history = []
+
+        eval_weights = equally_spaced_weights(self.reward_dim, n=eval_weights_number_for_front)
+
+        for iter in range(1, max_iter + 1):
+            if weight_selection_algo == "ols" or weight_selection_algo == "gpi-ls":
+                if weight_selection_algo == "gpi-ls":
+                    self.set_weight_support(linear_support.get_weight_support())
+                    w = linear_support.next_weight(algo="gpi-ls", gpi_agent=self, env=eval_env)
+                else:
+                    w = linear_support.next_weight(algo="ols")
+
+                if w is None:
+                    break
+            else:
+                raise ValueError(f"Unknown algorithm {weight_selection_algo}.")
+
+            print("Next weight vector:", w)
+            weight_history.append(w)
+            if weight_selection_algo == "gpi-ls":
+                M = linear_support.get_weight_support() + linear_support.get_corner_weights(top_k=4) + [w]
+            elif weight_selection_algo == "ols":
+                M = linear_support.get_weight_support() + [w]
+            else:
+                M = None
+
+            self.train_iteration(
+                total_timesteps=timesteps_per_iter,
+                weight=w,
+                weight_support=M,
+                change_w_every_episode=weight_selection_algo == "gpi-ls",
+                eval_env=eval_env,
+                eval_freq=1000,
+                reset_num_timesteps=False,
+                reset_learning_starts=False,
+            )
+
+            if weight_selection_algo == "ols":
+                value = policy_evaluation_mo(self, eval_env, w, rep=5)[3]
+                linear_support.add_solution(value, w)
+            elif weight_selection_algo == "gpi-ls":
+                for wcw in M:
+                    n_value = policy_evaluation_mo(self, eval_env, wcw, rep=5)[3]
+                    linear_support.add_solution(n_value, wcw)
+
+            if self.log:
+                # Evaluation
+                gpi_returns_test_tasks = [policy_evaluation_mo(self, eval_env, w, rep=5)[3] for w in eval_weights]
+                log_all_multi_policy_metrics(
+                    current_front=gpi_returns_test_tasks,
+                    hv_ref_point=ref_point,
+                    reward_dim=self.reward_dim,
+                    global_step=self.global_step,
+                    writer=self.writer,
+                    ref_front=known_pareto_front,
+                )
+                # This is the EU computed in the paper
+                mean_gpi_returns_test_tasks = np.mean(
+                    [np.dot(w, q) for w, q in zip(eval_weights, gpi_returns_test_tasks)], axis=0
+                )
+                wb.log({"eval/Mean Utility - GPI": mean_gpi_returns_test_tasks, "iteration": iter})
+
+            self.save(filename=f"GPI-PD {weight_selection_algo} iter={iter}", save_replay_buffer=False)
+
+        self.close_wandb()
diff --git a/morl-baselines/morl_baselines/multi_policy/gpi_pd/gpi_pd_continuous_action.py b/morl-baselines/morl_baselines/multi_policy/gpi_pd/gpi_pd_continuous_action.py
new file mode 100644
index 0000000..f0fe328
--- /dev/null
+++ b/morl-baselines/morl_baselines/multi_policy/gpi_pd/gpi_pd_continuous_action.py
@@ -0,0 +1,649 @@
+"""GPI-PD algorithm with continuous actions."""
+import os
+import random
+from itertools import chain
+from typing import List, Optional, Union
+
+import gymnasium
+import numpy as np
+import torch as th
+import torch.nn as nn
+import torch.nn.functional as F
+import torch.optim as optim
+import wandb as wb
+
+from morl_baselines.common.buffer import ReplayBuffer
+from morl_baselines.common.evaluation import policy_evaluation_mo
+from morl_baselines.common.model_based.probabilistic_ensemble import (
+    ProbabilisticEnsemble,
+)
+from morl_baselines.common.model_based.utils import ModelEnv, visualize_eval
+from morl_baselines.common.morl_algorithm import MOAgent, MOPolicy
+from morl_baselines.common.networks import mlp
+from morl_baselines.common.prioritized_buffer import PrioritizedReplayBuffer
+from morl_baselines.common.utils import (
+    equally_spaced_weights,
+    layer_init,
+    log_all_multi_policy_metrics,
+    log_episode_info,
+    polyak_update,
+)
+from morl_baselines.multi_policy.linear_support.linear_support import LinearSupport
+
+
+class Policy(nn.Module):
+    """Policy network."""
+
+    def __init__(self, obs_dim, rew_dim, output_dim, action_space, net_arch=[256, 256]):
+        """Initialize the policy network."""
+        super().__init__()
+        self.action_space = action_space
+        self.latent_pi = mlp(obs_dim + rew_dim, -1, net_arch)
+        self.mean = nn.Linear(net_arch[-1], output_dim)
+
+        # action rescaling
+        self.register_buffer("action_scale", th.tensor((action_space.high - action_space.low) / 2.0, dtype=th.float32))
+        self.register_buffer("action_bias", th.tensor((action_space.high + action_space.low) / 2.0, dtype=th.float32))
+
+        self.apply(layer_init)
+
+    def forward(self, obs, w, noise=None, noise_clip=None):
+        """Forward pass of the policy network."""
+        h = self.latent_pi(th.concat((obs, w), dim=obs.dim() - 1))
+        action = self.mean(h)
+        action = th.tanh(action)
+        if noise is not None:
+            n = (th.randn_like(action) * noise).clamp(-noise_clip, noise_clip)
+            action = (action + n).clamp(-1, 1)
+        return action * self.action_scale + self.action_bias
+
+
+class QNetwork(nn.Module):
+    """Q-network S x Ax W -> R^reward_dim."""
+
+    def __init__(self, obs_dim, action_dim, rew_dim, net_arch=[256, 256], layer_norm=True, drop_rate=0.01):
+        """Initialize the Q-network."""
+        super().__init__()
+        self.net = mlp(obs_dim + action_dim + rew_dim, rew_dim, net_arch, drop_rate=drop_rate, layer_norm=layer_norm)
+        self.apply(layer_init)
+
+    def forward(self, obs, action, w):
+        """Forward pass of the Q-network."""
+        q_values = self.net(th.cat((obs, action, w), dim=obs.dim() - 1))
+        return q_values
+
+
+class GPIPDContinuousAction(MOAgent, MOPolicy):
+    """GPI-PD algorithm with continuous actions.
+
+    Sample-Efficient Multi-Objective Learning via Generalized Policy Improvement Prioritization
+    Lucas N. Alegre, Ana L. C. Bazzan, Diederik M. Roijers, Ann Nowé, Bruno C. da Silva
+    AAMAS 2023
+    Paper: https://arxiv.org/abs/2301.07784
+    See Appendix for Continuous Action details.
+    """
+
+    def __init__(
+        self,
+        env,
+        learning_rate: float = 3e-4,
+        gamma: float = 0.99,
+        tau: float = 0.005,
+        buffer_size: int = int(4e5),
+        net_arch: List = [256, 256],
+        dynamics_net_arch: List = [200, 200, 200, 200],
+        batch_size: int = 128,
+        num_q_nets: int = 2,
+        delay_policy_update: int = 2,
+        learning_starts: int = 100,
+        gradient_updates: int = 1,
+        use_gpi: bool = True,
+        policy_noise: float = 0.2,
+        noise_clip: float = 0.5,
+        per: bool = False,
+        min_priority: float = 0.1,
+        alpha: float = 0.6,
+        dyna: bool = False,
+        dynamics_train_freq: int = 250,
+        dynamics_rollout_len: int = 5,
+        dynamics_rollout_starts: int = 1000,
+        dynamics_rollout_freq: int = 250,
+        dynamics_rollout_batch_size: int = 10000,
+        dynamics_buffer_size: int = 200000,
+        dynamics_min_uncertainty: float = 2.0,
+        dynamics_real_ratio: float = 0.1,
+        project_name: str = "MORL-Baselines",
+        experiment_name: str = "GPI-PD Continuous Action",
+        log: bool = True,
+        device: Union[th.device, str] = "auto",
+    ):
+        """GPI-PD algorithm with continuous actions.
+
+        It extends the TD3 algorithm to multi-objective RL.
+        It learns the policy and Q-networks conditioned on the weight vector.
+
+        Args:
+            env (gym.Env): The environment to train on.
+            learning_rate (float, optional): The learning rate. Defaults to 3e-4.
+            gamma (float, optional): The discount factor. Defaults to 0.99.
+            tau (float, optional): The soft update coefficient. Defaults to 0.005.
+            buffer_size (int, optional): The size of the replay buffer. Defaults to int(1e6).
+            net_arch (List, optional): The network architecture for the policy and Q-networks.
+            dynamics_net_arch (List, optional): The network architecture for the dynamics model.
+            batch_size (int, optional): The batch size for training. Defaults to 256.
+            num_q_nets (int, optional): The number of Q-networks to use. Defaults to 2.
+            delay_policy_update (int, optional): The number of gradient steps to take before updating the policy. Defaults to 2.
+            learning_starts (int, optional): The number of steps to take before starting to train. Defaults to 100.
+            gradient_updates (int, optional): The number of gradient steps to take per update. Defaults to 1.
+            use_gpi (bool, optional): Whether to use GPI for selecting actions. Defaults to True.
+            policy_noise (float, optional): The noise to add to the policy. Defaults to 0.2.
+            noise_clip (float, optional): The noise clipping value. Defaults to 0.5.
+            per (bool, optional): Whether to use prioritized experience replay. Defaults to False.
+            min_priority (float, optional): The minimum priority to use for prioritized experience replay. Defaults to 0.1.
+            alpha (float, optional): The alpha value for prioritized experience replay. Defaults to 0.6.
+            dyna (bool, optional): Whether to use Dyna. Defaults to False.
+            dynamics_train_freq (int, optional): The frequency with which to train the dynamics model. Defaults to 1000.
+            dynamics_rollout_len (int, optional): The rollout length for the dynamics model. Defaults to 1.
+            dynamics_rollout_starts (int, optional): The number of steps to take before starting to train the dynamics model. Defaults to 5000.
+            dynamics_rollout_freq (int, optional): The frequency with which to rollout the dynamics model. Defaults to 250.
+            dynamics_rollout_batch_size (int, optional): The batch size for the dynamics model rollout. Defaults to 10000.
+            dynamics_buffer_size (int, optional): The size of the dynamics model replay buffer. Defaults to 400000.
+            dynamics_min_uncertainty (float, optional): The minimum uncertainty to use for the dynamics model. Defaults to 1.0.
+            dynamics_real_ratio (float, optional): The ratio of real data to use for the dynamics model. Defaults to 0.1.
+            project_name (str, optional): The name of the project. Defaults to "MORL Baselines".
+            experiment_name (str, optional): The name of the experiment. Defaults to "GPI-PD Continuous Action".
+            log (bool, optional): Whether to log to wandb. Defaults to True.
+            device (Union[th.device, str], optional): The device to use for training. Defaults to "auto".
+        """
+        MOAgent.__init__(self, env, device=device)
+        MOPolicy.__init__(self, device)
+        self.learning_rate = learning_rate
+        self.tau = tau
+        self.gamma = gamma
+        self.use_gpi = use_gpi
+        self.policy_noise = policy_noise
+        self.noise_clip = noise_clip
+        self.buffer_size = buffer_size
+        self.num_q_nets = num_q_nets
+        self.delay_policy_update = delay_policy_update
+        self.net_arch = net_arch
+        self.dynamics_net_arch = dynamics_net_arch
+        self.learning_starts = learning_starts
+        self.batch_size = batch_size
+        self.gradient_updates = gradient_updates
+        self.per = per
+        self.min_priority = min_priority
+        self.alpha = alpha
+        if self.per:
+            self.replay_buffer = PrioritizedReplayBuffer(
+                self.observation_shape, self.action_dim, rew_dim=self.reward_dim, max_size=buffer_size
+            )
+        else:
+            self.replay_buffer = ReplayBuffer(
+                self.observation_shape, self.action_dim, rew_dim=self.reward_dim, max_size=buffer_size
+            )
+
+        self.q_nets = [
+            QNetwork(self.observation_dim, self.action_dim, self.reward_dim, net_arch=net_arch).to(self.device)
+            for _ in range(num_q_nets)
+        ]
+        self.target_q_nets = [
+            QNetwork(self.observation_dim, self.action_dim, self.reward_dim, net_arch=net_arch).to(self.device)
+            for _ in range(num_q_nets)
+        ]
+        for q_net, target_q_net in zip(self.q_nets, self.target_q_nets):
+            target_q_net.load_state_dict(q_net.state_dict())
+            for param in target_q_net.parameters():
+                param.requires_grad = False
+
+        self.policy = Policy(
+            self.observation_dim, self.reward_dim, self.action_dim, self.env.action_space, net_arch=net_arch
+        ).to(self.device)
+        self.target_policy = Policy(
+            self.observation_dim, self.reward_dim, self.action_dim, self.env.action_space, net_arch=net_arch
+        ).to(self.device)
+        self.target_policy.load_state_dict(self.policy.state_dict())
+        for param in self.target_policy.parameters():
+            param.requires_grad = False
+
+        self.q_optim = optim.Adam(chain(*[net.parameters() for net in self.q_nets]), lr=self.learning_rate)
+        self.policy_optim = optim.Adam(list(self.policy.parameters()), lr=self.learning_rate)
+
+        self.dyna = dyna
+        if self.dyna:
+            self.dynamics = ProbabilisticEnsemble(
+                input_dim=self.observation_dim + self.action_dim,
+                output_dim=self.observation_dim + self.reward_dim,
+                arch=self.dynamics_net_arch,
+                device=self.device,
+            )
+            self.dynamics_buffer = ReplayBuffer(
+                self.observation_shape, self.action_dim, rew_dim=self.reward_dim, max_size=dynamics_buffer_size
+            )
+        else:
+            self.dynamics = None
+        self.dynamics_train_freq = dynamics_train_freq
+        self.dynamics_rollout_len = dynamics_rollout_len
+        self.dynamics_rollout_starts = dynamics_rollout_starts
+        self.dynamics_rollout_freq = dynamics_rollout_freq
+        self.dynamics_rollout_batch_size = dynamics_rollout_batch_size
+        self.dynamics_min_uncertainty = dynamics_min_uncertainty
+        self.dynamics_real_ratio = dynamics_real_ratio
+
+        self.weight_support = []
+        self.stacked_weight_support = []
+
+        self._n_updates = 0
+
+        self.log = log
+        if self.log:
+            self.setup_wandb(project_name, experiment_name)
+
+    def get_config(self):
+        """Get the configuration of the agent."""
+        return {
+            "env_id": self.env.unwrapped.spec.id,
+            "learning_rate": self.learning_rate,
+            "num_q_nets": self.num_q_nets,
+            "batch_size": self.batch_size,
+            "tau": self.tau,
+            "gamma": self.gamma,
+            "policy_noise": self.policy_noise,
+            "net_arch": self.net_arch,
+            "gradient_updates": self.gradient_updates,
+            "delay_policy_update": self.delay_policy_update,
+            "min_priority": self.min_priority,
+            "per": self.per,
+            "buffer_size": self.buffer_size,
+            "alpha": self.alpha,
+            "learning_starts": self.learning_starts,
+            "dyna": self.dyna,
+            "dynamics_net_arch": self.dynamics_net_arch,
+            "dynamics_rollout_len": self.dynamics_rollout_len,
+            "dynamics_min_uncertainty": self.dynamics_min_uncertainty,
+            "dynamics_real_ratio": self.dynamics_real_ratio,
+        }
+
+    def save(self, save_dir="weights/", filename=None, save_replay_buffer=True):
+        """Save the agent's weights and replay buffer."""
+        if not os.path.isdir(save_dir):
+            os.makedirs(save_dir)
+
+        saved_params = {
+            "policy_state_dict": self.policy.state_dict(),
+            "policy_optimizer_state_dict": self.policy_optim.state_dict(),
+        }
+        for i, (q_net, target_q_net) in enumerate(zip(self.q_nets, self.target_q_nets)):
+            saved_params["q_net_" + str(i) + "_state_dict"] = q_net.state_dict()
+            saved_params["target_q_net_" + str(i) + "_state_dict"] = target_q_net.state_dict()
+        saved_params["q_nets_optimizer_state_dict"] = self.q_optim.state_dict()
+        saved_params["M"] = self.weight_support
+        if self.dyna:
+            saved_params["dynamics_state_dict"] = self.dynamics.state_dict()
+        if save_replay_buffer:
+            saved_params["replay_buffer"] = self.replay_buffer
+        filename = self.experiment_name if filename is None else filename
+        th.save(saved_params, save_dir + "/" + filename + ".tar")
+
+    def load(self, path, load_replay_buffer=True):
+        """Load the agent weights from a file."""
+        params = th.load(path, map_location=self.device)
+        self.weight_support = params["M"]
+        self.stacked_weight_support = th.stack(self.weight_support)
+        self.policy.load_state_dict(params["policy_state_dict"])
+        self.policy_optim.load_state_dict(params["policy_optimizer_state_dict"])
+        for i, (q_net, target_q_net) in enumerate(zip(self.q_nets, self.target_q_nets)):
+            q_net.load_state_dict(params["q_net_" + str(i) + "_state_dict"])
+            target_q_net.load_state_dict(params["target_q_net_" + str(i) + "_state_dict"])
+        self.q_optim.load_state_dict(params["q_nets_optimizer_state_dict"])
+        if self.dyna:
+            self.dynamics.load_state_dict(params["dynamics_state_dict"])
+        if load_replay_buffer and "replay_buffer" in params:
+            self.replay_buffer = params["replay_buffer"]
+
+    def _sample_batch_experiences(self, deactivate_per=False):
+        if not self.dyna or self.global_step < self.dynamics_rollout_starts or len(self.dynamics_buffer) == 0:
+            if deactivate_per:
+                return self.replay_buffer.sample_uniform(self.batch_size, to_tensor=True, device=self.device)
+            return self.replay_buffer.sample(self.batch_size, to_tensor=True, device=self.device)
+        else:
+            num_real_samples = int(self.batch_size * self.dynamics_real_ratio)  # % of real world data
+            if self.per and not deactivate_per:
+                s_obs, s_actions, s_rewards, s_next_obs, s_dones, idxes = self.replay_buffer.sample(
+                    num_real_samples, to_tensor=True, device=self.device
+                )
+            else:
+                (s_obs, s_actions, s_rewards, s_next_obs, s_dones) = self.replay_buffer.sample_uniform(
+                    num_real_samples, to_tensor=True, device=self.device
+                )
+            (m_obs, m_actions, m_rewards, m_next_obs, m_dones) = self.dynamics_buffer.sample(
+                self.batch_size - num_real_samples, to_tensor=True, device=self.device
+            )
+            experience_tuples = (
+                th.cat([s_obs, m_obs], dim=0),
+                th.cat([s_actions, m_actions], dim=0),
+                th.cat([s_rewards, m_rewards], dim=0),
+                th.cat([s_next_obs, m_next_obs], dim=0),
+                th.cat([s_dones, m_dones], dim=0),
+            )
+            if self.per and not deactivate_per:
+                return experience_tuples + (idxes,)
+            return experience_tuples
+
+    @th.no_grad()
+    def _rollout_dynamics(self, weight: th.Tensor):
+        # Dyna Planning
+        num_times = int(np.ceil(self.dynamics_rollout_batch_size / 10000))
+        batch_size = min(self.dynamics_rollout_batch_size, 10000)
+        for _ in range(num_times):
+            obs = self.replay_buffer.sample_obs(batch_size, to_tensor=False)
+            model_env = ModelEnv(self.dynamics, self.env.unwrapped.spec.id, rew_dim=self.reward_dim)
+            for plan_step in range(self.dynamics_rollout_len):
+                obs = th.tensor(obs).to(self.device)
+                w = weight.repeat(obs.shape[0], 1)
+                actions = self.policy(obs, w, noise=self.policy_noise, noise_clip=self.noise_clip)
+
+                next_obs_pred, r_pred, dones, info = model_env.step(obs, actions)
+                obs, actions = (obs.detach().cpu().numpy(), actions.detach().cpu().numpy())
+
+                uncertainties = info["uncertainty"]
+                for i in range(len(obs)):
+                    if uncertainties[i] < self.dynamics_min_uncertainty:
+                        self.dynamics_buffer.add(obs[i], actions[i], r_pred[i], next_obs_pred[i], dones[i])
+
+                nonterm_mask = ~dones.squeeze(-1)
+                if nonterm_mask.sum() == 0:
+                    break
+
+                obs = next_obs_pred[nonterm_mask]
+
+        if self.log:
+            self.writer.add_scalar("dynamics/uncertainty_mean", uncertainties.mean(), self.global_step)
+            self.writer.add_scalar("dynamics/uncertainty_max", uncertainties.max(), self.global_step)
+            self.writer.add_scalar("dynamics/uncertainty_min", uncertainties.min(), self.global_step)
+
+    def update(self, weight: th.Tensor):
+        """Update the policy and the Q-nets."""
+        for _ in range(self.gradient_updates):
+            if self.per:
+                (s_obs, s_actions, s_rewards, s_next_obs, s_dones, idxes) = self._sample_batch_experiences()
+            else:
+                (s_obs, s_actions, s_rewards, s_next_obs, s_dones) = self._sample_batch_experiences()
+
+            if len(self.weight_support) > 1:
+                s_obs, s_actions, s_rewards, s_next_obs, s_dones = (
+                    s_obs.repeat(2, 1),
+                    s_actions.repeat(2, 1),
+                    s_rewards.repeat(2, 1),
+                    s_next_obs.repeat(2, 1),
+                    s_dones.repeat(2, 1),
+                )
+                w = th.vstack(
+                    [weight for _ in range(s_obs.size(0) // 2)] + random.choices(self.weight_support, k=s_obs.size(0) // 2)
+                )
+            else:
+                w = weight.repeat(s_obs.size(0), 1)
+
+            with th.no_grad():
+                next_actions = self.target_policy(s_next_obs, w, noise=self.policy_noise, noise_clip=self.noise_clip)
+                q_targets = th.stack([q_target(s_next_obs, next_actions, w) for q_target in self.target_q_nets])
+                scalarized_q_targets = th.einsum("nbr,br->nb", q_targets, w)
+                inds = th.argmin(scalarized_q_targets, dim=0, keepdim=True)
+                inds = inds.reshape(1, -1, 1).expand(1, q_targets.size(1), q_targets.size(2))
+                target_q = q_targets.gather(0, inds).squeeze(0)
+
+                target_q = (s_rewards + (1 - s_dones) * self.gamma * target_q).detach()
+
+            q_values = [q_net(s_obs, s_actions, w) for q_net in self.q_nets]
+            critic_loss = (1 / self.num_q_nets) * sum([F.mse_loss(q_value, target_q) for q_value in q_values])
+
+            self.q_optim.zero_grad()
+            critic_loss.backward()
+            self.q_optim.step()
+
+            if self.per:
+                per = (q_values[0] - target_q)[: len(idxes)].detach().abs() * 0.05
+                per = th.einsum("br,br->b", per, w[: len(idxes)])
+                priority = per.cpu().numpy().flatten()
+                priority = priority.clip(min=self.min_priority) ** self.alpha
+                self.replay_buffer.update_priorities(idxes, priority)
+
+            for q_net, target_q_net in zip(self.q_nets, self.target_q_nets):
+                polyak_update(q_net.parameters(), target_q_net.parameters(), self.tau)
+
+            if self._n_updates % self.delay_policy_update == 0:
+                # Policy update
+                actions = self.policy(s_obs, w)
+                q_values_pi = (1 / self.num_q_nets) * sum(q_net(s_obs, actions, w) for q_net in self.q_nets)
+                policy_loss = -th.einsum("br,br->b", q_values_pi, w).mean()
+
+                self.policy_optim.zero_grad()
+                policy_loss.backward()
+                self.policy_optim.step()
+
+                polyak_update(self.policy.parameters(), self.target_policy.parameters(), self.tau)
+
+            self._n_updates += 1
+
+        if self.log and self.global_step % 100 == 0:
+            if self.per:
+                self.writer.add_scalar("metrics/mean_priority", np.mean(priority), self.global_step)
+                self.writer.add_scalar("metrics/max_priority", np.max(priority), self.global_step)
+                self.writer.add_scalar("metrics/min_priority", np.min(priority), self.global_step)
+            self.writer.add_scalar("losses/critic_loss", critic_loss.item(), self.global_step)
+            self.writer.add_scalar("losses/policy_loss", policy_loss.item(), self.global_step)
+
+    @th.no_grad()
+    def eval(
+        self, obs: Union[np.ndarray, th.Tensor], w: Union[np.ndarray, th.Tensor], torch_action=False
+    ) -> Union[np.ndarray, th.Tensor]:
+        """Evaluate the policy action for the given observation and weight vector."""
+        if isinstance(obs, np.ndarray):
+            obs = th.tensor(obs).float().to(self.device)
+            w = th.tensor(w).float().to(self.device)
+
+        if self.use_gpi:
+            obs = obs.repeat(len(self.weight_support), 1)
+            actions_original = self.policy(obs, self.stacked_weight_support)
+
+            obs = obs.repeat(len(self.weight_support), 1, 1)
+            actions = actions_original.repeat(len(self.weight_support), 1, 1)
+            stackedM = self.stacked_weight_support.repeat_interleave(len(self.weight_support), dim=0).view(
+                len(self.weight_support), len(self.weight_support), self.reward_dim
+            )
+            values = self.q_nets[0](obs, actions, stackedM)
+
+            scalar_values = th.einsum("par,r->pa", values, w)
+            max_q, a = th.max(scalar_values, dim=1)
+            policy_index = th.argmax(max_q)  # max_i max_a q(s,a,w_i)
+            action = a[policy_index].detach().item()
+            action = actions_original[action]
+        else:
+            action = self.policy(obs, w)
+
+        if not torch_action:
+            action = action.detach().cpu().numpy()
+
+        return action
+
+    def set_weight_support(self, weight_list: List[np.ndarray]):
+        """Set the weight support set."""
+        self.weight_support = [th.tensor(w).float().to(self.device) for w in weight_list]
+        if len(self.weight_support) > 0:
+            self.stacked_weight_support = th.stack(self.weight_support)
+
+    def train_iteration(
+        self,
+        total_timesteps: int,
+        weight: np.ndarray,
+        weight_support: List[np.ndarray],
+        change_weight_every_episode: bool = False,
+        eval_env=None,
+        eval_freq: int = 1000,
+        reset_num_timesteps: bool = False,
+    ):
+        """Train the agent.
+
+        Args:
+            total_timesteps (int): Total number of timesteps to train the agent for.
+            weight (np.ndarray): Initial weight vector.
+            weight_support (List[np.ndarray]): List of weight vectors to use for the weight support set.
+            change_weight_every_episode (bool): Whether to change the weight vector at the end of each episode.
+            eval_env (Optional[gym.Env]): Environment to use for evaluation.
+            eval_freq (int): Number of timesteps between evaluations.
+            reset_num_timesteps (bool): Whether to reset the number of timesteps.
+        """
+        self.set_weight_support(weight_support)
+        tensor_w = th.tensor(weight).float().to(self.device)
+
+        self.global_step = 0 if reset_num_timesteps else self.global_step
+        self.num_episodes = 0 if reset_num_timesteps else self.num_episodes
+
+        obs, info = self.env.reset()
+        for _ in range(1, total_timesteps + 1):
+            self.global_step += 1
+
+            if self.global_step < self.learning_starts:
+                action = self.env.action_space.sample()
+            else:
+                with th.no_grad():
+                    action = (
+                        self.policy(
+                            th.tensor(obs).float().to(self.device),
+                            tensor_w,
+                            noise=self.policy_noise,
+                            noise_clip=self.noise_clip,
+                        )
+                        .detach()
+                        .cpu()
+                        .numpy()
+                    )
+
+            action_env = action
+
+            next_obs, vector_reward, terminated, truncated, info = self.env.step(action_env)
+
+            self.replay_buffer.add(obs, action, vector_reward, next_obs, terminated)
+
+            if self.global_step >= self.learning_starts:
+                if self.dyna:
+                    if self.global_step % self.dynamics_train_freq == 0:
+                        (m_obs, m_actions, m_rewards, m_next_obs, m_dones) = self.replay_buffer.get_all_data()
+                        X = np.hstack((m_obs, m_actions))
+                        Y = np.hstack((m_rewards, m_next_obs - m_obs))
+                        mean_holdout_loss = self.dynamics.fit(X, Y)
+                        if self.log:
+                            self.writer.add_scalar("dynamics/mean_holdout_loss", mean_holdout_loss, self.global_step)
+
+                    if self.global_step >= self.dynamics_rollout_starts and self.global_step % self.dynamics_rollout_freq == 0:
+                        self._rollout_dynamics(tensor_w)
+
+                self.update(tensor_w)
+
+            if eval_env is not None and self.log and self.global_step % eval_freq == 0:
+                self.policy_eval(eval_env, weights=weight, writer=self.writer)
+                plot = visualize_eval(self, eval_env, self.dynamics, w=weight, compound=False, horizon=1000)
+                wb.log({"dynamics/predictions": wb.Image(plot), "global_step": self.global_step})
+                plot.close()
+
+            if terminated or truncated:
+                obs, info = self.env.reset()
+                self.num_episodes += 1
+
+                if self.log and "episode" in info.keys():
+                    log_episode_info(info["episode"], np.dot, weight, self.global_step, writer=self.writer)
+
+                if change_weight_every_episode:
+                    weight = random.choice(weight_support)
+                    tensor_w = th.tensor(weight).float().to(self.device)
+            else:
+                obs = next_obs
+
+    def train(
+        self,
+        eval_env: gymnasium.Env,
+        ref_point: np.ndarray,
+        known_pareto_front: Optional[List[np.ndarray]] = None,
+        eval_weights_number_for_front: int = 100,
+        weight_selection_algo: str = "gpi-ls",
+        timesteps_per_iter: int = 10000,
+        max_iter: int = 10,
+        eval_freq: int = 1000,
+    ):
+        """Train the agent.
+
+        Args:
+            eval_env (gym.Env): Environment to use for evaluation.
+            ref_point (np.ndarray): Reference point for hypervolume calculation
+            known_pareto_front (Optional[List[np.ndarray]]): Optimal Pareto front, if known.
+            eval_weights_number_for_front (int): Number of weights to evaluate for the Pareto front
+            weight_selection_algo (str): Weight selection algorithm to use.
+            timesteps_per_iter (int): Number of timesteps to train the agent for each iteration.
+            max_iter (int): Maximum number of iterations to train the agent for.
+            eval_freq (int): Number of timesteps between evaluations during an iteration.
+        """
+        linear_support = LinearSupport(num_objectives=self.reward_dim, epsilon=0.0 if weight_selection_algo == "ols" else None)
+
+        eval_weights = equally_spaced_weights(self.reward_dim, n=eval_weights_number_for_front)
+
+        for iter in range(1, max_iter + 1):
+            if weight_selection_algo == "ols" or weight_selection_algo == "gpi-ls":
+                if weight_selection_algo == "gpi-ls":
+                    self.set_weight_support(linear_support.get_weight_support())
+                    self.use_gpi = True
+                    w = linear_support.next_weight(algo="gpi-ls", gpi_agent=self, env=eval_env)
+                    self.use_gpi = False
+                else:
+                    w = linear_support.next_weight(algo="ols")
+
+                if w is None:
+                    break
+            else:
+                raise ValueError(f"Unknown algorithm {weight_selection_algo}.")
+
+            print("Next weight vector:", w)
+            if weight_selection_algo == "gpi-ls":
+                M = linear_support.get_weight_support() + linear_support.get_corner_weights(top_k=4) + [w]
+            elif weight_selection_algo == "ols":
+                M = linear_support.get_weight_support() + [w]
+            else:
+                M = None
+
+            self.train_iteration(
+                total_timesteps=timesteps_per_iter,
+                weight=w,
+                weight_support=M,
+                change_weight_every_episode=weight_selection_algo == "gpi-ls",
+                eval_env=eval_env,
+                eval_freq=eval_freq,
+            )
+
+            if weight_selection_algo == "ols":
+                value = policy_evaluation_mo(self, eval_env, w, rep=5)[3]
+                linear_support.add_solution(value, w)
+            elif weight_selection_algo == "gpi-ls":
+                for wcw in M:
+                    n_value = policy_evaluation_mo(self, eval_env, wcw, rep=5)[3]
+                    linear_support.add_solution(n_value, wcw)
+
+            if self.log:
+                # Evaluation
+                gpi_returns_test_tasks = [policy_evaluation_mo(self, eval_env, w, rep=5)[3] for w in eval_weights]
+                log_all_multi_policy_metrics(
+                    current_front=gpi_returns_test_tasks,
+                    hv_ref_point=ref_point,
+                    reward_dim=self.reward_dim,
+                    global_step=self.global_step,
+                    writer=self.writer,
+                    ref_front=known_pareto_front,
+                )
+                # This is the EU computed in the paper
+                mean_gpi_returns_test_tasks = np.mean(
+                    [np.dot(w, q) for w, q in zip(eval_weights, gpi_returns_test_tasks)], axis=0
+                )
+                wb.log({"eval/Mean Utility - GPI": mean_gpi_returns_test_tasks, "iteration": iter})
+
+            # Checkpoint
+            self.save(filename=f"GPI-PD {weight_selection_algo} iter={iter}", save_replay_buffer=False)
+
+        self.close_wandb()
diff --git a/morl-baselines/morl_baselines/multi_policy/linear_support/__init__.py b/morl-baselines/morl_baselines/multi_policy/linear_support/__init__.py
new file mode 100644
index 0000000..a3117b9
--- /dev/null
+++ b/morl-baselines/morl_baselines/multi_policy/linear_support/__init__.py
@@ -0,0 +1 @@
+"""Linear support algorithm."""
diff --git a/morl-baselines/morl_baselines/multi_policy/linear_support/linear_support.py b/morl-baselines/morl_baselines/multi_policy/linear_support/linear_support.py
new file mode 100644
index 0000000..88fca8d
--- /dev/null
+++ b/morl-baselines/morl_baselines/multi_policy/linear_support/linear_support.py
@@ -0,0 +1,378 @@
+"""Linear Support implementation."""
+import random
+from copy import deepcopy
+from typing import List, Optional
+
+import cdd
+import cvxpy as cp
+import numpy as np
+from gymnasium.core import Env
+
+from morl_baselines.common.evaluation import policy_evaluation_mo
+from morl_baselines.common.morl_algorithm import MOPolicy
+from morl_baselines.common.performance_indicators import hypervolume
+from morl_baselines.common.utils import extrema_weights
+
+
+np.set_printoptions(precision=4)
+
+
+class LinearSupport:
+    """Linear Support for computing corner weights when using linear utility functions.
+
+    Implements both
+
+    Optimistic Linear Support (OLS) algorithm:
+    Paper: (Section 3.3 of http://roijers.info/pub/thesis.pdf).
+
+    Generalized Policy Improvement Linear Support (GPI-LS) algorithm:
+    Paper: https://arxiv.org/abs/2301.07784
+    """
+
+    def __init__(
+        self,
+        num_objectives: int,
+        epsilon: float = 0.0,
+        verbose: bool = True,
+    ):
+        """Initialize Linear Support.
+
+        Args:
+            num_objectives (int): Number of objectives
+            epsilon (float, optional): Minimum improvement per iteration. Defaults to 0.0.
+            verbose (bool): Defaults to False.
+        """
+        self.num_objectives = num_objectives
+        self.epsilon = epsilon
+        self.visited_weights = []  # List of already tested weight vectors
+        self.ccs = []
+        self.weight_support = []  # List of weight vectors for each value vector in the CCS
+        self.queue = []
+        self.iteration = 0
+        self.verbose = verbose
+        for w in extrema_weights(self.num_objectives):
+            self.queue.append((float("inf"), w))
+
+    def next_weight(
+        self, algo: str = "ols", gpi_agent: Optional[MOPolicy] = None, env: Optional[Env] = None, rep_eval: int = 1
+    ) -> np.ndarray:
+        """Returns the next weight vector with highest priority.
+
+        Args:
+            algo (str): Algorithm to use. Either 'ols' or 'gpi-ls'.
+            gpi_agent (Optional[MOPolicy]): Agent to use for GPI-LS.
+            env (Optional[Env]): Environment to use for GPI-LS.
+            rep_eval (int): Number of times to evaluate the agent in GPI-LS.
+
+        Returns:
+            np.ndarray: Next weight vector
+        """
+        if len(self.ccs) > 0:
+            W_corner = self.compute_corner_weights()
+            if self.verbose:
+                print("W_corner:", W_corner, "W_corner size:", len(W_corner))
+
+            self.queue = []
+            for wc in W_corner:
+                if algo == "ols":
+                    priority = self.ols_priority(wc)
+
+                elif algo == "gpi-ls":
+                    if gpi_agent is None:
+                        raise ValueError("GPI-LS requires passing a GPI agent.")
+                    gpi_expanded_set = [policy_evaluation_mo(gpi_agent, env, wc, rep=rep_eval)[3] for wc in W_corner]
+                    priority = self.gpi_ls_priority(wc, gpi_expanded_set)
+
+                if self.epsilon is None or priority >= self.epsilon:
+                    # OLS does not try the same weight vector twice
+                    if not (algo == "ols" and any([np.allclose(wc, w) for (p, w) in self.visited_weights])):
+                        self.queue.append((priority, wc))
+
+            if len(self.queue) > 0:
+                # Sort in descending order of priority
+                self.queue.sort(key=lambda t: t[0], reverse=True)
+                # If all priorities are 0, shuffle the queue to avoid repearting weights every iteration
+                if self.queue[0][0] == 0.0:
+                    random.shuffle(self.queue)
+
+        if self.verbose:
+            print("CCS:", self.ccs, "CCS size:", len(self.ccs))
+
+        if len(self.queue) == 0:
+            if self.verbose:
+                print("There are no corner weights in the queue. Returning None.")
+            return None
+        else:
+            next_w = self.queue.pop(0)[1]
+            if self.verbose:
+                print("Next weight:", next_w)
+            return next_w
+
+    def get_weight_support(self) -> List[np.ndarray]:
+        """Returns the weight support of the CCS.
+
+        Returns:
+            List[np.ndarray]: List of weight vectors of the CCS
+
+        """
+        return deepcopy(self.weight_support)
+
+    def get_corner_weights(self, top_k: Optional[int] = None) -> List[np.ndarray]:
+        """Returns the corner weights of the current CCS.
+
+        Args:
+            top_k: If not None, returns the top_k corner weights.
+
+        Returns:
+            List[np.ndarray]: List of corner weights.
+        """
+        weights = [w.copy() for (p, w) in self.queue]
+        if top_k is not None:
+            return weights[:top_k]
+        else:
+            return weights
+
+    def ended(self) -> bool:
+        """Returns True if the queue is empty."""
+        return len(self.queue) == 0
+
+    def add_solution(self, value: np.ndarray, w: np.ndarray) -> List[int]:
+        """Add new value vector optimal to weight w.
+
+        Args:
+            value (np.ndarray): New value vector
+            w (np.ndarray): Weight vector
+
+        Returns:
+            List of indices of value vectors removed from the CCS for being dominated.
+        """
+        if self.verbose:
+            print(f"Adding value: {value} to CCS.")
+
+        self.iteration += 1
+        self.visited_weights.append(w)
+
+        if self.is_dominated(value):
+            if self.verbose:
+                print(f"Value {value} is dominated. Discarding.")
+            return [len(self.ccs)]
+
+        removed_indx = self.remove_obsolete_values(value)
+
+        self.ccs.append(value)
+        self.weight_support.append(w)
+
+        return removed_indx
+
+    def ols_priority(self, w: np.ndarray) -> float:
+        """Get the priority of a weight vector for OLS.
+
+        Args:
+            w: Weight vector
+
+        Returns:
+            Priority of the weight vector.
+        """
+        max_value_ccs = self.max_scalarized_value(w)
+        max_optimistic_value = self.max_value_lp(w)
+        priority = max_optimistic_value - max_value_ccs
+        return priority
+
+    def gpi_ls_priority(self, w: np.ndarray, gpi_expanded_set: List[np.ndarray]) -> float:
+        """Get the priority of a weight vector for GPI-LS.
+
+        Args:
+            w: Weight vector
+
+        Returns:
+            Priority of the weight vector.
+        """
+
+        def best_vector(values, w):
+            max_v = values[0]
+            for i in range(1, len(values)):
+                if values[i] @ w > max_v @ w:
+                    max_v = values[i]
+            return max_v
+
+        max_value_ccs = self.max_scalarized_value(w)
+        max_value_gpi = best_vector(gpi_expanded_set, w)
+        max_value_gpi = np.dot(max_value_gpi, w)
+        priority = max_value_gpi - max_value_ccs
+
+        return priority
+
+    def max_scalarized_value(self, w: np.ndarray) -> Optional[float]:
+        """Returns the maximum scalarized value for weight vector w.
+
+        Args:
+            w: Weight vector
+
+        Returns:
+            Maximum scalarized value for weight vector w.
+        """
+        if len(self.ccs) == 0:
+            return None
+        return np.max([np.dot(v, w) for v in self.ccs])
+
+    def remove_obsolete_weights(self, new_value: np.ndarray) -> List[np.ndarray]:
+        """Remove from the queue the weight vectors for which the new value vector is better than previous values.
+
+        Args:
+            new_value: New value vector
+
+        Returns:
+            List of weight vectors removed from the queue.
+        """
+        if len(self.ccs) == 0:
+            return []
+        W_del = []
+        inds_remove = []
+        for i, (priority, cw) in enumerate(self.queue):
+            if np.dot(cw, new_value) > self.max_scalarized_value(cw):
+                W_del.append(cw)
+                inds_remove.append(i)
+        for i in reversed(inds_remove):
+            self.queue.pop(i)
+        return W_del
+
+    def remove_obsolete_values(self, value: np.ndarray) -> List[int]:
+        """Removes the values vectors which are dominated by the new value for all visited weight vectors.
+
+        Args:
+            value: New value vector
+
+        Returns:
+             the indices of the removed values.
+        """
+        removed_indx = []
+        for i in reversed(range(len(self.ccs))):
+            best_in_all = True
+            for j in range(len(self.visited_weights)):
+                w = self.visited_weights[j]
+                if np.dot(value, w) < np.dot(self.ccs[i], w):
+                    best_in_all = False
+                    break
+
+            if best_in_all:
+                removed_indx.append(i)
+                self.ccs.pop(i)
+                self.weight_support.pop(i)
+
+        return removed_indx
+
+    def max_value_lp(self, w_new: np.ndarray) -> float:
+        """Returns an upper-bound for the maximum value of the scalarized objective.
+
+        Args:
+            w_new: New weight vector
+
+        Returns:
+            Upper-bound for the maximum value of the scalarized objective.
+        """
+        # No upper bound if no values in CCS
+        if len(self.ccs) == 0:
+            return float("inf")
+
+        w = cp.Parameter(self.num_objectives)
+        w.value = w_new
+        v = cp.Variable(self.num_objectives)
+
+        W_ = np.vstack(self.visited_weights)
+        W = cp.Parameter(W_.shape)
+        W.value = W_
+
+        V_ = np.array([self.max_scalarized_value(weight) for weight in self.visited_weights])
+        V = cp.Parameter(V_.shape)
+        V.value = V_
+
+        # Maximum value for weight vector w
+        objective = cp.Maximize(w @ v)
+        # such that it is consistent with other optimal values for other visited weights
+        constraints = [W @ v <= V]
+        prob = cp.Problem(objective, constraints)
+        return prob.solve(verbose=False)
+
+    def compute_corner_weights(self) -> List[np.ndarray]:
+        """Returns the corner weights for the current set of values.
+
+        See http://roijers.info/pub/thesis.pdf Definition 19.
+        Obs: there is a typo in the definition of the corner weights in the thesis, the >= sign should be <=.
+
+        Returns:
+            List of corner weights.
+        """
+        A = np.vstack(self.ccs)
+        A = np.round_(A, decimals=4)  # Round to avoid numerical issues
+        A = np.concatenate((A, -np.ones(A.shape[0]).reshape(-1, 1)), axis=1)
+
+        A_plus = np.ones(A.shape[1]).reshape(1, -1)
+        A_plus[0, -1] = 0
+        A = np.concatenate((A, A_plus), axis=0)
+        A_plus = -np.ones(A.shape[1]).reshape(1, -1)
+        A_plus[0, -1] = 0
+        A = np.concatenate((A, A_plus), axis=0)
+
+        for i in range(self.num_objectives):
+            A_plus = np.zeros(A.shape[1]).reshape(1, -1)
+            A_plus[0, i] = -1
+            A = np.concatenate((A, A_plus), axis=0)
+
+        b = np.zeros(len(self.ccs) + 2 + self.num_objectives)
+        b[len(self.ccs)] = 1
+        b[len(self.ccs) + 1] = -1
+
+        def compute_poly_vertices(A, b):
+            # Based on https://stackoverflow.com/questions/65343771/solve-linear-inequalities
+            b = b.reshape((b.shape[0], 1))
+            mat = cdd.Matrix(np.hstack([b, -A]), number_type="float")
+            mat.rep_type = cdd.RepType.INEQUALITY
+            P = cdd.Polyhedron(mat)
+            g = P.get_generators()
+            V = np.array(g)
+            vertices = []
+            for i in range(V.shape[0]):
+                if V[i, 0] != 1:
+                    continue
+                if i not in g.lin_set:
+                    vertices.append(V[i, 1:])
+            return vertices
+
+        vertices = compute_poly_vertices(A, b)
+        corners = []
+        for v in vertices:
+            corners.append(v[:-1])
+
+        return corners
+
+    def is_dominated(self, value: np.ndarray) -> bool:
+        """Checks if the value is dominated by any of the values in the CCS.
+
+        Args:
+            value: Value vector
+
+        Returns:
+            True if the value is dominated by any of the values in the CCS, False otherwise.
+        """
+        if len(self.ccs) == 0:
+            return False
+        for w in self.visited_weights:
+            if np.dot(value, w) >= self.max_scalarized_value(w):
+                return False
+        return True
+
+
+if __name__ == "__main__":
+
+    def _solve(w):
+        return np.array(list(map(float, input().split())), dtype=np.float32)
+
+    num_objectives = 3
+    ols = LinearSupport(num_objectives=num_objectives, epsilon=0.0001, verbose=True)
+    while not ols.ended():
+        w = ols.next_weight()
+        print("w:", w)
+        value = _solve(w)
+        ols.add_solution(value, w)
+
+        print("hv:", hypervolume(np.zeros(num_objectives), ols.ccs))
diff --git a/morl-baselines/morl_baselines/multi_policy/multi_policy_moqlearning/__init__.py b/morl-baselines/morl_baselines/multi_policy/multi_policy_moqlearning/__init__.py
new file mode 100644
index 0000000..0496cbf
--- /dev/null
+++ b/morl-baselines/morl_baselines/multi_policy/multi_policy_moqlearning/__init__.py
@@ -0,0 +1 @@
+"""Outer-loop MOQ-learning algorithm."""
diff --git a/morl-baselines/morl_baselines/multi_policy/multi_policy_moqlearning/mp_mo_q_learning.py b/morl-baselines/morl_baselines/multi_policy/multi_policy_moqlearning/mp_mo_q_learning.py
new file mode 100644
index 0000000..a84e097
--- /dev/null
+++ b/morl-baselines/morl_baselines/multi_policy/multi_policy_moqlearning/mp_mo_q_learning.py
@@ -0,0 +1,239 @@
+"""Outer-loop MOQ-learning algorithm (uses multiple weights)."""
+import time
+from copy import deepcopy
+from typing import List, Optional
+from typing_extensions import override
+
+import gymnasium as gym
+import numpy as np
+
+from morl_baselines.common.evaluation import policy_evaluation_mo
+from morl_baselines.common.morl_algorithm import MOAgent
+from morl_baselines.common.scalarization import weighted_sum
+from morl_baselines.common.utils import (
+    equally_spaced_weights,
+    log_all_multi_policy_metrics,
+    random_weights,
+)
+from morl_baselines.multi_policy.linear_support.linear_support import LinearSupport
+from morl_baselines.single_policy.ser.mo_q_learning import MOQLearning
+
+
+class MPMOQLearning(MOAgent):
+    """Multi-policy MOQ-Learning: Outer loop version of mo_q_learning.
+
+    Paper: Paper: K. Van Moffaert, M. Drugan, and A. Nowe, Scalarized Multi-Objective Reinforcement Learning: Novel Design Techniques. 2013. doi: 10.1109/ADPRL.2013.6615007.
+    """
+
+    def __init__(
+        self,
+        env,
+        scalarization=weighted_sum,
+        learning_rate: float = 0.1,
+        gamma: float = 0.9,
+        initial_epsilon: float = 0.1,
+        final_epsilon: float = 0.1,
+        epsilon_decay_steps: int = None,
+        weight_selection_algo: str = "random",
+        epsilon_ols: Optional[float] = None,
+        use_gpi_policy: bool = False,
+        transfer_q_table: bool = True,
+        dyna: bool = False,
+        dyna_updates: int = 5,
+        project_name: str = "MORL-Baselines",
+        experiment_name: str = "MultiPolicy MO Q-Learning",
+        log: bool = True,
+    ):
+        """Initialize the Multi-policy MOQ-learning algorithm.
+
+        Args:
+            env: The environment to learn from.
+            scalarization: The scalarization function to use.
+            learning_rate: The learning rate.
+            gamma: The discount factor.
+            initial_epsilon: The initial epsilon value.
+            final_epsilon: The final epsilon value.
+            epsilon_decay_steps: The number of steps for epsilon decay.
+            weight_selection_algo: The algorithm to use for weight selection. Options: "random", "ols", "gpi-ls"
+            epsilon_ols: The epsilon value for the optimistic linear support.
+            use_gpi_policy: Whether to use the Generalized Policy Improvement (GPI) or not.
+            transfer_q_table: Whether to reuse a Q-table from a previous learned policy when initializing a new policy.
+            dyna: Whether to use Dyna-Q or not.
+            dyna_updates: The number of Dyna-Q updates to perform.
+            project_name: The name of the project for logging.
+            experiment_name: The name of the experiment for logging.
+            log: Whether to log or not.
+        """
+        MOAgent.__init__(self, env)
+        # Learning
+        self.scalarization = scalarization
+        self.learning_rate = learning_rate
+        self.gamma = gamma
+        self.initial_epsilon = initial_epsilon
+        self.final_epsilon = final_epsilon
+        self.epsilon_decay_steps = epsilon_decay_steps
+        self.use_gpi_policy = use_gpi_policy
+        self.dyna = dyna
+        self.dyna_updates = dyna_updates
+        self.transfer_q_table = transfer_q_table
+        # Linear support
+        self.policies = []
+        self.weight_selection_algo = weight_selection_algo
+        self.epsilon_ols = epsilon_ols
+        assert self.weight_selection_algo in [
+            "random",
+            "ols",
+            "gpi-ls",
+        ], f"Unknown weight selection algorithm: {self.weight_selection_algo}."
+        self.linear_support = LinearSupport(num_objectives=self.reward_dim, epsilon=epsilon_ols)
+
+        # Logging
+        self.project_name = project_name
+        self.experiment_name = experiment_name
+        self.log = log
+
+        if self.log:
+            self.setup_wandb(project_name=self.project_name, experiment_name=self.experiment_name)
+        else:
+            self.writer = None
+
+    @override
+    def get_config(self) -> dict:
+        return {
+            "env_id": self.env.unwrapped.spec.id,
+            "alpha": self.learning_rate,
+            "gamma": self.gamma,
+            "initial_epsilon": self.initial_epsilon,
+            "final_epsilon": self.final_epsilon,
+            "epsilon_decay_steps": self.epsilon_decay_steps,
+            "scalarization": self.scalarization.__name__,
+            "use_gpi_policy": self.use_gpi_policy,
+            "weight_selection_algo": self.weight_selection_algo,
+            "epsilon_ols": self.epsilon_ols,
+            "transfer_q_table": self.transfer_q_table,
+            "dyna": self.dyna,
+            "dyna_updates": self.dyna_updates,
+        }
+
+    def _gpi_action(self, state: np.ndarray, w: np.ndarray) -> int:
+        """Get the action given by the GPI policy.
+
+        GPI(s, w) = argmax_a max_pi Q^pi(s, a, w) .
+
+        Args:
+            state: The state to get the action for.
+            weights: The weights to use for the scalarization.
+
+        Returns:
+            The action to take.
+        """
+        q_vals = np.stack([policy.scalarized_q_values(state, w) for policy in self.policies])
+        _, action = np.unravel_index(np.argmax(q_vals), q_vals.shape)
+        return int(action)
+
+    def eval(self, obs: np.array, w: Optional[np.ndarray] = None) -> int:
+        """If use_gpi is True, return the action given by the GPI policy. Otherwise, chooses the best policy for w and follows it."""
+        if self.use_gpi_policy:
+            return self._gpi_action(obs, w)
+        else:
+            best_policy = np.argmax([np.dot(w, v) for v in self.linear_support.ccs])
+            return self.policies[best_policy].eval(obs, w)
+
+    def delete_policies(self, delete_indx: List[int]):
+        """Delete the policies with the given indices."""
+        for i in sorted(delete_indx, reverse=True):
+            self.policies.pop(i)
+
+    def train(
+        self,
+        eval_env: gym.Env,
+        ref_point: np.ndarray,
+        num_iterations: int,
+        timesteps_per_iteration: int,
+        known_pareto_front: Optional[List[np.ndarray]] = None,
+        eval_weights_number_for_front: int = 100,
+        eval_freq: int = 1000,
+        num_episodes_eval: int = 10,
+    ):
+        """Learn a set of policies.
+
+        Args:
+            eval_env: The environment to use for evaluation.
+            ref_point: The reference point for the hypervolume calculation.
+            num_iterations: The number of iterations/policies to train.
+            timesteps_per_iteration: The number of timesteps per iteration.
+            eval_freq: The frequency of evaluation.
+            known_pareto_front: The optimal Pareto front, if known. Used for metrics.
+            eval_weights_number_for_front: The number of weights to use to construct a Pareto front for evaluation.
+            epsilon_linear_support: The epsilon value for the linear support algorithm.
+            num_episodes_eval: The number of episodes used to evaluate the value of a policy.
+        """
+        if eval_env is None:
+            eval_env = deepcopy(self.env)
+
+        eval_weights = equally_spaced_weights(self.reward_dim, n=eval_weights_number_for_front)
+
+        for iter in range(num_iterations):
+            if self.weight_selection_algo == "ols" or self.weight_selection_algo == "gpi-ls":
+                w = self.linear_support.next_weight(
+                    algo=self.weight_selection_algo,
+                    gpi_agent=self if self.weight_selection_algo == "gpi-ls" else None,
+                    env=eval_env if self.weight_selection_algo == "gpi-ls" else None,
+                    rep_eval=num_episodes_eval,
+                )
+            elif self.weight_selection_algo == "random":
+                w = random_weights(self.reward_dim)
+
+            new_agent = MOQLearning(
+                env=self.env,
+                id=iter,
+                weights=w,
+                scalarization=self.scalarization,
+                learning_rate=self.learning_rate,
+                gamma=self.gamma,
+                initial_epsilon=self.initial_epsilon,
+                final_epsilon=self.final_epsilon,
+                epsilon_decay_steps=self.epsilon_decay_steps,
+                dyna=self.dyna,
+                dyna_updates=self.dyna_updates,
+                log=self.log,
+                parent_writer=self.writer,
+            )
+            if self.transfer_q_table and len(self.policies) > 0:
+                reuse_ind = np.argmax([np.dot(w, v) for v in self.linear_support.ccs])
+                new_agent.q_table = deepcopy(self.policies[reuse_ind].q_table)
+            self.policies.append(new_agent)
+
+            start_time = time.time()
+            new_agent.global_step = self.global_step
+            new_agent.train(
+                start_time=start_time,
+                total_timesteps=timesteps_per_iteration,
+                reset_num_timesteps=False,
+                eval_freq=eval_freq,
+                eval_env=eval_env,
+            )
+            self.global_step = new_agent.global_step
+
+            value = policy_evaluation_mo(agent=new_agent, env=eval_env, w=w, rep=num_episodes_eval)[3]
+            removed_inds = self.linear_support.add_solution(value, w)
+            self.delete_policies(removed_inds)
+
+            if self.log:
+                if self.use_gpi_policy:
+                    front = [
+                        policy_evaluation_mo(agent=self, env=eval_env, w=w, rep=num_episodes_eval)[3] for w in eval_weights
+                    ]
+                else:
+                    front = self.linear_support.ccs
+                log_all_multi_policy_metrics(
+                    current_front=front,
+                    hv_ref_point=ref_point,
+                    reward_dim=self.reward_dim,
+                    global_step=self.global_step,
+                    writer=self.writer,
+                    ref_front=known_pareto_front,
+                )
+
+        if self.writer is not None:
+            self.close_wandb()
diff --git a/morl-baselines/morl_baselines/multi_policy/pareto_q_learning/__init__.py b/morl-baselines/morl_baselines/multi_policy/pareto_q_learning/__init__.py
new file mode 100644
index 0000000..7594673
--- /dev/null
+++ b/morl-baselines/morl_baselines/multi_policy/pareto_q_learning/__init__.py
@@ -0,0 +1 @@
+"""Pareto Q-Learning."""
diff --git a/morl-baselines/morl_baselines/multi_policy/pareto_q_learning/pql.py b/morl-baselines/morl_baselines/multi_policy/pareto_q_learning/pql.py
new file mode 100644
index 0000000..8d2c2bc
--- /dev/null
+++ b/morl-baselines/morl_baselines/multi_policy/pareto_q_learning/pql.py
@@ -0,0 +1,301 @@
+"""Pareto Q-Learning."""
+from typing import Callable, List, Optional
+
+import numpy as np
+
+from morl_baselines.common.morl_algorithm import MOAgent
+from morl_baselines.common.pareto import get_non_dominated
+from morl_baselines.common.performance_indicators import hypervolume
+from morl_baselines.common.utils import log_all_multi_policy_metrics
+
+
+class PQL(MOAgent):
+    """Pareto Q-learning.
+
+    Tabular method relying on pareto pruning.
+    Paper: K. Van Moffaert and A. Nowé, “Multi-objective reinforcement learning using sets of pareto dominating policies,” The Journal of Machine Learning Research, vol. 15, no. 1, pp. 3483–3512, 2014.
+    """
+
+    def __init__(
+        self,
+        env,
+        ref_point: np.ndarray,
+        gamma: float = 0.8,
+        initial_epsilon: float = 1.0,
+        epsilon_decay: float = 0.99,
+        final_epsilon: float = 0.1,
+        seed: int = None,
+        project_name: str = "MORL-Baselines",
+        experiment_name: str = "Pareto Q-Learning",
+        log: bool = True,
+    ):
+        """Initialize the Pareto Q-learning algorithm.
+
+        Args:
+            env: The environment.
+            ref_point: The reference point for the hypervolume metric.
+            gamma: The discount factor.
+            initial_epsilon: The initial epsilon value.
+            epsilon_decay: The epsilon decay rate.
+            final_epsilon: The final epsilon value.
+            seed: The random seed.
+            project_name: The name of the project used for logging.
+            experiment_name: The name of the experiment used for logging.
+            log: Whether to log or not.
+        """
+        super().__init__(env)
+        # Learning parameters
+        self.gamma = gamma
+        self.epsilon = initial_epsilon
+        self.initial_epsilon = initial_epsilon
+        self.epsilon_decay = epsilon_decay
+        self.final_epsilon = final_epsilon
+
+        # Algorithm setup
+        self.seed = seed
+        self.rng = np.random.default_rng(seed)
+        self.ref_point = ref_point
+
+        self.num_actions = self.env.action_space.n
+        low_bound = self.env.observation_space.low
+        high_bound = self.env.observation_space.high
+        self.env_shape = (high_bound[0] - low_bound[0] + 1, high_bound[1] - low_bound[1] + 1)
+        self.num_states = np.prod(self.env_shape)
+        self.num_objectives = self.env.reward_space.shape[0]
+        self.counts = np.zeros((self.num_states, self.num_actions))
+        self.non_dominated = [
+            [{tuple(np.zeros(self.num_objectives))} for _ in range(self.num_actions)] for _ in range(self.num_states)
+        ]
+        self.avg_reward = np.zeros((self.num_states, self.num_actions, self.num_objectives))
+
+        # Logging
+        self.project_name = project_name
+        self.experiment_name = experiment_name
+        self.log = log
+
+        if self.log:
+            self.setup_wandb(project_name=self.project_name, experiment_name=self.experiment_name)
+
+    def get_config(self) -> dict:
+        """Get the configuration dictionary.
+
+        Returns:
+            Dict: A dictionary of parameters and values.
+        """
+        return {
+            "env_id": self.env.unwrapped.spec.id,
+            "ref_point": list(self.ref_point),
+            "gamma": self.gamma,
+            "initial_epsilon": self.initial_epsilon,
+            "epsilon_decay": self.epsilon_decay,
+            "final_epsilon": self.final_epsilon,
+            "seed": self.seed,
+        }
+
+    def score_pareto_cardinality(self, state: int):
+        """Compute the action scores based upon the Pareto cardinality metric.
+
+        Args:
+            state (int): The current state.
+
+        Returns:
+            ndarray: A score per action.
+        """
+        q_sets = [self.get_q_set(state, action) for action in range(self.num_actions)]
+        candidates = set().union(*q_sets)
+        non_dominated = get_non_dominated(candidates)
+        scores = np.zeros(self.num_actions)
+
+        for vec in non_dominated:
+            for action, q_set in enumerate(q_sets):
+                if vec in q_set:
+                    scores[action] += 1
+
+        return scores
+
+    def score_hypervolume(self, state: int):
+        """Compute the action scores based upon the hypervolume metric.
+
+        Args:
+            state (int): The current state.
+
+        Returns:
+            ndarray: A score per action.
+        """
+        q_sets = [self.get_q_set(state, action) for action in range(self.num_actions)]
+        action_scores = [hypervolume(self.ref_point, list(q_set)) for q_set in q_sets]
+        return action_scores
+
+    def get_q_set(self, state: int, action: int):
+        """Compute the Q-set for a given state-action pair.
+
+        Args:
+            state (int): The current state.
+            action (int): The action.
+
+        Returns:
+            A set of Q vectors.
+        """
+        nd_array = np.array(list(self.non_dominated[state][action]))
+        q_array = self.avg_reward[state, action] + self.gamma * nd_array
+        return {tuple(vec) for vec in q_array}
+
+    def select_action(self, state: int, score_func: Callable):
+        """Select an action in the current state.
+
+        Args:
+            state (int): The current state.
+            score_func (callable): A function that returns a score per action.
+
+        Returns:
+            int: The selected action.
+        """
+        if self.rng.uniform(0, 1) < self.epsilon:
+            return self.rng.integers(self.num_actions)
+        else:
+            action_scores = score_func(state)
+            return self.rng.choice(np.argwhere(action_scores == np.max(action_scores)).flatten())
+
+    def calc_non_dominated(self, state: int):
+        """Get the non-dominated vectors in a given state.
+
+        Args:
+            state (int): The current state.
+
+        Returns:
+            Set: A set of Pareto non-dominated vectors.
+        """
+        candidates = set().union(*[self.get_q_set(state, action) for action in range(self.num_actions)])
+        non_dominated = get_non_dominated(candidates)
+        return non_dominated
+
+    def train(
+        self,
+        eval_ref_point: np.ndarray,
+        known_pareto_front: Optional[List[np.ndarray]] = None,
+        num_episodes: Optional[int] = 3000,
+        log_every: Optional[int] = 100,
+        action_eval: Optional[str] = "hypervolume",
+    ):
+        """Learn the Pareto front.
+
+        Args:
+            eval_ref_point (ndarray): The reference point for the hypervolume metric during evaluation.
+            known_pareto_front (List[ndarray], optional): The optimal Pareto front, if known.
+            num_episodes (int, optional): The number of episodes to train for.
+            log_every (int, optional): Log the results every number of episodes. (Default value = 100)
+            action_eval (str, optional): The action evaluation function name. (Default value = 'hypervolume')
+
+        Returns:
+            Set: The final Pareto front.
+        """
+        if action_eval == "hypervolume":
+            score_func = self.score_hypervolume
+        elif action_eval == "pareto_cardinality":
+            score_func = self.score_pareto_cardinality
+        else:
+            raise Exception("No other method implemented yet")
+
+        for episode in range(num_episodes):
+            if episode % log_every == 0:
+                print(f"Training episode {episode + 1}")
+
+            state, _ = self.env.reset()
+            state = int(np.ravel_multi_index(state, self.env_shape))
+            terminated = False
+            truncated = False
+
+            while not (terminated or truncated):
+                action = self.select_action(state, score_func)
+                next_state, reward, terminated, truncated, _ = self.env.step(action)
+                self.global_step += 1
+                next_state = int(np.ravel_multi_index(next_state, self.env_shape))
+
+                self.counts[state, action] += 1
+                self.non_dominated[state][action] = self.calc_non_dominated(next_state)
+                self.avg_reward[state, action] += (reward - self.avg_reward[state, action]) / self.counts[state, action]
+                state = next_state
+
+            self.epsilon = max(self.final_epsilon, self.epsilon * self.epsilon_decay)
+
+            if self.log and episode % log_every == 0:
+                self.writer.add_scalar("global_step", self.global_step, self.global_step)
+                pf = self._eval_all_policies()
+                log_all_multi_policy_metrics(
+                    current_front=pf,
+                    hv_ref_point=eval_ref_point,
+                    reward_dim=self.reward_dim,
+                    global_step=self.global_step,
+                    writer=self.writer,
+                    ref_front=known_pareto_front,
+                )
+
+        return self.get_local_pcs(state=0)
+
+    def _eval_all_policies(self) -> List[np.ndarray]:
+        """Evaluate all learned policies by tracking them."""
+        pf = []
+        for vec in self.get_local_pcs(state=0):
+            pf.append(self.track_policy(vec))
+
+        return pf
+
+    def track_policy(self, vec, tol=1e-3):
+        """Track a policy from its return vector.
+
+        Args:
+            vec (array_like): The return vector to track.
+            tol (float, optional): The tolerance for the return vector. (Default value = 1e-3)
+        """
+        target = np.array(vec)
+        state, _ = self.env.reset()
+        terminated = False
+        truncated = False
+        total_rew = np.zeros(self.num_objectives)
+        current_gamma = 1.0
+
+        while not (terminated or truncated):
+            state = np.ravel_multi_index(state, self.env_shape)
+            closest_dist = np.inf
+            closest_action = 0
+            found_action = False
+            new_target = target
+
+            for action in range(self.num_actions):
+                im_rew = self.avg_reward[state, action]
+                non_dominated_set = self.non_dominated[state][action]
+
+                for q in non_dominated_set:
+                    q = np.array(q)
+                    dist = np.sum(np.abs(self.gamma * q + im_rew - target))
+                    if dist < closest_dist:
+                        closest_dist = dist
+                        closest_action = action
+                        new_target = q
+
+                        if dist < tol:
+                            found_action = True
+                            break
+
+                if found_action:
+                    break
+
+            state, reward, terminated, truncated, _ = self.env.step(closest_action)
+            total_rew += current_gamma * reward
+            current_gamma *= self.gamma
+            target = new_target
+
+        return total_rew
+
+    def get_local_pcs(self, state: int = 0):
+        """Collect the local PCS in a given state.
+
+        Args:
+            state (int): The state to get a local PCS for. (Default value = 0)
+
+        Returns:
+            Set: A set of Pareto optimal vectors.
+        """
+        q_sets = [self.get_q_set(state, action) for action in range(self.num_actions)]
+        candidates = set().union(*q_sets)
+        return get_non_dominated(candidates)
diff --git a/morl-baselines/morl_baselines/multi_policy/pcn/__init__.py b/morl-baselines/morl_baselines/multi_policy/pcn/__init__.py
new file mode 100644
index 0000000..cc17b34
--- /dev/null
+++ b/morl-baselines/morl_baselines/multi_policy/pcn/__init__.py
@@ -0,0 +1 @@
+"""Pareto Conditioned Networks (PCN) for multi-policy learning."""
diff --git a/morl-baselines/morl_baselines/multi_policy/pcn/pcn.py b/morl-baselines/morl_baselines/multi_policy/pcn/pcn.py
new file mode 100644
index 0000000..4ab571d
--- /dev/null
+++ b/morl-baselines/morl_baselines/multi_policy/pcn/pcn.py
@@ -0,0 +1,431 @@
+"""Pareto Conditioned Network. Code adapted from https://github.com/mathieu-reymond/pareto-conditioned-networks ."""
+import heapq
+import os
+from dataclasses import dataclass
+from typing import List, Optional, Union
+
+import gymnasium as gym
+import numpy as np
+import torch as th
+import torch.nn as nn
+import torch.nn.functional as F
+
+from morl_baselines.common.morl_algorithm import MOAgent, MOPolicy
+from morl_baselines.common.pareto import get_non_dominated_inds
+from morl_baselines.common.performance_indicators import hypervolume
+from morl_baselines.common.utils import log_all_multi_policy_metrics
+
+
+def crowding_distance(points):
+    """Compute the crowding distance of a set of points."""
+    # first normalize across dimensions
+    points = (points - points.min(axis=0)) / (points.ptp(axis=0) + 1e-8)
+    # sort points per dimension
+    dim_sorted = np.argsort(points, axis=0)
+    point_sorted = np.take_along_axis(points, dim_sorted, axis=0)
+    # compute distances between lower and higher point
+    distances = np.abs(point_sorted[:-2] - point_sorted[2:])
+    # pad extrema's with 1, for each dimension
+    distances = np.pad(distances, ((1,), (0,)), constant_values=1)
+    # sum distances of each dimension of the same point
+    crowding = np.zeros(points.shape)
+    crowding[dim_sorted, np.arange(points.shape[-1])] = distances
+    crowding = np.sum(crowding, axis=-1)
+    return crowding
+
+
+@dataclass
+class Transition:
+    """Transition dataclass."""
+
+    observation: np.ndarray
+    action: int
+    reward: np.ndarray
+    next_observation: np.ndarray
+    terminal: bool
+
+
+class Model(nn.Module):
+    """Model for the PCN."""
+
+    def __init__(self, state_dim: int, action_dim: int, reward_dim: int, scaling_factor: np.ndarray, hidden_dim: int = 64):
+        """Initialize the PCN model."""
+        super().__init__()
+        self.state_dim = state_dim
+        self.action_dim = action_dim
+        self.reward_dim = reward_dim
+        self.scaling_factor = nn.Parameter(th.tensor(scaling_factor).float(), requires_grad=False)
+        self.hidden_dim = hidden_dim
+
+        self.s_emb = nn.Sequential(nn.Linear(self.state_dim, self.hidden_dim), nn.Sigmoid())
+        self.c_emb = nn.Sequential(nn.Linear(self.reward_dim + 1, self.hidden_dim), nn.Sigmoid())
+        self.fc = nn.Sequential(
+            nn.Linear(self.hidden_dim, self.hidden_dim),
+            nn.ReLU(),
+            nn.Linear(self.hidden_dim, self.action_dim),
+            nn.LogSoftmax(1),
+        )
+
+    def forward(self, state, desired_return, desired_horizon):
+        """Return log-probabilities of actions."""
+        c = th.cat((desired_return, desired_horizon), dim=-1)
+        # commands are scaled by a fixed factor
+        c = c * self.scaling_factor
+        s = self.s_emb(state.float())
+        c = self.c_emb(c)
+        # element-wise multiplication of state-embedding and command
+        log_prob = self.fc(s * c)
+        return log_prob
+
+
+class PCN(MOAgent, MOPolicy):
+    """Pareto Conditioned Networks (PCN).
+
+    Reymond, M., Bargiacchi, E., & Nowé, A. (2022, May). Pareto Conditioned Networks.
+    In Proceedings of the 21st International Conference on Autonomous Agents
+    and Multiagent Systems (pp. 1110-1118).
+    https://www.ifaamas.org/Proceedings/aamas2022/pdfs/p1110.pdf
+
+    ## Credits
+
+    This code is a refactor of the code from the authors of the paper, available at:
+    https://github.com/mathieu-reymond/pareto-conditioned-networks
+    """
+
+    def __init__(
+        self,
+        env: Optional[gym.Env],
+        scaling_factor: np.ndarray,
+        learning_rate: float = 1e-2,
+        gamma: float = 1.0,
+        batch_size: int = 32,
+        hidden_dim: int = 64,
+        project_name: str = "MORL-Baselines",
+        experiment_name: str = "PCN",
+        log: bool = False,
+        device: Union[th.device, str] = "auto",
+    ) -> None:
+        """Initialize PCN agent.
+
+        Args:
+            env (Optional[gym.Env]): Gym environment.
+            scaling_factor (np.ndarray): Scaling factor for the desired return and horizon used in the model.
+            learning_rate (float, optional): Learning rate. Defaults to 1e-2.
+            gamma (float, optional): Discount factor. Defaults to 1.0.
+            batch_size (int, optional): Batch size. Defaults to 32.
+            hidden_dim (int, optional): Hidden dimension. Defaults to 64.
+            project_name (str, optional): Name of the project for wandb. Defaults to "MORL-Baselines".
+            experiment_name (str, optional): Name of the experiment for wandb. Defaults to "PCN".
+            log (bool, optional): Whether to log to wandb. Defaults to False.
+            device (Union[th.device, str], optional): Device to use. Defaults to "auto".
+        """
+        MOAgent.__init__(self, env, device=device)
+        MOPolicy.__init__(self, device)
+
+        self.experience_replay = []  # List of (distance, time_step, transition)
+        self.batch_size = batch_size
+        self.gamma = gamma
+        self.learning_rate = learning_rate
+        self.hidden_dim = hidden_dim
+        self.scaling_factor = scaling_factor
+        self.desired_return = None
+        self.desired_horizon = None
+
+        self.model = Model(
+            self.observation_dim, self.action_dim, self.reward_dim, self.scaling_factor, hidden_dim=self.hidden_dim
+        ).to(self.device)
+        self.opt = th.optim.Adam(self.model.parameters(), lr=self.learning_rate)
+
+        self.log = log
+        if log:
+            self.setup_wandb(project_name, experiment_name)
+
+    def get_config(self) -> dict:
+        """Get configuration of PCN model."""
+        return {
+            "env_id": self.env.unwrapped.spec.id,
+            "batch_size": self.batch_size,
+            "gamma": self.gamma,
+            "learning_rate": self.learning_rate,
+            "hidden_dim": self.hidden_dim,
+            "scaling_factor": self.scaling_factor,
+        }
+
+    def update(self):
+        """Update PCN model."""
+        batch = []
+        # randomly choose episodes from experience buffer
+        s_i = np.random.choice(np.arange(len(self.experience_replay)), size=self.batch_size, replace=True)
+        for i in s_i:
+            # episode is tuple (return, transitions)
+            ep = self.experience_replay[i][2]
+            # choose random timestep from episode,
+            # use it's return and leftover timesteps as desired return and horizon
+            t = np.random.randint(0, len(ep))
+            # reward contains return until end of episode
+            s_t, a_t, r_t, h_t = ep[t].observation, ep[t].action, np.float32(ep[t].reward), np.float32(len(ep) - t)
+            batch.append((s_t, a_t, r_t, h_t))
+
+        obs, actions, desired_return, desired_horizon = zip(*batch)
+        log_prob = self.model(
+            th.tensor(obs).to(self.device),
+            th.tensor(desired_return).to(self.device),
+            th.tensor(desired_horizon).unsqueeze(1).to(self.device),
+        )
+
+        self.opt.zero_grad()
+        # one-hot of action for CE loss
+        actions = F.one_hot(th.tensor(actions).long().to(self.device), len(log_prob[0]))
+        # cross-entropy loss
+        l = th.sum(-actions * log_prob, -1)
+        l = l.mean()
+        l.backward()
+        self.opt.step()
+
+        return l, log_prob
+
+    def _add_episode(self, transitions: List[Transition], max_size: int, step: int) -> None:
+        # compute return
+        for i in reversed(range(len(transitions) - 1)):
+            transitions[i].reward += self.gamma * transitions[i + 1].reward
+        # pop smallest episode of heap if full, add new episode
+        # heap is sorted by negative distance, (updated in nlargest)
+        # put positive number to ensure that new item stays in the heap
+        if len(self.experience_replay) == max_size:
+            heapq.heappushpop(self.experience_replay, (1, step, transitions))
+        else:
+            heapq.heappush(self.experience_replay, (1, step, transitions))
+
+    def _nlargest(self, n, threshold=0.2):
+        """See Section 4.4 of https://arxiv.org/pdf/2204.05036.pdf for details."""
+        returns = np.array([e[2][0].reward for e in self.experience_replay])
+        # crowding distance of each point, check ones that are too close together
+        distances = crowding_distance(returns)
+        sma = np.argwhere(distances <= threshold).flatten()
+
+        non_dominated_i = get_non_dominated_inds(returns)
+        non_dominated = returns[non_dominated_i]
+        # we will compute distance of each point with each non-dominated point,
+        # duplicate each point with number of non_dominated to compute respective distance
+        returns_exp = np.tile(np.expand_dims(returns, 1), (1, len(non_dominated), 1))
+        # distance to closest non_dominated point
+        l2 = np.min(np.linalg.norm(returns_exp - non_dominated, axis=-1), axis=-1) * -1
+        # all points that are too close together (crowding distance < threshold) get a penalty
+        non_dominated_i = np.nonzero(non_dominated_i)[0]
+        _, unique_i = np.unique(non_dominated, axis=0, return_index=True)
+        unique_i = non_dominated_i[unique_i]
+        duplicates = np.ones(len(l2), dtype=bool)
+        duplicates[unique_i] = False
+        l2[duplicates] -= 1e-5
+        l2[sma] *= 2
+
+        sorted_i = np.argsort(l2)
+        largest = [self.experience_replay[i] for i in sorted_i[-n:]]
+        # before returning largest elements, update all distances in heap
+        for i in range(len(l2)):
+            self.experience_replay[i] = (l2[i], self.experience_replay[i][1], self.experience_replay[i][2])
+        heapq.heapify(self.experience_replay)
+        return largest
+
+    def _choose_commands(self, num_episodes: int):
+        # get best episodes, according to their crowding distance
+        episodes = self._nlargest(num_episodes)
+        returns, horizons = list(zip(*[(e[2][0].reward, len(e[2])) for e in episodes]))
+        # keep only non-dominated returns
+        nd_i = get_non_dominated_inds(np.array(returns))
+        returns = np.array(returns)[nd_i]
+        horizons = np.array(horizons)[nd_i]
+        # pick random return from random best episode
+        r_i = np.random.randint(0, len(returns))
+        desired_horizon = np.float32(horizons[r_i] - 2)
+        # mean and std per objective
+        _, s = np.mean(returns, axis=0), np.std(returns, axis=0)
+        # desired return is sampled from [M, M+S], to try to do better than mean return
+        desired_return = returns[r_i].copy()
+        # random objective
+        r_i = np.random.randint(0, len(desired_return))
+        desired_return[r_i] += np.random.uniform(high=s[r_i])
+        desired_return = np.float32(desired_return)
+        return desired_return, desired_horizon
+
+    def _act(self, obs: np.ndarray, desired_return, desired_horizon) -> int:
+        log_probs = self.model(
+            th.tensor([obs]).float().to(self.device),
+            th.tensor([desired_return]).float().to(self.device),
+            th.tensor([desired_horizon]).unsqueeze(1).float().to(self.device),
+        )
+        log_probs = log_probs.detach().cpu().numpy()[0]
+        action = np.random.choice(np.arange(len(log_probs)), p=np.exp(log_probs))
+        return action
+
+    def _run_episode(self, env, desired_return, desired_horizon, max_return):
+        transitions = []
+        obs, _ = env.reset()
+        done = False
+        while not done:
+            action = self._act(obs, desired_return, desired_horizon)
+            n_obs, reward, terminated, truncated, _ = env.step(action)
+            done = terminated or truncated
+
+            transitions.append(
+                Transition(
+                    observation=obs,
+                    action=action,
+                    reward=np.float32(reward).copy(),
+                    next_observation=n_obs,
+                    terminal=terminated,
+                )
+            )
+
+            obs = n_obs
+            # clip desired return, to return-upper-bound,
+            # to avoid negative returns giving impossible desired returns
+            desired_return = np.clip(desired_return - reward, None, max_return, dtype=np.float32)
+            # clip desired horizon to avoid negative horizons
+            desired_horizon = np.float32(max(desired_horizon - 1, 1.0))
+        return transitions
+
+    def set_desired_return_and_horizon(self, desired_return: np.ndarray, desired_horizon: int):
+        """Set desired return and horizon for evaluation."""
+        self.desired_return = desired_return
+        self.desired_horizon = desired_horizon
+
+    def eval(self, obs, w=None):
+        """Evaluate policy action for a given observation."""
+        return self._act(obs, self.desired_return, self.desired_horizon)
+
+    def evaluate(self, env, max_return, n=10):
+        """Evaluate policy in the given environment."""
+        episodes = self._nlargest(n)
+        returns, horizons = list(zip(*[(e[2][0].reward, len(e[2])) for e in episodes]))
+        returns = np.float32(returns)
+        horizons = np.float32(horizons)
+        e_returns = []
+        for i in range(n):
+            transitions = self._run_episode(env, returns[i], np.float32(horizons[i] - 2), max_return)
+            # compute return
+            for i in reversed(range(len(transitions) - 1)):
+                transitions[i].reward += self.gamma * transitions[i + 1].reward
+            e_returns.append(transitions[0].reward)
+
+        distances = np.linalg.norm(np.array(returns) - np.array(e_returns), axis=-1)
+        return e_returns, np.array(returns), distances
+
+    def save(self, filename: str = "PCN_model", savedir: str = "weights"):
+        """Save PCN."""
+        if not os.path.isdir(savedir):
+            os.makedirs(savedir)
+        th.save(self.model, f"{savedir}/{filename}.pt")
+
+    def train(
+        self,
+        env: gym.Env,
+        ref_point: np.ndarray,
+        known_pareto_front: Optional[List[np.ndarray]] = None,
+        num_er_episodes: int = 500,
+        total_time_steps: int = 1e7,
+        num_step_episodes: int = 10,
+        num_model_updates: int = 100,
+        max_return: np.ndarray = 250.0,
+        max_buffer_size: int = 500,
+    ):
+        """Train PCN.
+
+        Args:
+            env: environment
+            ref_point: reference point for hypervolume calculation
+            known_pareto_front: Optimal pareto front for metrics calculation, if known.
+            num_er_episodes: number of episodes to fill experience replay buffer
+            total_time_steps: total number of time steps to train for
+            num_step_episodes: number of steps per episode
+            num_model_updates: number of model updates per episode
+            max_return: maximum return for clipping desired return
+            max_buffer_size: maximum buffer size
+        """
+        self.global_step = 0
+        total_episodes = num_er_episodes
+        n_checkpoints = 0
+
+        # fill buffer with random episodes
+        self.experience_replay = []
+        for _ in range(num_er_episodes):
+            transitions = []
+            obs, _ = env.reset()
+            done = False
+            while not done:
+                action = env.action_space.sample()
+                n_obs, reward, terminated, truncated, _ = env.step(action)
+                transitions.append(Transition(obs, action, np.float32(reward).copy(), n_obs, terminated))
+                done = terminated or truncated
+                obs = n_obs
+                self.global_step += 1
+            # add episode in-place
+            self._add_episode(transitions, max_size=max_buffer_size, step=self.global_step)
+
+        while self.global_step < total_time_steps:
+            loss = []
+            entropy = []
+            for _ in range(num_model_updates):
+                l, lp = self.update()
+                loss.append(l.detach().cpu().numpy())
+                lp = lp.detach().cpu().numpy()
+                ent = np.sum(-np.exp(lp) * lp)
+                entropy.append(ent)
+
+            desired_return, desired_horizon = self._choose_commands(num_er_episodes)
+
+            # get all leaves, contain biggest elements, experience_replay got heapified in choose_commands
+            leaves_r = np.array([e[2][0].reward for e in self.experience_replay[len(self.experience_replay) // 2 :]])
+            # leaves_h = np.array([len(e[2]) for e in self.experience_replay[len(self.experience_replay) // 2 :]])
+
+            if self.log:
+                hv = hypervolume(ref_point, leaves_r)
+                hv_est = hv
+                self.writer.add_scalar("train/hypervolume", hv_est, self.global_step)
+                self.writer.add_scalar("train/loss", np.mean(loss), self.global_step)
+                self.writer.add_scalar("train/entropy", np.mean(entropy), self.global_step)
+
+            returns = []
+            horizons = []
+            for _ in range(num_step_episodes):
+                transitions = self._run_episode(env, desired_return, desired_horizon, max_return)
+                self.global_step += len(transitions)
+                self._add_episode(transitions, max_size=max_buffer_size, step=self.global_step)
+                returns.append(transitions[0].reward)
+                horizons.append(len(transitions))
+
+            total_episodes += num_step_episodes
+            if self.log:
+                self.writer.add_scalar("train/episode", total_episodes, self.global_step)
+                self.writer.add_scalar("train/horizon_desired", desired_horizon, self.global_step)
+                self.writer.add_scalar(
+                    "train/mean_horizon_distance", np.linalg.norm(np.mean(horizons) - desired_horizon), self.global_step
+                )
+
+                for i in range(self.reward_dim):
+                    self.writer.add_scalar(f"train/desired_return_{i}", desired_return[i], self.global_step)
+                    self.writer.add_scalar(f"train/mean_return_{i}", np.mean(np.array(returns)[:, i]), self.global_step)
+                    self.writer.add_scalar(
+                        f"train/mean_return_distance_{i}",
+                        np.linalg.norm(np.mean(np.array(returns)[:, i]) - desired_return[i]),
+                        self.global_step,
+                    )
+            print(
+                f"step {self.global_step} \t return {np.mean(returns, axis=0)}, ({np.std(returns, axis=0)}) \t loss {np.mean(loss):.3E}"
+            )
+
+            if self.global_step >= (n_checkpoints + 1) * total_time_steps / 100:
+                self.save()
+                n_checkpoints += 1
+                n_points = 10
+                e_returns, _, _ = self.evaluate(env, max_return, n=n_points)
+
+                if self.log:
+                    log_all_multi_policy_metrics(
+                        current_front=e_returns,
+                        hv_ref_point=ref_point,
+                        reward_dim=self.reward_dim,
+                        global_step=self.global_step,
+                        writer=self.writer,
+                        ref_front=known_pareto_front,
+                    )
diff --git a/morl-baselines/morl_baselines/multi_policy/pgmorl/__init__.py b/morl-baselines/morl_baselines/multi_policy/pgmorl/__init__.py
new file mode 100644
index 0000000..dacf5d9
--- /dev/null
+++ b/morl-baselines/morl_baselines/multi_policy/pgmorl/__init__.py
@@ -0,0 +1 @@
+"""Prediction Guided Multi-Objective Reinforcement Learning (PGMORL)."""
diff --git a/morl-baselines/morl_baselines/multi_policy/pgmorl/pgmorl.py b/morl-baselines/morl_baselines/multi_policy/pgmorl/pgmorl.py
new file mode 100644
index 0000000..fa00cde
--- /dev/null
+++ b/morl-baselines/morl_baselines/multi_policy/pgmorl/pgmorl.py
@@ -0,0 +1,668 @@
+"""PGMORL algorithm implementation.
+
+Some code in this file has been adapted from the original code provided by the authors of the paper https://github.com/mit-gfx/PGMORL.
+(!) Limited to 2 objectives for now.
+(!) The post-processing phase has not been implemented yet.
+"""
+import random
+import time
+from copy import deepcopy
+from typing import List, Optional, Tuple, Union
+from typing_extensions import override
+
+import gymnasium as gym
+import mo_gymnasium as mo_gym
+import numpy as np
+import torch as th
+from scipy.optimize import least_squares
+
+from morl_baselines.common.morl_algorithm import MOAgent
+from morl_baselines.common.pareto import ParetoArchive
+from morl_baselines.common.performance_indicators import hypervolume, sparsity
+from morl_baselines.common.utils import log_all_multi_policy_metrics
+from morl_baselines.single_policy.ser.mo_ppo import MOPPO, MOPPONet, make_env
+
+
+class PerformancePredictor:
+    """Performance prediction model.
+
+    Stores the performance deltas along with the used weights after each generation.
+    Then, uses these stored samples to perform a regression for predicting the performance of using a given weight
+    to train a given policy.
+    Predicts: Weight & performance -> delta performance
+    """
+
+    def __init__(
+        self,
+        neighborhood_threshold: float = 0.1,
+        sigma: float = 0.03,
+        A_bound_min: float = 1.0,
+        A_bound_max: float = 500.0,
+        f_scale: float = 20.0,
+    ):
+        """Initialize the performance predictor.
+
+        Args:
+            neighborhood_threshold: The threshold for the neighborhood of an evaluation.
+            sigma: The sigma value for the prediction model
+            A_bound_min: The minimum value for the A parameter of the prediction model.
+            A_bound_max: The maximum value for the A parameter of the prediction model.
+            f_scale: The scale value for the prediction model.
+        """
+        # Memory
+        self.previous_performance = []
+        self.next_performance = []
+        self.used_weight = []
+
+        # Prediction model parameters
+        self.neighborhood_threshold = neighborhood_threshold
+        self.A_bound_min = A_bound_min
+        self.A_bound_max = A_bound_max
+        self.f_scale = f_scale
+        self.sigma = sigma
+
+    def add(self, weight: np.ndarray, eval_before_pg: np.ndarray, eval_after_pg: np.ndarray) -> None:
+        """Add a new sample to the performance predictor.
+
+        Args:
+            weight: The weight used to train the policy.
+            eval_before_pg: The evaluation before training the policy.
+            eval_after_pg: The evaluation after training the policy.
+
+        Returns:
+            None
+        """
+        self.previous_performance.append(eval_before_pg)
+        self.next_performance.append(eval_after_pg)
+        self.used_weight.append(weight)
+
+    def __build_model_and_predict(
+        self,
+        training_weights,
+        training_deltas,
+        training_next_perfs,
+        current_dim,
+        current_eval: np.ndarray,
+        weight_candidate: np.ndarray,
+        sigma: float,
+    ):
+        """Uses the hyperbolic model on the training data: weights, deltas and next_perfs to predict the next delta given the current evaluation and weight.
+
+        Returns:
+             The expected delta from current_eval by using weight_candidate.
+        """
+
+        def __f(x, A, a, b, c):
+            return A * (np.exp(a * (x - b)) - 1) / (np.exp(a * (x - b)) + 1) + c
+
+        def __hyperbolic_model(params, x, y):
+            # f = A * (exp(a(x - b)) - 1) / (exp(a(x - b)) + 1) + c
+            return (
+                params[0] * (np.exp(params[1] * (x - params[2])) - 1.0) / (np.exp(params[1] * (x - params[2])) + 1)
+                + params[3]
+                - y
+            ) * w
+
+        def __jacobian(params, x, y):
+            A, a, b, _ = params[0], params[1], params[2], params[3]
+            J = np.zeros([len(params), len(x)])
+            # df_dA = (exp(a(x - b)) - 1) / (exp(a(x - b)) + 1)
+            J[0] = ((np.exp(a * (x - b)) - 1) / (np.exp(a * (x - b)) + 1)) * w
+            # df_da = A(x - b)(2exp(a(x-b)))/(exp(a(x-b)) + 1)^2
+            J[1] = (A * (x - b) * (2.0 * np.exp(a * (x - b))) / ((np.exp(a * (x - b)) + 1) ** 2)) * w
+            # df_db = A(-a)(2exp(a(x-b)))/(exp(a(x-b)) + 1)^2
+            J[2] = (A * (-a) * (2.0 * np.exp(a * (x - b))) / ((np.exp(a * (x - b)) + 1) ** 2)) * w
+            # df_dc = 1
+            J[3] = w
+
+            return np.transpose(J)
+
+        train_x = []
+        train_y = []
+        w = []
+        for i in range(len(training_weights)):
+            train_x.append(training_weights[i][current_dim])
+            train_y.append(training_deltas[i][current_dim])
+            diff = np.abs(training_next_perfs[i] - current_eval)
+            dist = np.linalg.norm(diff / np.abs(current_eval))
+            coef = np.exp(-((dist / sigma) ** 2) / 2.0)
+            w.append(coef)
+
+        train_x = np.array(train_x)
+        train_y = np.array(train_y)
+        w = np.array(w)
+
+        A_upperbound = np.clip(np.max(train_y) - np.min(train_y), 1.0, 500.0)
+        initial_guess = np.ones(4)
+        res_robust = least_squares(
+            __hyperbolic_model,
+            initial_guess,
+            loss="soft_l1",
+            f_scale=self.f_scale,
+            args=(train_x, train_y),
+            jac=__jacobian,
+            bounds=([0, 0.1, -5.0, -500.0], [A_upperbound, 20.0, 5.0, 500.0]),
+        )
+
+        return __f(weight_candidate[current_dim], *res_robust.x)
+
+    def predict_next_evaluation(self, weight_candidate: np.ndarray, policy_eval: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
+        """Predict the next evaluation of the policy.
+
+        Use a part of the collected data (determined by the neighborhood threshold) to predict the performance
+        after using weight to train the policy whose current evaluation is policy_eval.
+
+        Args:
+            weight_candidate: weight candidate
+            policy_eval: current evaluation of the policy
+
+        Returns:
+            the delta prediction, along with the predicted next evaluations
+        """
+        neighbor_weights = []
+        neighbor_deltas = []
+        neighbor_next_perf = []
+        current_sigma = self.sigma / 2.0
+        current_neighb_threshold = self.neighborhood_threshold / 2.0
+        # Iterates until we find at least 4 neighbors, enlarges the neighborhood at each iteration
+        while len(neighbor_weights) < 4:
+            # Enlarging neighborhood
+            current_sigma *= 2.0
+            current_neighb_threshold *= 2.0
+
+            print(f"current_neighb_threshold: {current_neighb_threshold}")
+            print(f"np.abs(policy_eval): {np.abs(policy_eval)}")
+            if current_neighb_threshold == np.inf or current_sigma == np.inf:
+                raise ValueError("Cannot find at least 4 neighbors by enlarging the neighborhood.")
+
+            # Filtering for neighbors
+            for previous_perf, next_perf, neighb_w in zip(self.previous_performance, self.next_performance, self.used_weight):
+                if np.all(np.abs(previous_perf - policy_eval) < current_neighb_threshold * np.abs(policy_eval)) and tuple(
+                    next_perf
+                ) not in list(map(tuple, neighbor_next_perf)):
+                    neighbor_weights.append(neighb_w)
+                    neighbor_deltas.append(next_perf - previous_perf)
+                    neighbor_next_perf.append(next_perf)
+
+        # constructing a prediction model for each objective dimension, and using it to construct the delta predictions
+        delta_predictions = [
+            self.__build_model_and_predict(
+                training_weights=neighbor_weights,
+                training_deltas=neighbor_deltas,
+                training_next_perfs=neighbor_next_perf,
+                current_dim=obj_num,
+                current_eval=policy_eval,
+                weight_candidate=weight_candidate,
+                sigma=current_sigma,
+            )
+            for obj_num in range(weight_candidate.size)
+        ]
+        delta_predictions = np.array(delta_predictions)
+        return delta_predictions, delta_predictions + policy_eval
+
+
+def generate_weights(delta_weight: float) -> np.ndarray:
+    """Generates weights uniformly distributed over the objective dimensions. These weight vectors are separated by delta_weight distance.
+
+    Args:
+        delta_weight: distance between weight vectors
+    Returns:
+        all the candidate weights
+    """
+    return np.linspace((0.0, 1.0), (1.0, 0.0), int(1 / delta_weight) + 1, dtype=np.float32)
+
+
+class PerformanceBuffer:
+    """Stores the population. Divides the objective space in to n bins of size max_size.
+
+    (!) restricted to 2D objective space (!)
+    """
+
+    def __init__(self, num_bins: int, max_size: int, origin: np.ndarray):
+        """Initializes the buffer.
+
+        Args:
+            num_bins: number of bins
+            max_size: maximum size of each bin
+            origin: origin of the objective space (to have only positive values)
+        """
+        self.num_bins = num_bins
+        self.max_size = max_size
+        self.origin = -origin
+        self.dtheta = np.pi / 2.0 / self.num_bins
+        self.bins = [[] for _ in range(self.num_bins)]
+        self.bins_evals = [[] for _ in range(self.num_bins)]
+
+    @property
+    def evaluations(self) -> List[np.ndarray]:
+        """Returns the evaluations of the individuals in the buffer."""
+        # flatten
+        return [e for l in self.bins_evals for e in l]
+
+    @property
+    def individuals(self) -> list:
+        """Returns the individuals in the buffer."""
+        return [i for l in self.bins for i in l]
+
+    def add(self, candidate, evaluation: np.ndarray):
+        """Adds a candidate to the buffer.
+
+        Args:
+            candidate: candidate to add
+            evaluation: evaluation of the candidate
+        """
+
+        def center_eval(eval):
+            # Objectives must be positive
+            return np.clip(eval + self.origin, 0.0, float("inf"))
+
+        centered_eval = center_eval(evaluation)
+        norm_eval = np.linalg.norm(centered_eval)
+        theta = np.arccos(np.clip(centered_eval[1] / (norm_eval + 1e-3), -1.0, 1.0))
+        buffer_id = int(theta // self.dtheta)
+
+        if buffer_id < 0 or buffer_id >= self.num_bins:
+            return
+
+        if len(self.bins[buffer_id]) < self.max_size:
+            self.bins[buffer_id].append(deepcopy(candidate))
+            self.bins_evals[buffer_id].append(evaluation)
+        else:
+            for i in range(len(self.bins[buffer_id])):
+                stored_eval_centered = center_eval(self.bins_evals[buffer_id][i])
+                if np.linalg.norm(stored_eval_centered) < np.linalg.norm(centered_eval):
+                    self.bins[buffer_id][i] = deepcopy(candidate)
+                    self.bins_evals[buffer_id][i] = evaluation
+                    break
+
+
+class PGMORL(MOAgent):
+    """Prediction Guided Multi-Objective Reinforcement Learning.
+
+    Reference: J. Xu, Y. Tian, P. Ma, D. Rus, S. Sueda, and W. Matusik,
+    “Prediction-Guided Multi-Objective Reinforcement Learning for Continuous Robot Control,”
+    in Proceedings of the 37th International Conference on Machine Learning,
+    Nov. 2020, pp. 10607–10616. Available: https://proceedings.mlr.press/v119/xu20h.html
+
+    Paper: https://people.csail.mit.edu/jiex/papers/PGMORL/paper.pdf
+    Supplementary materials: https://people.csail.mit.edu/jiex/papers/PGMORL/supp.pdf
+    """
+
+    def __init__(
+        self,
+        env_id: str,
+        ref_point: np.ndarray,
+        known_pareto_front: Optional[List[np.ndarray]] = None,
+        num_envs: int = 4,
+        pop_size: int = 6,
+        warmup_iterations: int = 80,
+        steps_per_iteration: int = 2048,
+        limit_env_steps: int = int(5e6),
+        evolutionary_iterations: int = 20,
+        num_weight_candidates: int = 7,
+        num_performance_buffer: int = 100,
+        performance_buffer_size: int = 2,
+        min_weight: float = 0.0,
+        max_weight: float = 1.0,
+        delta_weight: float = 0.2,
+        env=None,
+        gamma: float = 0.995,
+        project_name: str = "MORL-baselines",
+        experiment_name: str = "PGMORL",
+        seed: int = 0,
+        torch_deterministic: bool = True,
+        log: bool = True,
+        net_arch: List = [64, 64],
+        num_minibatches: int = 32,
+        update_epochs: int = 10,
+        learning_rate: float = 3e-4,
+        anneal_lr: bool = False,
+        clip_coef: float = 0.2,
+        ent_coef: float = 0.0,
+        vf_coef: float = 0.5,
+        clip_vloss: bool = True,
+        max_grad_norm: float = 0.5,
+        norm_adv: bool = True,
+        target_kl: Optional[float] = None,
+        gae: bool = True,
+        gae_lambda: float = 0.95,
+        device: Union[th.device, str] = "auto",
+    ):
+        """Initializes the PGMORL agent.
+
+        Args:
+            env_id: environment id
+            ref_point: reference point for the hypervolume calculation
+            known_pareto_front: known pareto front, if available
+            num_envs: number of environments to use (VectorizedEnvs)
+            pop_size: population size
+            warmup_iterations: number of warmup iterations
+            steps_per_iteration: number of steps per iteration
+            limit_env_steps: limit the number of environment steps
+            evolutionary_iterations: number of evolutionary iterations
+            num_weight_candidates: number of weight candidates
+            num_performance_buffer: number of performance buffers
+            performance_buffer_size: size of the performance buffers
+            min_weight: minimum weight
+            max_weight: maximum weight
+            delta_weight: delta weight for weight generation
+            env: environment
+            gamma: discount factor
+            project_name: name of the project. Usually MORL-baselines.
+            experiment_name: name of the experiment. Usually PGMORL.
+            seed: seed for the random number generator
+            torch_deterministic: whether to use deterministic torch operations
+            log: whether to log the results
+            net_arch: number of units per layer
+            num_minibatches: number of minibatches
+            update_epochs: number of update epochs
+            learning_rate: learning rate
+            anneal_lr: whether to anneal the learning rate
+            clip_coef: coefficient for the policy gradient clipping
+            ent_coef: coefficient for the entropy term
+            vf_coef: coefficient for the value function loss
+            clip_vloss: whether to clip the value function loss
+            max_grad_norm: maximum gradient norm
+            norm_adv: whether to normalize the advantages
+            target_kl: target KL divergence
+            gae: whether to use generalized advantage estimation
+            gae_lambda: lambda parameter for GAE
+            device: device on which the code should run
+        """
+        super().__init__(env, device=device)
+        # Env dimensions
+        self.tmp_env = mo_gym.make(env_id)
+        self.extract_env_info(self.tmp_env)
+        self.env_id = env_id
+        self.num_envs = num_envs
+        assert isinstance(self.action_space, gym.spaces.Box), "only continuous action space is supported"
+        self.tmp_env.close()
+        self.gamma = gamma
+        self.ref_point = ref_point
+        self.known_pareto_front = known_pareto_front
+
+        # EA parameters
+        self.pop_size = pop_size
+        self.warmup_iterations = warmup_iterations
+        self.steps_per_iteration = steps_per_iteration
+        self.evolutionary_iterations = evolutionary_iterations
+        self.num_weight_candidates = num_weight_candidates
+        self.min_weight = min_weight
+        self.max_weight = max_weight
+        self.delta_weight = delta_weight
+        self.limit_env_steps = limit_env_steps
+        self.max_iterations = self.limit_env_steps // self.steps_per_iteration // self.num_envs
+        self.iteration = 0
+        self.num_performance_buffer = num_performance_buffer
+        self.performance_buffer_size = performance_buffer_size
+        self.archive = ParetoArchive()
+        self.population = PerformanceBuffer(
+            num_bins=self.num_performance_buffer,
+            max_size=self.performance_buffer_size,
+            origin=self.ref_point,
+        )
+        self.predictor = PerformancePredictor()
+
+        # PPO Parameters
+        self.net_arch = net_arch
+        self.batch_size = int(self.num_envs * self.steps_per_iteration)
+        self.num_minibatches = num_minibatches
+        self.minibatch_size = int(self.batch_size // self.num_minibatches)
+        self.update_epochs = update_epochs
+        self.learning_rate = learning_rate
+        self.anneal_lr = anneal_lr
+        self.clip_coef = clip_coef
+        self.vf_coef = vf_coef
+        self.ent_coef = ent_coef
+        self.max_grad_norm = max_grad_norm
+        self.norm_adv = norm_adv
+        self.target_kl = target_kl
+        self.clip_vloss = clip_vloss
+        self.gae_lambda = gae_lambda
+        self.gae = gae
+
+        # seeding
+        self.seed = seed
+        random.seed(self.seed)
+        np.random.seed(self.seed)
+        th.manual_seed(self.seed)
+        th.backends.cudnn.deterministic = torch_deterministic
+
+        # env setup
+        if env is None:
+            self.env = mo_gym.MOSyncVectorEnv(
+                # Video recording is disabled since broken for now
+                [make_env(env_id, self.seed + i, i, experiment_name, self.gamma) for i in range(1, self.num_envs + 1)]
+            )
+        else:
+            raise ValueError("Environments should be vectorized for PPO. You should provide an environment id instead.")
+
+        # Logging
+        self.log = log
+        if self.log:
+            self.setup_wandb(project_name, experiment_name)
+        else:
+            self.writer = None
+
+        self.networks = [
+            MOPPONet(
+                self.observation_shape,
+                self.action_space.shape,
+                self.reward_dim,
+                self.net_arch,
+            ).to(self.device)
+            for _ in range(self.pop_size)
+        ]
+
+        weights = generate_weights(self.delta_weight)
+        print(f"Warmup phase - sampled weights: {weights}")
+
+        self.agents = [
+            MOPPO(
+                i,
+                self.networks[i],
+                weights[i],
+                self.env,
+                self.writer,
+                gamma=self.gamma,
+                device=self.device,
+                seed=self.seed,
+                steps_per_iteration=self.steps_per_iteration,
+                num_minibatches=self.num_minibatches,
+                update_epochs=self.update_epochs,
+                learning_rate=self.learning_rate,
+                anneal_lr=self.anneal_lr,
+                clip_coef=self.clip_coef,
+                ent_coef=self.ent_coef,
+                vf_coef=self.vf_coef,
+                clip_vloss=self.clip_vloss,
+                max_grad_norm=self.max_grad_norm,
+                norm_adv=self.norm_adv,
+                target_kl=self.target_kl,
+                gae=self.gae,
+                gae_lambda=self.gae_lambda,
+            )
+            for i in range(self.pop_size)
+        ]
+
+    @override
+    def get_config(self) -> dict:
+        return {
+            "env_id": self.env_id,
+            "ref_point": self.ref_point,
+            "num_envs": self.num_envs,
+            "pop_size": self.pop_size,
+            "warmup_iterations": self.warmup_iterations,
+            "evolutionary_iterations": self.evolutionary_iterations,
+            "steps_per_iteration": self.steps_per_iteration,
+            "limit_env_steps": self.limit_env_steps,
+            "max_iterations": self.max_iterations,
+            "num_weight_candidates": self.num_weight_candidates,
+            "num_performance_buffer": self.num_performance_buffer,
+            "performance_buffer_size": self.performance_buffer_size,
+            "min_weight": self.min_weight,
+            "max_weight": self.max_weight,
+            "delta_weight": self.delta_weight,
+            "gamma": self.gamma,
+            "seed": self.seed,
+            "net_arch": self.net_arch,
+            "batch_size": self.batch_size,
+            "minibatch_size": self.minibatch_size,
+            "update_epochs": self.update_epochs,
+            "learning_rate": self.learning_rate,
+            "anneal_lr": self.anneal_lr,
+            "clip_coef": self.clip_coef,
+            "vf_coef": self.vf_coef,
+            "ent_coef": self.ent_coef,
+            "max_grad_norm": self.max_grad_norm,
+            "norm_adv": self.norm_adv,
+            "target_kl": self.target_kl,
+            "clip_vloss": self.clip_vloss,
+            "gae": self.gae,
+            "gae_lambda": self.gae_lambda,
+        }
+
+    def __train_all_agents(self):
+        for i, agent in enumerate(self.agents):
+            agent.train(self.start_time, self.iteration, self.max_iterations)
+
+    def __eval_all_agents(self, evaluations_before_train: List[np.ndarray], add_to_prediction: bool = True):
+        """Evaluates all agents and store their current performances on the buffer and pareto archive."""
+        for i, agent in enumerate(self.agents):
+            _, _, _, discounted_reward = agent.policy_eval(self.env.envs[0], weights=agent.weights, writer=self.writer)
+            # Storing current results
+            self.population.add(agent, discounted_reward)
+            self.archive.add(agent, discounted_reward)
+            if add_to_prediction:
+                self.predictor.add(
+                    agent.weights.detach().cpu().numpy(),
+                    evaluations_before_train[i],
+                    discounted_reward,
+                )
+            evaluations_before_train[i] = discounted_reward
+
+        if self.log:
+            print("Current pareto archive:")
+            print(self.archive.evaluations)
+            log_all_multi_policy_metrics(
+                current_front=self.archive.evaluations,
+                hv_ref_point=self.ref_point,
+                reward_dim=self.reward_dim,
+                global_step=self.global_step,
+                writer=self.writer,
+                ref_front=self.known_pareto_front,
+            )
+
+    def __task_weight_selection(self):
+        """Chooses agents and weights to train at the next iteration based on the current population and prediction model."""
+        candidate_weights = generate_weights(self.delta_weight / 2.0)  # Generates more weights than agents
+        np.random.shuffle(candidate_weights)  # Randomize
+
+        current_front = deepcopy(self.archive.evaluations)
+        population = self.population.individuals
+        population_eval = self.population.evaluations
+        selected_tasks = []
+        # For each worker, select a (policy, weight) tuple
+        for i in range(len(self.agents)):
+            max_improv = float("-inf")
+            best_candidate = None
+            best_eval = None
+            best_predicted_eval = None
+
+            # In each selection, look at every possible candidate in the current population and every possible weight generated
+            for candidate, last_candidate_eval in zip(population, population_eval):
+                # Pruning the already selected (candidate, weight) pairs
+                candidate_tuples = [
+                    (last_candidate_eval, weight)
+                    for weight in candidate_weights
+                    if (tuple(last_candidate_eval), tuple(weight)) not in selected_tasks
+                ]
+
+                # Prediction of improvements of each pair
+                delta_predictions, predicted_evals = map(
+                    list,
+                    zip(
+                        *[
+                            self.predictor.predict_next_evaluation(weight, candidate_eval)
+                            for candidate_eval, weight in candidate_tuples
+                        ]
+                    ),
+                )
+                # optimization criterion is a hypervolume - sparsity
+                mixture_metrics = [
+                    hypervolume(self.ref_point, current_front + [predicted_eval]) - sparsity(current_front + [predicted_eval])
+                    for predicted_eval in predicted_evals
+                ]
+                # Best among all the weights for the current candidate
+                current_candidate_weight = np.argmax(np.array(mixture_metrics))
+                current_candidate_improv = np.max(np.array(mixture_metrics))
+
+                # Best among all candidates, weight tuple update
+                if max_improv < current_candidate_improv:
+                    max_improv = current_candidate_improv
+                    best_candidate = (
+                        candidate,
+                        candidate_tuples[current_candidate_weight][1],
+                    )
+                    best_eval = last_candidate_eval
+                    best_predicted_eval = predicted_evals[current_candidate_weight]
+
+            selected_tasks.append((tuple(best_eval), tuple(best_candidate[1])))
+            # Append current estimate to the estimated front (to compute the next predictions)
+            current_front.append(best_predicted_eval)
+
+            # Assigns best predicted (weight-agent) pair to the worker
+            copied_agent = deepcopy(best_candidate[0])
+            copied_agent.global_step = self.agents[i].global_step
+            copied_agent.id = i
+            copied_agent.change_weights(deepcopy(best_candidate[1]))
+            self.agents[i] = copied_agent
+
+            print(f"Agent #{self.agents[i].id} - weights {best_candidate[1]}")
+            print(
+                f"current eval: {best_eval} - estimated next: {best_predicted_eval} - deltas {(best_predicted_eval - best_eval)}"
+            )
+
+    def train(self):
+        """Trains the agents."""
+        # Init
+        current_evaluations = [np.zeros(self.reward_dim) for _ in range(len(self.agents))]
+        self.__eval_all_agents(current_evaluations, add_to_prediction=False)
+        self.start_time = time.time()
+
+        # Warmup
+        for i in range(1, self.warmup_iterations + 1):
+            print(f"Warmup iteration #{self.iteration}")
+            if self.log:
+                self.writer.add_scalar("charts/warmup_iterations", i)
+            self.__train_all_agents()
+            self.iteration += 1
+        self.__eval_all_agents(current_evaluations)
+
+        # Evolution
+        max_iterations = max(self.max_iterations, self.warmup_iterations + self.evolutionary_iterations)
+        evolutionary_generation = 1
+        while self.iteration < max_iterations:
+            # Every evolutionary iterations, change the task - weight assignments
+            self.__task_weight_selection()
+            print(f"Evolutionary generation #{evolutionary_generation}")
+            if self.log:
+                self.writer.add_scalar("charts/evolutionary_generation", evolutionary_generation)
+
+            for _ in range(self.evolutionary_iterations):
+                # Run training of every agent for evolutionary iterations.
+                if self.log:
+                    print(f"Evolutionary iteration #{self.iteration - self.warmup_iterations}")
+                    self.writer.add_scalar(
+                        "charts/evolutionary_iterations",
+                        self.iteration - self.warmup_iterations,
+                    )
+                self.__train_all_agents()
+                self.iteration += 1
+            self.__eval_all_agents(current_evaluations)
+            evolutionary_generation += 1
+
+        print("Done training!")
+        self.env.close()
+        if self.log:
+            self.close_wandb()
diff --git a/morl-baselines/morl_baselines/single_policy/__init__.py b/morl-baselines/morl_baselines/single_policy/__init__.py
new file mode 100644
index 0000000..8b497d7
--- /dev/null
+++ b/morl-baselines/morl_baselines/single_policy/__init__.py
@@ -0,0 +1 @@
+"""This package contains all single-policy RL algorithms. They usually rely on scalarization to convert the MOMDP into an MDP."""
diff --git a/morl-baselines/morl_baselines/single_policy/esr/__init__.py b/morl-baselines/morl_baselines/single_policy/esr/__init__.py
new file mode 100644
index 0000000..3728c1c
--- /dev/null
+++ b/morl-baselines/morl_baselines/single_policy/esr/__init__.py
@@ -0,0 +1 @@
+"""ESR Algorithms."""
diff --git a/morl-baselines/morl_baselines/single_policy/esr/eupg.py b/morl-baselines/morl_baselines/single_policy/esr/eupg.py
new file mode 100644
index 0000000..2c95bed
--- /dev/null
+++ b/morl-baselines/morl_baselines/single_policy/esr/eupg.py
@@ -0,0 +1,253 @@
+"""EUPG is an ESR algorithm based on Policy Gradient (REINFORCE like)."""
+from typing import List, Optional, Union
+from typing_extensions import override
+
+import gymnasium as gym
+import numpy as np
+import torch as th
+import torch.nn as nn
+import torch.optim as optim
+from torch.distributions import Categorical
+
+from morl_baselines.common.accrued_reward_buffer import AccruedRewardReplayBuffer
+from morl_baselines.common.morl_algorithm import MOAgent, MOPolicy
+from morl_baselines.common.networks import mlp
+from morl_baselines.common.utils import layer_init, log_episode_info
+
+
+class PolicyNet(nn.Module):
+    """Policy network."""
+
+    def __init__(self, obs_shape, action_dim, rew_dim, net_arch):
+        """Initialize the policy network.
+
+        Args:
+            obs_shape: Observation shape
+            action_dim: Action dimension
+            rew_dim: Reward dimension
+            net_arch: Number of units per layer
+        """
+        super().__init__()
+        self.obs_shape = obs_shape
+        self.action_dim = action_dim
+        self.rew_dim = rew_dim
+
+        # Conditioned on accrued reward, so input takes reward
+        input_dim = obs_shape[0] + rew_dim
+
+        # |S|+|R| -> ... -> |A|
+        self.net = mlp(input_dim, action_dim, net_arch, activation_fn=nn.Tanh)
+        self.apply(layer_init)
+
+    def forward(self, obs: th.Tensor, acc_reward: th.Tensor):
+        """Forward pass.
+
+        Args:
+            obs: Observation
+            acc_reward: accrued reward
+
+        Returns: Probability of each action
+
+        """
+        input = th.cat((obs, acc_reward), dim=acc_reward.dim() - 1)
+        pi = self.net(input)
+        # Normalized sigmoid
+        x_exp = th.sigmoid(pi)
+        probas = x_exp / th.sum(x_exp)
+        return probas.view(-1, self.action_dim)  # Batch Size x |Actions|
+
+    def distribution(self, obs: th.Tensor, acc_reward: th.Tensor):
+        """Categorical distribution based on the action probabilities.
+
+        Args:
+            obs: observation
+            acc_reward: accrued reward
+
+        Returns: action distribution.
+
+        """
+        probas = self.forward(obs, acc_reward)
+        distribution = Categorical(probas)
+        return distribution
+
+
+class EUPG(MOPolicy, MOAgent):
+    """Expected Utility Policy Gradient Algorithm.
+
+    The idea is to condition the network on the accrued reward and to scalarize the rewards based on the episodic return (accrued + future rewards)
+    Paper: D. Roijers, D. Steckelmacher, and A. Nowe, Multi-objective Reinforcement Learning for the Expected Utility of the Return. 2018.
+    """
+
+    def __init__(
+        self,
+        env: gym.Env,
+        scalarization,
+        buffer_size: int = int(1e5),
+        net_arch: List = [50],
+        gamma: float = 0.99,
+        learning_rate: float = 1e-3,
+        project_name: str = "MORL-Baselines",
+        experiment_name: str = "EUPG",
+        log: bool = True,
+        device: Union[th.device, str] = "auto",
+    ):
+        """Initialize the EUPG algorithm.
+
+        Args:
+            env: Environment
+            scalarization: Scalarization function to use (can be non-linear)
+            buffer_size: Size of the replay buffer
+            net_arch: Number of units per layer
+            gamma: Discount factor
+            learning_rate: Learning rate (alpha)
+            project_name: Name of the project (for logging)
+            experiment_name: Name of the experiment (for logging)
+            log: Whether to log or not
+            device: Device to use for NN. Can be "cpu", "cuda" or "auto".
+        """
+        MOAgent.__init__(self, env, device)
+        MOPolicy.__init__(self, None, device)
+
+        self.env = env
+        # RL
+        self.scalarization = scalarization
+        self.gamma = gamma
+
+        # Learning
+        self.buffer_size = buffer_size
+        self.net_arch = net_arch
+        self.learning_rate = learning_rate
+        self.buffer = AccruedRewardReplayBuffer(
+            obs_shape=self.observation_shape,
+            action_shape=self.action_shape,
+            rew_dim=self.reward_dim,
+            max_size=self.buffer_size,
+            obs_dtype=np.int32,
+            action_dtype=np.int32,
+        )
+        self.net = PolicyNet(
+            obs_shape=self.observation_shape,
+            rew_dim=self.reward_dim,
+            action_dim=self.action_dim,
+            net_arch=self.net_arch,
+        )
+        self.optimizer = optim.Adam(self.net.parameters(), lr=self.learning_rate)
+
+        # Logging
+        self.project_name = project_name
+        self.experiment_name = experiment_name
+        self.log = log
+        if log:
+            self.setup_wandb(project_name, experiment_name)
+
+    @override
+    def eval(self, obs: np.ndarray, accrued_reward: Optional[np.ndarray]) -> Union[int, np.ndarray]:
+        if type(obs) is int:
+            obs = th.as_tensor([obs]).to(self.device)
+        else:
+            obs = th.as_tensor(obs).to(self.device)
+        accrued_reward = th.as_tensor(accrued_reward).float().to(self.device)
+        return self.__choose_action(obs, accrued_reward)
+
+    @th.no_grad()
+    def __choose_action(self, obs: th.Tensor, accrued_reward: th.Tensor) -> int:
+        action = self.net.distribution(obs, accrued_reward).sample().detach().item()
+        return action
+
+    @override
+    def update(self):
+        (
+            obs,
+            accrued_rewards,
+            actions,
+            rewards,
+            next_obs,
+            terminateds,
+        ) = self.buffer.get_all_data(to_tensor=True, device=self.device)
+        # Scalarized episodic reward, our target :-)
+        episodic_return = th.sum(rewards, dim=0)
+        scalarized_return = self.scalarization(episodic_return)
+
+        # For each sample in the batch, get the distribution over actions
+        current_distribution = self.net.distribution(obs, accrued_rewards)
+        # Policy gradient
+        log_probs = current_distribution.log_prob(actions)
+        loss = -th.mean(log_probs * scalarized_return)
+
+        self.optimizer.zero_grad()
+        loss.backward()
+        self.optimizer.step()
+
+        if self.log:
+            self.writer.add_scalar("losses/loss", loss, self.global_step)
+            self.writer.add_scalar(
+                "metrics/scalarized_episodic_return",
+                scalarized_return,
+                self.global_step,
+            )
+
+    def train(
+        self,
+        total_timesteps: int,
+        eval_env: Optional[gym.Env] = None,
+        eval_freq: int = 1000,
+    ):
+        """Train the agent.
+
+        Args:
+            total_timesteps: Number of timesteps to train for
+            eval_env: Environment to run policy evaluation on
+            eval_freq: Frequency of policy evaluation
+        """
+        # Init
+        (
+            obs,
+            _,
+        ) = self.env.reset()
+        accrued_reward_tensor = th.zeros(self.reward_dim, dtype=th.float32).float().to(self.device)
+
+        # Training loop
+        for _ in range(1, total_timesteps + 1):
+            self.global_step += 1
+
+            with th.no_grad():
+                # For training, takes action randomly according to the policy
+                action = self.__choose_action(th.Tensor([obs]).to(self.device), accrued_reward_tensor)
+            next_obs, vec_reward, terminated, truncated, info = self.env.step(action)
+
+            # Memory update
+            self.buffer.add(obs, accrued_reward_tensor, action, vec_reward, next_obs, terminated)
+            accrued_reward_tensor += th.from_numpy(vec_reward).to(self.device)
+
+            if eval_env is not None and self.log and self.global_step % eval_freq == 0:
+                self.policy_eval_esr(eval_env, scalarization=self.scalarization, writer=self.writer)
+
+            if terminated or truncated:
+                # NN is updated at the end of each episode
+                self.update()
+                self.buffer.cleanup()
+                obs, _ = self.env.reset()
+                self.num_episodes += 1
+                accrued_reward_tensor = th.zeros(self.reward_dim).float().to(self.device)
+
+                if self.log and "episode" in info.keys():
+                    log_episode_info(
+                        info["episode"],
+                        scalarization=self.scalarization,
+                        weights=None,
+                        global_timestep=self.global_step,
+                        writer=self.writer,
+                    )
+
+            else:
+                obs = next_obs
+
+    @override
+    def get_config(self) -> dict:
+        return {
+            "env_id": self.env.unwrapped.spec.id,
+            "learning_rate": self.learning_rate,
+            "buffer_size": self.buffer_size,
+            "gamma": self.gamma,
+            "net_arch": self.net_arch,
+        }
diff --git a/morl-baselines/morl_baselines/single_policy/ser/__init__.py b/morl-baselines/morl_baselines/single_policy/ser/__init__.py
new file mode 100644
index 0000000..f240b89
--- /dev/null
+++ b/morl-baselines/morl_baselines/single_policy/ser/__init__.py
@@ -0,0 +1 @@
+"""SER Algorithms."""
diff --git a/morl-baselines/morl_baselines/single_policy/ser/mo_ppo.py b/morl-baselines/morl_baselines/single_policy/ser/mo_ppo.py
new file mode 100644
index 0000000..d9ac7dd
--- /dev/null
+++ b/morl-baselines/morl_baselines/single_policy/ser/mo_ppo.py
@@ -0,0 +1,600 @@
+"""Multi-Objective PPO Algorithm."""
+import time
+from copy import deepcopy
+from typing import List, Optional, Union
+from typing_extensions import override
+
+import gymnasium as gym
+import mo_gymnasium as mo_gym
+import numpy as np
+import torch as th
+from mo_gymnasium import MORecordEpisodeStatistics
+from torch import nn, optim
+from torch.distributions import Normal
+from torch.utils.tensorboard import SummaryWriter
+
+from morl_baselines.common.morl_algorithm import MOPolicy
+from morl_baselines.common.networks import mlp
+from morl_baselines.common.utils import layer_init, log_episode_info
+
+
+class PPOReplayBuffer:
+    """Replay buffer."""
+
+    def __init__(
+        self,
+        size: int,
+        num_envs: int,
+        obs_shape: tuple,
+        action_shape: tuple,
+        reward_dim: int,
+        device: Union[th.device, str],
+    ):
+        """Initialize the replay buffer.
+
+        Args:
+            size: Buffer size
+            num_envs: Number of environments (for VecEnv)
+            obs_shape: Observation shape
+            action_shape: Action shape
+            reward_dim: Reward dimension
+            device: Device where the tensors are stored
+        """
+        self.size = size
+        self.ptr = 0
+        self.num_envs = num_envs
+        self.device = device
+        self.obs = th.zeros((self.size, self.num_envs) + obs_shape).to(device)
+        self.actions = th.zeros((self.size, self.num_envs) + action_shape).to(device)
+        self.logprobs = th.zeros((self.size, self.num_envs)).to(device)
+        self.rewards = th.zeros((self.size, self.num_envs, reward_dim), dtype=th.float32).to(device)
+        self.dones = th.zeros((self.size, self.num_envs)).to(device)
+        self.values = th.zeros((self.size, self.num_envs, reward_dim), dtype=th.float32).to(device)
+
+    def add(self, obs, actions, logprobs, rewards, dones, values):
+        """Add a bunch of new transition to the buffer. (VecEnv makes more transitions at once).
+
+        Args:
+            obs: Observations
+            actions: Actions
+            logprobs: Log probabilities of the actions
+            rewards: Rewards
+            dones: Done signals
+            values: Values
+        """
+        self.obs[self.ptr] = obs
+        self.actions[self.ptr] = actions
+        self.logprobs[self.ptr] = logprobs
+        self.rewards[self.ptr] = rewards
+        self.dones[self.ptr] = dones
+        self.values[self.ptr] = values
+        self.ptr = (self.ptr + 1) % self.size
+
+    def get(self, step: int):
+        """Get data from the buffer at a specific step.
+
+        Args:
+            step: step
+
+        Returns: A tuple of (obs, actions, logprobs, rewards, dones, values)
+
+        """
+        return (
+            self.obs[step],
+            self.actions[step],
+            self.logprobs[step],
+            self.rewards[step],
+            self.dones[step],
+            self.values[step],
+        )
+
+    def get_all(self):
+        """Get all data from the buffer.
+
+        Returns: A tuple of (obs, actions, logprobs, rewards, dones, values) containing all the data in the buffer.
+        """
+        return (
+            self.obs,
+            self.actions,
+            self.logprobs,
+            self.rewards,
+            self.dones,
+            self.values,
+        )
+
+
+def make_env(env_id, seed, idx, run_name, gamma):
+    """Returns a function to create environments. This is because PPO works better with vectorized environments. Also, some tricks like clipping and normalizing the environments' features are applied.
+
+    Args:
+        env_id: Environment ID (for MO-Gymnasium)
+        seed: Seed
+        idx: Index of the environment
+        run_name: Name of the run
+        gamma: Discount factor
+
+    Returns:
+        A function to create environments
+    """
+
+    def thunk():
+        env = mo_gym.make(env_id)
+        reward_dim = env.reward_space.shape[0]
+        if idx == 0:
+            env = gym.wrappers.RecordVideo(
+                env,
+                f"videos/{run_name}_{seed}",
+                episode_trigger=lambda e: e % 1000 == 0,
+            )
+        env = gym.wrappers.ClipAction(env)
+        env = gym.wrappers.NormalizeObservation(env)
+        env = gym.wrappers.TransformObservation(env, lambda obs: np.clip(obs, -10, 10))
+        for o in range(reward_dim):
+            env = mo_gym.utils.MONormalizeReward(env, idx=o, gamma=gamma)
+            env = mo_gym.utils.MOClipReward(env, idx=o, min_r=-10, max_r=10)
+        env = MORecordEpisodeStatistics(env, gamma=gamma)
+        env.reset(seed=seed)
+        env.action_space.seed(seed)
+        env.observation_space.seed(seed)
+        return env
+
+    return thunk
+
+
+def _hidden_layer_init(layer):
+    layer_init(layer, weight_gain=np.sqrt(2), bias_const=0.0)
+
+
+def _critic_init(layer):
+    layer_init(layer, weight_gain=1.0)
+
+
+def _value_init(layer):
+    layer_init(layer, weight_gain=0.01)
+
+
+class MOPPONet(nn.Module):
+    """Actor-Critic network."""
+
+    def __init__(
+        self,
+        obs_shape: tuple,
+        action_shape: tuple,
+        reward_dim: int,
+        net_arch: List = [64, 64],
+    ):
+        """Initialize the network.
+
+        Args:
+            obs_shape: Observation shape
+            action_shape: Action shape
+            reward_dim: Reward dimension
+            net_arch: Number of units per layer
+        """
+        super().__init__()
+        self.obs_shape = obs_shape
+        self.action_shape = action_shape
+        self.reward_dim = reward_dim
+        self.net_arch = net_arch
+
+        # S -> ... -> |R| (multi-objective)
+        self.critic = mlp(
+            input_dim=np.array(self.obs_shape).prod(),
+            output_dim=self.reward_dim,
+            net_arch=net_arch,
+            activation_fn=nn.Tanh,
+        )
+        self.critic.apply(_hidden_layer_init)
+        _critic_init(list(self.critic.modules())[-1])
+
+        # S -> ... -> A (continuous)
+        self.actor_mean = mlp(
+            input_dim=np.array(self.obs_shape).prod(),
+            output_dim=np.array(self.action_shape).prod(),
+            net_arch=net_arch,
+            activation_fn=nn.Tanh,
+        )
+        self.actor_mean.apply(_hidden_layer_init)
+        _value_init(list(self.actor_mean.modules())[-1])
+        self.actor_logstd = nn.Parameter(th.zeros(1, np.array(self.action_shape).prod()))
+
+    def get_value(self, obs):
+        """Get the value of an observation.
+
+        Args:
+            obs: Observation
+
+        Returns: The predicted value of the observation.
+        """
+        return self.critic(obs)
+
+    def get_action_and_value(self, obs, action=None):
+        """Get the action and value of an observation.
+
+        Args:
+            obs: Observation
+            action: Action. If None, a new action is sampled.
+
+        Returns: A tuple of (action, logprob, entropy, value)
+        """
+        action_mean = self.actor_mean(obs)
+        action_logstd = self.actor_logstd.expand_as(action_mean)
+        action_std = th.exp(action_logstd)
+        probs = Normal(action_mean, action_std)
+        if action is None:
+            action = probs.sample()
+        return (
+            action,
+            probs.log_prob(action).sum(1),
+            probs.entropy().sum(1),
+            self.critic(obs),
+        )
+
+
+class MOPPO(MOPolicy):
+    """Modified PPO to have a multi-objective value net (returning a vector) and applying weighted sum scalarization.
+
+    This code has been adapted from the PPO implementation of clean RL https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/ppo_continuous_action.py
+    """
+
+    def __init__(
+        self,
+        id: int,
+        networks: MOPPONet,
+        weights: np.ndarray,
+        envs: gym.vector.SyncVectorEnv,
+        writer: Optional[SummaryWriter],
+        steps_per_iteration: int = 2048,
+        num_minibatches: int = 32,
+        update_epochs: int = 10,
+        learning_rate: float = 3e-4,
+        gamma: float = 0.995,
+        anneal_lr: bool = False,
+        clip_coef: float = 0.2,
+        ent_coef: float = 0.0,
+        vf_coef: float = 0.5,
+        clip_vloss: bool = True,
+        max_grad_norm: float = 0.5,
+        norm_adv: bool = True,
+        target_kl: Optional[float] = None,
+        gae: bool = True,
+        gae_lambda: float = 0.95,
+        device: Union[th.device, str] = "auto",
+        seed: int = 42,
+    ):
+        """Multi-objective PPO.
+
+        Args:
+            id: Policy ID
+            networks: Actor-Critic networks
+            weights: Weights of the objectives
+            envs: Vectorized environments
+            writer: Tensorboard writer for logging
+            steps_per_iteration: Number of steps per iteration
+            num_minibatches: Number of minibatches
+            update_epochs: Number of epochs to update the network
+            learning_rate: Learning rate
+            gamma: Discount factor
+            anneal_lr: Whether to anneal the learning rate
+            clip_coef: PPO clipping coefficient
+            ent_coef: Entropy coefficient
+            vf_coef: Value function coefficient
+            clip_vloss: Whether to clip the value loss
+            max_grad_norm: Maximum gradient norm
+            norm_adv: Whether to normalize the advantage
+            target_kl: Target KL divergence
+            gae: Whether to use Generalized Advantage Estimation
+            gae_lambda: GAE lambda
+            device: Device to use
+            seed: Random seed
+        """
+        super().__init__(id, device)
+        self.id = id
+        self.envs = envs
+        self.num_envs = envs.num_envs
+        self.networks = networks
+        self.device = device
+        self.seed = seed
+
+        # PPO Parameters
+        self.steps_per_iteration = steps_per_iteration
+        self.weights = th.from_numpy(weights).to(self.device)
+        self.batch_size = int(self.num_envs * self.steps_per_iteration)
+        self.num_minibatches = num_minibatches
+        self.minibatch_size = int(self.batch_size // num_minibatches)
+        self.update_epochs = update_epochs
+        self.learning_rate = learning_rate
+        self.gamma = gamma
+        self.anneal_lr = anneal_lr
+        self.clip_coef = clip_coef
+        self.vf_coef = vf_coef
+        self.ent_coef = ent_coef
+        self.max_grad_norm = max_grad_norm
+        self.norm_adv = norm_adv
+        self.target_kl = target_kl
+        self.clip_vloss = clip_vloss
+        self.gae_lambda = gae_lambda
+        self.writer = writer
+        self.gae = gae
+
+        self.optimizer = optim.Adam(networks.parameters(), lr=self.learning_rate, eps=1e-5)
+
+        # Storage setup (the batch)
+        self.batch = PPOReplayBuffer(
+            self.steps_per_iteration,
+            self.num_envs,
+            self.networks.obs_shape,
+            self.networks.action_shape,
+            self.networks.reward_dim,
+            self.device,
+        )
+
+    def __deepcopy__(self, memo):
+        """Deepcopy method.
+
+        Useful for genetic algorithms stuffs.
+        """
+        copied_net = deepcopy(self.networks)
+        copied = type(self)(
+            self.id,
+            copied_net,
+            self.weights.detach().cpu().numpy(),
+            self.envs,
+            self.writer,
+            self.steps_per_iteration,
+            self.num_minibatches,
+            self.update_epochs,
+            self.learning_rate,
+            self.gamma,
+            self.anneal_lr,
+            self.clip_coef,
+            self.ent_coef,
+            self.vf_coef,
+            self.clip_vloss,
+            self.max_grad_norm,
+            self.norm_adv,
+            self.target_kl,
+            self.gae,
+            self.gae_lambda,
+            self.device,
+        )
+
+        copied.global_step = self.global_step
+        copied.optimizer = optim.Adam(copied_net.parameters(), lr=self.learning_rate, eps=1e-5)
+        copied.batch = deepcopy(self.batch)
+        return copied
+
+    def change_weights(self, new_weights: np.ndarray):
+        """Change the weights of the scalarization function.
+
+        Args:
+            new_weights: New weights to apply.
+        """
+        self.weights = th.from_numpy(deepcopy(new_weights)).to(self.device)
+
+    def __extend_to_reward_dim(self, tensor: th.Tensor):
+        # This allows to broadcast the tensor to match the additional dimension of rewards
+        return tensor.unsqueeze(1).repeat(1, self.networks.reward_dim)
+
+    def __collect_samples(self, obs: th.Tensor, done: th.Tensor):
+        """Fills the batch with {self.steps_per_iteration} samples collected from the environments.
+
+        Args:
+            obs: current observations
+            done: current dones
+
+        Returns:
+            next observation and dones
+        """
+        for step in range(0, self.steps_per_iteration):
+            self.global_step += 1 * self.num_envs
+            # Compute best action
+            with th.no_grad():
+                action, logprob, _, value = self.networks.get_action_and_value(obs)
+                value = value.view(self.num_envs, self.networks.reward_dim)
+
+            # Perform action on the environment
+            next_obs, reward, next_terminated, _, info = self.envs.step(action.cpu().numpy())
+            reward = th.tensor(reward).to(self.device).view(self.num_envs, self.networks.reward_dim)
+            # storing to batch
+            self.batch.add(obs, action, logprob, reward, done, value)
+
+            # Next iteration
+            obs, done = th.Tensor(next_obs).to(self.device), th.Tensor(next_terminated).to(self.device)
+
+            # Episode info logging
+            if "episode" in info.keys():
+                for item in info["episode"]:
+                    log_episode_info(
+                        item,
+                        np.dot,
+                        self.weights,
+                        self.global_step,
+                        self.id,
+                        self.writer,
+                    )
+                    break
+
+        return obs, done
+
+    def __compute_advantages(self, next_obs, next_done):
+        """Computes the advantages by replaying experiences from the buffer in reverse.
+
+        Returns:
+            MO returns, scalarized advantages
+        """
+        with th.no_grad():
+            next_value = self.networks.get_value(next_obs).reshape(self.num_envs, -1)
+            if self.gae:
+                advantages = th.zeros_like(self.batch.rewards).to(self.device)
+                lastgaelam = 0
+                for t in reversed(range(self.steps_per_iteration)):
+                    if t == self.steps_per_iteration - 1:
+                        nextnonterminal = 1.0 - next_done
+                        nextvalues = next_value
+                    else:
+                        _, _, _, _, done_t1, value_t1 = self.batch.get(t + 1)
+                        nextnonterminal = 1.0 - done_t1
+                        nextvalues = value_t1
+
+                    nextnonterminal = self.__extend_to_reward_dim(nextnonterminal)
+                    _, _, _, reward_t, _, value_t = self.batch.get(t)
+                    delta = reward_t + self.gamma * nextvalues * nextnonterminal - value_t
+                    advantages[t] = lastgaelam = delta + self.gamma * self.gae_lambda * nextnonterminal * lastgaelam
+                returns = advantages + self.batch.values
+            else:
+                returns = th.zeros_like(self.batch.rewards).to(self.device)
+                for t in reversed(range(self.steps_per_iteration)):
+                    if t == self.steps_per_iteration - 1:
+                        nextnonterminal = 1.0 - next_done
+                        next_return = next_value
+                    else:
+                        _, _, _, _, done_t1, _ = self.batch.get(t + 1)
+                        nextnonterminal = 1.0 - done_t1
+                        next_return = returns[t + 1]
+
+                    nextnonterminal = self.__extend_to_reward_dim(nextnonterminal)
+                    _, _, _, reward_t, _, _ = self.batch.get(t)
+                    returns[t] = reward_t + self.gamma * nextnonterminal * next_return
+                advantages = returns - self.batch.values
+
+        # Scalarization of the advantages (weighted sum)
+        advantages = advantages @ self.weights
+        return returns, advantages
+
+    @override
+    def eval(self, obs: np.ndarray, w):
+        """Returns the best action to perform for the given obs
+
+        Returns:
+            action as a numpy array (continuous actions)
+        """
+        obs = th.as_tensor(obs).float().to(self.device)
+        obs = obs.unsqueeze(0).repeat(self.num_envs, 1)  # duplicate observation to fit the NN input
+        with th.no_grad():
+            action, _, _, _ = self.networks.get_action_and_value(obs)
+
+        return action[0].detach().cpu().numpy()
+
+    @override
+    def update(self):
+        # flatten the batch (b == batch)
+        obs, actions, logprobs, _, _, values = self.batch.get_all()
+        b_obs = obs.reshape((-1,) + self.networks.obs_shape)
+        b_logprobs = logprobs.reshape(-1)
+        b_actions = actions.reshape((-1,) + self.networks.action_shape)
+        b_advantages = self.advantages.reshape(-1)
+        b_returns = self.returns.reshape(-1, self.networks.reward_dim)
+        b_values = values.reshape(-1, self.networks.reward_dim)
+
+        # Optimizing the policy and value network
+        b_inds = np.arange(self.batch_size)
+        clipfracs = []
+        # Perform multiple passes on the batch (that is shuffled every time)
+        for epoch in range(self.update_epochs):
+            np.random.shuffle(b_inds)
+            for start in range(0, self.batch_size, self.minibatch_size):
+                end = start + self.minibatch_size
+                # mb == minibatch
+                mb_inds = b_inds[start:end]
+
+                _, newlogprob, entropy, newvalue = self.networks.get_action_and_value(b_obs[mb_inds], b_actions[mb_inds])
+                logratio = newlogprob - b_logprobs[mb_inds]
+                ratio = logratio.exp()
+
+                with th.no_grad():
+                    # calculate approx_kl http://joschu.net/blog/kl-approx.html
+                    old_approx_kl = (-logratio).mean()
+                    approx_kl = ((ratio - 1) - logratio).mean()
+                    clipfracs += [((ratio - 1.0).abs() > self.clip_coef).float().mean().item()]
+
+                mb_advantages = b_advantages[mb_inds]
+                if self.norm_adv:
+                    mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)
+
+                # Policy loss
+                pg_loss1 = -mb_advantages * ratio
+                pg_loss2 = -mb_advantages * th.clamp(ratio, 1 - self.clip_coef, 1 + self.clip_coef)
+                pg_loss = th.max(pg_loss1, pg_loss2).mean()
+
+                # Value loss
+                newvalue = newvalue.view(-1, self.networks.reward_dim)
+                if self.clip_vloss:
+                    v_loss_unclipped = (newvalue - b_returns[mb_inds]) ** 2
+                    v_clipped = b_values[mb_inds] + th.clamp(
+                        newvalue - b_values[mb_inds],
+                        -self.clip_coef,
+                        self.clip_coef,
+                    )
+                    v_loss_clipped = (v_clipped - b_returns[mb_inds]) ** 2
+                    v_loss_max = th.max(v_loss_unclipped, v_loss_clipped)
+                    v_loss = 0.5 * v_loss_max.mean()
+                else:
+                    v_loss = 0.5 * ((newvalue - b_returns[mb_inds]) ** 2).mean()
+
+                entropy_loss = entropy.mean()
+                loss = pg_loss - self.ent_coef * entropy_loss + v_loss * self.vf_coef
+
+                self.optimizer.zero_grad()
+                loss.backward()
+                nn.utils.clip_grad_norm_(self.networks.parameters(), self.max_grad_norm)
+                self.optimizer.step()
+
+            if self.target_kl is not None:
+                if approx_kl > self.target_kl:
+                    break
+
+        y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()
+        var_y = np.var(y_true)
+        explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y
+
+        # record rewards for plotting purposes
+        if self.writer is not None:
+            self.writer.add_scalar(
+                f"charts_{self.id}/learning_rate",
+                self.optimizer.param_groups[0]["lr"],
+                self.global_step,
+            )
+            self.writer.add_scalar(f"losses_{self.id}/value_loss", v_loss.item(), self.global_step)
+            self.writer.add_scalar(f"losses_{self.id}/policy_loss", pg_loss.item(), self.global_step)
+            self.writer.add_scalar(f"losses_{self.id}/entropy", entropy_loss.item(), self.global_step)
+            self.writer.add_scalar(f"losses_{self.id}/old_approx_kl", old_approx_kl.item(), self.global_step)
+            self.writer.add_scalar(f"losses_{self.id}/approx_kl", approx_kl.item(), self.global_step)
+            self.writer.add_scalar(f"losses_{self.id}/clipfrac", np.mean(clipfracs), self.global_step)
+            self.writer.add_scalar(f"losses_{self.id}/explained_variance", explained_var, self.global_step)
+
+    def train(self, start_time, current_iteration: int, max_iterations: int):
+        """A training iteration: trains MOPPO for self.steps_per_iteration * self.num_envs.
+
+        Args:
+            start_time: time.time() when the training started
+            current_iteration: current iteration number
+            max_iterations: maximum number of iterations
+        """
+        next_obs, _ = self.envs.reset(seed=self.seed)
+        next_obs = th.Tensor(next_obs).to(self.device)  # num_envs x obs
+        next_done = th.zeros(self.num_envs).to(self.device)
+
+        # Annealing the rate if instructed to do so.
+        if self.anneal_lr:
+            frac = 1.0 - (current_iteration - 1.0) / max_iterations
+            lrnow = frac * self.learning_rate
+            self.optimizer.param_groups[0]["lr"] = lrnow
+
+        # Fills buffer
+        next_obs, next_done = self.__collect_samples(next_obs, next_done)
+
+        # Compute advantage on collected samples
+        self.returns, self.advantages = self.__compute_advantages(next_obs, next_done)
+
+        # Update neural networks from batch
+        self.update()
+
+        # Logging
+        print("SPS:", int(self.global_step / (time.time() - start_time)))
+        if self.writer is not None:
+            self.writer.add_scalar(
+                "charts/SPS",
+                int(self.global_step / (time.time() - start_time)),
+                self.global_step,
+            )
diff --git a/morl-baselines/morl_baselines/single_policy/ser/mo_q_learning.py b/morl-baselines/morl_baselines/single_policy/ser/mo_q_learning.py
new file mode 100644
index 0000000..9841cd1
--- /dev/null
+++ b/morl-baselines/morl_baselines/single_policy/ser/mo_q_learning.py
@@ -0,0 +1,241 @@
+"""Scalarized Q-learning for single policy multi-objective reinforcement learning."""
+import time
+from typing import Optional
+from typing_extensions import override
+
+import gymnasium as gym
+import numpy as np
+from torch.utils.tensorboard import SummaryWriter
+
+from morl_baselines.common.model_based.tabular_model import TabularModel
+from morl_baselines.common.morl_algorithm import MOAgent, MOPolicy
+from morl_baselines.common.scalarization import weighted_sum
+from morl_baselines.common.utils import linearly_decaying_value, log_episode_info
+
+
+class MOQLearning(MOPolicy, MOAgent):
+    """Scalarized Q learning for single policy multi-objective reinforcement learning.
+
+    Maintains one Q-table per objective, rely on a scalarization function to choose the moves.
+    Paper: K. Van Moffaert, M. Drugan, and A. Nowe, Scalarized Multi-Objective Reinforcement Learning: Novel Design Techniques. 2013. doi: 10.1109/ADPRL.2013.6615007.
+    """
+
+    def __init__(
+        self,
+        env,
+        id: Optional[int] = None,
+        weights: np.ndarray = np.array([0.5, 0.5]),
+        scalarization=weighted_sum,
+        learning_rate: float = 0.1,
+        gamma: float = 0.9,
+        initial_epsilon: float = 0.1,
+        final_epsilon: float = 0.1,
+        epsilon_decay_steps: int = None,
+        learning_starts: int = 0,
+        dyna: bool = False,
+        dyna_updates: int = 5,
+        project_name: str = "MORL-baselines",
+        experiment_name: str = "MO Q-Learning",
+        log: bool = True,
+        parent_writer: Optional[SummaryWriter] = None,
+    ):
+        """Initializes the MOQ-learning algorithm.
+
+        Args:
+            env: The environment to train on
+            id: The id of the policy
+            weights: The weights to use for the scalarization function
+            scalarization: The scalarization function to use
+            learning_rate: The learning rate
+            gamma: The discount factor
+            initial_epsilon: The initial epsilon value
+            final_epsilon: The final epsilon value
+            epsilon_decay_steps: The number of steps to decay epsilon over
+            learning_starts: The number of steps to wait before starting to learn
+            dyna: Whether to use Dyna-Q or not
+            dyna_updates: The number of Dyna-Q updates to perform each step
+            project_name: The name of the project used for logging
+            experiment_name: The name of the experiment used for logging
+            log: Whether to log or not
+            parent_writer: The writer to use for logging. If None, a new writer is created.
+        """
+        MOAgent.__init__(self, env)
+        MOPolicy.__init__(self, id)
+        self.learning_rate = learning_rate
+        self.id = id
+        if self.id is not None:
+            self.idstr = f"_{self.id}"
+        else:
+            self.idstr = ""
+        self.gamma = gamma
+        self.initial_epsilon = initial_epsilon
+        self.epsilon = initial_epsilon
+        self.final_epsilon = final_epsilon
+        self.epsilon_decay_steps = epsilon_decay_steps
+        self.learning_starts = learning_starts
+        self.dyna = dyna
+        self.dyna_updates = dyna_updates
+
+        self.weights = weights
+        self.scalarization = scalarization
+
+        self.q_table = dict()
+
+        if self.dyna:
+            self.model = TabularModel()
+
+        self.log = log
+        if parent_writer is not None:
+            self.writer = parent_writer
+        if self.log and parent_writer is None:
+            self.setup_wandb(project_name, experiment_name)
+
+    def __act(self, obs: np.array):
+        # epsilon-greedy
+        coin = np.random.rand()
+        if coin < self.epsilon:
+            return int(self.env.action_space.sample())
+        else:
+            return self.eval(obs)
+
+    def scalarized_q_values(self, obs, w: np.ndarray) -> np.ndarray:
+        """Returns the scalarized Q values for each action, given observation and weights."""
+        t_obs = tuple(obs)
+        if t_obs not in self.q_table:
+            return np.zeros(self.action_dim)
+        return np.array([self.scalarization(state_action_value, w) for state_action_value in self.q_table[t_obs]])
+
+    @override
+    def eval(self, obs: np.array, w: Optional[np.ndarray] = None) -> int:
+        """Greedily chooses best action using the scalarization method"""
+        t_obs = tuple(obs)
+        if t_obs not in self.q_table:
+            return int(self.env.action_space.sample())
+        scalarized = np.array(
+            [self.scalarization(state_action_value, self.weights) for state_action_value in self.q_table[t_obs]]
+        )
+        return int(np.argmax(scalarized))
+
+    @override
+    def update(self):
+        """Updates the Q table."""
+        obs = tuple(self.obs)
+        next_obs = tuple(self.next_obs)
+        if obs not in self.q_table:
+            self.q_table[obs] = np.zeros((self.action_dim, self.reward_dim))
+        if next_obs not in self.q_table:
+            self.q_table[next_obs] = np.zeros((self.action_dim, self.reward_dim))
+
+        max_q = self.q_table[next_obs][self.eval(self.next_obs)]
+        td_error = self.reward + (1 - self.terminated) * self.gamma * max_q - self.q_table[obs][self.action]
+        self.q_table[obs][self.action] += self.learning_rate * td_error
+
+        # Dyna updates
+        if self.dyna:
+            self.model.update(obs, self.action, self.reward, next_obs, self.terminated)
+            for _ in range(self.dyna_updates):
+                s, a, r, next_s, terminal = self.model.random_transition()
+                if s not in self.q_table:
+                    self.q_table[s] = np.zeros((self.action_dim, self.reward_dim))
+                if next_s not in self.q_table:
+                    self.q_table[next_s] = np.zeros((self.action_dim, self.reward_dim))
+                max_q = self.q_table[next_s][self.eval(next_s)]
+                model_td = r + (1 - terminal) * self.gamma * max_q - self.q_table[s][a]
+                self.q_table[s][a] += self.learning_rate * model_td
+
+        if self.epsilon_decay_steps is not None:
+            self.epsilon = linearly_decaying_value(
+                self.initial_epsilon,
+                self.epsilon_decay_steps,
+                self.global_step,
+                self.learning_starts,
+                self.final_epsilon,
+            )
+
+        if self.log and self.global_step % 1000 == 0:
+            self.writer.add_scalar(f"charts{self.idstr}/epsilon", self.epsilon, self.global_step)
+            self.writer.add_scalar(
+                f"losses{self.idstr}/scalarized_td_error",
+                self.scalarization(td_error, self.weights),
+                self.global_step,
+            )
+            self.writer.add_scalar(f"losses{self.idstr}/mean_td_error", np.mean(td_error), self.global_step)
+
+    @override
+    def get_config(self) -> dict:
+        return {
+            "env_id": self.env.unwrapped.spec.id,
+            "alpha": self.learning_rate,
+            "gamma": self.gamma,
+            "initial_epsilon": self.initial_epsilon,
+            "final_epsilon": self.final_epsilon,
+            "epsilon_decay_steps": self.epsilon_decay_steps,
+            "dyna": self.dyna,
+            "dyna_updates": self.dyna_updates,
+            "weight": self.weights,
+            "scalarization": self.scalarization.__name__,
+        }
+
+    def train(
+        self,
+        start_time,
+        total_timesteps: int = int(5e5),
+        reset_num_timesteps: bool = True,
+        eval_env: gym.Env = None,
+        eval_freq: int = 1000,
+    ):
+        """Learning for the agent.
+
+        Args:
+            start_time: time when the training started
+            total_timesteps: max number of timesteps to learn
+            reset_num_timesteps: whether to reset timesteps or not when recalling learn
+            eval_env: other environment to launch greedy evaluations
+            eval_freq: number of timesteps between each policy evaluation
+        """
+        num_episodes = 0
+        self.obs, _ = self.env.reset()
+
+        self.global_step = 0 if reset_num_timesteps else self.global_step
+        self.num_episodes = 0 if reset_num_timesteps else self.num_episodes
+
+        for _ in range(1, total_timesteps + 1):
+            self.global_step += 1
+
+            self.action = self.__act(self.obs)
+            (
+                self.next_obs,
+                self.reward,
+                self.terminated,
+                self.truncated,
+                info,
+            ) = self.env.step(self.action)
+
+            self.update()
+
+            if eval_env is not None and self.log and self.global_step % eval_freq == 0:
+                self.policy_eval(eval_env, scalarization=self.scalarization, weights=self.weights, writer=self.writer)
+
+            if self.terminated or self.truncated:
+                self.obs, _ = self.env.reset()
+                num_episodes += 1
+                self.num_episodes += 1
+
+                if self.log and self.global_step % 1000 == 0:
+                    self.writer.add_scalar(
+                        f"charts{self.idstr}/SPS",
+                        int(self.global_step / (time.time() - start_time)),
+                        self.global_step,
+                    )
+                    if "episode" in info:
+                        log_episode_info(
+                            info["episode"],
+                            self.scalarization,
+                            self.weights,
+                            self.global_step,
+                            self.id,
+                            self.writer,
+                            verbose=False,
+                        )
+            else:
+                self.obs = self.next_obs
diff --git a/morl-baselines/pyproject.toml b/morl-baselines/pyproject.toml
new file mode 100644
index 0000000..543115b
--- /dev/null
+++ b/morl-baselines/pyproject.toml
@@ -0,0 +1,81 @@
+[build-system]
+requires = ["setuptools >= 61.0.0"]
+build-backend = "setuptools.build_meta"
+
+[project]
+name = "morl-baselines"
+description="Implementations of multi-objective reinforcement learning (MORL) algorithms."
+readme = "README.md"
+requires-python = ">= 3.7"
+authors = [{ name = "Florian Felten & Lucas Alegre", email = "lucasnale@gmail.com" }]
+license = { text = "MIT License" }
+keywords = ["Reinforcement Learning", "Multi-Objective", "RL", "AI", "gymnasium"]
+classifiers = [
+    "Development Status :: 4 - Beta",  # change to `5 - Production/Stable` when ready
+    "License :: OSI Approved :: MIT License",
+    "Programming Language :: Python :: 3",
+    "Programming Language :: Python :: 3.7",
+    "Programming Language :: Python :: 3.8",
+    "Programming Language :: Python :: 3.9",
+    "Programming Language :: Python :: 3.10",
+    'Intended Audience :: Science/Research',
+    'Topic :: Scientific/Engineering :: Artificial Intelligence',
+]
+dependencies = [
+    "mo-gymnasium >=0.3.4",
+    "gymnasium >=0.27",
+    "numpy >=1.21.0",
+    "torch >=1.11.0",
+    "pygame >=2.1.0",
+    "scipy >=1.7.3",
+    "pymoo >=0.6.0",
+    "wandb",
+    "seaborn",
+    "tensorboard",
+    "cvxpy",
+    "fire",
+]
+dynamic = ["version"]
+
+[project.optional-dependencies]
+# Update dependencies in `all` if any are added or removed
+# OLS requires pycddlib and libgmp to be installed, which does not work on MacOS for now.
+ols = ["pycddlib"]
+gpi = ["pycddlib"]
+
+all = [
+    # OLS & GPI
+    "pycddlib",
+]
+testing = ["pytest ==7.1.3"]
+
+[project.urls]
+Homepage = "https://lucasalegre.github.io/morl-baselines/"
+Repository = "https://github.com/LucasAlegre/morl-baselines"
+Documentation = "https://lucasalegre.github.io/morl-baselines/"
+"Bug Report" = "https://github.com/LucasAlegre/morl-baselines/issues"
+
+[tool.setuptools]
+include-package-data = true
+
+[tool.setuptools.packages.find]
+include = ["morl_baselines", "morl_baselines.*", "examples", "examples.*"]
+
+[tool.setuptools.package-data]
+morl_baselines = ["*.json", "assets/*"]
+
+# Linting, testing, ... ########################################################
+[tool.black]
+safe = true
+line-length = 127
+target-version = ['py37', 'py38', 'py39', 'py310']
+include = '\.pyi?$'
+
+[tool.isort]
+atomic = true
+profile = "black"
+src_paths = ["morl_baselines"]
+extra_standard_library = ["typing_extensions"]
+indent = 4
+lines_after_imports = 2
+multi_line_output = 3
diff --git a/morl-baselines/setup.py b/morl-baselines/setup.py
new file mode 100644
index 0000000..00ac7f2
--- /dev/null
+++ b/morl-baselines/setup.py
@@ -0,0 +1,8 @@
+from setuptools import setup
+
+
+setup(
+    name="morl-baselines",
+    description="Implementations of multi-objective reinforcement learning (MORL) algorithms.",
+    version="0.1",
+)
diff --git a/morl-baselines/tests/test_algos.py b/morl-baselines/tests/test_algos.py
new file mode 100644
index 0000000..f6edc01
--- /dev/null
+++ b/morl-baselines/tests/test_algos.py
@@ -0,0 +1,269 @@
+"""Mostly tests to make sure the algorithms are able to run."""
+import time
+
+import mo_gymnasium as mo_gym
+import numpy as np
+from mo_gymnasium.envs.deep_sea_treasure.deep_sea_treasure import CONCAVE_MAP
+
+from morl_baselines.common.evaluation import eval_mo, eval_mo_reward_conditioned
+from morl_baselines.common.scalarization import tchebicheff
+from morl_baselines.multi_policy.envelope.envelope import Envelope
+from morl_baselines.multi_policy.gpi_pd.gpi_pd import GPIPD
+from morl_baselines.multi_policy.gpi_pd.gpi_pd_continuous_action import (
+    GPIPDContinuousAction,
+)
+from morl_baselines.multi_policy.linear_support.linear_support import LinearSupport
+from morl_baselines.multi_policy.multi_policy_moqlearning.mp_mo_q_learning import (
+    MPMOQLearning,
+)
+from morl_baselines.multi_policy.pareto_q_learning.pql import PQL
+from morl_baselines.multi_policy.pcn.pcn import PCN
+from morl_baselines.multi_policy.pgmorl.pgmorl import PGMORL
+from morl_baselines.single_policy.esr.eupg import EUPG
+from morl_baselines.single_policy.ser.mo_ppo import make_env
+from morl_baselines.single_policy.ser.mo_q_learning import MOQLearning
+
+
+def test_pql():
+    env_id = "deep-sea-treasure-v0"
+    env = mo_gym.make(env_id, dst_map=CONCAVE_MAP)
+    ref_point = np.array([0, -25])
+
+    agent = PQL(
+        env,
+        ref_point,
+        gamma=1.0,
+        initial_epsilon=1.0,
+        epsilon_decay=0.997,
+        final_epsilon=0.2,
+        seed=1,
+        log=False,
+    )
+
+    # Training
+    pf = agent.train(num_episodes=1000, log_every=100, action_eval="hypervolume", eval_ref_point=ref_point)
+    assert len(pf) > 0
+
+    # Policy following
+    target = np.array(pf.pop())
+    tracked = agent.track_policy(target)
+    assert np.all(tracked == target)
+
+
+def test_eupg():
+    env = mo_gym.make("fishwood-v0")
+    eval_env = mo_gym.make("fishwood-v0")
+
+    def scalarization(reward: np.ndarray):
+        return min(reward[0], reward[1] // 2)
+
+    agent = EUPG(env, scalarization=scalarization, gamma=0.99, log=False)
+    agent.train(total_timesteps=10000, eval_env=eval_env, eval_freq=100)
+
+    scalar_return, scalarized_disc_return, vec_ret, vec_disc_ret = eval_mo_reward_conditioned(
+        agent, env=eval_env, scalarization=scalarization
+    )
+    assert scalar_return > scalarized_disc_return
+    assert scalarized_disc_return > 0
+    assert vec_ret[0] > vec_disc_ret[0]
+    assert vec_ret[1] > vec_disc_ret[1]
+
+
+def test_moql():
+    env_id = "deep-sea-treasure-v0"
+    env = mo_gym.make(env_id, dst_map=CONCAVE_MAP)
+    eval_env = mo_gym.make(env_id, dst_map=CONCAVE_MAP)
+    scalarization = tchebicheff(tau=4.0, reward_dim=2)
+    weights = np.array([0.3, 0.7])
+
+    agent = MOQLearning(env, scalarization=scalarization, weights=weights, log=False)
+    agent.train(
+        total_timesteps=1000,
+        start_time=time.time(),
+        eval_freq=100,
+        eval_env=eval_env,
+    )
+
+    scalar_return, scalarized_disc_return, vec_ret, vec_disc_ret = eval_mo(agent, env=eval_env, w=weights)
+    assert scalar_return != 0
+    assert scalarized_disc_return != 0
+    assert len(vec_ret) == 2
+    assert len(vec_disc_ret) == 2
+
+
+def test_mp_moql():
+    env_id = "deep-sea-treasure-v0"
+    env = mo_gym.make(env_id, dst_map=CONCAVE_MAP)
+    eval_env = mo_gym.make(env_id, dst_map=CONCAVE_MAP)
+    scalarization = tchebicheff(tau=4.0, reward_dim=2)
+
+    agent = MPMOQLearning(
+        env,
+        scalarization=scalarization,
+        initial_epsilon=0.9,
+        epsilon_decay_steps=int(1e3),
+        log=False,
+    )
+    agent.train(eval_env=eval_env, ref_point=np.array([0.0, -25.0]), num_iterations=1, timesteps_per_iteration=1000)
+
+    front = agent.linear_support.ccs
+    assert len(front) > 0
+
+
+def test_ols():
+    env = mo_gym.make("deep-sea-treasure-v0")
+
+    ols = LinearSupport(num_objectives=2, epsilon=0.1, verbose=False)
+    policies = []
+    while not ols.ended():
+        w = ols.next_weight()
+
+        new_policy = MOQLearning(
+            env,
+            weights=w,
+            learning_rate=0.3,
+            gamma=0.9,
+            initial_epsilon=1,
+            final_epsilon=0.01,
+            epsilon_decay_steps=int(1e5),
+            log=False,
+        )
+        new_policy.train(0, total_timesteps=int(1e4))
+
+        _, _, vec, discounted_vec = new_policy.policy_eval(eval_env=env, weights=w)
+        policies.append(new_policy)
+
+        removed_inds = ols.add_solution(discounted_vec, w)
+
+        for ind in removed_inds:
+            policies.pop(ind)  # remove policies that are no longer needed
+
+
+def test_envelope():
+    env = mo_gym.make("minecart-v0")
+    eval_env = mo_gym.make("minecart-v0")
+
+    agent = Envelope(
+        env,
+        log=False,
+    )
+
+    agent.train(
+        total_timesteps=1000,
+        eval_env=eval_env,
+        ref_point=np.array([0.0, 0.0, -200.0]),
+        eval_freq=100,
+    )
+
+    scalar_return, scalarized_disc_return, vec_ret, vec_disc_ret = eval_mo(agent, env=eval_env, w=np.array([0.5, 0.4, 0.1]))
+    assert scalar_return != 0
+    assert scalarized_disc_return != 0
+    assert len(vec_ret) == 3
+    assert len(vec_disc_ret) == 3
+
+
+def test_gpi_pd():
+    env = mo_gym.make("minecart-v0")
+    eval_env = mo_gym.make("minecart-v0")
+
+    agent = GPIPD(
+        env,
+        log=False,
+    )
+
+    agent.train_iteration(
+        total_timesteps=1000,
+        weight=np.array([1.0, 0.0, 0.0]),
+        weight_support=[np.array([1.0, 0.0, 0.0]), np.array([0.0, 1.0, 0.0])],
+        eval_env=eval_env,
+        eval_freq=100,
+    )
+
+    scalar_return, scalarized_disc_return, vec_ret, vec_disc_ret = eval_mo(agent, env=eval_env, w=np.array([0.5, 0.4, 0.1]))
+    assert scalar_return != 0
+    assert scalarized_disc_return != 0
+    assert len(vec_ret) == 3
+    assert len(vec_disc_ret) == 3
+
+
+def test_gpi_pd_continuous_action():
+    env = mo_gym.make("mo-hopper-v4", cost_objective=False, max_episode_steps=500)
+    eval_env = mo_gym.make("mo-hopper-v4", cost_objective=False, max_episode_steps=500)
+
+    agent = GPIPDContinuousAction(
+        env,
+        log=False,
+    )
+
+    agent.train_iteration(
+        total_timesteps=1000,
+        weight=np.array([1.0, 0.0]),
+        weight_support=[np.array([0.0, 1.0])],
+        eval_env=eval_env,
+        eval_freq=100,
+    )
+
+    scalar_return, scalarized_disc_return, vec_ret, vec_disc_ret = eval_mo(agent, env=eval_env, w=np.array([0.5, 0.5]))
+    assert scalar_return != 0
+    assert scalarized_disc_return != 0
+    assert len(vec_ret) == 2
+    assert len(vec_disc_ret) == 2
+
+
+# This test is a bit long to execute, idk what to do with it.
+def test_pgmorl():
+    env_id = "mo-mountaincarcontinuous-v0"
+    algo = PGMORL(
+        env_id=env_id,
+        ref_point=np.array([0.0, 0.0]),
+        num_envs=4,
+        pop_size=6,
+        warmup_iterations=2,
+        evolutionary_iterations=2,
+        num_weight_candidates=5,
+        limit_env_steps=int(1e4),
+        log=False,
+    )
+    algo.train()
+    env = make_env(env_id, 422, 1, "PGMORL_test", gamma=0.995)()  # idx != 0 to avoid taking videos
+
+    # Execution of trained policies
+    for a in algo.archive.individuals:
+        scalarized, discounted_scalarized, reward, discounted_reward = eval_mo(
+            agent=a, env=env, w=np.array([1.0, 1.0]), render=False
+        )
+        assert scalarized != 0
+        assert discounted_scalarized != 0
+        assert len(reward) == 2
+        assert len(discounted_reward) == 2
+
+
+def test_pcn():
+    env = mo_gym.make("minecart-deterministic-v0")
+
+    agent = PCN(
+        env,
+        scaling_factor=np.array([1, 1, 0.1, 0.1]),
+        learning_rate=1e-3,
+        batch_size=256,
+        log=False,
+    )
+
+    agent.train(
+        env,
+        ref_point=np.array([0, 0, -200.0]),
+        num_er_episodes=1,
+        max_buffer_size=50,
+        num_model_updates=50,
+        total_time_steps=10,
+        max_return=np.array([1.5, 1.5, -0.0]),
+    )
+
+    agent.set_desired_return_and_horizon(np.array([1.5, 1.5, -0.0]), 100)
+    scalar_return, scalarized_disc_return, vec_ret, vec_disc_ret = eval_mo(
+        agent, env=env, w=np.array([0.4, 0.4, 0.2]), render=False
+    )
+    assert scalar_return != 0
+    assert scalarized_disc_return != 0
+    assert len(vec_ret) == 3
+    assert len(vec_disc_ret) == 3

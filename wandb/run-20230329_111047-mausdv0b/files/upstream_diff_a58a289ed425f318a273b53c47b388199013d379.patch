diff --git a/.gitmodules b/.gitmodules
deleted file mode 100644
index ffdee8d..0000000
--- a/.gitmodules
+++ /dev/null
@@ -1,6 +0,0 @@
-[submodule "MO-Gymnasium"]
-	path = MO-Gymnasium
-	url = git@github.com:Farama-Foundation/MO-Gymnasium.git
-[submodule "morl-baselines"]
-	path = morl-baselines
-	url = git@github.com:LucasAlegre/morl-baselines.git
diff --git a/Gpi/main.py b/Gpi/main.py
index 23bb996..e20ac49 100644
--- a/Gpi/main.py
+++ b/Gpi/main.py
@@ -3,26 +3,21 @@ import mo_gymnasium as mo_gym
 import numpy as np
 from morl_baselines.multi_policy.gpi_pd.gpi_pd_continuous_action import GPIPDContinuousAction
 
-# Error with the dimensions of the reward array when penalized=False (probably a bug when summing the reward with penalty=0.0)
-#   Fixed by downgrading to numpy 1.21 fromn 1.24
-
-env = mo_gym.make('water-reservoir-v0', normalized_action=True, nO=4, penalize=False)
-# Bug when penalize=True
-# using numpy.random._generator.Generator (default in gym), the np_random.randint method does not work on default np.Generator (dam_env.py line 140)
-#   This functionality has been removed in numpy 1.23 (required version in pyproject is 1.21=<)
-#   When downgrading to numpy 1.21 bug persists
-#   also see gym/utils/seeding.py
-#   Perhaps change randint to integers?
-#print(env.np_random)
-
-# AttributeError: 'GPIPDContinuousAction' object has no attribute 'dynamics' (line 543 in gpi_pd_continuous_action.py)
-#   occurs only when dyna=False (maybe due to if check not having else branch such that self.dynamics=None at line 213?)
-#   Fixed: added else branch locally after if-test line 213
+
+env = mo_gym.make('water-reservoir-v0', normalized_action=False, nO=2, penalize=True, time_limit=365)
+
+#wrapped_env = mo_gym.MOClipReward(env, 1, -50, 0)
+
+# bug??: algorithm returns nan as values for ccs when normalized_action=False
+# C:\Users\liamm\anaconda3\lib\site-packages\mo_gymnasium\envs\water_reservoir\dam_env.py:261: RuntimeWarning: invalid value encountered in multiply penalty = -self.penalize * np.abs(bounded_action - action)
+# TEMP Fix: when placing an upper bound on the action other than np.inf, agent trains
+
+# normalized_action=True results in faster learning
 
 # When using the model based version (dyna=True, per=True), NotImplementedError is raised by:
 #   morl-baselines/common/model-based/utils.py (env_id==water-reservoir-v0 is not in the if-statement in the contructor of ModelEnv)
-agent = GPIPDContinuousAction(env=env, per=False, dyna=False)
+agent = GPIPDContinuousAction(env=env, per=True, dyna=False, experiment_name='gpi-ls_2_obj_RestrAction200_pen')
 
-#agent.train(env, ref_point=np.array([0,0], dtype=np.float32)) #random ref_point
+agent.train(365000, env, ref_point=np.array([0,0], dtype=np.float32), timesteps_per_iter=36500, eval_freq=3650) #random ref_point
 
 #agent.load("./weights/GPI-PD gpi-ls iter=10.tar")
diff --git a/Gpi/pf_obj3_obj4.png b/Gpi/pf_obj3_obj4.png
deleted file mode 100644
index a44caf7..0000000
Binary files a/Gpi/pf_obj3_obj4.png and /dev/null differ
diff --git a/Gpi/plot_pf.py b/Gpi/plot_pf.py
index fe0bfd5..6cb5cc2 100644
--- a/Gpi/plot_pf.py
+++ b/Gpi/plot_pf.py
@@ -1,9 +1,11 @@
 import pandas as pd
 from matplotlib import pyplot as plt
 
-columns = ["objective_1", "objective_2", "objective_3", "objective_4"]
+columns = ["objective_1", "objective_2"]
 
-df = pd.read_csv("GPI/wandb_export_2023-03-23T18_23_52.285+01_00.csv", usecols=columns)
+df = pd.read_csv("GPI/2obj_norm_pen.csv", usecols=columns)
 
-plt.plot(df.objective_3, df.objective_4)
+plt.plot(df.objective_1, df.objective_2, 'o')
+plt.xlabel('Cost due to excess water level wrt flooding threshold upstream')
+plt.ylabel('Deficit in water supply wrt demand')
 plt.show()
\ No newline at end of file
Submodule MO-Gymnasium 3a9c3e8...0000000 (submodule deleted)
Submodule morl-baselines 9f683af...0000000 (submodule deleted)
diff --git a/weights/GPI-PD gpi-ls iter=1.tar b/weights/GPI-PD gpi-ls iter=1.tar
index 1823dab..3766d5a 100644
Binary files a/weights/GPI-PD gpi-ls iter=1.tar and b/weights/GPI-PD gpi-ls iter=1.tar differ
diff --git a/weights/GPI-PD gpi-ls iter=10.tar b/weights/GPI-PD gpi-ls iter=10.tar
index ab43834..555d621 100644
Binary files a/weights/GPI-PD gpi-ls iter=10.tar and b/weights/GPI-PD gpi-ls iter=10.tar differ
diff --git a/weights/GPI-PD gpi-ls iter=2.tar b/weights/GPI-PD gpi-ls iter=2.tar
index 36a937c..d21e6c1 100644
Binary files a/weights/GPI-PD gpi-ls iter=2.tar and b/weights/GPI-PD gpi-ls iter=2.tar differ
diff --git a/weights/GPI-PD gpi-ls iter=3.tar b/weights/GPI-PD gpi-ls iter=3.tar
index b1f1576..29840d8 100644
Binary files a/weights/GPI-PD gpi-ls iter=3.tar and b/weights/GPI-PD gpi-ls iter=3.tar differ
diff --git a/weights/GPI-PD gpi-ls iter=4.tar b/weights/GPI-PD gpi-ls iter=4.tar
index bcaf24d..52f6906 100644
Binary files a/weights/GPI-PD gpi-ls iter=4.tar and b/weights/GPI-PD gpi-ls iter=4.tar differ
diff --git a/weights/GPI-PD gpi-ls iter=5.tar b/weights/GPI-PD gpi-ls iter=5.tar
index a9d43da..a123874 100644
Binary files a/weights/GPI-PD gpi-ls iter=5.tar and b/weights/GPI-PD gpi-ls iter=5.tar differ
diff --git a/weights/GPI-PD gpi-ls iter=6.tar b/weights/GPI-PD gpi-ls iter=6.tar
index e00b273..196d0c9 100644
Binary files a/weights/GPI-PD gpi-ls iter=6.tar and b/weights/GPI-PD gpi-ls iter=6.tar differ
diff --git a/weights/GPI-PD gpi-ls iter=7.tar b/weights/GPI-PD gpi-ls iter=7.tar
index 5976cf3..57245a0 100644
Binary files a/weights/GPI-PD gpi-ls iter=7.tar and b/weights/GPI-PD gpi-ls iter=7.tar differ
diff --git a/weights/GPI-PD gpi-ls iter=8.tar b/weights/GPI-PD gpi-ls iter=8.tar
index 4923399..d025382 100644
Binary files a/weights/GPI-PD gpi-ls iter=8.tar and b/weights/GPI-PD gpi-ls iter=8.tar differ
diff --git a/weights/GPI-PD gpi-ls iter=9.tar b/weights/GPI-PD gpi-ls iter=9.tar
index 0a61e2a..2a2642d 100644
Binary files a/weights/GPI-PD gpi-ls iter=9.tar and b/weights/GPI-PD gpi-ls iter=9.tar differ

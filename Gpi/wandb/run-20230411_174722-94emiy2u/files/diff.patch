diff --git a/Gpi/main.ipynb b/Gpi/main.ipynb
index b49b4d4..15ebeaa 100644
--- a/Gpi/main.ipynb
+++ b/Gpi/main.ipynb
@@ -49,19 +49,25 @@
     "## GPI-ls with TD3\n",
     "Try this without the dynamics model for a fair comparison with PCN (later).\n",
     "\n",
-    "Parameters:\n",
+    "Algo parameters:\n",
     "- per = True\n",
     "- policy_noise = 0.002\n",
     "- noise_clip = 0.005\n",
+    "- rest is default\n",
     "\n",
-    "Opt for smaller policy noise due to normalised actions.\n",
+    "Opt for smaller policy noise due to normalized actions.\n",
+    "\n",
+    "Environment parameters:\n",
+    "- normalized_action = True\n",
+    "- penalize = True\n",
+    "- time_limit = 365 \n",
     "\n",
     "### Bug?\n",
     "Algorithm returns nan as values for ccs when normalized_action=False\n",
     "\n",
     "    C:\\Users\\liamm\\anaconda3\\lib\\site-packages\\mo_gymnasium\\envs\\water_reservoir\\dam_env.py:261: RuntimeWarning: invalid value encountered in multiply penalty = -self.penalize * np.abs(bounded_action - action)\n",
     "    \n",
-    "=> TEMP Fix: when placing an upper bound on the action other than np.inf, agent trains. Maybe agents selects np.inf as action sometimes?"
+    "=> TEMP Fix: when placing an upper bound on the action other than np.inf (dam_env.py, line 113), agent trains. Maybe agents selects np.inf as action sometimes?"
    ]
   },
   {
@@ -104,6 +110,28 @@
     "plt.show()"
    ]
   },
+  {
+   "attachments": {},
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "### Experiment with TD3 hyperparameters\n",
+    "\n"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "env = mo_gym.make('water-reservoir-v0', normalized_action=True, nO=2, penalize=True, time_limit=365)\n",
+    "\n",
+    "GPIAgent = GPIPDContinuousAction(env=env, per=True, dyna=False, experiment_name='gpi-ls_2_obj_NormAction_lessPolicyNoise_lessHiddenLayers', policy_noise=0.002, noise_clip=0.005, net_arch=[500])\n",
+    "\n",
+    "GPIAgent.train(365000, env, ref_point=np.array([0,0], dtype=np.float32), timesteps_per_iter=36500, eval_freq=3650) #random ref_point, hv is not used in the algorithm so this does not matter for training"
+   ]
+  },
   {
    "attachments": {},
    "cell_type": "markdown",
diff --git a/Gpi/main.py b/Gpi/main.py
index 23bb996..ca0fc45 100644
--- a/Gpi/main.py
+++ b/Gpi/main.py
@@ -3,26 +3,21 @@ import mo_gymnasium as mo_gym
 import numpy as np
 from morl_baselines.multi_policy.gpi_pd.gpi_pd_continuous_action import GPIPDContinuousAction
 
-# Error with the dimensions of the reward array when penalized=False (probably a bug when summing the reward with penalty=0.0)
-#   Fixed by downgrading to numpy 1.21 fromn 1.24
-
-env = mo_gym.make('water-reservoir-v0', normalized_action=True, nO=4, penalize=False)
-# Bug when penalize=True
-# using numpy.random._generator.Generator (default in gym), the np_random.randint method does not work on default np.Generator (dam_env.py line 140)
-#   This functionality has been removed in numpy 1.23 (required version in pyproject is 1.21=<)
-#   When downgrading to numpy 1.21 bug persists
-#   also see gym/utils/seeding.py
-#   Perhaps change randint to integers?
-#print(env.np_random)
-
-# AttributeError: 'GPIPDContinuousAction' object has no attribute 'dynamics' (line 543 in gpi_pd_continuous_action.py)
-#   occurs only when dyna=False (maybe due to if check not having else branch such that self.dynamics=None at line 213?)
-#   Fixed: added else branch locally after if-test line 213
+
+env = mo_gym.make('water-reservoir-v0', normalized_action=True, nO=2, penalize=True, time_limit=365)
+
+#wrapped_env = mo_gym.MOClipReward(env, 1, -50, 0)
+
+# bug??: algorithm returns nan as values for ccs when normalized_action=False
+# C:\Users\liamm\anaconda3\lib\site-packages\mo_gymnasium\envs\water_reservoir\dam_env.py:261: RuntimeWarning: invalid value encountered in multiply penalty = -self.penalize * np.abs(bounded_action - action)
+# TEMP Fix: when placing an upper bound on the action other than np.inf, agent trains
+
+# normalized_action=True results in faster learning
 
 # When using the model based version (dyna=True, per=True), NotImplementedError is raised by:
 #   morl-baselines/common/model-based/utils.py (env_id==water-reservoir-v0 is not in the if-statement in the contructor of ModelEnv)
-agent = GPIPDContinuousAction(env=env, per=False, dyna=False)
+GPIAgent = GPIPDContinuousAction(env=env, per=True, dyna=False, experiment_name='gpi-ls_2_obj_NormAction_lessPolicyNoise', policy_noise=0.002, noise_clip=0.005)
 
-#agent.train(env, ref_point=np.array([0,0], dtype=np.float32)) #random ref_point
+GPIAgent.train(365000, env, ref_point=np.array([0,0], dtype=np.float32), timesteps_per_iter=36500, eval_freq=3650) #random ref_point
 
 #agent.load("./weights/GPI-PD gpi-ls iter=10.tar")
diff --git a/Gpi/plot_pf.py b/Gpi/plot_pf.py
index fe0bfd5..6cb5cc2 100644
--- a/Gpi/plot_pf.py
+++ b/Gpi/plot_pf.py
@@ -1,9 +1,11 @@
 import pandas as pd
 from matplotlib import pyplot as plt
 
-columns = ["objective_1", "objective_2", "objective_3", "objective_4"]
+columns = ["objective_1", "objective_2"]
 
-df = pd.read_csv("GPI/wandb_export_2023-03-23T18_23_52.285+01_00.csv", usecols=columns)
+df = pd.read_csv("GPI/2obj_norm_pen.csv", usecols=columns)
 
-plt.plot(df.objective_3, df.objective_4)
+plt.plot(df.objective_1, df.objective_2, 'o')
+plt.xlabel('Cost due to excess water level wrt flooding threshold upstream')
+plt.ylabel('Deficit in water supply wrt demand')
 plt.show()
\ No newline at end of file
diff --git a/weights/GPI-PD gpi-ls iter=1.tar b/weights/GPI-PD gpi-ls iter=1.tar
index 1823dab..f675fd9 100644
Binary files a/weights/GPI-PD gpi-ls iter=1.tar and b/weights/GPI-PD gpi-ls iter=1.tar differ
diff --git a/weights/GPI-PD gpi-ls iter=10.tar b/weights/GPI-PD gpi-ls iter=10.tar
index ab43834..fc1ca50 100644
Binary files a/weights/GPI-PD gpi-ls iter=10.tar and b/weights/GPI-PD gpi-ls iter=10.tar differ
diff --git a/weights/GPI-PD gpi-ls iter=2.tar b/weights/GPI-PD gpi-ls iter=2.tar
index 36a937c..a0a4af3 100644
Binary files a/weights/GPI-PD gpi-ls iter=2.tar and b/weights/GPI-PD gpi-ls iter=2.tar differ
diff --git a/weights/GPI-PD gpi-ls iter=3.tar b/weights/GPI-PD gpi-ls iter=3.tar
index b1f1576..8a32d9a 100644
Binary files a/weights/GPI-PD gpi-ls iter=3.tar and b/weights/GPI-PD gpi-ls iter=3.tar differ
diff --git a/weights/GPI-PD gpi-ls iter=4.tar b/weights/GPI-PD gpi-ls iter=4.tar
index bcaf24d..57e089a 100644
Binary files a/weights/GPI-PD gpi-ls iter=4.tar and b/weights/GPI-PD gpi-ls iter=4.tar differ
diff --git a/weights/GPI-PD gpi-ls iter=5.tar b/weights/GPI-PD gpi-ls iter=5.tar
index a9d43da..cce2a4c 100644
Binary files a/weights/GPI-PD gpi-ls iter=5.tar and b/weights/GPI-PD gpi-ls iter=5.tar differ
diff --git a/weights/GPI-PD gpi-ls iter=6.tar b/weights/GPI-PD gpi-ls iter=6.tar
index e00b273..db24a94 100644
Binary files a/weights/GPI-PD gpi-ls iter=6.tar and b/weights/GPI-PD gpi-ls iter=6.tar differ
diff --git a/weights/GPI-PD gpi-ls iter=7.tar b/weights/GPI-PD gpi-ls iter=7.tar
index 5976cf3..cd51995 100644
Binary files a/weights/GPI-PD gpi-ls iter=7.tar and b/weights/GPI-PD gpi-ls iter=7.tar differ
diff --git a/weights/GPI-PD gpi-ls iter=8.tar b/weights/GPI-PD gpi-ls iter=8.tar
index 4923399..e018540 100644
Binary files a/weights/GPI-PD gpi-ls iter=8.tar and b/weights/GPI-PD gpi-ls iter=8.tar differ
diff --git a/weights/GPI-PD gpi-ls iter=9.tar b/weights/GPI-PD gpi-ls iter=9.tar
index 0a61e2a..913a0ec 100644
Binary files a/weights/GPI-PD gpi-ls iter=9.tar and b/weights/GPI-PD gpi-ls iter=9.tar differ

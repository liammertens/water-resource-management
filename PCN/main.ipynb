{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pareto Conditioned networks\n",
    "\n",
    "Using the modified dam environment (less variance in the inflow), test PCN. This modifications is necessary because PCN expects deterministic transitions.\n",
    "\n",
    "Also a modification to the original PCN code in morl-baselines is necessary in order to support the continuous actions of the environment:\n",
    "- continuous_actions is a parameter given to the algorithm\n",
    "- During training, actions are no longer sampled from a categorical distribution as in the [original paper](https://www.ifaamas.org/Proceedings/aamas2022/pdfs/p1110.pdf). This might hinder exploration...\n",
    "- The logsoftmax function of the original model is no longer used, instead its output is a single value corresponding to an action\n",
    "- The loss used in the model is now MSE instead of CE (regression loss vs classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mo_gymnasium as mo_gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from morl_baselines.multi_policy.pcn.pcn import PCN\n",
    "\n",
    "\"\"\"\n",
    "Helper function to plot pf\n",
    "\"\"\"\n",
    "# TODO: add no objectives as parameter (later)\n",
    "def plot_pf(file):\n",
    "    columns = [\"objective_1\", \"objective_2\"]\n",
    "    df = pd.read_csv(file, usecols=columns)\n",
    "\n",
    "    plt.plot(df.objective_1, df.objective_2, 'o')\n",
    "    plt.xlabel('Cost due to excess water level wrt flooding threshold upstream')\n",
    "    plt.ylabel('Deficit in water supply wrt demand')\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First attempt\n",
    "\n",
    "- Dam inflow: mean = 40.0 and stdev = 1.0 \n",
    "- Actions capped between [0.0;250.0]\n",
    "- Scaling factor for desired horizon/return = [0.1, 0.1]\n",
    "- Hypervolume ref point = [-100, -1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "wandb: Currently logged in as: liam-mertens02 (vub-ai). Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\liamm\\water-resource-management\\PCN\\wandb\\run-20230510_143056-t4ohzmny</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/vub-ai/MORL-Baselines/runs/t4ohzmny' target=\"_blank\">water-reservoir-v0__PCN__None__1683721852</a></strong> to <a href='https://wandb.ai/vub-ai/MORL-Baselines' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/vub-ai/MORL-Baselines' target=\"_blank\">https://wandb.ai/vub-ai/MORL-Baselines</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/vub-ai/MORL-Baselines/runs/t4ohzmny' target=\"_blank\">https://wandb.ai/vub-ai/MORL-Baselines/runs/t4ohzmny</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Found log directory outside of given root_logdir, dropping given root_logdir for event file in /tmp/PCN\n",
      "c:\\Users\\liamm\\anaconda3\\lib\\site-packages\\morl_baselines\\multi_policy\\pcn\\pcn.py:190: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:233.)\n",
      "  th.tensor(obs).to(self.device),\n",
      "c:\\Users\\liamm\\anaconda3\\lib\\site-packages\\morl_baselines\\multi_policy\\pcn\\pcn.py:404: RuntimeWarning: overflow encountered in multiply\n",
      "  ent = np.sum(-np.exp(lp) * lp)\n",
      "c:\\Users\\liamm\\anaconda3\\lib\\site-packages\\morl_baselines\\multi_policy\\pcn\\pcn.py:404: RuntimeWarning: overflow encountered in exp\n",
      "  ent = np.sum(-np.exp(lp) * lp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 60000 \t return [-7082.882  -8074.1567], ([ 6.5656724 22.188858 ]) \t loss 8.543E+03\n",
      "step 70000 \t return [-6626.084 -7616.352], ([ 2.439899 23.981667]) \t loss 4.269E+03\n",
      "step 80000 \t return [-6622.7817 -7617.29  ], ([ 2.0073726 21.380568 ]) \t loss 3.058E+03\n",
      "step 90000 \t return [-6606.732 -7597.325], ([ 2.0039642 20.370197 ]) \t loss 2.254E+03\n",
      "step 100000 \t return [-6607.9556 -7603.368 ], ([ 1.8573393 20.589802 ]) \t loss 1.112E+03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Step cannot be set when using syncing with tensorboard. Please log your step values as a metric such as 'global_step'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 110000 \t return [-6606.01  -7599.614], ([ 1.8485326 21.356756 ]) \t loss 9.544E+01\n",
      "step 120000 \t return [-6608.456  -7602.7637], ([ 1.7181865 22.72263  ]) \t loss 3.081E+01\n",
      "step 130000 \t return [-6609.399  -7603.0107], ([ 1.9992337 21.411774 ]) \t loss 2.368E+01\n",
      "step 140000 \t return [-6606.9736 -7597.839 ], ([ 1.9234885 22.724628 ]) \t loss 2.476E+01\n",
      "step 150000 \t return [-6605.486  -7601.4287], ([ 1.6822094 24.264341 ]) \t loss 1.563E+01\n",
      "step 160000 \t return [-6601.6533 -7595.4688], ([ 1.851816 23.232746]) \t loss 2.511E+01\n",
      "step 170000 \t return [-6599.0127 -7590.5093], ([ 1.6485417 21.793571 ]) \t loss 1.866E+01\n",
      "step 180000 \t return [-6598.538  -7589.8276], ([ 1.5868927 21.772291 ]) \t loss 1.480E+01\n",
      "step 190000 \t return [-6599.2964 -7590.256 ], ([ 1.6704288 22.318876 ]) \t loss 1.119E+01\n",
      "step 200000 \t return [-6597.275  -7589.1504], ([ 1.6236256 21.443516 ]) \t loss 1.150E+01\n",
      "step 210000 \t return [-6597.937  -7588.1626], ([ 2.4227304 21.657925 ]) \t loss 1.217E+01\n",
      "step 220000 \t return [-6600.089  -7589.6816], ([ 5.5243926 17.576597 ]) \t loss 8.533E+00\n",
      "step 230000 \t return [-6595.832 -7588.911], ([ 1.5148383 23.044638 ]) \t loss 9.688E+00\n",
      "step 240000 \t return [-6592.955 -7587.011], ([ 1.1380457 21.167076 ]) \t loss 1.151E+01\n",
      "step 250000 \t return [-6592.9424 -7586.3833], ([ 2.7833083 20.124525 ]) \t loss 1.372E+01\n",
      "step 260000 \t return [-6591.425  -7582.9204], ([ 3.7770398 19.807098 ]) \t loss 7.363E+00\n",
      "step 270000 \t return [-6619.056  -7611.5107], ([11.691225  11.4395895]) \t loss 4.460E+00\n",
      "step 280000 \t return [-6590.149  -7587.1313], ([ 3.758488 18.166714]) \t loss 7.566E+00\n",
      "step 290000 \t return [-6590.5454 -7579.0117], ([ 4.896113 19.343256]) \t loss 1.225E+01\n",
      "step 300000 \t return [-6591.549 -7587.149], ([ 8.371932 14.156521]) \t loss 9.350E+00\n",
      "step 310000 \t return [-6586.187 -7578.773], ([ 3.6825123 16.121304 ]) \t loss 8.683E+00\n",
      "step 320000 \t return [-6604.9985 -7599.8433], ([15.265616   8.9256735]) \t loss 8.104E+00\n",
      "step 330000 \t return [-6600.476 -7594.056], ([12.9498415  9.639978 ]) \t loss 2.276E+01\n",
      "step 340000 \t return [-6589.0854 -7584.909 ], ([10.222402  13.2808075]) \t loss 7.171E+00\n",
      "step 350000 \t return [-6605.4736 -7597.9536], ([13.77177    8.4741955]) \t loss 6.860E+00\n",
      "step 360000 \t return [-6601.4893 -7597.4067], ([14.051552  9.988075]) \t loss 1.146E+01\n",
      "step 370000 \t return [-6596.795 -7590.486], ([12.220298 11.093889]) \t loss 1.495E+01\n",
      "step 380000 \t return [-6595.6426 -7594.7715], ([13.011322 11.090014]) \t loss 3.299E+00\n",
      "step 390000 \t return [-6622.1743 -7616.3306], ([13.057366  8.090261]) \t loss 1.288E+01\n",
      "step 400000 \t return [-6584.582  -7582.4434], ([ 8.903521 13.41261 ]) \t loss 1.187E+01\n",
      "step 410000 \t return [-6588.957  -7583.5015], ([11.927612 14.109924]) \t loss 1.469E+01\n",
      "step 420000 \t return [-6588.719  -7588.5757], ([13.363064 14.547358]) \t loss 1.171E+01\n",
      "step 430000 \t return [-6601.9937 -7598.865 ], ([16.3679   10.194133]) \t loss 6.464E+00\n",
      "step 440000 \t return [-6603.7163 -7599.933 ], ([15.165915  9.130203]) \t loss 1.255E+01\n",
      "step 450000 \t return [-6587.7637 -7587.87  ], ([10.647347 14.885624]) \t loss 1.885E+01\n",
      "step 460000 \t return [-6593.007  -7589.4575], ([12.605346 14.008787]) \t loss 1.085E+01\n",
      "step 470000 \t return [-6586.5024 -7584.169 ], ([ 8.637987 16.945065]) \t loss 8.474E+00\n",
      "step 480000 \t return [-6603.05  -7593.074], ([12.717419  8.829767]) \t loss 7.791E+00\n",
      "step 490000 \t return [-6605.064  -7599.5776], ([14.447465  9.478261]) \t loss 4.127E+00\n",
      "step 500000 \t return [-6604.7954 -7601.8887], ([14.803101 10.594369]) \t loss 1.121E+01\n",
      "step 510000 \t return [-6588.728 -7585.313], ([10.108458 14.3158  ]) \t loss 9.878E+00\n",
      "step 520000 \t return [-6585.2524 -7582.0186], ([10.05303  15.857713]) \t loss 1.123E+01\n",
      "step 530000 \t return [-6579.3574 -7571.7344], ([ 8.432414 16.17751 ]) \t loss 6.591E+00\n",
      "step 540000 \t return [-6586.675 -7577.484], ([ 1.0923326 21.27325  ]) \t loss 5.957E+00\n",
      "step 550000 \t return [-6575.915 -7568.522], ([ 6.409607 15.358725]) \t loss 1.352E+01\n",
      "step 560000 \t return [-6577.912 -7572.078], ([ 3.2815588 17.669058 ]) \t loss 9.097E+00\n",
      "step 570000 \t return [-6576.4336 -7568.781 ], ([ 2.5659282 20.954618 ]) \t loss 1.229E+01\n",
      "step 580000 \t return [-6573.4077 -7567.2563], ([ 6.775502 16.298021]) \t loss 1.014E+01\n",
      "step 590000 \t return [-6572.9775 -7567.141 ], ([ 4.1569686 22.333096 ]) \t loss 2.442E+01\n",
      "step 600000 \t return [-6571.741 -7566.909], ([ 7.4223576 18.784494 ]) \t loss 1.005E+01\n",
      "step 610000 \t return [-6565.7812 -7561.091 ], ([ 4.1246824 16.371256 ]) \t loss 1.525E+01\n",
      "step 620000 \t return [-6564.846 -7557.825], ([ 2.8136222 20.864754 ]) \t loss 2.679E+01\n",
      "step 630000 \t return [-6563.6763 -7557.912 ], ([ 1.8529724 22.059292 ]) \t loss 5.078E+00\n",
      "step 640000 \t return [-6558.4536 -7549.2563], ([ 3.7274337 20.233564 ]) \t loss 7.809E+00\n",
      "step 650000 \t return [-6564.132  -7555.8657], ([11.438824 13.041796]) \t loss 2.408E+00\n",
      "step 660000 \t return [-6561.459 -7553.994], ([ 0.8749366 22.321705 ]) \t loss 1.178E+01\n",
      "step 670000 \t return [-6556.4126 -7550.0186], ([ 3.0734234 20.050983 ]) \t loss 2.447E+00\n",
      "step 680000 \t return [-6562.4155 -7554.587 ], ([ 8.400601 18.552242]) \t loss 1.315E+01\n",
      "step 690000 \t return [-6553.8794 -7546.2505], ([ 1.9693483 22.087988 ]) \t loss 1.560E+01\n",
      "step 700000 \t return [-6547.221 -7538.835], ([ 2.7602284 18.729336 ]) \t loss 1.049E+01\n",
      "step 710000 \t return [-6544.881  -7539.8394], ([ 1.7838125 19.70017  ]) \t loss 8.614E+00\n",
      "step 720000 \t return [-6587.759 -7580.273], ([ 1.1441448 25.47107  ]) \t loss 1.107E+01\n",
      "step 730000 \t return [-6540.7793 -7533.0386], ([ 1.598641 21.436644]) \t loss 9.008E+00\n",
      "step 740000 \t return [-6544.338 -7537.054], ([ 8.808715 15.099278]) \t loss 2.674E+00\n",
      "step 750000 \t return [-6533.9243 -7532.998 ], ([ 5.332174 15.633583]) \t loss 7.304E+00\n",
      "step 760000 \t return [-6531.4014 -7527.291 ], ([ 2.3017278 20.538305 ]) \t loss 3.737E+00\n",
      "step 770000 \t return [-6590.747  -7585.5425], ([ 0.97297317 24.81909   ]) \t loss 2.302E+00\n",
      "step 780000 \t return [-6532.286 -7524.21 ], ([ 6.6861315 16.874962 ]) \t loss 3.911E+00\n",
      "step 790000 \t return [-6531.587  -7520.0283], ([ 1.3698118 21.450766 ]) \t loss 7.268E+00\n",
      "step 800000 \t return [-6539.5986 -7528.6724], ([ 9.614831 15.526591]) \t loss 8.162E+00\n",
      "step 810000 \t return [-6535.0405 -7527.6743], ([ 5.197026 16.523897]) \t loss 6.239E+00\n",
      "step 820000 \t return [-6535.991  -7529.7324], ([ 7.1312113 17.072939 ]) \t loss 1.391E+01\n",
      "step 830000 \t return [-6591.9443 -7584.514 ], ([ 1.250611 20.906197]) \t loss 4.140E+00\n",
      "step 840000 \t return [-6526.419 -7516.061], ([ 9.094822 15.391713]) \t loss 1.729E+01\n",
      "step 850000 \t return [-6522.957  -7512.7905], ([ 1.5395794 23.911259 ]) \t loss 8.588E+00\n",
      "step 860000 \t return [-6520.946  -7514.2925], ([ 6.393122 17.472446]) \t loss 7.967E+00\n",
      "step 870000 \t return [-6521.1973 -7516.0767], ([ 7.488022 15.555555]) \t loss 8.954E+00\n",
      "step 880000 \t return [-6529.937  -7522.1357], ([11.811402 13.666991]) \t loss 2.322E+01\n",
      "step 890000 \t return [-6512.7285 -7500.938 ], ([ 5.152238 17.935844]) \t loss 1.739E+01\n",
      "step 900000 \t return [-6528.0674 -7519.6963], ([ 8.744364 14.166093]) \t loss 9.614E+00\n",
      "step 910000 \t return [-6504.4883 -7498.058 ], ([ 5.6855383 21.742872 ]) \t loss 1.578E+01\n",
      "step 920000 \t return [-6502.4434 -7495.213 ], ([ 3.5325975 20.328451 ]) \t loss 1.294E+01\n",
      "step 930000 \t return [-6511.6123 -7504.1875], ([ 8.810606 15.254041]) \t loss 1.656E+01\n",
      "step 940000 \t return [-6504.804 -7495.484], ([ 1.4454802 23.01004  ]) \t loss 2.514E+00\n",
      "step 950000 \t return [-6517.859 -7508.989], ([ 7.815964 16.601353]) \t loss 1.430E+01\n",
      "step 960000 \t return [-6507.2373 -7499.5786], ([ 1.1765814 20.924782 ]) \t loss 2.118E+00\n",
      "step 970000 \t return [-6501.7607 -7496.4795], ([ 1.2284732 22.081907 ]) \t loss 1.067E+00\n",
      "step 980000 \t return [-6511.6924 -7505.2905], ([ 8.137973 16.886505]) \t loss 9.630E-01\n",
      "step 990000 \t return [-6502.5513 -7498.178 ], ([ 3.4702542 22.494148 ]) \t loss 1.524E+00\n",
      "step 1000000 \t return [-6505.9395 -7500.989 ], ([ 6.8778615 19.012934 ]) \t loss 8.367E-01\n",
      "step 1010000 \t return [-6501.41   -7496.9917], ([ 3.9516459 18.99581  ]) \t loss 7.008E-01\n",
      "step 1020000 \t return [-6502.59   -7495.6475], ([ 2.1906595 22.62159  ]) \t loss 6.549E-01\n",
      "step 1030000 \t return [-6501.338  -7494.6875], ([ 4.0755005 19.625605 ]) \t loss 6.818E-01\n",
      "step 1040000 \t return [-6522.305 -7516.114], ([ 9.401669 15.709308]) \t loss 8.198E-01\n",
      "step 1050000 \t return [-6500.7124 -7492.5737], ([ 1.6358261 25.363525 ]) \t loss 1.801E+00\n",
      "step 1060000 \t return [-6501.279  -7495.4336], ([ 1.3186606 24.32211  ]) \t loss 1.149E+00\n",
      "step 1070000 \t return [-6498.882 -7492.713], ([ 1.0812368 22.252914 ]) \t loss 7.034E-01\n",
      "step 1080000 \t return [-6499.2524 -7486.8926], ([ 1.5665584 25.116665 ]) \t loss 5.943E-01\n",
      "step 1090000 \t return [-6499.945 -7491.891], ([ 5.648186 16.946875]) \t loss 6.659E-01\n",
      "step 1100000 \t return [-6500.393  -7496.7817], ([ 1.8566803 23.304487 ]) \t loss 6.468E-01\n",
      "step 1110000 \t return [-6497.939 -7493.036], ([ 3.70861  21.219011]) \t loss 7.690E-01\n",
      "step 1120000 \t return [-6503.0137 -7493.915 ], ([ 8.573511 18.024332]) \t loss 7.094E-01\n",
      "step 1130000 \t return [-6501.575  -7493.3613], ([ 2.722223 22.978909]) \t loss 9.833E-01\n",
      "step 1140000 \t return [-6500.9023 -7498.4   ], ([ 2.9231892 23.864794 ]) \t loss 8.570E-01\n",
      "step 1150000 \t return [-6498.094  -7491.0015], ([ 1.8654485 21.892319 ]) \t loss 6.956E-01\n",
      "step 1160000 \t return [-6498.723 -7490.507], ([ 2.7714205 25.834974 ]) \t loss 4.359E-01\n",
      "step 1170000 \t return [-6496.8477 -7488.1987], ([ 2.6640325 24.721075 ]) \t loss 4.360E-01\n",
      "step 1180000 \t return [-6494.583  -7488.6157], ([ 3.1666722 24.469404 ]) \t loss 3.421E-01\n",
      "step 1190000 \t return [-6488.866 -7482.123], ([ 1.8284276 24.294603 ]) \t loss 3.151E-01\n",
      "step 1200000 \t return [-6495.0283 -7487.175 ], ([ 4.148044 29.081161]) \t loss 2.289E-01\n",
      "step 1210000 \t return [-6493.7285 -7487.687 ], ([ 2.920419 23.027647]) \t loss 2.847E-01\n",
      "step 1220000 \t return [-6495.6436 -7492.3306], ([ 4.2057443 25.924717 ]) \t loss 2.391E-01\n",
      "step 1230000 \t return [-6491.0376 -7485.911 ], ([ 3.5043714 26.158606 ]) \t loss 3.758E-01\n",
      "step 1240000 \t return [-6488.006  -7482.3223], ([ 2.7885141 23.669271 ]) \t loss 3.490E-01\n",
      "step 1250000 \t return [-6493.025  -7485.6646], ([ 5.3733773 28.590748 ]) \t loss 3.139E-01\n",
      "step 1260000 \t return [-6494.7344 -7490.257 ], ([ 5.744634 27.507664]) \t loss 4.814E-01\n",
      "step 1270000 \t return [-6487.676  -7477.0664], ([ 4.037273 25.434734]) \t loss 6.607E-01\n",
      "step 1280000 \t return [-6494.6064 -7488.8994], ([ 7.3257556 32.585136 ]) \t loss 5.010E-01\n",
      "step 1290000 \t return [-6516.4375 -7510.402 ], ([ 8.799264 30.922014]) \t loss 1.050E+00\n",
      "step 1300000 \t return [-6490.6816 -7482.667 ], ([ 5.431711 29.33703 ]) \t loss 1.584E+00\n",
      "step 1310000 \t return [-6490.0044 -7484.8535], ([ 5.291619 26.603273]) \t loss 1.410E+00\n",
      "step 1320000 \t return [-6499.0874 -7493.7227], ([ 6.960519 26.887428]) \t loss 6.486E-01\n",
      "step 1330000 \t return [-6489.21  -7487.052], ([ 5.6394997 28.114056 ]) \t loss 7.173E-01\n",
      "step 1340000 \t return [-6497.016  -7492.2676], ([ 7.772011 32.286736]) \t loss 8.589E-01\n",
      "step 1350000 \t return [-6488.9194 -7482.313 ], ([ 6.399716 29.948631]) \t loss 6.666E-01\n",
      "step 1360000 \t return [-6489.801  -7480.0825], ([ 7.072239 30.553404]) \t loss 5.558E-01\n",
      "step 1370000 \t return [-6491.5063 -7485.8965], ([ 8.098455 31.983925]) \t loss 6.272E-01\n",
      "step 1380000 \t return [-6502.134  -7495.5327], ([ 9.432165 32.077026]) \t loss 7.274E-01\n",
      "step 1390000 \t return [-6491.033  -7483.6177], ([ 6.2484365 28.349579 ]) \t loss 9.361E-01\n",
      "step 1400000 \t return [-6495.915  -7486.8906], ([ 7.950565 30.86827 ]) \t loss 6.512E-01\n",
      "step 1410000 \t return [-6515.8955 -7508.795 ], ([13.808239 38.59675 ]) \t loss 8.864E-01\n",
      "step 1420000 \t return [-6510.333  -7501.4443], ([11.437457 31.595764]) \t loss 2.728E+00\n",
      "step 1430000 \t return [-6500.0137 -7495.064 ], ([ 8.920236 29.52741 ]) \t loss 4.426E+00\n",
      "step 1440000 \t return [-6498.3267 -7492.425 ], ([ 8.202881 30.581743]) \t loss 2.635E+00\n",
      "step 1450000 \t return [-6492.4473 -7484.588 ], ([ 9.047705 31.903461]) \t loss 1.027E+00\n",
      "step 1460000 \t return [-6505.4844 -7498.535 ], ([10.277334 32.487003]) \t loss 1.202E+00\n",
      "step 1470000 \t return [-6495.06  -7486.408], ([ 9.366979 31.645304]) \t loss 1.036E+00\n",
      "step 1480000 \t return [-6499.1567 -7491.2656], ([ 9.37287  31.281195]) \t loss 9.817E-01\n",
      "step 1490000 \t return [-6493.3145 -7484.5015], ([ 9.230513 32.199154]) \t loss 8.266E-01\n",
      "step 1500000 \t return [-6517.1294 -7510.7974], ([14.3972   38.115646]) \t loss 1.687E+00\n",
      "step 1510000 \t return [-6505.283 -7494.918], ([12.12718  36.176105]) \t loss 1.764E+00\n",
      "step 1520000 \t return [-6497.828 -7487.872], ([ 8.107828 28.15687 ]) \t loss 2.073E+00\n",
      "step 1530000 \t return [-6505.545  -7493.7427], ([11.202855 33.896297]) \t loss 1.379E+00\n",
      "step 1540000 \t return [-6495.937 -7489.084], ([ 9.963092 32.26569 ]) \t loss 1.361E+00\n",
      "step 1550000 \t return [-6508.5464 -7494.7974], ([14.163401 38.861492]) \t loss 8.876E-01\n",
      "step 1560000 \t return [-6507.5137 -7497.351 ], ([12.2309  33.54509]) \t loss 3.561E+00\n",
      "step 1570000 \t return [-6507.2305 -7497.8184], ([12.509132 35.901806]) \t loss 1.408E+00\n",
      "step 1580000 \t return [-6503.3477 -7496.0127], ([11.573518 33.886757]) \t loss 1.343E+00\n",
      "step 1590000 \t return [-6509.0283 -7503.4507], ([13.083264 35.427288]) \t loss 1.752E+00\n",
      "step 1600000 \t return [-6507.1294 -7497.7495], ([11.999095 33.984398]) \t loss 2.259E+00\n",
      "step 1610000 \t return [-6507.9717 -7500.757 ], ([12.228443 33.513706]) \t loss 1.682E+00\n",
      "step 1620000 \t return [-6505.7056 -7496.814 ], ([13.086283 34.517704]) \t loss 1.447E+00\n",
      "step 1630000 \t return [-6518.66   -7513.0215], ([16.316357 39.790436]) \t loss 1.562E+00\n",
      "step 1640000 \t return [-6513.7886 -7507.368 ], ([15.442958 38.678173]) \t loss 2.801E+00\n",
      "step 1650000 \t return [-6501.6724 -7492.29  ], ([13.451877 40.334644]) \t loss 2.452E+00\n",
      "step 1660000 \t return [-6515.9956 -7502.448 ], ([15.705738 37.96092 ]) \t loss 1.162E+00\n",
      "step 1670000 \t return [-6516.7026 -7509.344 ], ([17.429737 40.5827  ]) \t loss 3.571E+00\n",
      "step 1680000 \t return [-6498.1045 -7487.268 ], ([12.025275 37.638916]) \t loss 5.363E+00\n",
      "step 1690000 \t return [-6506.2485 -7498.108 ], ([14.477682 37.614803]) \t loss 1.138E+00\n",
      "step 1700000 \t return [-6506.866  -7496.8633], ([14.819043 37.03459 ]) \t loss 1.163E+00\n",
      "step 1710000 \t return [-6501.4136 -7494.5986], ([14.305128 39.278835]) \t loss 1.228E+00\n",
      "step 1720000 \t return [-6498.873 -7489.092], ([11.650045 33.995373]) \t loss 1.123E+00\n",
      "step 1730000 \t return [-6501.61   -7491.7236], ([14.750004 38.23988 ]) \t loss 1.076E+00\n",
      "step 1740000 \t return [-6509.917  -7502.5786], ([17.241962 40.422493]) \t loss 1.340E+00\n",
      "step 1750000 \t return [-6506.484  -7499.7285], ([16.292517 41.15099 ]) \t loss 3.325E+00\n",
      "step 1760000 \t return [-6502.2373 -7494.6587], ([13.633124 35.423817]) \t loss 2.172E+00\n",
      "step 1770000 \t return [-6502.584 -7490.819], ([13.362545 33.994186]) \t loss 2.180E+00\n",
      "step 1780000 \t return [-6522.516  -7515.2373], ([21.78223 48.27197]) \t loss 1.321E+00\n",
      "step 1790000 \t return [-6525.8975 -7514.308 ], ([19.578953 41.921043]) \t loss 7.411E+00\n",
      "step 1800000 \t return [-6502.129  -7493.5405], ([13.126332 36.657597]) \t loss 7.222E+00\n",
      "step 1810000 \t return [-6491.813 -7482.186], ([10.308761 34.479015]) \t loss 1.926E+00\n",
      "step 1820000 \t return [-6501.076  -7495.1763], ([11.154621 32.127758]) \t loss 1.069E+00\n",
      "step 1830000 \t return [-6502.9756 -7499.7437], ([13.21877 36.69157]) \t loss 8.363E-01\n",
      "step 1840000 \t return [-6508.089  -7500.8223], ([15.880081 40.93619 ]) \t loss 1.002E+00\n",
      "step 1850000 \t return [-6503.588 -7498.175], ([14.139677 37.919857]) \t loss 1.768E+00\n",
      "step 1860000 \t return [-6507.632  -7504.1426], ([14.944563 37.863895]) \t loss 1.599E+00\n",
      "step 1870000 \t return [-6502.7505 -7492.239 ], ([14.16783  38.381477]) \t loss 1.557E+00\n",
      "step 1880000 \t return [-6506.4844 -7494.478 ], ([14.425958 37.4221  ]) \t loss 1.080E+00\n",
      "step 1890000 \t return [-6515.602 -7508.638], ([18.011333 43.357082]) \t loss 1.843E+00\n",
      "step 1900000 \t return [-6519.358 -7511.92 ], ([16.902447 38.032257]) \t loss 2.642E+00\n",
      "step 1910000 \t return [-6530.701  -7524.9443], ([19.408546 40.9308  ]) \t loss 4.200E+00\n",
      "step 1920000 \t return [-6512.044 -7502.308], ([17.136002 40.39248 ]) \t loss 3.811E+00\n",
      "step 1930000 \t return [-6508.5874 -7501.507 ], ([16.919271 41.996693]) \t loss 3.223E+00\n",
      "step 1940000 \t return [-6523.1577 -7508.096 ], ([18.585997 39.440987]) \t loss 1.866E+00\n",
      "step 1950000 \t return [-6510.7437 -7501.984 ], ([16.20238  39.447105]) \t loss 3.397E+00\n",
      "step 1960000 \t return [-6511.303  -7500.0386], ([16.169048 37.78802 ]) \t loss 1.952E+00\n",
      "step 1970000 \t return [-6514.3813 -7507.7256], ([17.337051 39.156452]) \t loss 2.793E+00\n",
      "step 1980000 \t return [-6502.107  -7494.5264], ([11.01069  32.431885]) \t loss 4.535E+00\n",
      "step 1990000 \t return [-6503.7373 -7495.1934], ([12.622547 37.089027]) \t loss 9.995E-01\n",
      "step 2000000 \t return [-6505.1733 -7497.4375], ([11.889241 33.44384 ]) \t loss 1.034E+00\n",
      "step 2010000 \t return [-6501.6807 -7491.3965], ([11.306582 33.847145]) \t loss 8.987E-01\n",
      "step 2020000 \t return [-6505.386  -7496.5464], ([12.077194 34.348812]) \t loss 9.060E-01\n",
      "step 2030000 \t return [-6512.177 -7506.26 ], ([12.505701 33.341766]) \t loss 1.015E+00\n",
      "step 2040000 \t return [-6509.7124 -7501.9023], ([12.910195 34.83595 ]) \t loss 1.949E+00\n",
      "step 2050000 \t return [-6508.4155 -7500.895 ], ([14.858909 40.578255]) \t loss 1.162E+00\n",
      "step 2060000 \t return [-6503.87  -7494.791], ([12.227398 34.5757  ]) \t loss 1.843E+00\n",
      "step 2070000 \t return [-6507.906 -7497.16 ], ([14.289039 38.87946 ]) \t loss 1.371E+00\n",
      "step 2080000 \t return [-6506.6357 -7497.8057], ([14.222263 38.83162 ]) \t loss 1.391E+00\n",
      "step 2090000 \t return [-6523.844 -7516.622], ([16.156124 37.70894 ]) \t loss 1.995E+00\n",
      "step 2100000 \t return [-6501.3477 -7493.2246], ([12.287364 36.942097]) \t loss 4.195E+00\n",
      "step 2110000 \t return [-6503.8174 -7493.2583], ([11.702057 34.042423]) \t loss 1.481E+00\n",
      "step 2120000 \t return [-6502.352  -7493.7104], ([11.132228 32.494617]) \t loss 1.734E+00\n",
      "step 2130000 \t return [-6504.7695 -7494.569 ], ([13.371101 38.37273 ]) \t loss 2.193E+00\n",
      "step 2140000 \t return [-6502.136  -7491.3237], ([12.446485 37.10086 ]) \t loss 1.619E+00\n",
      "step 2150000 \t return [-6498.523 -7490.018], ([11.633091 37.28346 ]) \t loss 1.184E+00\n",
      "step 2160000 \t return [-6500.8994 -7490.1177], ([11.960324 35.961414]) \t loss 1.232E+00\n",
      "step 2170000 \t return [-6510.837 -7506.306], ([13.356546 37.067703]) \t loss 1.356E+00\n",
      "step 2180000 \t return [-6519.5605 -7516.5605], ([13.689285 34.88315 ]) \t loss 1.711E+00\n",
      "step 2190000 \t return [-6519.4575 -7509.4014], ([15.134346 37.233147]) \t loss 1.831E+00\n",
      "step 2200000 \t return [-6511.0337 -7503.2344], ([13.064642 34.3833  ]) \t loss 2.874E+00\n",
      "step 2210000 \t return [-6517.659  -7510.3345], ([14.36724 37.0681 ]) \t loss 1.617E+00\n",
      "step 2220000 \t return [-6503.8633 -7496.824 ], ([12.121261 34.599506]) \t loss 2.631E+00\n",
      "step 2230000 \t return [-6503.5176 -7495.3374], ([12.212254 35.85431 ]) \t loss 1.348E+00\n",
      "step 2240000 \t return [-6507.5337 -7499.0024], ([12.35227  33.686127]) \t loss 1.001E+00\n",
      "step 2250000 \t return [-6510.608  -7503.6514], ([14.36133  38.699432]) \t loss 1.389E+00\n",
      "step 2260000 \t return [-6511.377 -7503.105], ([12.259276 32.829304]) \t loss 1.667E+00\n",
      "step 2270000 \t return [-6509.61  -7505.116], ([12.985894 35.248974]) \t loss 2.258E+00\n",
      "step 2280000 \t return [-6510.967  -7502.3735], ([12.899273 34.05949 ]) \t loss 1.061E+00\n",
      "step 2290000 \t return [-6516.227 -7511.683], ([14.524096 36.69231 ]) \t loss 2.060E+00\n",
      "step 2300000 \t return [-6511.112 -7509.323], ([13.237764 36.482025]) \t loss 1.867E+00\n",
      "step 2310000 \t return [-6516.092  -7506.8164], ([15.720088 39.34483 ]) \t loss 1.515E+00\n",
      "step 2320000 \t return [-6516.9277 -7512.8164], ([13.937838 35.129604]) \t loss 2.201E+00\n",
      "step 2330000 \t return [-6512.0024 -7502.5005], ([13.802595 34.696205]) \t loss 2.986E+00\n",
      "step 2340000 \t return [-6512.6    -7505.0015], ([13.589536 35.026634]) \t loss 2.922E+00\n",
      "step 2350000 \t return [-6518.835 -7508.676], ([15.1847   36.514996]) \t loss 2.202E+00\n",
      "step 2360000 \t return [-6520.272 -7511.683], ([15.463039 37.237724]) \t loss 4.596E+00\n",
      "step 2370000 \t return [-6510.2896 -7502.5186], ([13.931176 36.826424]) \t loss 2.936E+00\n",
      "step 2380000 \t return [-6512.074 -7503.477], ([14.562034 37.314182]) \t loss 1.493E+00\n",
      "step 2390000 \t return [-6517.168  -7513.1064], ([14.855402 36.89778 ]) \t loss 1.806E+00\n",
      "step 2400000 \t return [-6508.0645 -7501.058 ], ([12.951679 34.38688 ]) \t loss 3.087E+00\n",
      "step 2410000 \t return [-6509.2925 -7500.9175], ([14.236064 36.862194]) \t loss 1.450E+00\n",
      "step 2420000 \t return [-6508.3223 -7501.7925], ([13.471402 35.9862  ]) \t loss 1.721E+00\n",
      "step 2430000 \t return [-6519.7793 -7511.5854], ([17.141329 40.298225]) \t loss 1.443E+00\n",
      "step 2440000 \t return [-6518.4814 -7509.2725], ([15.553475 37.38366 ]) \t loss 3.276E+00\n",
      "step 2450000 \t return [-6510.0996 -7504.81  ], ([14.494089 37.51201 ]) \t loss 2.741E+00\n",
      "step 2460000 \t return [-6519.187 -7512.397], ([15.657591 37.525135]) \t loss 2.779E+00\n",
      "step 2470000 \t return [-6505.681 -7499.245], ([12.263127 32.674763]) \t loss 2.003E+00\n",
      "step 2480000 \t return [-6514.17   -7504.6167], ([14.942629 36.58233 ]) \t loss 1.211E+00\n",
      "step 2490000 \t return [-6513.0977 -7503.107 ], ([16.062914 39.676823]) \t loss 1.521E+00\n",
      "step 2500000 \t return [-6513.608  -7505.7573], ([17.976719 44.97792 ]) \t loss 1.670E+00\n",
      "step 2510000 \t return [-6516.2856 -7512.6006], ([16.061764 39.807262]) \t loss 1.368E+00\n",
      "step 2520000 \t return [-6508.089 -7501.053], ([13.563314 35.74713 ]) \t loss 2.550E+00\n",
      "step 2530000 \t return [-6514.227 -7503.176], ([14.685438 35.769684]) \t loss 1.068E+00\n",
      "step 2540000 \t return [-6517.9956 -7509.1226], ([16.106651 37.76503 ]) \t loss 1.345E+00\n",
      "step 2550000 \t return [-6519.734 -7512.523], ([15.892793 36.90534 ]) \t loss 2.313E+00\n",
      "step 2560000 \t return [-6519.6655 -7512.5337], ([16.405586 37.608795]) \t loss 4.428E+00\n",
      "step 2570000 \t return [-6519.9287 -7516.625 ], ([17.952045 42.373676]) \t loss 4.201E+00\n",
      "step 2580000 \t return [-6512.119 -7504.335], ([15.384623 37.823563]) \t loss 5.240E+00\n",
      "step 2590000 \t return [-6519.5596 -7514.1025], ([15.276145 36.101734]) \t loss 3.055E+00\n",
      "step 2600000 \t return [-6507.0405 -7494.576 ], ([13.871841 35.90704 ]) \t loss 2.377E+00\n",
      "step 2610000 \t return [-6509.473 -7501.641], ([14.856841 38.173126]) \t loss 1.301E+00\n",
      "step 2620000 \t return [-6512.361 -7500.92 ], ([14.432266 35.435535]) \t loss 1.483E+00\n",
      "step 2630000 \t return [-6518.77   -7513.5337], ([15.461761 36.26927 ]) \t loss 1.459E+00\n",
      "step 2640000 \t return [-6517.938 -7508.687], ([15.132643 35.310448]) \t loss 2.871E+00\n",
      "step 2650000 \t return [-6523.302 -7515.48 ], ([16.270393 36.485004]) \t loss 2.499E+00\n",
      "step 2660000 \t return [-6517.4814 -7506.0083], ([18.066395 41.54064 ]) \t loss 4.379E+00\n",
      "step 2670000 \t return [-6520.154  -7513.9326], ([17.10011 38.35885]) \t loss 7.482E+00\n",
      "step 2680000 \t return [-6526.3345 -7520.0713], ([20.500826 44.547607]) \t loss 3.680E+00\n",
      "step 2690000 \t return [-6523.7812 -7515.252 ], ([18.249056 39.338886]) \t loss 2.441E+00\n",
      "step 2700000 \t return [-6523.875 -7517.98 ], ([18.846403 41.834816]) \t loss 4.288E+00\n",
      "step 2710000 \t return [-6530.925  -7527.5474], ([21.063702 45.7082  ]) \t loss 5.720E+00\n",
      "step 2720000 \t return [-6523.6675 -7513.6504], ([16.777288 37.027294]) \t loss 3.936E+00\n",
      "step 2730000 \t return [-6525.256  -7515.7764], ([18.191051 39.089962]) \t loss 3.676E+00\n",
      "step 2740000 \t return [-6528.3643 -7519.2773], ([21.72194  46.521824]) \t loss 4.502E+00\n",
      "step 2750000 \t return [-6516.541 -7504.389], ([17.85884 41.02813]) \t loss 6.397E+00\n",
      "step 2760000 \t return [-6524.6494 -7517.294 ], ([18.627495 41.63788 ]) \t loss 5.151E+00\n",
      "step 2770000 \t return [-6516.337  -7509.0933], ([17.81558 42.693  ]) \t loss 4.903E+00\n",
      "step 2780000 \t return [-6518.658  -7509.5986], ([17.072876 38.958305]) \t loss 3.066E+00\n",
      "step 2790000 \t return [-6522.2407 -7515.0425], ([16.356699 36.22017 ]) \t loss 2.435E+00\n",
      "step 2800000 \t return [-6526.8687 -7523.381 ], ([21.01997 46.20813]) \t loss 2.460E+00\n",
      "step 2810000 \t return [-6521.1274 -7510.6523], ([18.54489  40.794857]) \t loss 4.271E+00\n",
      "step 2820000 \t return [-6531.2705 -7524.741 ], ([21.826303 45.742863]) \t loss 4.082E+00\n",
      "step 2830000 \t return [-6518.9277 -7507.409 ], ([21.357943 47.644955]) \t loss 7.513E+00\n",
      "step 2840000 \t return [-6537.96   -7526.4595], ([26.108912 50.31463 ]) \t loss 5.240E+00\n",
      "step 2850000 \t return [-6535.4727 -7529.035 ], ([21.419157 44.06598 ]) \t loss 1.307E+01\n",
      "step 2860000 \t return [-6525.042  -7515.3096], ([19.17123 39.81617]) \t loss 1.009E+01\n",
      "step 2870000 \t return [-6535.547 -7529.254], ([22.155556 44.712994]) \t loss 9.086E+00\n",
      "step 2880000 \t return [-6538.291  -7528.8706], ([23.332918 45.37977 ]) \t loss 1.056E+01\n",
      "step 2890000 \t return [-6531.984  -7526.9214], ([20.630857 42.3244  ]) \t loss 1.172E+01\n",
      "step 2900000 \t return [-6541.8774 -7533.476 ], ([26.529022 51.731197]) \t loss 7.076E+00\n",
      "step 2910000 \t return [-6535.0186 -7525.772 ], ([24.33057  48.237446]) \t loss 7.011E+00\n",
      "step 2920000 \t return [-6534.549 -7523.588], ([18.733896 37.16503 ]) \t loss 5.909E+00\n",
      "step 2930000 \t return [-6528.5693 -7519.4673], ([23.069214 46.88656 ]) \t loss 8.762E+00\n",
      "step 2940000 \t return [-6528.0293 -7519.015 ], ([23.150772 46.97898 ]) \t loss 6.517E+00\n",
      "step 2950000 \t return [-6531.025  -7521.4775], ([19.068275 37.659782]) \t loss 3.436E+00\n",
      "step 2960000 \t return [-6517.325 -7507.156], ([19.205786 40.87365 ]) \t loss 7.143E+00\n",
      "step 2970000 \t return [-6523.091  -7514.2725], ([19.692629 41.496773]) \t loss 6.025E+00\n",
      "step 2980000 \t return [-6528.0293 -7524.7183], ([23.517908 48.2772  ]) \t loss 5.476E+00\n",
      "step 2990000 \t return [-6529.1816 -7522.2056], ([24.906963 48.4158  ]) \t loss 6.394E+00\n",
      "step 3000000 \t return [-6528.427  -7521.8345], ([23.694098 45.83365 ]) \t loss 7.759E+00\n",
      "step 3010000 \t return [-6536.382  -7529.7485], ([27.154053 51.477737]) \t loss 4.560E+00\n",
      "step 3020000 \t return [-6539.92 -7530.08], ([29.916626 53.90291 ]) \t loss 7.504E+00\n",
      "step 3030000 \t return [-6558.2627 -7549.194 ], ([28.30119 48.78086]) \t loss 9.457E+00\n",
      "step 3040000 \t return [-6546.933 -7541.045], ([27.09489  48.740955]) \t loss 1.301E+01\n",
      "step 3050000 \t return [-6545.022 -7538.935], ([30.052042 53.45796 ]) \t loss 1.570E+01\n",
      "step 3060000 \t return [-6550.3706 -7544.1875], ([27.444567 48.00745 ]) \t loss 1.153E+01\n",
      "step 3070000 \t return [-6536.411  -7527.4785], ([31.155365 57.0461  ]) \t loss 7.459E+00\n",
      "step 3080000 \t return [-6538.3726 -7534.714 ], ([30.001545 54.459293]) \t loss 5.440E+00\n",
      "step 3090000 \t return [-6540.259 -7531.09 ], ([29.008265 53.02679 ]) \t loss 6.661E+00\n",
      "step 3100000 \t return [-6532.6514 -7523.3506], ([25.17461  47.709232]) \t loss 9.125E+00\n",
      "step 3110000 \t return [-6520.7437 -7511.096 ], ([20.22096  41.244316]) \t loss 7.944E+00\n",
      "step 3120000 \t return [-6531.21   -7519.4814], ([22.771408 43.558796]) \t loss 5.967E+00\n",
      "step 3130000 \t return [-6561.0225 -7554.5986], ([30.652597 52.308666]) \t loss 6.793E+00\n",
      "step 3140000 \t return [-6533.273  -7523.5215], ([22.025827 42.695198]) \t loss 1.613E+01\n",
      "step 3150000 \t return [-6531.8745 -7522.993 ], ([24.628653 47.16979 ]) \t loss 9.364E+00\n",
      "step 3160000 \t return [-6548.5986 -7542.8013], ([29.884407 54.37551 ]) \t loss 8.704E+00\n",
      "step 3170000 \t return [-6545.909 -7536.237], ([27.527344 48.683956]) \t loss 1.229E+01\n",
      "step 3180000 \t return [-6530.8364 -7524.38  ], ([26.10727  49.111725]) \t loss 1.266E+01\n",
      "step 3190000 \t return [-6529.108  -7520.8687], ([24.663757 46.744514]) \t loss 1.118E+01\n",
      "step 3200000 \t return [-6541.7227 -7536.3345], ([28.31471  50.870308]) \t loss 7.531E+00\n",
      "step 3210000 \t return [-6536.0767 -7523.9043], ([24.978477 46.33568 ]) \t loss 1.143E+01\n",
      "step 3220000 \t return [-6550.7485 -7540.6816], ([27.280796 49.20982 ]) \t loss 1.027E+01\n",
      "step 3230000 \t return [-6542.898 -7536.266], ([26.173014 47.430893]) \t loss 1.231E+01\n",
      "step 3240000 \t return [-6530.723 -7519.196], ([29.337719 54.607372]) \t loss 9.280E+00\n",
      "step 3250000 \t return [-6538.766  -7528.4937], ([27.434748 49.960327]) \t loss 7.891E+00\n",
      "step 3260000 \t return [-6533.8394 -7524.143 ], ([25.579329 47.021328]) \t loss 1.061E+01\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-af7cd74c2e09>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mPCNAgent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_desired_return_and_horizon\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdesired_return\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdesired_return\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdesired_horizon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mPCNAgent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10000000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m100.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_step_episodes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax_return\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_return\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\liamm\\anaconda3\\lib\\site-packages\\morl_baselines\\multi_policy\\pcn\\pcn.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, total_timesteps, eval_env, ref_point, known_pareto_front, num_er_episodes, num_step_episodes, num_model_updates, max_return, max_buffer_size)\u001b[0m\n\u001b[0;32m    421\u001b[0m             \u001b[0mhorizons\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    422\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_step_episodes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 423\u001b[1;33m                 \u001b[0mtransitions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_episode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdesired_return\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdesired_horizon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_return\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    424\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_step\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransitions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    425\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_add_episode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransitions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_buffer_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\liamm\\anaconda3\\lib\\site-packages\\morl_baselines\\multi_policy\\pcn\\pcn.py\u001b[0m in \u001b[0;36m_run_episode\u001b[1;34m(self, env, desired_return, desired_horizon, max_return)\u001b[0m\n\u001b[0;32m    294\u001b[0m         \u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 296\u001b[1;33m             \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_act\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdesired_return\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdesired_horizon\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    297\u001b[0m             \u001b[0mn_obs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m             \u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mterminated\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\liamm\\anaconda3\\lib\\site-packages\\morl_baselines\\multi_policy\\pcn\\pcn.py\u001b[0m in \u001b[0;36m_act\u001b[1;34m(self, obs, desired_return, desired_horizon)\u001b[0m\n\u001b[0;32m    276\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    277\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_act\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdesired_return\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdesired_horizon\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 278\u001b[1;33m         log_probs = self.model(\n\u001b[0m\u001b[0;32m    279\u001b[0m             \u001b[0mth\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    280\u001b[0m             \u001b[0mth\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdesired_return\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\liamm\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1191\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\liamm\\anaconda3\\lib\\site-packages\\morl_baselines\\multi_policy\\pcn\\pcn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, state, desired_return, desired_horizon)\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_emb\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m         \u001b[1;31m# element-wise multiplication of state-embedding and command\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m         \u001b[0mlog_prob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlog_prob\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\liamm\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1191\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\liamm\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    202\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    205\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\liamm\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1191\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\liamm\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = mo_gym.make('water-reservoir-v0', normalized_action=False, nO=2, penalize=True, time_limit=100)\n",
    "\n",
    "PCNAgent = PCN(env, np.array((0.1, 0.1, 0.1), dtype=np.float32), continuous_actions=True)\n",
    "\n",
    "max_return = np.zeros(2)\n",
    "\n",
    "desired_return = np.array((-20.0, -630.0))\n",
    "PCNAgent.set_desired_return_and_horizon(desired_return=desired_return, desired_horizon=0)\n",
    "\n",
    "PCNAgent.train(10000000, env, np.array((-100.0,-1000), dtype=np.float32), num_step_episodes=100,max_return=max_return)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopped training at 30% mark because no improvement is seen..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pareto Conditioned networks\n",
    "\n",
    "Using the modified dam environment (less variance in the inflow), test PCN. This modifications is necessary because PCN expects deterministic transitions.\n",
    "\n",
    "Also a modification to the original PCN code in morl-baselines is necessary in order to support the continuous actions of the environment:\n",
    "- continuous_actions is a parameter given to the algorithm\n",
    "- During training, actions are no longer sampled from a categorical distribution as in the [original paper](https://www.ifaamas.org/Proceedings/aamas2022/pdfs/p1110.pdf). This might hinder exploration...\n",
    "- The logsoftmax function of the original model is no longer used, instead its output is a single value corresponding to an action\n",
    "- The loss used in the model is now MSE instead of CE (regression loss vs classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium.core import Env\n",
    "import mo_gymnasium as mo_gym\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from morl_baselines.multi_policy.pcn.pcn import PCN\n",
    "from gymnasium.utils import EzPickle\n",
    "\n",
    "\"\"\"\n",
    "Helper function to plot pf\n",
    "\"\"\"\n",
    "def plot_pf(file):\n",
    "    columns = [\"objective_1\", \"objective_2\"]\n",
    "    df = pd.read_csv(file, usecols=columns)\n",
    "\n",
    "    plt.plot(df.objective_1, df.objective_2, 'o')\n",
    "    plt.xlabel('Cost due to excess water level wrt flooding threshold upstream')\n",
    "    plt.ylabel('Deficit in water supply wrt demand')\n",
    "    plt.show()\n",
    "\n",
    "#\n",
    "\"\"\"\n",
    "Scale rewards down to improve PCN's performance\n",
    "\"\"\"\n",
    "class ScaleReward(gym.RewardWrapper, EzPickle):\n",
    "    def __init__(self, env: Env):\n",
    "        super().__init__(env)\n",
    "        EzPickle.__init__(env)\n",
    "\n",
    "    def reward(self, reward):\n",
    "        reward = (reward / 100.0)\n",
    "        return reward"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First attempt\n",
    "\n",
    "- Dam inflow: mean = 40.0 and stdev = 1.0 \n",
    "- Actions capped between [0.0;250.0] (motivation in gpi-ls notebook)\n",
    "- Scaling factor for desired horizon/return = [0.1, 0.1]\n",
    "- Hypervolume ref point = [-100, -1000]\n",
    "- Random noise: X~(0;5.0) Noise is added directly to action.\n",
    "\n",
    "Also, actions are scaled down with factor 100 because PCN is sensitive to the range of actions..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:945fleok) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">water-reservoir-v0__PCN__None__1683884788</strong> at: <a href='https://wandb.ai/vub-ai/MORL-Baselines/runs/945fleok' target=\"_blank\">https://wandb.ai/vub-ai/MORL-Baselines/runs/945fleok</a><br/>Synced 7 W&B file(s), 2 media file(s), 2 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230512_114631-945fleok\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:945fleok). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f437587e5fb0460bb1c7970ad6221443",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01693333333338766, max=1.0)â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\liamm\\water-resource-management\\PCN\\wandb\\run-20230512_115046-36t8gmck</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/vub-ai/MORL-Baselines/runs/36t8gmck' target=\"_blank\">water-reservoir-v0__PCN__None__1683885046</a></strong> to <a href='https://wandb.ai/vub-ai/MORL-Baselines' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/vub-ai/MORL-Baselines' target=\"_blank\">https://wandb.ai/vub-ai/MORL-Baselines</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/vub-ai/MORL-Baselines/runs/36t8gmck' target=\"_blank\">https://wandb.ai/vub-ai/MORL-Baselines/runs/36t8gmck</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Found log directory outside of given root_logdir, dropping given root_logdir for event file in /tmp/PCN\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 60000 \t return [-82.26866 -92.20118], ([0.54408187 0.6290396 ]) \t loss 8.769E+03\n",
      "step 70000 \t return [-75.48533 -85.41563], ([0.5414608  0.70820844]) \t loss 4.207E+03\n",
      "step 80000 \t return [-77.73574 -87.67751], ([0.5131005 0.6531166]) \t loss 3.062E+03\n",
      "step 90000 \t return [-74.35394 -84.27745], ([0.43701044 0.5445091 ]) \t loss 2.178E+03\n",
      "step 100000 \t return [-74.99455 -84.91675], ([0.4653714  0.57966554]) \t loss 1.479E+03\n",
      "step 110000 \t return [-70.18356 -80.09503], ([0.36434454 0.5328669 ]) \t loss 1.028E+03\n",
      "step 120000 \t return [-72.50139 -82.44405], ([0.43516037 0.56056863]) \t loss 4.680E+02\n",
      "step 130000 \t return [-71.97411 -81.9075 ], ([0.4072029 0.5142844]) \t loss 3.648E+02\n",
      "step 140000 \t return [-71.36834 -81.30413], ([0.3687014  0.46494484]) \t loss 2.884E+02\n",
      "step 150000 \t return [-68.200485 -78.16499 ], ([0.2952062 0.4498387]) \t loss 2.678E+02\n",
      "step 160000 \t return [-67.69813  -77.617485], ([0.28538334 0.37763762]) \t loss 1.725E+02\n",
      "step 170000 \t return [-68.75096 -78.64478], ([0.30835903 0.44247094]) \t loss 1.263E+02\n",
      "step 180000 \t return [-68.80709 -78.76581], ([0.27858108 0.38858253]) \t loss 8.228E+01\n",
      "step 190000 \t return [-67.73604 -77.6855 ], ([0.30087698 0.42158175]) \t loss 4.699E+01\n",
      "step 200000 \t return [-67.43521 -77.37225], ([0.3306705  0.44646665]) \t loss 3.208E+01\n",
      "step 210000 \t return [-67.48959 -77.40315], ([0.28759617 0.41088992]) \t loss 2.629E+01\n",
      "step 220000 \t return [-67.112404 -77.03537 ], ([0.3180332  0.45150715]) \t loss 2.481E+01\n",
      "step 230000 \t return [-66.979   -76.86838], ([0.29041228 0.395005  ]) \t loss 2.514E+01\n",
      "step 240000 \t return [-66.920494 -76.85382 ], ([0.33106416 0.43439448]) \t loss 2.601E+01\n",
      "step 250000 \t return [-67.36074 -77.26793], ([0.28862312 0.43353605]) \t loss 2.563E+01\n",
      "step 260000 \t return [-66.77646  -76.724686], ([0.2494196  0.36209282]) \t loss 2.598E+01\n",
      "step 270000 \t return [-67.02495 -76.91251], ([0.27895907 0.40075633]) \t loss 2.514E+01\n",
      "step 280000 \t return [-66.111984 -76.043274], ([0.26729804 0.36114293]) \t loss 2.604E+01\n",
      "step 290000 \t return [-66.11423 -76.01558], ([0.30288684 0.4323739 ]) \t loss 2.546E+01\n",
      "step 300000 \t return [-65.83922 -75.71489], ([0.25574347 0.36298737]) \t loss 2.659E+01\n",
      "step 310000 \t return [-65.72376 -75.65153], ([0.2866806  0.39280885]) \t loss 2.692E+01\n",
      "step 320000 \t return [-65.64738 -75.57706], ([0.24111962 0.34992334]) \t loss 2.501E+01\n",
      "step 330000 \t return [-64.97246 -74.86764], ([0.26172644 0.41026783]) \t loss 2.619E+01\n",
      "step 340000 \t return [-64.99722 -74.93995], ([0.32090318 0.42046514]) \t loss 2.543E+01\n",
      "step 350000 \t return [-64.65781 -74.57018], ([0.2689616  0.40238753]) \t loss 2.545E+01\n",
      "step 360000 \t return [-64.540306 -74.434074], ([0.22344562 0.3337848 ]) \t loss 2.607E+01\n",
      "step 370000 \t return [-64.46595 -74.38199], ([0.24307361 0.38063824]) \t loss 2.613E+01\n",
      "step 380000 \t return [-64.6964  -74.66571], ([0.27200398 0.3798049 ]) \t loss 2.534E+01\n",
      "step 390000 \t return [-64.42585  -74.347336], ([0.27227414 0.4125419 ]) \t loss 2.607E+01\n",
      "step 400000 \t return [-63.814064 -73.72607 ], ([0.27790737 0.42132673]) \t loss 2.596E+01\n",
      "step 410000 \t return [-63.350777 -73.26365 ], ([0.2352181  0.35660276]) \t loss 2.486E+01\n",
      "step 420000 \t return [-62.739136 -72.64884 ], ([0.24868552 0.3769937 ]) \t loss 2.686E+01\n",
      "step 430000 \t return [-62.33011 -72.24665], ([0.24822003 0.35990363]) \t loss 2.605E+01\n",
      "step 440000 \t return [-62.555286 -72.45976 ], ([0.2611362  0.40222317]) \t loss 2.660E+01\n",
      "step 450000 \t return [-61.432236 -71.30055 ], ([0.2815083  0.39256674]) \t loss 2.715E+01\n",
      "step 460000 \t return [-61.750195 -71.68011 ], ([0.24165334 0.35195148]) \t loss 2.725E+01\n",
      "step 470000 \t return [-61.013283 -70.9219  ], ([0.23931795 0.3807856 ]) \t loss 2.685E+01\n",
      "step 480000 \t return [-61.207695 -71.05305 ], ([0.23660888 0.34094515]) \t loss 2.639E+01\n",
      "step 490000 \t return [-61.145893 -71.08588 ], ([0.24336925 0.3846117 ]) \t loss 2.705E+01\n",
      "step 500000 \t return [-60.90431 -70.86216], ([0.25672236 0.36853626]) \t loss 2.584E+01\n",
      "step 510000 \t return [-61.689003 -71.596146], ([0.23708738 0.2769932 ]) \t loss 2.734E+01\n",
      "step 520000 \t return [-60.590042 -70.493805], ([0.21799234 0.33593068]) \t loss 2.901E+01\n",
      "step 530000 \t return [-60.21028 -70.11579], ([0.23314276 0.35044828]) \t loss 2.612E+01\n",
      "step 540000 \t return [-60.5628 -70.4562], ([0.20861192 0.3512106 ]) \t loss 2.718E+01\n",
      "step 550000 \t return [-60.008385 -69.93503 ], ([0.25068933 0.33475974]) \t loss 2.704E+01\n",
      "step 560000 \t return [-60.257412 -70.15381 ], ([0.24643625 0.35584748]) \t loss 2.651E+01\n",
      "step 570000 \t return [-59.757042 -69.71036 ], ([0.18549137 0.2992925 ]) \t loss 2.490E+01\n",
      "step 580000 \t return [-59.04188 -68.94754], ([0.21894751 0.33561924]) \t loss 2.610E+01\n",
      "step 590000 \t return [-59.411877 -69.33235 ], ([0.24066435 0.3349784 ]) \t loss 2.737E+01\n",
      "step 600000 \t return [-59.34901 -69.27464], ([0.23007263 0.32735175]) \t loss 2.540E+01\n",
      "step 610000 \t return [-58.595448 -68.51647 ], ([0.19802581 0.3125198 ]) \t loss 2.676E+01\n",
      "step 620000 \t return [-58.383446 -68.27425 ], ([0.19518381 0.29826322]) \t loss 2.612E+01\n",
      "step 630000 \t return [-57.945435 -67.86907 ], ([0.21059588 0.29997376]) \t loss 2.567E+01\n",
      "step 640000 \t return [-57.51093  -67.447586], ([0.21689427 0.30283955]) \t loss 2.705E+01\n",
      "step 650000 \t return [-57.278847 -67.20563 ], ([0.19640337 0.33700216]) \t loss 2.615E+01\n",
      "step 660000 \t return [-57.258053 -67.14672 ], ([0.22132313 0.3151734 ]) \t loss 2.684E+01\n",
      "step 670000 \t return [-57.998417 -67.92436 ], ([0.17766163 0.29210833]) \t loss 2.876E+01\n",
      "step 680000 \t return [-57.169975 -67.039536], ([0.2227581  0.34124467]) \t loss 2.773E+01\n",
      "step 690000 \t return [-56.991302 -66.921364], ([0.22719498 0.28552967]) \t loss 2.676E+01\n",
      "step 700000 \t return [-56.996655 -66.8928  ], ([0.19268905 0.27103046]) \t loss 2.750E+01\n",
      "step 710000 \t return [-57.32378 -67.2563 ], ([0.20921446 0.3225305 ]) \t loss 2.678E+01\n",
      "step 720000 \t return [-56.707573 -66.61517 ], ([0.19483437 0.28960848]) \t loss 2.577E+01\n",
      "step 730000 \t return [-56.809956 -66.67817 ], ([0.1945681  0.28318918]) \t loss 2.804E+01\n",
      "step 740000 \t return [-57.176582 -67.114265], ([0.20078678 0.29672757]) \t loss 2.641E+01\n",
      "step 750000 \t return [-56.918564 -66.83646 ], ([0.21241672 0.34854764]) \t loss 2.859E+01\n",
      "step 760000 \t return [-55.95058 -65.82183], ([0.19765593 0.29340208]) \t loss 2.685E+01\n",
      "step 770000 \t return [-55.655514 -65.594345], ([0.2148086  0.30142206]) \t loss 2.645E+01\n",
      "step 780000 \t return [-56.306473 -66.17569 ], ([0.2085941 0.3133083]) \t loss 2.679E+01\n",
      "step 790000 \t return [-56.09867 -65.99385], ([0.21531828 0.29520363]) \t loss 2.697E+01\n",
      "step 800000 \t return [-55.60769 -65.50557], ([0.19679306 0.32420006]) \t loss 2.801E+01\n",
      "step 810000 \t return [-55.51856 -65.43669], ([0.16813007 0.26892823]) \t loss 2.750E+01\n",
      "step 820000 \t return [-55.637386 -65.53525 ], ([0.18442592 0.261321  ]) \t loss 2.813E+01\n",
      "step 830000 \t return [-55.404346 -65.3093  ], ([0.1812198  0.33035752]) \t loss 2.703E+01\n",
      "step 840000 \t return [-55.214798 -65.13144 ], ([0.20520411 0.26733574]) \t loss 2.729E+01\n",
      "step 850000 \t return [-54.98118  -64.899345], ([0.15711246 0.30732834]) \t loss 2.699E+01\n",
      "step 860000 \t return [-55.342087 -65.247406], ([0.19556262 0.27432337]) \t loss 2.649E+01\n",
      "step 870000 \t return [-54.951595 -64.830444], ([0.17627725 0.2827973 ]) \t loss 2.629E+01\n",
      "step 880000 \t return [-55.26326 -65.19872], ([0.19268772 0.31209642]) \t loss 2.563E+01\n",
      "step 890000 \t return [-55.287907 -65.184364], ([0.19798048 0.28831798]) \t loss 2.704E+01\n",
      "step 900000 \t return [-55.29208  -65.208435], ([0.19165833 0.31394395]) \t loss 2.910E+01\n",
      "step 910000 \t return [-54.658737 -64.60145 ], ([0.16412327 0.27199665]) \t loss 2.699E+01\n",
      "step 920000 \t return [-54.79583 -64.70003], ([0.18434528 0.26691183]) \t loss 2.664E+01\n",
      "step 930000 \t return [-54.73183 -64.62619], ([0.17745796 0.26804513]) \t loss 2.680E+01\n",
      "step 940000 \t return [-54.028706 -63.858383], ([0.19393972 0.27072448]) \t loss 2.652E+01\n",
      "step 950000 \t return [-53.926723 -63.830605], ([0.18318625 0.27426594]) \t loss 2.629E+01\n",
      "step 960000 \t return [-54.039707 -63.89274 ], ([0.18250038 0.26042724]) \t loss 2.761E+01\n",
      "step 970000 \t return [-54.002186 -63.897842], ([0.1572928 0.269554 ]) \t loss 2.730E+01\n",
      "step 980000 \t return [-54.03926 -63.94985], ([0.15817697 0.2528287 ]) \t loss 2.809E+01\n",
      "step 990000 \t return [-53.750084 -63.649746], ([0.18318015 0.26551643]) \t loss 2.667E+01\n",
      "step 1000000 \t return [-53.686558 -63.59499 ], ([0.17162813 0.2861498 ]) \t loss 2.597E+01\n",
      "step 1010000 \t return [-53.709835 -63.61146 ], ([0.1831467  0.31755474]) \t loss 2.736E+01\n",
      "step 1020000 \t return [-53.55078 -63.42309], ([0.16694419 0.24280521]) \t loss 2.709E+01\n",
      "step 1030000 \t return [-53.445923 -63.352184], ([0.16910024 0.2657936 ]) \t loss 2.743E+01\n",
      "step 1040000 \t return [-53.662357 -63.554848], ([0.16109176 0.25148392]) \t loss 2.766E+01\n",
      "step 1050000 \t return [-53.813564 -63.74105 ], ([0.18262602 0.2615019 ]) \t loss 2.664E+01\n",
      "step 1060000 \t return [-53.40797 -63.31151], ([0.17130262 0.25287762]) \t loss 2.880E+01\n",
      "step 1070000 \t return [-53.624653 -63.54524 ], ([0.18002218 0.2451886 ]) \t loss 2.738E+01\n",
      "step 1080000 \t return [-53.285435 -63.220856], ([0.17024983 0.26302686]) \t loss 2.685E+01\n",
      "step 1090000 \t return [-53.175117 -63.067448], ([0.17349647 0.24017555]) \t loss 2.686E+01\n",
      "step 1100000 \t return [-53.11217 -62.99367], ([0.18208994 0.28428906]) \t loss 2.693E+01\n",
      "step 1110000 \t return [-53.241165 -63.17419 ], ([0.18807694 0.22993737]) \t loss 2.649E+01\n",
      "step 1120000 \t return [-52.84858  -62.749813], ([0.16809987 0.24445927]) \t loss 2.631E+01\n",
      "step 1130000 \t return [-52.956932 -62.82731 ], ([0.16404009 0.23405224]) \t loss 2.749E+01\n",
      "step 1140000 \t return [-52.688873 -62.620697], ([0.15564303 0.25653297]) \t loss 2.650E+01\n",
      "step 1150000 \t return [-52.872417 -62.787308], ([0.16870506 0.25352547]) \t loss 2.659E+01\n",
      "step 1160000 \t return [-52.27835  -62.131454], ([0.18044867 0.25708112]) \t loss 2.752E+01\n",
      "step 1170000 \t return [-51.638966 -61.546528], ([0.181783  0.2723178]) \t loss 2.735E+01\n",
      "step 1180000 \t return [-51.757866 -61.68039 ], ([0.16892315 0.2680367 ]) \t loss 2.995E+01\n",
      "step 1190000 \t return [-51.8499   -61.754604], ([0.17335159 0.2579811 ]) \t loss 2.826E+01\n",
      "step 1200000 \t return [-51.203003 -61.078915], ([0.16711861 0.22846511]) \t loss 2.795E+01\n",
      "step 1210000 \t return [-50.948135 -60.84829 ], ([0.17621547 0.24961081]) \t loss 2.743E+01\n",
      "step 1220000 \t return [-50.864815 -60.74346 ], ([0.16400029 0.24483964]) \t loss 2.867E+01\n",
      "step 1230000 \t return [-50.39007  -60.315197], ([0.15547772 0.25665003]) \t loss 2.798E+01\n",
      "step 1240000 \t return [-50.611916 -60.468163], ([0.19776168 0.2672904 ]) \t loss 2.670E+01\n",
      "step 1250000 \t return [-50.178783 -60.066998], ([0.17901447 0.24733357]) \t loss 2.739E+01\n",
      "step 1260000 \t return [-50.222378 -60.11253 ], ([0.1719851  0.26410374]) \t loss 2.761E+01\n",
      "step 1270000 \t return [-50.469704 -60.396107], ([0.18433498 0.2931256 ]) \t loss 2.790E+01\n",
      "step 1280000 \t return [-50.090927 -59.9896  ], ([0.15578526 0.22894128]) \t loss 2.699E+01\n",
      "step 1290000 \t return [-49.78307  -59.660255], ([0.17368513 0.217817  ]) \t loss 2.694E+01\n",
      "step 1300000 \t return [-50.118496 -60.033485], ([0.16798046 0.2394885 ]) \t loss 2.591E+01\n",
      "step 1310000 \t return [-49.65207 -59.5297 ], ([0.16955766 0.23429003]) \t loss 2.690E+01\n",
      "step 1320000 \t return [-49.821136 -59.69551 ], ([0.21443094 0.25396016]) \t loss 2.623E+01\n",
      "step 1330000 \t return [-49.830746 -59.735546], ([0.17636912 0.21842639]) \t loss 2.811E+01\n",
      "step 1340000 \t return [-49.52559 -59.36681], ([0.18575579 0.24513522]) \t loss 2.683E+01\n",
      "step 1350000 \t return [-49.32267 -59.2305 ], ([0.18964823 0.25777152]) \t loss 2.666E+01\n",
      "step 1360000 \t return [-49.04613  -58.931187], ([0.1685734 0.2969382]) \t loss 2.620E+01\n",
      "step 1370000 \t return [-48.60316 -58.46284], ([0.17562938 0.26959735]) \t loss 2.578E+01\n",
      "step 1380000 \t return [-48.894863 -58.73711 ], ([0.19444579 0.22455646]) \t loss 2.675E+01\n",
      "step 1390000 \t return [-48.33049 -58.22709], ([0.15953861 0.28402996]) \t loss 2.749E+01\n",
      "step 1400000 \t return [-48.353863 -58.245464], ([0.16888466 0.2441144 ]) \t loss 2.753E+01\n",
      "step 1410000 \t return [-48.737553 -58.634457], ([0.172002   0.27414793]) \t loss 2.935E+01\n",
      "step 1420000 \t return [-48.75096  -58.671432], ([0.17512237 0.2651507 ]) \t loss 2.766E+01\n",
      "step 1430000 \t return [-47.973576 -57.81885 ], ([0.16256028 0.24232489]) \t loss 2.916E+01\n",
      "step 1440000 \t return [-48.175323 -58.07636 ], ([0.18402919 0.25807735]) \t loss 2.991E+01\n",
      "step 1450000 \t return [-47.84153  -57.701485], ([0.15580995 0.2602206 ]) \t loss 2.606E+01\n",
      "step 1460000 \t return [-47.759865 -57.661606], ([0.17083645 0.2501001 ]) \t loss 2.513E+01\n",
      "step 1470000 \t return [-47.44402 -57.33695], ([0.18431562 0.2680271 ]) \t loss 2.672E+01\n",
      "step 1480000 \t return [-47.59188  -57.484695], ([0.1872171  0.23493078]) \t loss 2.624E+01\n",
      "step 1490000 \t return [-47.167362 -57.030865], ([0.1575361  0.21437849]) \t loss 2.684E+01\n",
      "step 1500000 \t return [-47.11227 -56.99797], ([0.14997274 0.24244678]) \t loss 2.675E+01\n",
      "step 1510000 \t return [-46.963135 -56.878693], ([0.14747208 0.26805264]) \t loss 2.706E+01\n",
      "step 1520000 \t return [-46.74672  -56.591343], ([0.1681394  0.24466856]) \t loss 2.579E+01\n",
      "step 1530000 \t return [-46.633213 -56.52932 ], ([0.15075356 0.26583833]) \t loss 2.637E+01\n",
      "step 1540000 \t return [-46.625015 -56.460106], ([0.16054794 0.24135049]) \t loss 2.402E+01\n",
      "step 1550000 \t return [-46.178883 -56.049316], ([0.15826476 0.2392065 ]) \t loss 2.648E+01\n",
      "step 1560000 \t return [-46.330334 -56.187252], ([0.17901227 0.22264734]) \t loss 2.679E+01\n",
      "step 1570000 \t return [-46.055595 -55.89027 ], ([0.1667641  0.25306496]) \t loss 2.847E+01\n",
      "step 1580000 \t return [-46.165768 -56.066616], ([0.16977303 0.23491661]) \t loss 2.647E+01\n",
      "step 1590000 \t return [-46.350845 -56.17576 ], ([0.170638   0.27757403]) \t loss 2.630E+01\n",
      "step 1600000 \t return [-45.902454 -55.799175], ([0.14892566 0.24482846]) \t loss 2.862E+01\n",
      "step 1610000 \t return [-46.012787 -55.864857], ([0.17832096 0.22028945]) \t loss 2.633E+01\n",
      "step 1620000 \t return [-45.465336 -55.33051 ], ([0.15855782 0.26539204]) \t loss 2.870E+01\n",
      "step 1630000 \t return [-45.38285  -55.316387], ([0.1639209  0.24965963]) \t loss 2.896E+01\n",
      "step 1640000 \t return [-45.420544 -55.322037], ([0.19800335 0.21762691]) \t loss 2.782E+01\n",
      "step 1650000 \t return [-44.968678 -54.832386], ([0.15547185 0.26010776]) \t loss 2.885E+01\n",
      "step 1660000 \t return [-44.631084 -54.527576], ([0.19355932 0.26801792]) \t loss 2.999E+01\n",
      "step 1670000 \t return [-44.24995  -54.074066], ([0.16383202 0.25472233]) \t loss 2.903E+01\n",
      "step 1680000 \t return [-44.13643  -53.997528], ([0.19469032 0.20347786]) \t loss 2.884E+01\n",
      "step 1690000 \t return [-44.130783 -54.008656], ([0.18018217 0.2161951 ]) \t loss 2.692E+01\n",
      "step 1700000 \t return [-44.101254 -53.993484], ([0.18421614 0.25356707]) \t loss 2.748E+01\n",
      "step 1710000 \t return [-43.88376 -53.78476], ([0.1660134  0.21772358]) \t loss 2.567E+01\n",
      "step 1720000 \t return [-43.708683 -53.593594], ([0.16078477 0.23713373]) \t loss 2.639E+01\n",
      "step 1730000 \t return [-43.638084 -53.52245 ], ([0.18839562 0.20314679]) \t loss 2.806E+01\n",
      "step 1740000 \t return [-43.512676 -53.35579 ], ([0.18121916 0.23129778]) \t loss 2.569E+01\n",
      "step 1750000 \t return [-43.172993 -53.05499 ], ([0.14198455 0.22875652]) \t loss 2.675E+01\n",
      "step 1760000 \t return [-43.004173 -52.88951 ], ([0.1447796  0.23210901]) \t loss 2.617E+01\n",
      "step 1770000 \t return [-43.06735  -52.926754], ([0.19542341 0.23703165]) \t loss 2.698E+01\n",
      "step 1780000 \t return [-43.013042 -52.875175], ([0.17827801 0.20391698]) \t loss 2.666E+01\n",
      "step 1790000 \t return [-42.70928  -52.573563], ([0.16854036 0.22665408]) \t loss 2.837E+01\n",
      "step 1800000 \t return [-42.4966  -52.37906], ([0.168222   0.21302508]) \t loss 2.751E+01\n",
      "step 1810000 \t return [-42.58454  -52.503273], ([0.20058793 0.21595679]) \t loss 2.648E+01\n",
      "step 1820000 \t return [-42.3606   -52.238003], ([0.18933393 0.22149512]) \t loss 2.779E+01\n",
      "step 1830000 \t return [-42.03272  -51.914623], ([0.17844045 0.2039625 ]) \t loss 2.622E+01\n",
      "step 1840000 \t return [-42.055336 -51.87207 ], ([0.17451255 0.25878105]) \t loss 2.710E+01\n",
      "step 1850000 \t return [-41.94189  -51.822598], ([0.14460908 0.27499256]) \t loss 2.595E+01\n",
      "step 1860000 \t return [-41.89859  -51.757748], ([0.16972534 0.23562406]) \t loss 2.585E+01\n",
      "step 1870000 \t return [-41.642216 -51.496056], ([0.18418612 0.24079257]) \t loss 2.637E+01\n",
      "step 1880000 \t return [-41.3936  -51.30775], ([0.1458135  0.25013208]) \t loss 2.661E+01\n",
      "step 1890000 \t return [-41.3682  -51.22229], ([0.16926959 0.20963603]) \t loss 2.726E+01\n",
      "step 1900000 \t return [-41.06245 -50.93706], ([0.14905651 0.23536773]) \t loss 2.680E+01\n",
      "step 1910000 \t return [-40.94014  -50.793644], ([0.15809676 0.24810466]) \t loss 2.804E+01\n",
      "step 1920000 \t return [-40.603745 -50.44296 ], ([0.2147097  0.19862732]) \t loss 2.543E+01\n",
      "step 1930000 \t return [-40.19948  -50.002144], ([0.18691479 0.23209262]) \t loss 2.673E+01\n",
      "step 1940000 \t return [-40.002274 -49.846813], ([0.18268712 0.23685919]) \t loss 2.632E+01\n",
      "step 1950000 \t return [-39.9944  -49.85111], ([0.21871574 0.18147914]) \t loss 2.801E+01\n",
      "step 1960000 \t return [-39.929276 -49.75558 ], ([0.19848438 0.20030919]) \t loss 2.703E+01\n",
      "step 1970000 \t return [-39.4082   -49.274815], ([0.14849454 0.24401939]) \t loss 2.696E+01\n",
      "step 1980000 \t return [-39.2978   -49.135666], ([0.18365902 0.24019133]) \t loss 2.707E+01\n",
      "step 1990000 \t return [-38.932175 -48.789993], ([0.15614729 0.24819416]) \t loss 2.773E+01\n",
      "step 2000000 \t return [-38.812565 -48.704517], ([0.13956584 0.2298284 ]) \t loss 2.628E+01\n",
      "step 2010000 \t return [-38.884426 -48.76448 ], ([0.18100542 0.21805042]) \t loss 2.567E+01\n",
      "step 2020000 \t return [-38.69828  -48.529152], ([0.13417657 0.22895704]) \t loss 2.761E+01\n",
      "step 2030000 \t return [-38.28454  -48.137196], ([0.16781984 0.2773422 ]) \t loss 2.697E+01\n",
      "step 2040000 \t return [-38.075054 -47.973755], ([0.17951347 0.18827505]) \t loss 2.685E+01\n",
      "step 2050000 \t return [-37.81683  -47.657154], ([0.14551187 0.2560433 ]) \t loss 2.691E+01\n",
      "step 2060000 \t return [-37.85204  -47.738262], ([0.15501328 0.250481  ]) \t loss 2.684E+01\n",
      "step 2070000 \t return [-37.718243 -47.623554], ([0.18115763 0.26433066]) \t loss 2.796E+01\n",
      "step 2080000 \t return [-37.578865 -47.423084], ([0.22742489 0.19671689]) \t loss 2.795E+01\n",
      "step 2090000 \t return [-37.46988  -47.303226], ([0.16976753 0.26596183]) \t loss 2.802E+01\n",
      "step 2100000 \t return [-37.307003 -47.16138 ], ([0.21193285 0.22672553]) \t loss 2.742E+01\n",
      "step 2110000 \t return [-37.2032  -47.04033], ([0.20383187 0.21170865]) \t loss 2.780E+01\n",
      "step 2120000 \t return [-37.38175  -47.250153], ([0.18033607 0.2167946 ]) \t loss 2.593E+01\n",
      "step 2130000 \t return [-37.297752 -47.18028 ], ([0.16840172 0.22061726]) \t loss 2.932E+01\n",
      "step 2140000 \t return [-36.952515 -46.80631 ], ([0.16086681 0.23911186]) \t loss 2.731E+01\n",
      "step 2150000 \t return [-36.897972 -46.76601 ], ([0.3168785  0.22586639]) \t loss 2.829E+01\n",
      "step 2160000 \t return [-36.804882 -46.67377 ], ([0.21055606 0.21940416]) \t loss 2.702E+01\n",
      "step 2170000 \t return [-36.74187  -46.601284], ([0.14400256 0.21931568]) \t loss 2.764E+01\n",
      "step 2180000 \t return [-36.478065 -46.357906], ([0.16776252 0.22129844]) \t loss 2.647E+01\n",
      "step 2190000 \t return [-36.537235 -46.355167], ([0.2015598  0.20916344]) \t loss 2.715E+01\n",
      "step 2200000 \t return [-36.594227 -46.42648 ], ([0.24682684 0.18398945]) \t loss 2.725E+01\n",
      "step 2210000 \t return [-36.169296 -46.030514], ([0.14116102 0.2340243 ]) \t loss 2.906E+01\n",
      "step 2220000 \t return [-36.19053  -46.027267], ([0.19755076 0.20862857]) \t loss 2.766E+01\n",
      "step 2230000 \t return [-36.09119 -45.94298], ([0.19408497 0.18976404]) \t loss 2.823E+01\n",
      "step 2240000 \t return [-35.9991   -45.806816], ([0.18512037 0.20797391]) \t loss 2.650E+01\n",
      "step 2250000 \t return [-35.902916 -45.79276 ], ([0.15941694 0.21973063]) \t loss 2.697E+01\n",
      "step 2260000 \t return [-35.904644 -45.70642 ], ([0.23541424 0.2115113 ]) \t loss 2.645E+01\n",
      "step 2270000 \t return [-35.938198 -45.81876 ], ([0.24217941 0.21429464]) \t loss 2.732E+01\n",
      "step 2280000 \t return [-35.967945 -45.835033], ([0.2416125  0.19810191]) \t loss 2.505E+01\n",
      "step 2290000 \t return [-36.03033 -45.83513], ([0.19199279 0.22115113]) \t loss 2.650E+01\n",
      "step 2300000 \t return [-35.725967 -45.590145], ([0.16857243 0.24850726]) \t loss 2.713E+01\n",
      "step 2310000 \t return [-35.569134 -45.43346 ], ([0.17214929 0.20567325]) \t loss 2.697E+01\n",
      "step 2320000 \t return [-35.481895 -45.305363], ([0.21990173 0.21223074]) \t loss 2.694E+01\n",
      "step 2330000 \t return [-35.202263 -45.06711 ], ([0.16999686 0.2230381 ]) \t loss 2.690E+01\n",
      "step 2340000 \t return [-35.021656 -44.82895 ], ([0.17893016 0.26022264]) \t loss 2.694E+01\n",
      "step 2350000 \t return [-34.809315 -44.57973 ], ([0.28820053 0.23714511]) \t loss 2.622E+01\n",
      "step 2360000 \t return [-34.75881  -44.574814], ([0.19386098 0.19467281]) \t loss 2.722E+01\n",
      "step 2370000 \t return [-34.773335 -44.59791 ], ([0.21378775 0.18609484]) \t loss 2.638E+01\n",
      "step 2380000 \t return [-34.651405 -44.51531 ], ([0.20098758 0.18627934]) \t loss 2.621E+01\n",
      "step 2390000 \t return [-34.496113 -44.333023], ([0.1435634  0.22535342]) \t loss 2.672E+01\n",
      "step 2400000 \t return [-34.36344  -44.163662], ([0.18327995 0.20890588]) \t loss 2.627E+01\n",
      "step 2410000 \t return [-34.016685 -43.867348], ([0.17788114 0.2475942 ]) \t loss 2.769E+01\n",
      "step 2420000 \t return [-34.0469   -43.906456], ([0.23028478 0.23395249]) \t loss 2.755E+01\n",
      "step 2430000 \t return [-33.656784 -43.505127], ([0.1479757 0.243916 ]) \t loss 2.745E+01\n",
      "step 2440000 \t return [-33.571995 -43.439774], ([0.20515658 0.22266087]) \t loss 2.588E+01\n",
      "step 2450000 \t return [-33.362766 -43.216236], ([0.21437877 0.2260422 ]) \t loss 2.702E+01\n",
      "step 2460000 \t return [-33.26745  -43.088837], ([0.21589641 0.20665473]) \t loss 2.657E+01\n",
      "step 2470000 \t return [-33.6225  -43.41413], ([0.24064383 0.24618505]) \t loss 2.647E+01\n",
      "step 2480000 \t return [-33.301334 -43.13196 ], ([0.21280053 0.23203214]) \t loss 2.677E+01\n",
      "step 2490000 \t return [-33.106846 -42.88087 ], ([0.29092103 0.23012923]) \t loss 2.771E+01\n",
      "step 2500000 \t return [-33.156662 -42.999546], ([0.15979488 0.23417026]) \t loss 2.734E+01\n",
      "step 2510000 \t return [-33.114532 -42.92132 ], ([0.1672798  0.23776396]) \t loss 2.531E+01\n",
      "step 2520000 \t return [-32.8344  -42.76297], ([0.16789599 0.2155999 ]) \t loss 2.648E+01\n",
      "step 2530000 \t return [-32.809216 -42.629745], ([0.26442206 0.19844668]) \t loss 2.641E+01\n",
      "step 2540000 \t return [-33.00176  -42.729847], ([0.34231016 0.24610026]) \t loss 2.765E+01\n",
      "step 2550000 \t return [-32.825977 -42.55533 ], ([0.37723333 0.22620872]) \t loss 2.584E+01\n",
      "step 2560000 \t return [-32.85759  -42.588104], ([0.2515104  0.21428825]) \t loss 2.621E+01\n",
      "step 2570000 \t return [-32.370262 -42.19064 ], ([0.1405799  0.24923638]) \t loss 2.808E+01\n",
      "step 2580000 \t return [-32.28667  -42.097866], ([0.2753434  0.20845734]) \t loss 2.825E+01\n",
      "step 2590000 \t return [-32.428127 -42.265556], ([0.27293235 0.25074646]) \t loss 2.726E+01\n",
      "step 2600000 \t return [-32.310272 -42.073906], ([0.22620487 0.19978122]) \t loss 2.747E+01\n",
      "step 2610000 \t return [-32.141586 -42.0099  ], ([0.2729373  0.18865374]) \t loss 2.743E+01\n",
      "step 2620000 \t return [-31.925909 -41.775246], ([0.23213372 0.19204606]) \t loss 2.653E+01\n",
      "step 2630000 \t return [-31.728554 -41.590607], ([0.17421597 0.21541485]) \t loss 2.745E+01\n",
      "step 2640000 \t return [-31.899204 -41.698524], ([0.17335968 0.25105512]) \t loss 2.574E+01\n",
      "step 2650000 \t return [-31.661554 -41.517677], ([0.17795958 0.20376231]) \t loss 2.600E+01\n",
      "step 2660000 \t return [-31.718773 -41.50063 ], ([0.1830311  0.24859059]) \t loss 2.746E+01\n",
      "step 2670000 \t return [-31.696491 -41.487026], ([0.2784814  0.21494123]) \t loss 2.771E+01\n",
      "step 2680000 \t return [-31.662111 -41.46925 ], ([0.22607556 0.229219  ]) \t loss 2.685E+01\n",
      "step 2690000 \t return [-31.522024 -41.33356 ], ([0.13551438 0.2088923 ]) \t loss 2.753E+01\n",
      "step 2700000 \t return [-31.435041 -41.336617], ([0.15077664 0.22193006]) \t loss 2.728E+01\n",
      "step 2710000 \t return [-31.669981 -41.435444], ([0.23824388 0.16140112]) \t loss 2.656E+01\n",
      "step 2720000 \t return [-31.931868 -41.73269 ], ([0.3033885  0.24321017]) \t loss 2.495E+01\n",
      "step 2730000 \t return [-31.885174 -41.76018 ], ([0.14274101 0.26522368]) \t loss 2.750E+01\n",
      "step 2740000 \t return [-31.422148 -41.20652 ], ([0.2595098  0.23443682]) \t loss 2.728E+01\n",
      "step 2750000 \t return [-31.485153 -41.16935 ], ([0.61616695 0.27378038]) \t loss 2.777E+01\n",
      "step 2760000 \t return [-31.441889 -41.230064], ([0.1532973  0.28733557]) \t loss 2.883E+01\n",
      "step 2770000 \t return [-31.280823 -41.12211 ], ([0.19008423 0.23518878]) \t loss 2.554E+01\n",
      "step 2780000 \t return [-31.041786 -40.855267], ([0.17223564 0.20486735]) \t loss 2.549E+01\n",
      "step 2790000 \t return [-30.919983 -40.741844], ([0.14975007 0.26222768]) \t loss 2.623E+01\n",
      "step 2800000 \t return [-30.988197 -40.847855], ([0.23482884 0.20962176]) \t loss 2.605E+01\n",
      "step 2810000 \t return [-30.992603 -40.810116], ([0.20262961 0.23331368]) \t loss 2.682E+01\n",
      "step 2820000 \t return [-30.872246 -40.725937], ([0.1585571  0.18843408]) \t loss 2.743E+01\n",
      "step 2830000 \t return [-30.883055 -40.683037], ([0.1914531  0.22837333]) \t loss 2.797E+01\n",
      "step 2840000 \t return [-30.834091 -40.657593], ([0.15380515 0.22919966]) \t loss 2.701E+01\n",
      "step 2850000 \t return [-30.759521 -40.61596 ], ([0.1619152  0.20585376]) \t loss 2.613E+01\n",
      "step 2860000 \t return [-30.763283 -40.650017], ([0.14446054 0.2574414 ]) \t loss 2.580E+01\n",
      "step 2870000 \t return [-30.670488 -40.487892], ([0.18104818 0.22414239]) \t loss 2.624E+01\n",
      "step 2880000 \t return [-30.656233 -40.47242 ], ([0.18139832 0.23431845]) \t loss 2.687E+01\n",
      "step 2890000 \t return [-30.623491 -40.43438 ], ([0.25825474 0.18173715]) \t loss 2.777E+01\n",
      "step 2900000 \t return [-30.669336 -40.47554 ], ([0.24940161 0.19792847]) \t loss 2.788E+01\n",
      "step 2910000 \t return [-30.66123  -40.534855], ([0.22950573 0.16739935]) \t loss 2.548E+01\n",
      "step 2920000 \t return [-30.637842 -40.493465], ([0.21086745 0.19882603]) \t loss 2.642E+01\n",
      "step 2930000 \t return [-30.371933 -40.185738], ([0.12373942 0.2567659 ]) \t loss 2.789E+01\n",
      "step 2940000 \t return [-30.540379 -40.371902], ([0.2062528  0.16185245]) \t loss 2.930E+01\n",
      "step 2950000 \t return [-30.278433 -40.06599 ], ([0.12270658 0.22228897]) \t loss 2.812E+01\n",
      "step 2960000 \t return [-30.193096 -40.047474], ([0.14803538 0.25087103]) \t loss 2.646E+01\n",
      "step 2970000 \t return [-30.296658 -40.13804 ], ([0.19945852 0.19926167]) \t loss 2.811E+01\n",
      "step 2980000 \t return [-30.18293  -39.958725], ([0.1392297  0.27225867]) \t loss 2.712E+01\n",
      "step 2990000 \t return [-30.156006 -40.02584 ], ([0.21929331 0.2104252 ]) \t loss 2.614E+01\n",
      "step 3000000 \t return [-30.175797 -39.98723 ], ([0.22871615 0.19368662]) \t loss 2.681E+01\n",
      "step 3010000 \t return [-30.181625 -40.04689 ], ([0.14897361 0.22844449]) \t loss 2.540E+01\n",
      "step 3020000 \t return [-30.009941 -39.86112 ], ([0.14579625 0.2498256 ]) \t loss 2.527E+01\n",
      "step 3030000 \t return [-29.893305 -39.70417 ], ([0.15531205 0.23682024]) \t loss 2.586E+01\n",
      "step 3040000 \t return [-29.839619 -39.68602 ], ([0.1601862  0.21943347]) \t loss 2.642E+01\n",
      "step 3050000 \t return [-29.766006 -39.61283 ], ([0.16192506 0.23420578]) \t loss 2.588E+01\n",
      "step 3060000 \t return [-29.77402  -39.630775], ([0.17119215 0.21676466]) \t loss 2.563E+01\n",
      "step 3070000 \t return [-29.85881  -39.703655], ([0.23624615 0.19340149]) \t loss 2.652E+01\n",
      "step 3080000 \t return [-29.667587 -39.484737], ([0.20807861 0.16697906]) \t loss 2.589E+01\n",
      "step 3090000 \t return [-29.513172 -39.353977], ([0.17043923 0.2103605 ]) \t loss 2.681E+01\n",
      "step 3100000 \t return [-29.413828 -39.255657], ([0.21809824 0.19920813]) \t loss 2.545E+01\n",
      "step 3110000 \t return [-29.4227   -39.265392], ([0.16841389 0.23719688]) \t loss 2.778E+01\n",
      "step 3120000 \t return [-29.406809 -39.256805], ([0.21952216 0.15764001]) \t loss 2.484E+01\n",
      "step 3130000 \t return [-29.334131 -39.175037], ([0.186213   0.20777899]) \t loss 2.519E+01\n",
      "step 3140000 \t return [-29.368427 -39.241215], ([0.1453296  0.30421215]) \t loss 2.755E+01\n",
      "step 3150000 \t return [-29.397749 -39.21582 ], ([0.24759002 0.17956568]) \t loss 2.649E+01\n",
      "step 3160000 \t return [-29.476469 -39.327133], ([0.10325272 0.28769484]) \t loss 2.716E+01\n",
      "step 3170000 \t return [-29.02021  -38.846622], ([0.14337404 0.2514587 ]) \t loss 2.647E+01\n",
      "step 3180000 \t return [-28.826956 -38.660934], ([0.15695874 0.2779161 ]) \t loss 2.739E+01\n",
      "step 3190000 \t return [-28.84716  -38.634186], ([0.12253992 0.31376475]) \t loss 2.835E+01\n",
      "step 3200000 \t return [-28.784784 -38.619076], ([0.10219413 0.30569777]) \t loss 2.657E+01\n",
      "step 3210000 \t return [-28.68887  -38.569492], ([0.18920882 0.23629421]) \t loss 2.737E+01\n",
      "step 3220000 \t return [-28.865246 -38.718628], ([0.10180961 0.2973599 ]) \t loss 2.813E+01\n",
      "step 3230000 \t return [-28.850231 -38.700794], ([0.11083233 0.30096793]) \t loss 2.920E+01\n",
      "step 3240000 \t return [-28.603277 -38.451424], ([0.12766473 0.25846252]) \t loss 2.628E+01\n",
      "step 3250000 \t return [-28.512712 -38.345627], ([0.11537348 0.24493471]) \t loss 2.773E+01\n",
      "step 3260000 \t return [-28.512764 -38.301563], ([0.19046916 0.24476409]) \t loss 2.708E+01\n",
      "step 3270000 \t return [-28.41823  -38.293953], ([0.21721868 0.18308558]) \t loss 2.634E+01\n",
      "step 3280000 \t return [-28.39726  -38.189587], ([0.18089743 0.28488258]) \t loss 2.662E+01\n",
      "step 3290000 \t return [-28.426353 -38.277702], ([0.25990117 0.19097327]) \t loss 2.700E+01\n",
      "step 3300000 \t return [-28.393658 -38.20133 ], ([0.11017431 0.31798276]) \t loss 2.774E+01\n",
      "step 3310000 \t return [-28.291601 -38.13999 ], ([0.21163286 0.22076635]) \t loss 2.703E+01\n",
      "step 3320000 \t return [-28.228445 -38.05309 ], ([0.20653707 0.25224674]) \t loss 2.757E+01\n",
      "step 3330000 \t return [-28.164423 -37.957775], ([0.16137652 0.26468158]) \t loss 2.669E+01\n",
      "step 3340000 \t return [-27.987595 -37.827736], ([0.19845761 0.21905833]) \t loss 2.567E+01\n",
      "step 3350000 \t return [-28.337114 -38.104374], ([0.26702175 0.28701702]) \t loss 2.800E+01\n",
      "step 3360000 \t return [-27.922083 -37.65746 ], ([0.3095018 0.2241886]) \t loss 2.719E+01\n",
      "step 3370000 \t return [-27.953821 -37.745796], ([0.29072228 0.19406813]) \t loss 2.493E+01\n",
      "step 3380000 \t return [-27.758474 -37.543808], ([0.14084782 0.28090748]) \t loss 2.615E+01\n",
      "step 3390000 \t return [-27.672861 -37.50292 ], ([0.15835822 0.2504391 ]) \t loss 2.646E+01\n",
      "step 3400000 \t return [-27.901817 -37.620453], ([0.29025075 0.3073855 ]) \t loss 2.620E+01\n",
      "step 3410000 \t return [-27.684038 -37.47613 ], ([0.12159044 0.3204819 ]) \t loss 2.696E+01\n",
      "step 3420000 \t return [-27.741709 -37.57136 ], ([0.21724802 0.19697826]) \t loss 2.633E+01\n",
      "step 3430000 \t return [-27.740261 -37.41452 ], ([0.2646711  0.25765526]) \t loss 2.667E+01\n",
      "step 3440000 \t return [-27.579685 -37.31986 ], ([0.4004762  0.27286533]) \t loss 2.666E+01\n",
      "step 3450000 \t return [-27.586105 -37.515156], ([0.25752062 0.17551577]) \t loss 2.676E+01\n",
      "step 3460000 \t return [-27.38987  -37.079365], ([0.2740764 0.2553718]) \t loss 2.512E+01\n",
      "step 3470000 \t return [-27.33819 -37.09846], ([0.2730985  0.19908237]) \t loss 2.512E+01\n",
      "step 3480000 \t return [-27.232262 -37.020832], ([0.1424954 0.3142834]) \t loss 2.542E+01\n",
      "step 3490000 \t return [-27.957901 -37.152184], ([1.2945119  0.28384992]) \t loss 2.777E+01\n",
      "step 3500000 \t return [-27.138357 -36.95594 ], ([0.13731611 0.3296093 ]) \t loss 3.427E+01\n",
      "step 3510000 \t return [-26.780048 -36.592545], ([0.1477715 0.275785 ]) \t loss 2.905E+01\n",
      "step 3520000 \t return [-26.670631 -36.457302], ([0.17818037 0.23659062]) \t loss 2.849E+01\n",
      "step 3530000 \t return [-26.579592 -36.314747], ([0.27821937 0.22735433]) \t loss 2.642E+01\n",
      "step 3540000 \t return [-26.508162 -36.30867 ], ([0.29756182 0.2234092 ]) \t loss 2.680E+01\n",
      "step 3550000 \t return [-26.488594 -36.27521 ], ([0.13533764 0.27570635]) \t loss 2.714E+01\n",
      "step 3560000 \t return [-26.194265 -36.002186], ([0.13950925 0.25839353]) \t loss 2.719E+01\n",
      "step 3570000 \t return [-26.126137 -35.930958], ([0.21964464 0.21919614]) \t loss 2.618E+01\n",
      "step 3580000 \t return [-26.301785 -36.12147 ], ([0.27172342 0.18506676]) \t loss 2.632E+01\n",
      "step 3590000 \t return [-26.133612 -35.903824], ([0.25696594 0.2374703 ]) \t loss 2.852E+01\n",
      "step 3600000 \t return [-26.341692 -35.954426], ([0.6228275  0.19477305]) \t loss 2.732E+01\n",
      "step 3610000 \t return [-26.08465 -35.84623], ([0.2119118  0.22141215]) \t loss 2.915E+01\n",
      "step 3620000 \t return [-26.16536 -35.96733], ([0.41743085 0.17258815]) \t loss 2.624E+01\n",
      "step 3630000 \t return [-26.046686 -35.75383 ], ([0.69974625 0.21015072]) \t loss 2.688E+01\n",
      "step 3640000 \t return [-25.907188 -35.672794], ([0.13258877 0.27928162]) \t loss 2.730E+01\n",
      "step 3650000 \t return [-25.980556 -35.703217], ([0.22722323 0.24545981]) \t loss 2.609E+01\n",
      "step 3660000 \t return [-25.713005 -35.542786], ([0.21129203 0.23706178]) \t loss 2.639E+01\n",
      "step 3670000 \t return [-25.843042 -35.57221 ], ([0.18712258 0.23891538]) \t loss 2.685E+01\n",
      "step 3680000 \t return [-25.734846 -35.48048 ], ([0.30297288 0.2660173 ]) \t loss 2.656E+01\n",
      "step 3690000 \t return [-25.438572 -35.259945], ([0.20535466 0.24579978]) \t loss 2.754E+01\n",
      "step 3700000 \t return [-25.62458 -35.42927], ([0.2458222 0.1788597]) \t loss 2.873E+01\n",
      "step 3710000 \t return [-25.853836 -35.530266], ([0.68373805 0.20235102]) \t loss 2.690E+01\n",
      "step 3720000 \t return [-25.434326 -35.225494], ([0.21411811 0.21708046]) \t loss 2.621E+01\n",
      "step 3730000 \t return [-25.04325 -34.86485], ([0.1998354  0.24314085]) \t loss 2.545E+01\n",
      "step 3740000 \t return [-25.151426 -35.011295], ([0.13429882 0.3139309 ]) \t loss 2.576E+01\n",
      "step 3750000 \t return [-25.070127 -34.909744], ([0.20844351 0.20575145]) \t loss 2.711E+01\n",
      "step 3760000 \t return [-24.87166 -34.74305], ([0.16218351 0.23638742]) \t loss 2.684E+01\n",
      "step 3770000 \t return [-24.865835 -34.590946], ([0.22250117 0.2405154 ]) \t loss 2.610E+01\n",
      "step 3780000 \t return [-24.642384 -34.412197], ([0.1663475  0.30215916]) \t loss 2.702E+01\n",
      "step 3790000 \t return [-24.448269 -34.19718 ], ([0.19296907 0.25295505]) \t loss 3.020E+01\n",
      "step 3800000 \t return [-24.17668 -33.91157], ([0.2559409  0.23954546]) \t loss 2.851E+01\n",
      "step 3810000 \t return [-24.061129 -33.832264], ([0.14243695 0.3018083 ]) \t loss 2.794E+01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\liamm\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:179: RuntimeWarning: overflow encountered in reduce\n",
      "  ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3820000 \t return [-23.941242 -33.672466], ([0.15343323 0.3125724 ]) \t loss 2.878E+01\n",
      "step 3830000 \t return [-23.84443  -33.655746], ([0.13369799 0.33795318]) \t loss 2.727E+01\n",
      "step 3840000 \t return [-23.89511  -33.689785], ([0.11704697 0.31761557]) \t loss 2.745E+01\n",
      "step 3850000 \t return [-23.664959 -33.388313], ([0.17217508 0.30874997]) \t loss 2.572E+01\n",
      "step 3860000 \t return [-23.750769 -33.52252 ], ([0.17355207 0.28053993]) \t loss 2.570E+01\n",
      "step 3870000 \t return [-23.525791 -33.31076 ], ([0.15543494 0.31101   ]) \t loss 2.681E+01\n",
      "step 3880000 \t return [-23.611275 -33.38776 ], ([0.12027194 0.41373432]) \t loss 2.620E+01\n",
      "step 3890000 \t return [-23.68736  -33.494637], ([0.23515293 0.220105  ]) \t loss 2.615E+01\n",
      "step 3900000 \t return [-23.58222 -33.3204 ], ([0.12913597 0.3709557 ]) \t loss 2.557E+01\n",
      "step 3910000 \t return [-23.388151 -33.173153], ([0.16334233 0.29875168]) \t loss 2.688E+01\n",
      "step 3920000 \t return [-23.567053 -33.38491 ], ([0.24214691 0.21523795]) \t loss 2.606E+01\n",
      "step 3930000 \t return [-23.486435 -33.22747 ], ([0.24676901 0.24481349]) \t loss 2.650E+01\n",
      "step 3940000 \t return [-23.44644  -33.202877], ([0.15174022 0.31931505]) \t loss 2.763E+01\n",
      "step 3950000 \t return [-23.43491 -33.16441], ([0.21254963 0.26280537]) \t loss 2.524E+01\n",
      "step 3960000 \t return [-23.49718 -33.17534], ([0.17556918 0.3083022 ]) \t loss 2.656E+01\n",
      "step 3970000 \t return [-23.35382  -33.175484], ([0.22476327 0.24079908]) \t loss 2.815E+01\n",
      "step 3980000 \t return [-23.579384 -33.301197], ([0.14826143 0.39759955]) \t loss 2.659E+01\n",
      "step 3990000 \t return [-23.23841  -32.947014], ([0.21203811 0.2979154 ]) \t loss 2.604E+01\n",
      "step 4000000 \t return [-23.188377 -32.990837], ([0.13960521 0.28421122]) \t loss 2.719E+01\n",
      "step 4010000 \t return [-23.279163 -33.00802 ], ([0.15150163 0.30081585]) \t loss 2.665E+01\n",
      "step 4020000 \t return [-23.065971 -32.822495], ([0.22454618 0.28352627]) \t loss 2.638E+01\n",
      "step 4030000 \t return [-23.142378 -32.980053], ([0.28539616 0.15102631]) \t loss 2.802E+01\n",
      "step 4040000 \t return [-23.580303 -33.284172], ([0.09554264 0.37757125]) \t loss 2.735E+01\n",
      "step 4050000 \t return [-23.003277 -32.779915], ([0.18058674 0.24356192]) \t loss 2.697E+01\n",
      "step 4060000 \t return [-22.853731 -32.581783], ([0.20043425 0.29925713]) \t loss 2.728E+01\n",
      "step 4070000 \t return [-22.995491 -32.76545 ], ([0.284345   0.20976004]) \t loss 2.680E+01\n",
      "step 4080000 \t return [-23.486528 -33.334217], ([0.08164911 0.35868463]) \t loss 2.601E+01\n",
      "step 4090000 \t return [-23.378574 -33.110973], ([0.09883744 0.41585767]) \t loss 2.780E+01\n",
      "step 4100000 \t return [-23.416353 -33.14129 ], ([0.08987271 0.42837834]) \t loss 2.724E+01\n",
      "step 4110000 \t return [-22.746906 -32.474796], ([0.1558709  0.32013956]) \t loss 2.614E+01\n",
      "step 4120000 \t return [-22.847385 -32.61748 ], ([0.22583649 0.27435216]) \t loss 2.793E+01\n",
      "step 4130000 \t return [-22.75774 -32.50786], ([0.18042317 0.31798044]) \t loss 2.867E+01\n",
      "step 4140000 \t return [-22.752165 -32.528225], ([0.11593942 0.33291978]) \t loss 2.753E+01\n",
      "step 4150000 \t return [-23.03138  -32.772552], ([0.10599457 0.37953788]) \t loss 2.807E+01\n",
      "step 4160000 \t return [-22.688238 -32.44359 ], ([0.25310498 0.24937378]) \t loss 2.728E+01\n",
      "step 4170000 \t return [-22.599428 -32.33398 ], ([0.18945764 0.24671814]) \t loss 2.735E+01\n",
      "step 4180000 \t return [-22.437231 -32.267693], ([0.23035286 0.24632034]) \t loss 2.676E+01\n",
      "step 4190000 \t return [-22.568476 -32.255054], ([0.23477896 0.30657384]) \t loss 2.617E+01\n",
      "step 4200000 \t return [-22.602633 -32.394012], ([0.12008843 0.37597293]) \t loss 2.670E+01\n",
      "step 4210000 \t return [-22.863625 -32.5679  ], ([0.10287116 0.39912865]) \t loss 2.728E+01\n",
      "step 4220000 \t return [-22.480145 -32.23246 ], ([0.28631413 0.21544698]) \t loss 2.574E+01\n",
      "step 4230000 \t return [-22.816938 -32.610645], ([0.09830148 0.39311716]) \t loss 2.661E+01\n",
      "step 4240000 \t return [-22.30468 -32.08412], ([0.19166762 0.31807113]) \t loss 2.819E+01\n",
      "step 4250000 \t return [-22.387808 -32.0871  ], ([0.29882652 0.21956286]) \t loss 2.616E+01\n",
      "step 4260000 \t return [-22.165133 -31.934088], ([0.15163334 0.35103437]) \t loss 2.615E+01\n",
      "step 4270000 \t return [-22.159409 -31.800322], ([0.2791791 0.273016 ]) \t loss 2.876E+01\n",
      "step 4280000 \t return [-21.94301  -31.709412], ([0.22942205 0.24972458]) \t loss 2.684E+01\n",
      "step 4290000 \t return [-21.91614 -31.67189], ([0.26395243 0.25505954]) \t loss 2.717E+01\n",
      "step 4300000 \t return [-21.76722 -31.45677], ([0.23190191 0.2804573 ]) \t loss 2.654E+01\n",
      "step 4310000 \t return [-21.689705 -31.314577], ([0.12209494 0.36508915]) \t loss 2.690E+01\n",
      "step 4320000 \t return [-21.82541  -31.489653], ([0.11007089 0.46456903]) \t loss 2.692E+01\n",
      "step 4330000 \t return [-21.817436 -31.475273], ([0.12049462 0.43724024]) \t loss 2.657E+01\n",
      "step 4340000 \t return [-21.480888 -31.2807  ], ([0.14124984 0.26353163]) \t loss 2.748E+01\n",
      "step 4350000 \t return [-21.376488 -31.001047], ([0.17444177 0.361042  ]) \t loss 2.577E+01\n",
      "step 4360000 \t return [-21.349133 -31.096546], ([0.22869386 0.29614636]) \t loss 2.646E+01\n",
      "step 4370000 \t return [-21.385347 -31.03716 ], ([0.25325346 0.32433867]) \t loss 2.650E+01\n",
      "step 4380000 \t return [-21.190882 -30.906101], ([0.14017344 0.3972238 ]) \t loss 2.628E+01\n",
      "step 4390000 \t return [-21.218233 -30.933426], ([0.12924708 0.3721691 ]) \t loss 2.529E+01\n",
      "step 4400000 \t return [-21.041122 -30.72987 ], ([0.19508584 0.35832012]) \t loss 2.544E+01\n",
      "step 4410000 \t return [-21.14885  -30.888372], ([0.21831079 0.3054675 ]) \t loss 2.633E+01\n",
      "step 4420000 \t return [-21.121565 -30.82067 ], ([0.12017667 0.3963832 ]) \t loss 2.659E+01\n",
      "step 4430000 \t return [-21.095562 -30.864748], ([0.23279567 0.2681084 ]) \t loss 2.564E+01\n",
      "step 4440000 \t return [-20.980417 -30.696373], ([0.18889971 0.31471357]) \t loss 2.563E+01\n",
      "step 4450000 \t return [-21.074085 -30.795046], ([0.1252631  0.40453747]) \t loss 2.763E+01\n",
      "step 4460000 \t return [-20.887283 -30.581123], ([0.23105699 0.3316153 ]) \t loss 2.770E+01\n",
      "step 4470000 \t return [-20.914223 -30.565794], ([0.18147957 0.33270293]) \t loss 2.684E+01\n",
      "step 4480000 \t return [-20.680359 -30.400461], ([0.23893672 0.32633302]) \t loss 2.738E+01\n",
      "step 4490000 \t return [-20.796593 -30.533463], ([0.13310148 0.41227877]) \t loss 2.601E+01\n",
      "step 4500000 \t return [-20.483047 -30.195208], ([0.19635163 0.34833375]) \t loss 2.690E+01\n",
      "step 4510000 \t return [-20.55104 -30.16996], ([0.1622973  0.44047254]) \t loss 2.827E+01\n",
      "step 4520000 \t return [-20.473364 -30.171467], ([0.2067281  0.35316613]) \t loss 2.759E+01\n",
      "step 4530000 \t return [-20.50131  -30.091578], ([0.25479135 0.33533177]) \t loss 2.582E+01\n",
      "step 4540000 \t return [-20.569487 -30.194473], ([0.27919778 0.3057569 ]) \t loss 2.661E+01\n",
      "step 4550000 \t return [-20.65746  -30.379295], ([0.14334144 0.42875126]) \t loss 2.606E+01\n",
      "step 4560000 \t return [-20.438995 -30.145157], ([0.20423256 0.27318466]) \t loss 2.734E+01\n",
      "step 4570000 \t return [-20.43074  -30.194208], ([0.212703   0.34649932]) \t loss 2.556E+01\n",
      "step 4580000 \t return [-20.347929 -30.014603], ([0.25859424 0.30888316]) \t loss 2.781E+01\n",
      "step 4590000 \t return [-20.262613 -29.9535  ], ([0.20004499 0.36908787]) \t loss 2.781E+01\n",
      "step 4600000 \t return [-20.175714 -29.855433], ([0.20641659 0.3594225 ]) \t loss 2.680E+01\n",
      "step 4610000 \t return [-20.302984 -29.936169], ([0.1399863 0.4965941]) \t loss 2.490E+01\n",
      "step 4620000 \t return [-20.191057 -29.855392], ([0.11979262 0.49323192]) \t loss 2.681E+01\n",
      "step 4630000 \t return [-20.065811 -29.748753], ([0.17660727 0.46369815]) \t loss 2.722E+01\n",
      "step 4640000 \t return [-19.9488  -29.62589], ([0.19519821 0.44906014]) \t loss 2.621E+01\n",
      "step 4650000 \t return [-20.041574 -29.734661], ([0.1523594 0.4571746]) \t loss 2.786E+01\n",
      "step 4660000 \t return [-19.883957 -29.501785], ([0.2007002 0.3795869]) \t loss 2.708E+01\n",
      "step 4670000 \t return [-19.936811 -29.48503 ], ([0.27756885 0.38527536]) \t loss 2.722E+01\n",
      "step 4680000 \t return [-20.21346  -29.872646], ([0.12969385 0.43940338]) \t loss 2.573E+01\n",
      "step 4690000 \t return [-20.401283 -29.959314], ([0.12511341 0.6401522 ]) \t loss 2.743E+01\n",
      "step 4700000 \t return [-20.052124 -29.56719 ], ([0.15552321 0.5558134 ]) \t loss 2.704E+01\n",
      "step 4710000 \t return [-20.09156  -29.665895], ([0.12850936 0.5027719 ]) \t loss 2.555E+01\n",
      "step 4720000 \t return [-19.906273 -29.510818], ([0.19911738 0.48889583]) \t loss 2.612E+01\n",
      "step 4730000 \t return [-19.745457 -29.420208], ([0.24987479 0.36931738]) \t loss 2.629E+01\n",
      "step 4740000 \t return [-19.658497 -29.206219], ([0.24648863 0.39492783]) \t loss 2.723E+01\n",
      "step 4750000 \t return [-19.508928 -29.190386], ([0.24877203 0.32443652]) \t loss 2.637E+01\n",
      "step 4760000 \t return [-19.686573 -29.337498], ([0.19500078 0.4670233 ]) \t loss 2.611E+01\n",
      "step 4770000 \t return [-19.767464 -29.385376], ([0.1674476  0.46440667]) \t loss 2.722E+01\n",
      "step 4780000 \t return [-19.68841 -29.30895], ([0.16942649 0.44107032]) \t loss 2.749E+01\n",
      "step 4790000 \t return [-19.641495 -29.240301], ([0.20636211 0.4669115 ]) \t loss 2.678E+01\n",
      "step 4800000 \t return [-19.677704 -29.256403], ([0.16302061 0.49682903]) \t loss 2.792E+01\n",
      "step 4810000 \t return [-19.871563 -29.612377], ([0.15029317 0.48220888]) \t loss 2.629E+01\n",
      "step 4820000 \t return [-19.847223 -29.48497 ], ([0.18267287 0.46770057]) \t loss 2.627E+01\n",
      "step 4830000 \t return [-19.778576 -29.334837], ([0.156855 0.532289]) \t loss 2.619E+01\n",
      "step 4840000 \t return [-19.530106 -29.122776], ([0.20861024 0.46887785]) \t loss 2.598E+01\n",
      "step 4850000 \t return [-19.415533 -29.054731], ([0.21581396 0.36636502]) \t loss 2.574E+01\n",
      "step 4860000 \t return [-19.41167  -29.143671], ([0.26056045 0.32265195]) \t loss 2.557E+01\n",
      "step 4870000 \t return [-19.351826 -28.862957], ([0.27135614 0.4418307 ]) \t loss 2.573E+01\n",
      "step 4880000 \t return [-19.152077 -28.79237 ], ([0.2471282  0.37577468]) \t loss 2.736E+01\n",
      "step 4890000 \t return [-19.721546 -29.433327], ([0.10089169 0.5240165 ]) \t loss 2.524E+01\n",
      "step 4900000 \t return [-18.990301 -28.586584], ([0.26143813 0.43099105]) \t loss 2.592E+01\n",
      "step 4910000 \t return [-19.391233 -29.046032], ([0.15009907 0.5503766 ]) \t loss 2.642E+01\n",
      "step 4920000 \t return [-19.01163  -28.564259], ([0.26415157 0.46377614]) \t loss 2.570E+01\n",
      "step 4930000 \t return [-19.022495 -28.548033], ([0.18755771 0.56841666]) \t loss 2.596E+01\n",
      "step 4940000 \t return [-18.999392 -28.543808], ([0.29714918 0.41722566]) \t loss 2.601E+01\n",
      "step 4950000 \t return [-18.865904 -28.357855], ([0.17325373 0.64219487]) \t loss 2.717E+01\n",
      "step 4960000 \t return [-18.717665 -28.324306], ([0.2542372  0.45513666]) \t loss 2.526E+01\n",
      "step 4970000 \t return [-18.673693 -28.210533], ([0.24936481 0.48353004]) \t loss 2.506E+01\n",
      "step 4980000 \t return [-18.879858 -28.34075 ], ([0.16034931 0.63792163]) \t loss 2.438E+01\n",
      "step 4990000 \t return [-18.826363 -28.401396], ([0.33497742 0.33316228]) \t loss 2.713E+01\n",
      "step 5000000 \t return [-18.808199 -28.266829], ([0.1404872  0.58037025]) \t loss 2.673E+01\n",
      "step 5010000 \t return [-18.723774 -28.251394], ([0.25838757 0.4757329 ]) \t loss 2.840E+01\n",
      "step 5020000 \t return [-18.623055 -28.17219 ], ([0.33171913 0.38903874]) \t loss 2.582E+01\n",
      "step 5030000 \t return [-18.513287 -28.165224], ([0.31743884 0.33932924]) \t loss 2.618E+01\n",
      "step 5040000 \t return [-18.407585 -27.944387], ([0.28540632 0.46139687]) \t loss 2.589E+01\n",
      "step 5050000 \t return [-18.58742  -28.216225], ([0.13386413 0.5493515 ]) \t loss 2.751E+01\n",
      "step 5060000 \t return [-18.155003 -27.721634], ([0.25409833 0.5272063 ]) \t loss 2.709E+01\n",
      "step 5070000 \t return [-17.97218  -27.394663], ([0.31205037 0.5420606 ]) \t loss 2.565E+01\n",
      "step 5080000 \t return [-18.215885 -27.685375], ([0.20872492 0.55551976]) \t loss 2.788E+01\n",
      "step 5090000 \t return [-18.403645 -27.706438], ([0.16658759 0.7404951 ]) \t loss 2.592E+01\n",
      "step 5100000 \t return [-18.156582 -27.558321], ([0.1527619 0.6752155]) \t loss 2.677E+01\n",
      "step 5110000 \t return [-18.0859   -27.711004], ([0.18745385 0.56612116]) \t loss 2.555E+01\n",
      "step 5120000 \t return [-17.904495 -27.495344], ([0.2966739  0.41825005]) \t loss 2.539E+01\n",
      "step 5130000 \t return [-18.282438 -27.748066], ([0.12848654 0.68771976]) \t loss 2.599E+01\n",
      "step 5140000 \t return [-18.093496 -27.52629 ], ([0.18940203 0.6926231 ]) \t loss 2.641E+01\n",
      "step 5150000 \t return [-17.98778 -27.46112], ([0.1394806 0.522341 ]) \t loss 2.642E+01\n",
      "step 5160000 \t return [-18.253454 -27.806648], ([0.10673758 0.7603031 ]) \t loss 2.585E+01\n",
      "step 5170000 \t return [-17.846172 -27.353636], ([0.33170003 0.41759673]) \t loss 2.565E+01\n",
      "step 5180000 \t return [-17.838064 -27.218761], ([0.23687221 0.70921886]) \t loss 2.632E+01\n",
      "step 5190000 \t return [-18.149834 -27.640867], ([0.1119507 0.6948297]) \t loss 2.675E+01\n",
      "step 5200000 \t return [-17.70467  -27.141443], ([0.34880373 0.48831195]) \t loss 2.611E+01\n",
      "step 5210000 \t return [-17.605474 -27.026783], ([0.3015032 0.5344212]) \t loss 2.511E+01\n",
      "step 5220000 \t return [-17.554543 -27.078886], ([0.34660658 0.4667868 ]) \t loss 2.569E+01\n",
      "step 5230000 \t return [-17.500713 -26.905561], ([0.4659479  0.39765206]) \t loss 2.687E+01\n",
      "step 5240000 \t return [-17.865934 -27.278522], ([0.13886523 0.6856514 ]) \t loss 2.529E+01\n",
      "step 5250000 \t return [-17.656002 -27.127953], ([0.18613042 0.66167754]) \t loss 2.699E+01\n",
      "step 5260000 \t return [-17.68942 -27.11345], ([0.15117927 0.63033074]) \t loss 2.780E+01\n",
      "step 5270000 \t return [-17.630077 -27.12343 ], ([0.21320829 0.58190626]) \t loss 2.692E+01\n",
      "step 5280000 \t return [-17.49087  -26.858137], ([0.23625208 0.6166638 ]) \t loss 2.650E+01\n",
      "step 5290000 \t return [-17.37011 -26.83076], ([0.23809041 0.5277723 ]) \t loss 2.623E+01\n",
      "step 5300000 \t return [-18.01272  -27.499077], ([0.10648319 0.7273727 ]) \t loss 2.560E+01\n",
      "step 5310000 \t return [-17.453783 -27.064621], ([0.12059818 0.62204736]) \t loss 2.661E+01\n",
      "step 5320000 \t return [-17.318592 -26.661415], ([0.42426634 0.39687333]) \t loss 2.552E+01\n",
      "step 5330000 \t return [-17.764565 -27.15434 ], ([0.13334742 0.7658259 ]) \t loss 2.482E+01\n",
      "step 5340000 \t return [-17.120266 -26.571838], ([0.31542715 0.50813556]) \t loss 2.499E+01\n",
      "step 5350000 \t return [-17.756416 -27.258041], ([0.10225315 0.6496608 ]) \t loss 2.550E+01\n",
      "step 5360000 \t return [-17.071148 -26.512157], ([0.3888302  0.40979967]) \t loss 2.723E+01\n",
      "step 5370000 \t return [-17.751652 -27.272972], ([0.09932862 0.78092587]) \t loss 2.583E+01\n",
      "step 5380000 \t return [-17.348936 -26.769339], ([0.18117614 0.690651  ]) \t loss 2.561E+01\n",
      "step 5390000 \t return [-17.095213 -26.617231], ([0.3010124  0.56448895]) \t loss 2.663E+01\n",
      "step 5400000 \t return [-16.834778 -26.261972], ([0.30092722 0.54361844]) \t loss 2.618E+01\n",
      "step 5410000 \t return [-16.78271 -26.19885], ([0.27825487 0.585545  ]) \t loss 2.762E+01\n",
      "step 5420000 \t return [-17.063633 -26.501541], ([0.55405384 0.3158254 ]) \t loss 2.700E+01\n",
      "step 5430000 \t return [-17.672695 -26.961365], ([0.10318632 0.8671487 ]) \t loss 2.482E+01\n",
      "step 5440000 \t return [-17.978355 -27.320171], ([0.09222683 0.82921785]) \t loss 2.636E+01\n",
      "step 5450000 \t return [-17.124874 -26.320637], ([0.16153386 0.76193655]) \t loss 2.744E+01\n",
      "step 5460000 \t return [-16.66967  -26.105593], ([0.35781294 0.50057024]) \t loss 2.640E+01\n",
      "step 5470000 \t return [-16.678308 -26.09148 ], ([0.34379315 0.532199  ]) \t loss 2.610E+01\n",
      "step 5480000 \t return [-16.838291 -26.269505], ([0.16792987 0.7425436 ]) \t loss 2.673E+01\n",
      "step 5490000 \t return [-16.662539 -26.136791], ([0.28459623 0.57102275]) \t loss 2.638E+01\n",
      "step 5500000 \t return [-16.654879 -26.115307], ([0.24451607 0.62518287]) \t loss 2.747E+01\n",
      "step 5510000 \t return [-16.49419  -26.072748], ([0.30098    0.49112815]) \t loss 2.625E+01\n",
      "step 5520000 \t return [-16.809925 -26.294012], ([0.17242129 0.6643631 ]) \t loss 2.658E+01\n",
      "step 5530000 \t return [-16.594536 -26.037405], ([0.20634364 0.5839316 ]) \t loss 2.576E+01\n",
      "step 5540000 \t return [-16.95031 -26.46887], ([0.1464015  0.65650576]) \t loss 2.647E+01\n",
      "step 5550000 \t return [-16.616014 -26.09321 ], ([0.20195636 0.71920145]) \t loss 2.679E+01\n",
      "step 5560000 \t return [-16.41695  -25.931303], ([0.17448492 0.5994172 ]) \t loss 2.522E+01\n",
      "step 5570000 \t return [-16.547361 -26.041838], ([0.1897291  0.61366534]) \t loss 2.642E+01\n",
      "step 5580000 \t return [-16.600208 -26.119013], ([0.14406548 0.67964435]) \t loss 2.628E+01\n",
      "step 5590000 \t return [-16.409166 -25.871437], ([0.38943312 0.46431506]) \t loss 2.595E+01\n",
      "step 5600000 \t return [-16.976534 -26.182007], ([0.12512696 0.8615685 ]) \t loss 2.603E+01\n",
      "step 5610000 \t return [-16.430511 -25.807991], ([0.24505456 0.7178849 ]) \t loss 2.614E+01\n",
      "step 5620000 \t return [-16.551477 -25.859316], ([0.21517247 0.78849334]) \t loss 2.570E+01\n",
      "step 5630000 \t return [-16.5418   -25.736504], ([0.20109816 0.8179947 ]) \t loss 2.647E+01\n",
      "step 5640000 \t return [-16.423555 -25.832247], ([0.54831576 0.34152922]) \t loss 2.663E+01\n",
      "step 5650000 \t return [-16.291948 -25.661934], ([0.19950359 0.7561853 ]) \t loss 2.554E+01\n",
      "step 5660000 \t return [-16.16567  -25.593578], ([0.45689064 0.40320352]) \t loss 2.651E+01\n",
      "step 5670000 \t return [-16.2786   -25.641977], ([0.27317527 0.6209227 ]) \t loss 2.718E+01\n",
      "step 5680000 \t return [-16.09799 -25.50332], ([0.44096497 0.49277458]) \t loss 2.616E+01\n",
      "step 5690000 \t return [-16.402275 -25.858482], ([0.6035837 0.2907316]) \t loss 2.535E+01\n",
      "step 5700000 \t return [-16.589437 -25.999128], ([0.10811152 0.8186793 ]) \t loss 2.603E+01\n",
      "step 5710000 \t return [-16.288168 -25.734655], ([0.14255814 0.74111557]) \t loss 2.540E+01\n",
      "step 5720000 \t return [-16.162558 -25.501385], ([0.16017143 0.7518763 ]) \t loss 2.620E+01\n",
      "step 5730000 \t return [-16.078821 -25.47923 ], ([0.11731114 0.721357  ]) \t loss 2.501E+01\n",
      "step 5740000 \t return [-15.975727 -25.422571], ([0.28445506 0.6342607 ]) \t loss 2.588E+01\n",
      "step 5750000 \t return [-16.000835 -25.245508], ([0.29917845 0.69869083]) \t loss 2.581E+01\n",
      "step 5760000 \t return [-15.658797 -25.041607], ([0.28633693 0.60085076]) \t loss 2.523E+01\n",
      "step 5770000 \t return [-15.661905 -25.072495], ([0.6455513 0.3249861]) \t loss 2.611E+01\n",
      "step 5780000 \t return [-15.437614 -24.845356], ([0.5178166 0.3553921]) \t loss 2.593E+01\n",
      "step 5790000 \t return [-15.633148 -24.950075], ([0.37833965 0.5510301 ]) \t loss 2.806E+01\n",
      "step 5800000 \t return [-15.572334 -24.946758], ([0.26058158 0.6048604 ]) \t loss 2.687E+01\n",
      "step 5810000 \t return [-15.577518 -24.897383], ([0.26180094 0.68024963]) \t loss 2.605E+01\n",
      "step 5820000 \t return [-15.474222 -24.720753], ([0.4789257  0.46782055]) \t loss 2.624E+01\n",
      "step 5830000 \t return [-15.551104 -24.870613], ([0.35966733 0.58051777]) \t loss 2.544E+01\n",
      "step 5840000 \t return [-16.288963 -25.736216], ([0.09084015 0.7450116 ]) \t loss 2.534E+01\n",
      "step 5850000 \t return [-15.835226 -25.237114], ([0.17828627 0.74600446]) \t loss 2.736E+01\n",
      "step 5860000 \t return [-15.994254 -25.366644], ([0.11198681 0.69392234]) \t loss 2.595E+01\n",
      "step 5870000 \t return [-16.62691  -26.045506], ([0.0796993 0.8473798]) \t loss 2.715E+01\n",
      "step 5880000 \t return [-15.662594 -25.119448], ([0.24237195 0.60909593]) \t loss 2.632E+01\n",
      "step 5890000 \t return [-15.463139 -24.776672], ([0.5067433 0.4796424]) \t loss 2.563E+01\n",
      "step 5900000 \t return [-15.514697 -24.91192 ], ([0.26740986 0.7445913 ]) \t loss 2.649E+01\n",
      "step 5910000 \t return [-15.345977 -24.732388], ([0.5064012 0.4152793]) \t loss 2.634E+01\n",
      "step 5920000 \t return [-15.380553 -24.848944], ([0.21899082 0.7414552 ]) \t loss 2.579E+01\n",
      "step 5930000 \t return [-15.441612 -24.816246], ([0.5783946  0.33321202]) \t loss 2.539E+01\n",
      "step 5940000 \t return [-15.2961855 -24.567162 ], ([0.5054405  0.55086744]) \t loss 2.579E+01\n",
      "step 5950000 \t return [-15.382809 -24.558035], ([0.3250782 0.7597934]) \t loss 2.562E+01\n",
      "step 5960000 \t return [-15.166614 -24.500122], ([0.41176453 0.5087504 ]) \t loss 2.561E+01\n",
      "step 5970000 \t return [-15.427199 -24.772528], ([0.24678473 0.7224492 ]) \t loss 2.718E+01\n",
      "step 5980000 \t return [-15.784183 -25.06884 ], ([0.09748012 0.9211708 ]) \t loss 2.610E+01\n",
      "step 5990000 \t return [-15.208528 -24.576635], ([0.5233835 0.4120513]) \t loss 2.694E+01\n",
      "step 6000000 \t return [-15.320475 -24.503075], ([0.2851369  0.81584454]) \t loss 2.580E+01\n",
      "step 6010000 \t return [-15.041049 -24.38313 ], ([0.37833598 0.62437475]) \t loss 2.598E+01\n",
      "step 6020000 \t return [-15.337445 -24.554222], ([0.15466402 0.7922498 ]) \t loss 2.526E+01\n",
      "step 6030000 \t return [-14.999729 -24.376316], ([0.35993227 0.5779446 ]) \t loss 2.571E+01\n",
      "step 6040000 \t return [-14.800994 -23.91008 ], ([0.62750214 0.60581887]) \t loss 2.555E+01\n",
      "step 6050000 \t return [-15.004291 -24.303017], ([0.2902822 0.8029785]) \t loss 2.826E+01\n",
      "step 6060000 \t return [-15.061772 -24.299843], ([0.16347678 0.7828355 ]) \t loss 2.622E+01\n",
      "step 6070000 \t return [-15.323007 -24.65748 ], ([0.13271394 0.8146922 ]) \t loss 2.581E+01\n",
      "step 6080000 \t return [-15.190961 -24.527233], ([0.14664494 0.8460495 ]) \t loss 2.693E+01\n",
      "step 6090000 \t return [-14.496564 -23.825193], ([0.44175804 0.5233187 ]) \t loss 2.670E+01\n",
      "step 6100000 \t return [-15.896275 -25.189499], ([0.09552661 0.9827599 ]) \t loss 2.714E+01\n",
      "step 6110000 \t return [-15.247297 -24.45114 ], ([0.16674079 0.90325147]) \t loss 2.672E+01\n",
      "step 6120000 \t return [-14.479767 -23.778366], ([0.520424   0.52660304]) \t loss 2.629E+01\n",
      "step 6130000 \t return [-14.543017 -23.779234], ([0.69728994 0.35914332]) \t loss 2.651E+01\n",
      "step 6140000 \t return [-14.513428 -23.80135 ], ([0.4908674 0.600209 ]) \t loss 2.615E+01\n",
      "step 6150000 \t return [-14.3729   -23.706724], ([0.4153475 0.5959347]) \t loss 2.579E+01\n",
      "step 6160000 \t return [-14.408965 -23.68819 ], ([0.24713781 0.82017195]) \t loss 2.565E+01\n",
      "step 6170000 \t return [-14.372272 -23.662453], ([0.2945633  0.82653564]) \t loss 2.501E+01\n",
      "step 6180000 \t return [-14.179554 -23.51809 ], ([0.42482066 0.57344705]) \t loss 2.624E+01\n",
      "step 6190000 \t return [-14.335677 -23.594484], ([0.7136158 0.3065324]) \t loss 2.590E+01\n",
      "step 6200000 \t return [-14.228691 -23.462326], ([0.36869538 0.69552386]) \t loss 2.542E+01\n",
      "step 6210000 \t return [-14.196668 -23.442982], ([0.664483  0.3599576]) \t loss 2.644E+01\n",
      "step 6220000 \t return [-14.231763 -23.587059], ([0.3258438 0.751263 ]) \t loss 2.575E+01\n",
      "step 6230000 \t return [-14.134952 -23.417442], ([0.2816029 0.78715  ]) \t loss 2.623E+01\n",
      "step 6240000 \t return [-14.27248  -23.493969], ([0.6351251  0.52094924]) \t loss 2.531E+01\n",
      "step 6250000 \t return [-14.199563 -23.458767], ([0.3876124 0.7943613]) \t loss 2.623E+01\n",
      "step 6260000 \t return [-14.521014 -23.843193], ([0.11601912 0.91137725]) \t loss 2.520E+01\n",
      "step 6270000 \t return [-14.150033 -23.570337], ([0.35157374 0.6083411 ]) \t loss 2.726E+01\n",
      "step 6280000 \t return [-15.213929 -24.341703], ([0.08865851 0.9893358 ]) \t loss 2.756E+01\n",
      "step 6290000 \t return [-14.212821 -23.597794], ([0.22845285 0.754348  ]) \t loss 2.576E+01\n",
      "step 6300000 \t return [-14.285208 -23.525576], ([0.2775048 0.8818703]) \t loss 2.603E+01\n",
      "step 6310000 \t return [-14.141798 -23.508736], ([0.21079332 0.7506124 ]) \t loss 2.612E+01\n",
      "step 6320000 \t return [-13.899017 -23.245173], ([0.66227025 0.36443067]) \t loss 2.655E+01\n",
      "step 6330000 \t return [-14.150388 -23.401123], ([0.18435739 0.873168  ]) \t loss 2.691E+01\n",
      "step 6340000 \t return [-14.195083 -23.47835 ], ([0.56345594 0.5265381 ]) \t loss 2.690E+01\n",
      "step 6350000 \t return [-13.9607725 -23.344704 ], ([0.64338195 0.33121535]) \t loss 2.542E+01\n",
      "step 6360000 \t return [-13.923799 -23.34074 ], ([0.4927444  0.35893428]) \t loss 2.509E+01\n",
      "step 6370000 \t return [-14.140093 -23.549824], ([0.36965698 0.6134183 ]) \t loss 2.659E+01\n",
      "step 6380000 \t return [-14.273952 -23.415722], ([0.11960479 1.0235314 ]) \t loss 2.517E+01\n",
      "step 6390000 \t return [-14.254858 -23.359951], ([0.29184023 0.81775075]) \t loss 2.603E+01\n",
      "step 6400000 \t return [-13.920954 -23.337612], ([0.37499192 0.52978796]) \t loss 2.510E+01\n",
      "step 6410000 \t return [-14.172498 -23.514952], ([0.30530438 0.6596657 ]) \t loss 2.601E+01\n",
      "step 6420000 \t return [-14.038763 -23.30148 ], ([0.23257056 0.704064  ]) \t loss 2.680E+01\n",
      "step 6430000 \t return [-13.77397  -23.123873], ([0.3041639  0.68020123]) \t loss 2.565E+01\n",
      "step 6440000 \t return [-13.820632 -23.238375], ([0.6479546 0.3849934]) \t loss 2.677E+01\n",
      "step 6450000 \t return [-14.033949 -23.45287 ], ([0.60470974 0.22860311]) \t loss 2.610E+01\n",
      "step 6460000 \t return [-14.10947  -23.329405], ([0.7576131 0.2811973]) \t loss 2.467E+01\n",
      "step 6470000 \t return [-13.715436 -22.909962], ([0.53278404 0.5912852 ]) \t loss 2.630E+01\n",
      "step 6480000 \t return [-14.137277 -23.257462], ([0.13805641 0.98185664]) \t loss 2.656E+01\n",
      "step 6490000 \t return [-13.938206 -23.137634], ([0.3354992 0.8025299]) \t loss 2.539E+01\n",
      "step 6500000 \t return [-13.484084 -22.59523 ], ([0.51725185 0.6216963 ]) \t loss 2.616E+01\n",
      "step 6510000 \t return [-13.463339 -22.53008 ], ([0.39487028 0.7412695 ]) \t loss 2.609E+01\n",
      "step 6520000 \t return [-13.267627 -22.49576 ], ([0.49401286 0.6468228 ]) \t loss 2.547E+01\n",
      "step 6530000 \t return [-13.205693 -22.231905], ([0.5403011  0.64168125]) \t loss 2.607E+01\n",
      "step 6540000 \t return [-13.060894 -22.398226], ([0.614583   0.39323866]) \t loss 2.682E+01\n",
      "step 6550000 \t return [-13.642237 -22.764544], ([0.15870616 0.9701281 ]) \t loss 2.688E+01\n",
      "step 6560000 \t return [-13.64995  -22.835289], ([0.17963865 0.9069211 ]) \t loss 2.595E+01\n",
      "step 6570000 \t return [-13.040239 -22.298662], ([0.6033844  0.49629158]) \t loss 2.599E+01\n",
      "step 6580000 \t return [-13.04086  -22.235573], ([0.5857879 0.513192 ]) \t loss 2.597E+01\n",
      "step 6590000 \t return [-13.5828285 -22.772139 ], ([0.27549404 0.8907337 ]) \t loss 2.517E+01\n",
      "step 6600000 \t return [-13.395817 -22.67366 ], ([0.27348927 0.8222238 ]) \t loss 2.626E+01\n",
      "step 6610000 \t return [-13.242016 -22.469816], ([0.7782172  0.33767003]) \t loss 2.527E+01\n",
      "step 6620000 \t return [-13.127139 -22.216202], ([0.7567326  0.43316406]) \t loss 2.540E+01\n",
      "step 6630000 \t return [-12.773334 -22.072067], ([0.5406922  0.45197588]) \t loss 2.637E+01\n",
      "step 6640000 \t return [-12.742509 -21.917334], ([0.6676099  0.48988754]) \t loss 2.480E+01\n",
      "step 6650000 \t return [-14.069815 -23.275364], ([0.07170891 0.99065655]) \t loss 2.517E+01\n",
      "step 6660000 \t return [-14.2032175 -23.439817 ], ([0.0729264 1.108808 ]) \t loss 2.686E+01\n",
      "step 6670000 \t return [-13.136191 -22.380571], ([0.26901072 0.9085976 ]) \t loss 2.518E+01\n",
      "step 6680000 \t return [-13.499437 -22.63479 ], ([0.13884038 1.1375632 ]) \t loss 2.563E+01\n",
      "step 6690000 \t return [-12.646692 -21.875673], ([0.5718404 0.5980866]) \t loss 2.626E+01\n",
      "step 6700000 \t return [-12.954006 -22.008049], ([0.35615587 0.8652078 ]) \t loss 2.610E+01\n",
      "step 6710000 \t return [-13.469434 -22.609312], ([0.23727188 0.9599664 ]) \t loss 2.546E+01\n",
      "step 6720000 \t return [-12.978842 -21.978405], ([0.7586046  0.39470607]) \t loss 2.563E+01\n",
      "step 6730000 \t return [-12.560931 -21.586758], ([0.78813183 0.6652131 ]) \t loss 2.621E+01\n",
      "step 6740000 \t return [-12.681058 -21.874014], ([0.76889235 0.34502783]) \t loss 2.602E+01\n",
      "step 6750000 \t return [-12.392684 -21.748781], ([0.5417318 0.5880836]) \t loss 2.626E+01\n",
      "step 6760000 \t return [-12.581317 -21.851082], ([0.6887698  0.30001318]) \t loss 2.589E+01\n",
      "step 6770000 \t return [-13.449771 -22.634455], ([0.10831767 1.1195064 ]) \t loss 2.781E+01\n",
      "step 6780000 \t return [-12.422498 -21.653511], ([0.70467174 0.421677  ]) \t loss 2.549E+01\n",
      "step 6790000 \t return [-12.564279 -21.793015], ([0.4422764 0.6564524]) \t loss 2.643E+01\n",
      "step 6800000 \t return [-11.980553 -21.260784], ([0.80565053 0.44427192]) \t loss 2.698E+01\n",
      "step 6810000 \t return [-12.329988 -21.468657], ([0.8331951  0.39070225]) \t loss 2.531E+01\n",
      "step 6820000 \t return [-12.151027 -21.12003 ], ([0.99247223 0.39683428]) \t loss 2.750E+01\n",
      "step 6830000 \t return [-12.049529 -21.224268], ([0.98678356 0.3178098 ]) \t loss 2.649E+01\n",
      "step 6840000 \t return [-12.232749 -21.175247], ([0.7634243  0.61095196]) \t loss 2.573E+01\n",
      "step 6850000 \t return [-12.336404 -21.495674], ([0.51685387 0.6765135 ]) \t loss 2.612E+01\n",
      "step 6860000 \t return [-12.869899 -21.8848  ], ([0.27334493 1.0090978 ]) \t loss 2.483E+01\n",
      "step 6870000 \t return [-12.651176 -21.593723], ([0.421609  0.9949855]) \t loss 2.635E+01\n",
      "step 6880000 \t return [-12.017006 -21.21628 ], ([0.62423134 0.61875975]) \t loss 2.483E+01\n",
      "step 6890000 \t return [-12.678678 -21.690453], ([0.30345723 1.0738003 ]) \t loss 2.542E+01\n",
      "step 6900000 \t return [-11.996091 -21.16222 ], ([0.6398212 0.5681845]) \t loss 2.595E+01\n",
      "step 6910000 \t return [-11.821539 -20.778627], ([0.7312292  0.60832584]) \t loss 2.511E+01\n",
      "step 6920000 \t return [-12.762347 -21.793022], ([0.1407496 1.1774305]) \t loss 2.536E+01\n",
      "step 6930000 \t return [-12.260141 -21.177025], ([0.17890204 1.1271187 ]) \t loss 2.676E+01\n",
      "step 6940000 \t return [-11.893895 -20.90443 ], ([0.9270086  0.35592085]) \t loss 2.644E+01\n",
      "step 6950000 \t return [-12.506133 -21.565035], ([0.06227   1.2420169]) \t loss 2.756E+01\n",
      "step 6960000 \t return [-11.565424 -20.658947], ([0.9600982  0.37944147]) \t loss 2.585E+01\n",
      "step 6970000 \t return [-11.57965  -20.251144], ([0.676923  0.8526397]) \t loss 2.684E+01\n",
      "step 6980000 \t return [-12.163821 -21.189173], ([0.20710745 1.2424766 ]) \t loss 2.763E+01\n",
      "step 6990000 \t return [-11.343414 -20.36995 ], ([0.5958822 0.8346398]) \t loss 2.555E+01\n",
      "step 7000000 \t return [-12.083133 -21.16067 ], ([0.19633909 1.076674  ]) \t loss 2.788E+01\n",
      "step 7010000 \t return [-11.30798 -20.18328], ([0.6350065 0.9182244]) \t loss 2.635E+01\n",
      "step 7020000 \t return [-11.857496 -20.907444], ([0.3865309 1.1535527]) \t loss 2.731E+01\n",
      "step 7030000 \t return [-11.377442 -20.271925], ([0.8363382  0.70747864]) \t loss 2.570E+01\n",
      "step 7040000 \t return [-11.25725  -20.170994], ([0.574235  1.0145129]) \t loss 2.683E+01\n",
      "step 7050000 \t return [-11.496271 -20.580488], ([0.95219547 0.31878322]) \t loss 2.526E+01\n",
      "step 7060000 \t return [-12.542045 -21.665815], ([0.06387697 1.2041898 ]) \t loss 2.552E+01\n",
      "step 7070000 \t return [-11.094142 -20.003948], ([0.5204373  0.96466047]) \t loss 2.493E+01\n",
      "step 7080000 \t return [-11.488723 -20.344791], ([0.3378495 1.1959966]) \t loss 2.663E+01\n",
      "step 7090000 \t return [-11.070875 -19.967718], ([0.6223673  0.87505734]) \t loss 2.646E+01\n",
      "step 7100000 \t return [-11.112029 -20.212706], ([0.4634484  0.96007425]) \t loss 2.554E+01\n",
      "step 7110000 \t return [-10.934644 -19.991066], ([0.47109738 0.83621866]) \t loss 2.683E+01\n",
      "step 7120000 \t return [-12.128609 -20.918455], ([0.29327175 1.4459152 ]) \t loss 2.623E+01\n",
      "step 7130000 \t return [-12.726393 -21.612463], ([0.09637921 1.5166644 ]) \t loss 2.660E+01\n",
      "step 7140000 \t return [-12.535503 -21.491604], ([0.08251216 1.3106086 ]) \t loss 2.632E+01\n",
      "step 7150000 \t return [-11.18087  -20.093843], ([1.2669431  0.51994663]) \t loss 2.656E+01\n",
      "step 7160000 \t return [-11.130688 -20.117477], ([1.1639458 0.9190687]) \t loss 3.243E+01\n",
      "step 7170000 \t return [-10.853246 -19.948471], ([0.50593185 0.8022267 ]) \t loss 3.047E+01\n",
      "step 7180000 \t return [-12.121316 -20.761074], ([0.17502971 1.574809  ]) \t loss 2.652E+01\n",
      "step 7190000 \t return [-12.518789 -21.469336], ([0.09093919 1.2695073 ]) \t loss 2.609E+01\n",
      "step 7200000 \t return [-10.946813 -20.062077], ([0.39656886 1.004977  ]) \t loss 2.693E+01\n",
      "step 7210000 \t return [-12.434513 -21.219498], ([0.0646958 1.3961788]) \t loss 2.741E+01\n",
      "step 7220000 \t return [-10.887913 -19.723778], ([1.870326   0.51119804]) \t loss 2.647E+01\n",
      "step 7230000 \t return [-11.541487 -20.554811], ([0.14878234 1.1900566 ]) \t loss 2.781E+01\n",
      "step 7240000 \t return [-11.530061 -20.644285], ([0.1862229 1.2543805]) \t loss 2.588E+01\n",
      "step 7250000 \t return [-10.618363 -19.564058], ([0.89180785 0.4815101 ]) \t loss 2.746E+01\n",
      "step 7260000 \t return [-10.867346 -19.829462], ([0.73781484 0.9414766 ]) \t loss 2.584E+01\n",
      "step 7270000 \t return [-10.89993 -19.6159 ], ([1.9882582  0.56330556]) \t loss 2.627E+01\n",
      "step 7280000 \t return [-10.588493 -19.44022 ], ([0.99476594 0.38028204]) \t loss 2.876E+01\n",
      "step 7290000 \t return [-11.367115 -20.135483], ([0.38549432 1.3286726 ]) \t loss 2.709E+01\n",
      "step 7300000 \t return [-10.5621395 -19.49267  ], ([0.8720265 0.8150633]) \t loss 2.761E+01\n",
      "step 7310000 \t return [-11.04191  -19.845678], ([0.3529235 1.2308711]) \t loss 2.673E+01\n",
      "step 7320000 \t return [-11.046259 -20.248774], ([0.262737  1.1693703]) \t loss 2.627E+01\n",
      "step 7330000 \t return [-10.652169 -19.724373], ([0.43025684 0.9440154 ]) \t loss 2.687E+01\n",
      "step 7340000 \t return [-10.789375 -19.390114], ([2.2193642  0.33374077]) \t loss 2.547E+01\n",
      "step 7350000 \t return [-10.431652 -19.320745], ([0.7157499  0.76415443]) \t loss 2.710E+01\n",
      "step 7360000 \t return [-10.409699 -19.237862], ([0.7597138  0.76095706]) \t loss 2.560E+01\n",
      "step 7370000 \t return [-11.715824 -20.571684], ([0.16742125 1.4249827 ]) \t loss 2.611E+01\n",
      "step 7380000 \t return [-10.51162  -19.380608], ([0.3498964 1.104271 ]) \t loss 2.582E+01\n",
      "step 7390000 \t return [-11.082293 -19.861946], ([0.3328141 1.3037277]) \t loss 2.521E+01\n",
      "step 7400000 \t return [-10.828539 -19.935087], ([0.24859463 1.2077968 ]) \t loss 2.557E+01\n",
      "step 7410000 \t return [-10.558795 -19.42365 ], ([1.5764787 0.8727611]) \t loss 2.645E+01\n",
      "step 7420000 \t return [-11.284833 -20.025677], ([1.3549025 1.2821294]) \t loss 2.703E+01\n",
      "step 7430000 \t return [-10.274753 -19.048128], ([1.2083415  0.43691406]) \t loss 2.565E+01\n",
      "step 7440000 \t return [-10.758481 -19.734167], ([0.2272581 1.2119708]) \t loss 2.649E+01\n",
      "step 7450000 \t return [-10.362964 -19.297052], ([0.24953295 1.1151679 ]) \t loss 2.534E+01\n",
      "step 7460000 \t return [-10.54088  -19.018568], ([2.8348012 1.0337797]) \t loss 2.595E+01\n",
      "step 7470000 \t return [-10.8647995 -19.75827  ], ([0.20816056 1.2830328 ]) \t loss 2.872E+01\n",
      "step 7480000 \t return [-10.456642 -19.237778], ([1.4917994  0.99679375]) \t loss 2.596E+01\n",
      "step 7490000 \t return [-11.244801 -20.37287 ], ([0.41971162 1.0264856 ]) \t loss 2.725E+01\n",
      "step 7500000 \t return [-10.159969 -18.758394], ([1.0440872 0.5982666]) \t loss 2.544E+01\n",
      "step 7510000 \t return [-10.483133 -19.058308], ([2.0159674  0.69671667]) \t loss 2.700E+01\n",
      "step 7520000 \t return [-10.204907 -19.129993], ([0.2554119 1.1465899]) \t loss 2.602E+01\n",
      "step 7530000 \t return [-11.148716 -20.18208 ], ([0.28569666 1.1864316 ]) \t loss 2.484E+01\n",
      "step 7540000 \t return [-10.199649 -19.102634], ([1.1100113 1.121004 ]) \t loss 2.567E+01\n",
      "step 7550000 \t return [-10.02307  -18.914642], ([0.8591969 0.5649836]) \t loss 2.610E+01\n",
      "step 7560000 \t return [ -9.748265 -18.797556], ([0.40206924 1.0487791 ]) \t loss 2.454E+01\n",
      "step 7570000 \t return [-11.55114  -19.511602], ([4.0842466 1.1272658]) \t loss 2.614E+01\n",
      "step 7580000 \t return [ -9.642776 -18.42349 ], ([0.40479177 1.1322142 ]) \t loss 2.648E+01\n",
      "step 7590000 \t return [ -9.730136 -18.49492 ], ([0.58206964 1.1129481 ]) \t loss 2.596E+01\n",
      "step 7600000 \t return [ -9.869255 -18.731468], ([1.3114977 1.1759295]) \t loss 2.546E+01\n",
      "step 7610000 \t return [ -9.665043 -18.634495], ([0.41434166 1.0767449 ]) \t loss 2.641E+01\n",
      "step 7620000 \t return [ -9.6992855 -18.356968 ], ([0.5388849 1.0928249]) \t loss 2.544E+01\n",
      "step 7630000 \t return [ -9.575024 -18.363016], ([0.4770311 1.0085336]) \t loss 2.563E+01\n",
      "step 7640000 \t return [ -9.693459 -18.574608], ([0.80060375 1.1657977 ]) \t loss 2.582E+01\n",
      "step 7650000 \t return [ -9.491312 -18.3397  ], ([0.73633146 0.7876752 ]) \t loss 2.498E+01\n",
      "step 7660000 \t return [ -9.502812 -18.201506], ([0.90114975 0.73685056]) \t loss 2.634E+01\n",
      "step 7670000 \t return [ -9.536099 -18.115538], ([1.2120756  0.28551382]) \t loss 2.620E+01\n",
      "step 7680000 \t return [ -9.662565 -18.437048], ([1.2744458 1.2308639]) \t loss 2.709E+01\n",
      "step 7690000 \t return [ -9.573067 -18.069942], ([2.3778174  0.38629046]) \t loss 2.583E+01\n",
      "step 7700000 \t return [-13.3501625 -17.68783  ], ([8.121583  1.1702386]) \t loss 2.674E+01\n",
      "step 7710000 \t return [ -8.551594 -17.298796], ([1.2673302 0.9954885]) \t loss 2.809E+01\n",
      "step 7720000 \t return [ -9.4580145 -18.404913 ], ([0.44112   1.3752507]) \t loss 2.572E+01\n",
      "step 7730000 \t return [-12.645092 -16.186562], ([7.9112134 0.8986617]) \t loss 2.539E+01\n",
      "step 7740000 \t return [-14.364906 -15.099737], ([9.784087   0.77211237]) \t loss 2.823E+01\n",
      "step 7750000 \t return [-18.135866 -15.92351 ], ([10.638721   1.2356839]) \t loss 2.987E+01\n",
      "step 7760000 \t return [ -9.407102 -16.1866  ], ([5.102729  1.4404978]) \t loss 2.791E+01\n",
      "step 7770000 \t return [-12.589744 -15.47845 ], ([8.602858  1.0063794]) \t loss 2.653E+01\n",
      "step 7780000 \t return [-12.260455 -15.510723], ([8.735872   0.82668865]) \t loss 2.712E+01\n",
      "step 7790000 \t return [-11.17124  -15.588757], ([7.7405486 1.1636015]) \t loss 2.704E+01\n",
      "step 7800000 \t return [-10.927465 -15.067527], ([7.0940604 0.5285742]) \t loss 2.612E+01\n",
      "step 7810000 \t return [-14.428775 -15.665942], ([9.912505  1.3249289]) \t loss 2.683E+01\n",
      "step 7820000 \t return [ -9.63903   -14.7944975], ([6.530464  0.7417672]) \t loss 2.645E+01\n",
      "step 7830000 \t return [ -8.836607 -14.902004], ([4.9899387  0.48847032]) \t loss 2.646E+01\n",
      "step 7840000 \t return [ -9.679015 -14.496797], ([6.9286513 0.8594381]) \t loss 2.668E+01\n",
      "step 7850000 \t return [-18.909668 -14.827078], ([8.280399  1.3412845]) \t loss 2.503E+01\n",
      "step 7860000 \t return [-15.976836 -14.949542], ([9.351918  1.2182561]) \t loss 2.676E+01\n",
      "step 7870000 \t return [-16.91574  -14.583301], ([8.879931  1.1649896]) \t loss 2.647E+01\n",
      "step 7880000 \t return [-17.034975 -14.956259], ([9.927122  1.3744324]) \t loss 2.548E+01\n",
      "step 7890000 \t return [-16.124592 -15.285703], ([7.914194  0.9675595]) \t loss 2.749E+01\n",
      "step 7900000 \t return [-17.763264 -14.516932], ([9.043657  1.2263714]) \t loss 2.671E+01\n",
      "step 7910000 \t return [-14.274764 -14.194117], ([9.454271  1.1041502]) \t loss 2.616E+01\n",
      "step 7920000 \t return [-25.044975 -14.690421], ([8.764025  1.5126724]) \t loss 2.630E+01\n",
      "step 7930000 \t return [-17.788805 -14.166482], ([10.448194   1.1075295]) \t loss 2.665E+01\n",
      "step 7940000 \t return [-14.247179 -14.176256], ([7.695955   0.91220903]) \t loss 2.594E+01\n",
      "step 7950000 \t return [-10.5157385 -13.480154 ], ([7.230618  0.7849341]) \t loss 2.571E+01\n",
      "step 7960000 \t return [-19.705765 -13.434437], ([9.172308  1.2915133]) \t loss 2.633E+01\n",
      "step 7970000 \t return [-10.952507 -13.594972], ([7.7427373 0.7605658]) \t loss 2.684E+01\n",
      "step 7980000 \t return [ -9.072402 -13.623126], ([4.8817816 0.6357317]) \t loss 2.612E+01\n",
      "step 7990000 \t return [-12.378197 -13.974774], ([5.78847   0.6972549]) \t loss 2.605E+01\n",
      "step 8000000 \t return [ -9.345643 -13.130813], ([5.886403  0.6277288]) \t loss 2.552E+01\n",
      "step 8010000 \t return [-18.408663 -12.980283], ([9.722778 1.304819]) \t loss 2.583E+01\n",
      "step 8020000 \t return [-14.514248 -12.98736 ], ([9.219879  1.1299157]) \t loss 2.573E+01\n",
      "step 8030000 \t return [ -9.860988 -13.195016], ([6.101863   0.78237796]) \t loss 2.603E+01\n",
      "step 8040000 \t return [ -8.81881  -12.792149], ([4.7028627 0.5638342]) \t loss 2.623E+01\n",
      "step 8050000 \t return [-13.410303 -12.565574], ([10.094156   0.8577107]) \t loss 2.437E+01\n",
      "step 8060000 \t return [-11.793063 -13.208294], ([6.8300285 0.7675089]) \t loss 2.595E+01\n",
      "step 8070000 \t return [-14.1836815 -12.393633 ], ([8.305744  0.9292522]) \t loss 2.519E+01\n",
      "step 8080000 \t return [-12.688938 -12.610914], ([6.4011354 0.6782644]) \t loss 2.596E+01\n",
      "step 8090000 \t return [-17.091663 -12.158324], ([10.015344    0.83348453]) \t loss 2.476E+01\n",
      "step 8100000 \t return [-11.492572 -12.590311], ([6.0988517 0.6576879]) \t loss 2.559E+01\n",
      "step 8110000 \t return [-11.532696 -12.823087], ([4.712877  0.8554964]) \t loss 2.519E+01\n",
      "step 8120000 \t return [-15.393433 -12.360753], ([8.991919   0.92405355]) \t loss 2.668E+01\n",
      "step 8130000 \t return [-11.169696 -12.443019], ([6.5700088 0.6497931]) \t loss 2.527E+01\n",
      "step 8140000 \t return [-10.165    -12.501818], ([6.3655386 0.6058227]) \t loss 2.520E+01\n",
      "step 8150000 \t return [-13.116267 -12.257788], ([9.633948  0.7826542]) \t loss 2.570E+01\n",
      "step 8160000 \t return [ -8.929516 -12.636389], ([5.5649176 0.5504319]) \t loss 2.527E+01\n",
      "step 8170000 \t return [-17.333826 -12.225852], ([9.49765   0.9052422]) \t loss 2.539E+01\n",
      "step 8180000 \t return [-10.585297 -11.896078], ([7.1534896 0.5880496]) \t loss 2.532E+01\n",
      "step 8190000 \t return [-21.2101   -12.090381], ([10.685128   0.9817127]) \t loss 2.507E+01\n",
      "step 8200000 \t return [-15.0659275 -12.279449 ], ([7.8664393  0.67989224]) \t loss 2.454E+01\n",
      "step 8210000 \t return [ -9.483315 -12.836453], ([5.003174   0.51512045]) \t loss 2.505E+01\n",
      "step 8220000 \t return [ -6.555733 -12.269445], ([3.4707286 0.3311054]) \t loss 2.683E+01\n",
      "step 8230000 \t return [ -9.445913 -12.941118], ([5.9284334 0.545996 ]) \t loss 2.538E+01\n",
      "step 8240000 \t return [ -7.449653 -12.858656], ([3.2170463  0.42954084]) \t loss 2.455E+01\n",
      "step 8250000 \t return [-15.212556 -11.637301], ([11.464844   0.7741271]) \t loss 2.357E+01\n",
      "step 8260000 \t return [-23.43485  -11.946844], ([6.4209337 1.1576586]) \t loss 2.513E+01\n",
      "step 8270000 \t return [-28.850212 -11.179547], ([10.422831   1.2038013]) \t loss 2.582E+01\n",
      "step 8280000 \t return [-21.758196 -11.298931], ([10.584085  1.141189]) \t loss 2.543E+01\n",
      "step 8290000 \t return [-17.12821  -11.595028], ([9.596949  0.8870146]) \t loss 2.673E+01\n",
      "step 8300000 \t return [-19.646349 -11.286735], ([9.078623  0.8731939]) \t loss 2.503E+01\n",
      "step 8310000 \t return [-13.232761 -11.555268], ([7.6000667  0.60964996]) \t loss 2.498E+01\n",
      "step 8320000 \t return [-16.51618  -11.568452], ([10.788306   0.8792678]) \t loss 2.553E+01\n",
      "step 8330000 \t return [-18.089094 -11.683842], ([12.111984   0.9085546]) \t loss 2.586E+01\n",
      "step 8340000 \t return [-47.8151   -12.249634], ([4.5967593 1.0940948]) \t loss 2.567E+01\n",
      "step 8350000 \t return [-11.306123 -11.590287], ([7.846576   0.59408987]) \t loss 2.619E+01\n",
      "step 8360000 \t return [-43.984726 -12.726005], ([5.2328753 1.5210713]) \t loss 2.470E+01\n",
      "step 8370000 \t return [-31.264877 -12.142976], ([6.649052  1.3952023]) \t loss 2.614E+01\n",
      "step 8380000 \t return [-29.406862 -10.986114], ([15.126847   0.9931094]) \t loss 2.612E+01\n",
      "step 8390000 \t return [-10.995209 -11.673836], ([6.6411295 0.7243801]) \t loss 2.554E+01\n",
      "step 8400000 \t return [-26.569363 -11.240993], ([14.623223   1.1539649]) \t loss 2.412E+01\n",
      "step 8410000 \t return [-10.386547 -11.437445], ([8.103977  0.5627956]) \t loss 2.416E+01\n",
      "step 8420000 \t return [-26.206543 -12.337438], ([8.608567  1.6869664]) \t loss 2.583E+01\n",
      "step 8430000 \t return [-14.54412   -11.5406685], ([10.923553    0.76384866]) \t loss 2.651E+01\n",
      "step 8440000 \t return [-15.879748 -11.266891], ([7.574297  0.7269773]) \t loss 2.597E+01\n",
      "step 8450000 \t return [ -6.4676533 -11.855476 ], ([4.1331654 0.4349788]) \t loss 2.442E+01\n",
      "step 8460000 \t return [ -6.1332116 -12.038102 ], ([3.5277085  0.40537706]) \t loss 2.459E+01\n",
      "step 8470000 \t return [-16.630304 -11.103669], ([11.346814    0.98563945]) \t loss 2.436E+01\n",
      "step 8480000 \t return [-16.899107 -11.01212 ], ([9.092649  0.8642746]) \t loss 2.441E+01\n",
      "step 8490000 \t return [-10.242445 -11.374565], ([8.190081  0.6240316]) \t loss 2.580E+01\n",
      "step 8500000 \t return [ -7.311228 -11.891079], ([6.197119  0.5667702]) \t loss 2.418E+01\n",
      "step 8510000 \t return [ -9.879103 -11.562484], ([4.8509264  0.72441953]) \t loss 2.456E+01\n",
      "step 8520000 \t return [ -7.4822583 -11.4196205], ([4.986667   0.46911722]) \t loss 2.444E+01\n",
      "step 8530000 \t return [ -9.8941555 -11.557189 ], ([8.1538105 0.6164368]) \t loss 2.583E+01\n",
      "step 8540000 \t return [-11.421107 -11.054071], ([7.5568905  0.65863657]) \t loss 2.493E+01\n",
      "step 8550000 \t return [-12.362279 -11.068046], ([7.177984   0.66270643]) \t loss 2.512E+01\n",
      "step 8560000 \t return [ -8.884607 -11.665575], ([5.0363765 0.6578692]) \t loss 2.486E+01\n",
      "step 8570000 \t return [-13.523125 -11.398783], ([8.391349 0.948965]) \t loss 2.525E+01\n",
      "step 8580000 \t return [-20.529331 -12.820151], ([4.470068  1.6285539]) \t loss 2.490E+01\n",
      "step 8590000 \t return [ -7.6966224 -11.498981 ], ([5.4287252  0.51123077]) \t loss 2.599E+01\n",
      "step 8600000 \t return [-25.599092 -11.19607 ], ([16.390167   1.0373857]) \t loss 2.588E+01\n",
      "step 8610000 \t return [-21.401936 -11.894999], ([7.8602004 1.8722562]) \t loss 2.568E+01\n",
      "step 8620000 \t return [-38.926094 -11.624559], ([19.463945   1.5403934]) \t loss 2.637E+01\n",
      "step 8630000 \t return [ -7.1992583 -11.609784 ], ([4.638701  0.5168621]) \t loss 2.655E+01\n",
      "step 8640000 \t return [-19.042707 -10.954037], ([11.730843   0.8969222]) \t loss 2.494E+01\n",
      "step 8650000 \t return [-27.00832  -12.383747], ([8.254055  1.6750377]) \t loss 2.427E+01\n",
      "step 8660000 \t return [ -6.738314 -11.805033], ([4.268599  0.4790382]) \t loss 2.550E+01\n",
      "step 8670000 \t return [-26.224709 -11.00047 ], ([16.56624    1.1454568]) \t loss 2.414E+01\n",
      "step 8680000 \t return [ -8.529697 -11.453508], ([6.4937882  0.66158736]) \t loss 2.475E+01\n",
      "step 8690000 \t return [-20.323347  -11.8733225], ([8.768579  1.5862489]) \t loss 2.351E+01\n",
      "step 8700000 \t return [ -9.09008   -11.3455715], ([5.0224414 0.550744 ]) \t loss 2.514E+01\n",
      "step 8710000 \t return [-49.860577 -11.34961 ], ([18.449987   1.1380247]) \t loss 2.291E+01\n",
      "step 8720000 \t return [-21.948149 -10.851339], ([15.193425   0.9664583]) \t loss 2.434E+01\n",
      "step 8730000 \t return [-25.003897 -11.238736], ([9.904294  1.2003253]) \t loss 2.508E+01\n",
      "step 8740000 \t return [ -6.5728474 -11.856564 ], ([3.7811286  0.44925985]) \t loss 2.324E+01\n",
      "step 8750000 \t return [-11.90475  -10.917403], ([10.365927   0.7185905]) \t loss 2.531E+01\n",
      "step 8760000 \t return [ -6.685689 -11.724797], ([3.7075171 0.5105179]) \t loss 2.478E+01\n",
      "step 8770000 \t return [-21.25403  -11.717976], ([6.1181474 1.5045   ]) \t loss 2.459E+01\n",
      "step 8780000 \t return [-17.953892 -10.877777], ([13.975487   0.8602699]) \t loss 2.436E+01\n",
      "step 8790000 \t return [-29.89162  -10.759819], ([21.583204    0.90133625]) \t loss 2.452E+01\n",
      "step 8800000 \t return [ -5.76642  -11.960579], ([3.7563841  0.45273203]) \t loss 2.524E+01\n",
      "step 8810000 \t return [ -7.000425 -11.474042], ([4.36073    0.60651934]) \t loss 2.378E+01\n",
      "step 8820000 \t return [ -7.3369374 -11.497669 ], ([5.0886917 0.5283783]) \t loss 2.422E+01\n",
      "step 8830000 \t return [-21.036266 -10.942374], ([10.1506405  1.1245577]) \t loss 2.412E+01\n",
      "step 8840000 \t return [-18.401548 -11.180183], ([8.957496  1.3498076]) \t loss 2.507E+01\n",
      "step 8850000 \t return [-14.371577 -10.821297], ([11.032078   0.7161521]) \t loss 2.404E+01\n",
      "step 8860000 \t return [-37.817833 -11.044129], ([20.198395   0.9612459]) \t loss 2.279E+01\n",
      "step 8870000 \t return [-16.047537 -11.445382], ([9.686014  1.4757272]) \t loss 2.458E+01\n",
      "step 8880000 \t return [-22.38728  -12.015918], ([10.898609  2.211865]) \t loss 2.488E+01\n",
      "step 8890000 \t return [-28.112827 -13.5196  ], ([6.035848  3.0598862]) \t loss 2.558E+01\n",
      "step 8900000 \t return [-14.118822 -10.754128], ([12.562585    0.56216085]) \t loss 2.506E+01\n",
      "step 8910000 \t return [-38.140324 -11.109625], ([15.558069   1.1856474]) \t loss 2.399E+01\n",
      "step 8920000 \t return [-30.357576 -11.590127], ([13.75287    1.7899262]) \t loss 2.472E+01\n",
      "step 8930000 \t return [-18.711514 -10.748521], ([15.306065    0.73030216]) \t loss 2.358E+01\n",
      "step 8940000 \t return [-14.697016 -10.719187], ([13.189423   0.7153455]) \t loss 2.442E+01\n",
      "step 8950000 \t return [-12.200764 -12.12588 ], ([3.3668587 1.5179255]) \t loss 2.256E+01\n",
      "step 8960000 \t return [-23.871677 -12.609472], ([7.1932507 2.5439467]) \t loss 2.397E+01\n",
      "step 8970000 \t return [-16.92028  -10.739141], ([14.093211  0.738181]) \t loss 2.444E+01\n",
      "step 8980000 \t return [-10.550577 -11.248626], ([8.837889   0.60691637]) \t loss 2.339E+01\n",
      "step 8990000 \t return [-17.113174 -11.003222], ([14.098216    0.98858213]) \t loss 2.516E+01\n",
      "step 9000000 \t return [ -5.074181 -12.039634], ([2.801627   0.72022796]) \t loss 2.461E+01\n",
      "step 9010000 \t return [ -5.9896526 -11.661655 ], ([3.1969478 0.6037459]) \t loss 2.508E+01\n",
      "step 9020000 \t return [-25.179296 -11.26501 ], ([12.630896   1.3980036]) \t loss 2.355E+01\n",
      "step 9030000 \t return [-22.900425 -10.763472], ([17.171484    0.83702046]) \t loss 2.530E+01\n",
      "step 9040000 \t return [-29.492382 -11.466914], ([13.130851  2.150332]) \t loss 2.456E+01\n",
      "step 9050000 \t return [-10.857953 -11.821353], ([3.625988 1.061826]) \t loss 2.599E+01\n",
      "step 9060000 \t return [-16.710377  -11.9179325], ([4.3966236 1.6410388]) \t loss 2.465E+01\n",
      "step 9070000 \t return [ -8.933578 -11.223758], ([6.289077  0.6436318]) \t loss 2.466E+01\n",
      "step 9080000 \t return [ -7.629755 -11.471511], ([5.1124187 0.5940852]) \t loss 2.508E+01\n",
      "step 9090000 \t return [ -8.440298 -11.146886], ([5.893435  0.6595672]) \t loss 2.443E+01\n",
      "step 9100000 \t return [-11.851087 -10.859276], ([10.967366    0.83555895]) \t loss 2.531E+01\n",
      "step 9110000 \t return [-23.495    -11.462266], ([10.059909   1.9575934]) \t loss 2.506E+01\n",
      "step 9120000 \t return [-12.695701 -10.797948], ([10.079907   0.6809205]) \t loss 2.466E+01\n",
      "step 9130000 \t return [-22.83652 -10.66184], ([18.500046    0.91903317]) \t loss 2.416E+01\n",
      "step 9140000 \t return [-17.987635 -10.783971], ([13.847111    0.67849827]) \t loss 2.470E+01\n",
      "step 9150000 \t return [ -9.606495 -11.165546], ([7.9080386  0.55057836]) \t loss 2.484E+01\n",
      "step 9160000 \t return [ -7.5868034 -11.324798 ], ([6.050747   0.64473456]) \t loss 2.445E+01\n",
      "step 9170000 \t return [ -8.623793 -11.084865], ([7.0226507 0.5632206]) \t loss 2.476E+01\n",
      "step 9180000 \t return [-57.48805  -10.926983], ([13.000127  1.161709]) \t loss 2.504E+01\n",
      "step 9190000 \t return [-13.950806 -10.857136], ([14.197109    0.59691274]) \t loss 2.430E+01\n",
      "step 9200000 \t return [-19.718374 -13.826133], ([8.327201  2.5488598]) \t loss 2.510E+01\n",
      "step 9210000 \t return [-13.511575 -11.172061], ([11.430381  1.226628]) \t loss 2.525E+01\n",
      "step 9220000 \t return [-17.01419  -10.912533], ([17.323591   0.5763776]) \t loss 2.549E+01\n",
      "step 9230000 \t return [ -6.639819 -11.269727], ([4.958458  0.7034959]) \t loss 2.535E+01\n",
      "step 9240000 \t return [ -6.755733 -11.578405], ([4.4644957 0.6492074]) \t loss 2.411E+01\n",
      "step 9250000 \t return [ -8.456631 -11.275261], ([7.354831   0.56888163]) \t loss 2.434E+01\n",
      "step 9260000 \t return [ -4.8068857 -11.808255 ], ([2.9129183 0.5194408]) \t loss 2.374E+01\n",
      "step 9270000 \t return [ -8.2717905 -11.446556 ], ([6.5395303  0.52889323]) \t loss 2.546E+01\n",
      "step 9280000 \t return [-12.849937 -10.705009], ([10.962293   0.6621094]) \t loss 2.590E+01\n",
      "step 9290000 \t return [ -5.3846045 -11.961953 ], ([3.2046947 0.5902619]) \t loss 2.481E+01\n",
      "step 9300000 \t return [ -7.7981195 -11.335699 ], ([6.411946  0.7094094]) \t loss 2.362E+01\n",
      "step 9310000 \t return [-25.534683 -10.498031], ([20.84791   0.825547]) \t loss 2.524E+01\n",
      "step 9320000 \t return [-22.660252 -12.178546], ([5.871266  1.8758235]) \t loss 2.375E+01\n",
      "step 9330000 \t return [-39.595127 -15.03485 ], ([12.75319    3.3277962]) \t loss 2.641E+01\n",
      "step 9340000 \t return [-39.8627   -15.130277], ([10.649725   3.4161978]) \t loss 2.623E+01\n",
      "step 9350000 \t return [ -8.983086 -11.116105], ([9.415898 0.852537]) \t loss 2.460E+01\n",
      "step 9360000 \t return [ -5.6004167 -11.6398735], ([4.1177297 0.5601273]) \t loss 2.452E+01\n",
      "step 9370000 \t return [-22.619179 -11.797944], ([13.219203   2.1898954]) \t loss 2.432E+01\n",
      "step 9380000 \t return [ -9.68165  -11.019238], ([8.606997   0.60684884]) \t loss 2.513E+01\n",
      "step 9390000 \t return [-13.608176 -10.939863], ([12.211434   0.5042378]) \t loss 2.439E+01\n",
      "step 9400000 \t return [-17.922134 -12.216134], ([7.110621  2.1695838]) \t loss 2.556E+01\n",
      "step 9410000 \t return [-26.392218 -12.079163], ([10.751441   2.2181861]) \t loss 2.497E+01\n",
      "step 9420000 \t return [-35.47342  -10.424087], ([17.949331   0.8261763]) \t loss 2.454E+01\n",
      "step 9430000 \t return [-22.348316 -11.750096], ([11.051542   2.3704894]) \t loss 2.369E+01\n",
      "step 9440000 \t return [-23.195856 -12.349012], ([13.040352   2.6864452]) \t loss 2.502E+01\n",
      "step 9450000 \t return [-20.614117 -10.777505], ([17.99607     0.47380483]) \t loss 2.568E+01\n",
      "step 9460000 \t return [-19.86998 -12.30508], ([11.860104  2.202323]) \t loss 2.588E+01\n",
      "step 9470000 \t return [ -7.2645483 -11.47615  ], ([5.7564964  0.67807186]) \t loss 2.362E+01\n",
      "step 9480000 \t return [ -6.7430024 -11.511705 ], ([4.894128  0.5025362]) \t loss 2.428E+01\n",
      "step 9490000 \t return [-17.496387 -13.114409], ([6.7205215 2.3579497]) \t loss 2.368E+01\n",
      "step 9500000 \t return [-16.025663 -10.762673], ([13.694188   1.0070266]) \t loss 2.642E+01\n",
      "step 9510000 \t return [ -8.607472 -11.258073], ([6.3206487 0.6887958]) \t loss 2.465E+01\n",
      "step 9520000 \t return [-22.104929 -10.423659], ([18.234512   0.6652792]) \t loss 2.594E+01\n",
      "step 9530000 \t return [ -9.833152 -11.129444], ([9.890561  0.9036811]) \t loss 2.454E+01\n",
      "step 9540000 \t return [-53.477142 -11.003071], ([12.5819025  1.9710827]) \t loss 2.493E+01\n",
      "step 9550000 \t return [-17.191654 -11.323054], ([12.447048   1.6700904]) \t loss 2.438E+01\n",
      "step 9560000 \t return [-14.808889 -11.386753], ([8.645281  1.5860913]) \t loss 2.466E+01\n",
      "step 9570000 \t return [-11.911879 -10.947324], ([12.108023    0.72848946]) \t loss 2.495E+01\n",
      "step 9580000 \t return [-24.011791 -10.488943], ([20.937546   0.7748173]) \t loss 2.505E+01\n",
      "step 9590000 \t return [-14.218012 -11.24499 ], ([9.74298   1.2183099]) \t loss 2.530E+01\n",
      "step 9600000 \t return [-25.536924 -10.375044], ([14.765634  0.848897]) \t loss 2.596E+01\n",
      "step 9610000 \t return [ -5.8130536 -12.214788 ], ([3.3385966 0.74922  ]) \t loss 2.384E+01\n",
      "step 9620000 \t return [-22.408741 -11.621232], ([5.970327  1.9175248]) \t loss 2.476E+01\n",
      "step 9630000 \t return [-11.139698 -11.068263], ([9.605148  0.6101725]) \t loss 2.504E+01\n",
      "step 9640000 \t return [-19.706602 -10.563213], ([16.48848     0.49756703]) \t loss 2.343E+01\n",
      "step 9650000 \t return [-17.779512 -10.789918], ([16.028395    0.46259525]) \t loss 2.442E+01\n",
      "step 9660000 \t return [-22.680473 -10.595083], ([14.569973   1.1602646]) \t loss 2.561E+01\n",
      "step 9670000 \t return [-10.618583 -11.257756], ([9.053709   0.64295775]) \t loss 2.538E+01\n",
      "step 9680000 \t return [-18.304464 -11.228337], ([13.726108   1.6669344]) \t loss 2.508E+01\n",
      "step 9690000 \t return [-26.325    -11.557631], ([16.117025   2.0944765]) \t loss 2.384E+01\n",
      "step 9700000 \t return [-10.232716 -11.237792], ([8.026421   0.96223617]) \t loss 2.488E+01\n",
      "step 9710000 \t return [-45.677322 -13.974035], ([11.026176   4.6246476]) \t loss 2.431E+01\n",
      "step 9720000 \t return [-12.570885  -10.7595215], ([11.085381  0.88621 ]) \t loss 2.775E+01\n",
      "step 9730000 \t return [-14.903783 -10.768747], ([15.308855   0.9297627]) \t loss 2.466E+01\n",
      "step 9740000 \t return [ -8.959811 -11.343053], ([7.0686135 0.6595694]) \t loss 2.377E+01\n",
      "step 9750000 \t return [-18.56821  -10.939025], ([8.122546  1.4813673]) \t loss 2.712E+01\n",
      "step 9760000 \t return [-14.382873 -10.897125], ([12.277375   1.0057752]) \t loss 2.512E+01\n",
      "step 9770000 \t return [ -8.5621805 -11.373443 ], ([7.165907  0.7367183]) \t loss 2.345E+01\n",
      "step 9780000 \t return [ -4.882715 -11.855512], ([3.4122283  0.65465707]) \t loss 2.404E+01\n",
      "step 9790000 \t return [-25.518764 -10.952202], ([13.480909   1.5117689]) \t loss 2.850E+01\n",
      "step 9800000 \t return [-18.935097 -10.623721], ([15.107248    0.81965685]) \t loss 2.475E+01\n",
      "step 9810000 \t return [-16.904415 -11.815166], ([7.3503847 2.0409114]) \t loss 2.408E+01\n",
      "step 9820000 \t return [ -5.7800636 -11.916859 ], ([4.281356   0.67481565]) \t loss 2.514E+01\n",
      "step 9830000 \t return [-11.1847315 -10.975298 ], ([11.116517   0.6052322]) \t loss 2.892E+01\n",
      "step 9840000 \t return [-29.826242 -10.366311], ([21.862509    0.76873326]) \t loss 2.530E+01\n",
      "step 9850000 \t return [-23.607954 -11.925532], ([7.7792144 2.276172 ]) \t loss 2.557E+01\n",
      "step 9860000 \t return [-24.748642 -10.989182], ([12.402649   1.4824193]) \t loss 2.561E+01\n",
      "step 9870000 \t return [-16.365028 -10.746799], ([16.735945   0.9816404]) \t loss 2.480E+01\n",
      "step 9880000 \t return [-26.107527 -10.532997], ([21.208986    0.88637686]) \t loss 2.583E+01\n",
      "step 9890000 \t return [-21.719446 -10.981144], ([15.251345   1.5539018]) \t loss 2.603E+01\n",
      "step 9900000 \t return [ -9.111663 -11.21399 ], ([7.6967134 0.7127589]) \t loss 2.487E+01\n",
      "step 9910000 \t return [-20.997215 -10.833159], ([10.125868   1.3275076]) \t loss 2.494E+01\n",
      "step 9920000 \t return [-10.15995  -12.511147], ([4.2173753 1.4458601]) \t loss 2.541E+01\n",
      "step 9930000 \t return [ -7.102502 -11.549256], ([5.6683183 0.6998108]) \t loss 2.490E+01\n",
      "step 9940000 \t return [-10.229776 -11.304684], ([9.041286   0.88065374]) \t loss 2.545E+01\n",
      "step 9950000 \t return [-10.821 -11.598], ([8.177731  1.2238269]) \t loss 2.417E+01\n",
      "step 9960000 \t return [-10.31818  -11.104281], ([9.262835  0.5533243]) \t loss 2.529E+01\n",
      "step 9970000 \t return [-29.818256 -10.8046  ], ([15.320365  1.555224]) \t loss 2.712E+01\n",
      "step 9980000 \t return [-37.36702  -14.129595], ([6.737292  3.2074368]) \t loss 2.554E+01\n",
      "step 9990000 \t return [ -7.252165 -11.666091], ([5.6910324 0.9381694]) \t loss 2.602E+01\n",
      "step 10000000 \t return [-15.040275 -11.733166], ([5.9054227 1.5516132]) \t loss 2.594E+01\n"
     ]
    }
   ],
   "source": [
    "env = mo_gym.make('water-reservoir-v0', normalized_action=False, nO=2, penalize=True, time_limit=100)\n",
    "\n",
    "env = ScaleReward(env)\n",
    "\n",
    "PCNAgent = PCN(env, np.array((0.1, 0.1, 0.1), dtype=np.float32), continuous_actions=True)\n",
    "\n",
    "max_return = np.zeros(2)\n",
    "\n",
    "PCNAgent.train(10000000, env, np.array((-100.0,-1000), dtype=np.float32), num_step_episodes=100,max_return=max_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_pf('./noise5.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When looking at the returns, there is significant improvement during training. However, the returns are also significantly higher than those of the solution set in the GPI experiments (when scaled back up by factor 100). Learning becomes unstable after around 7000000 steps. => Decay noise (probably)\n",
    "\n",
    "## Attempt 2\n",
    "\n",
    "Added noise decay of 0.99 every 100000 steps\n",
    "\n",
    "# TODO:\n",
    "- Experiment with scaling factor for desired return\n",
    "- Experiment with noise: add as parameter, implement decay\n",
    "- Try to scale the action as in the TD3 policy net (tanh + noise, scale output up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "wandb: Currently logged in as: liam-mertens02 (vub-ai). Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\liamm\\water-resource-management\\PCN\\wandb\\run-20230512_133411-qrs4zuft</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/vub-ai/MORL-Baselines/runs/qrs4zuft' target=\"_blank\">water-reservoir-v0__PCN__None__1683891247</a></strong> to <a href='https://wandb.ai/vub-ai/MORL-Baselines' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/vub-ai/MORL-Baselines' target=\"_blank\">https://wandb.ai/vub-ai/MORL-Baselines</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/vub-ai/MORL-Baselines/runs/qrs4zuft' target=\"_blank\">https://wandb.ai/vub-ai/MORL-Baselines/runs/qrs4zuft</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Found log directory outside of given root_logdir, dropping given root_logdir for event file in /tmp/PCN\n",
      "c:\\Users\\liamm\\anaconda3\\lib\\site-packages\\morl_baselines\\multi_policy\\pcn\\pcn.py:191: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:233.)\n",
      "  th.tensor(obs).to(self.device),\n",
      "c:\\Users\\liamm\\anaconda3\\lib\\site-packages\\morl_baselines\\multi_policy\\pcn\\pcn.py:411: RuntimeWarning: overflow encountered in multiply\n",
      "  ent = np.sum(-np.exp(lp) * lp)\n",
      "c:\\Users\\liamm\\anaconda3\\lib\\site-packages\\morl_baselines\\multi_policy\\pcn\\pcn.py:411: RuntimeWarning: overflow encountered in exp\n",
      "  ent = np.sum(-np.exp(lp) * lp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 60000 \t return [-79.58982 -89.5748 ], ([0.6480278  0.78957564]) \t loss 8.624E+03\n",
      "step 70000 \t return [-75.86534  -85.801414], ([0.64037603 0.8032793 ]) \t loss 4.304E+03\n",
      "step 80000 \t return [-82.04873  -91.981476], ([0.70160776 0.8342531 ]) \t loss 3.142E+03\n",
      "step 90000 \t return [-72.82013 -82.78728], ([0.58069   0.7266203]) \t loss 2.105E+03\n",
      "step 100000 \t return [-73.29544 -83.21858], ([0.5463456 0.6802388]) \t loss 1.296E+03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Step cannot be set when using syncing with tensorboard. Please log your step values as a metric such as 'global_step'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 110000 \t return [-76.80146 -86.71199], ([0.58658165 0.7095399 ]) \t loss 1.040E+03\n",
      "step 120000 \t return [-74.63824 -84.60152], ([0.56829715 0.7062702 ]) \t loss 6.757E+02\n",
      "step 130000 \t return [-70.45668 -80.34142], ([0.5591226  0.71008235]) \t loss 3.531E+02\n",
      "step 140000 \t return [-73.23504 -83.09843], ([0.6290266 0.7712619]) \t loss 2.585E+02\n",
      "step 150000 \t return [-71.44258 -81.35122], ([0.5572675 0.7449787]) \t loss 1.810E+02\n",
      "step 160000 \t return [-72.991745 -82.92948 ], ([0.6102425  0.77556705]) \t loss 1.495E+02\n",
      "step 170000 \t return [-71.31282 -81.21912], ([0.5701509  0.70369935]) \t loss 1.315E+02\n",
      "step 180000 \t return [-73.70294 -83.6431 ], ([0.5453567  0.67572963]) \t loss 1.549E+02\n",
      "step 190000 \t return [-70.35511  -80.256874], ([0.5815265 0.7523881]) \t loss 7.593E+01\n",
      "step 200000 \t return [-70.86025  -80.812515], ([0.56378615 0.7118689 ]) \t loss 7.634E+01\n",
      "step 210000 \t return [-69.74624  -79.616425], ([0.55082756 0.70928127]) \t loss 8.295E+01\n",
      "step 220000 \t return [-70.56805 -80.50078], ([0.576959   0.75877947]) \t loss 8.283E+01\n",
      "step 230000 \t return [-68.84195 -78.74023], ([0.6256208 0.7910977]) \t loss 4.700E+01\n",
      "step 240000 \t return [-68.80594  -78.714066], ([0.6348071  0.80536205]) \t loss 6.903E+01\n",
      "step 250000 \t return [-68.83265 -78.74583], ([0.60756934 0.7712633 ]) \t loss 4.219E+01\n",
      "step 260000 \t return [-68.89874 -78.7859 ], ([0.55239165 0.7090663 ]) \t loss 7.299E+01\n",
      "step 270000 \t return [-68.50269 -78.37075], ([0.57207686 0.7314549 ]) \t loss 6.694E+01\n",
      "step 280000 \t return [-68.78887 -78.67032], ([0.5902142  0.73097825]) \t loss 3.514E+01\n",
      "step 290000 \t return [-69.80953 -79.74251], ([0.5615612 0.6906354]) \t loss 5.344E+01\n",
      "step 300000 \t return [-67.77135 -77.66389], ([0.53821796 0.6991786 ]) \t loss 5.788E+01\n",
      "step 310000 \t return [-67.58303 -77.49629], ([0.57368606 0.77455556]) \t loss 5.622E+01\n",
      "step 320000 \t return [-68.92618 -78.82062], ([0.57886094 0.7222937 ]) \t loss 6.102E+01\n",
      "step 330000 \t return [-67.90223 -77.84191], ([0.54515994 0.6986797 ]) \t loss 6.296E+01\n",
      "step 340000 \t return [-67.66645 -77.57565], ([0.54230493 0.6479498 ]) \t loss 5.628E+01\n",
      "step 350000 \t return [-68.139984 -78.09012 ], ([0.54987955 0.6969431 ]) \t loss 7.123E+01\n",
      "step 360000 \t return [-69.14046 -79.04233], ([0.5364986 0.6666341]) \t loss 7.035E+01\n",
      "step 370000 \t return [-67.54322 -77.45783], ([0.5795895 0.7635686]) \t loss 5.147E+01\n",
      "step 380000 \t return [-67.878746 -77.74074 ], ([0.6646303 0.7989434]) \t loss 4.353E+01\n",
      "step 390000 \t return [-66.42199 -76.32111], ([0.5649512  0.73111737]) \t loss 6.255E+01\n",
      "step 400000 \t return [-67.34606 -77.28878], ([0.47846764 0.6664612 ]) \t loss 5.706E+01\n",
      "step 410000 \t return [-66.114204 -76.02487 ], ([0.5650627 0.7483966]) \t loss 5.750E+01\n",
      "step 420000 \t return [-68.23993  -78.163475], ([0.53520006 0.69633794]) \t loss 6.042E+01\n",
      "step 430000 \t return [-66.64991  -76.537636], ([0.53315735 0.6926298 ]) \t loss 4.951E+01\n",
      "step 440000 \t return [-65.931335 -75.8669  ], ([0.5876008 0.7523861]) \t loss 3.476E+01\n",
      "step 450000 \t return [-68.41089  -78.317474], ([0.5469458  0.70073885]) \t loss 5.498E+01\n",
      "step 460000 \t return [-65.68326  -75.623535], ([0.5491415 0.6885933]) \t loss 7.332E+01\n",
      "step 470000 \t return [-69.50869 -79.42279], ([0.6097263  0.79636115]) \t loss 7.966E+01\n",
      "step 480000 \t return [-65.523384 -75.436905], ([0.5924176 0.7578879]) \t loss 6.826E+01\n",
      "step 490000 \t return [-67.10287  -77.028854], ([0.4941411 0.6544093]) \t loss 5.451E+01\n",
      "step 500000 \t return [-66.22549 -76.1275 ], ([0.5385648 0.6893032]) \t loss 5.326E+01\n",
      "step 510000 \t return [-65.72476 -75.62328], ([0.5797512 0.713982 ]) \t loss 3.081E+01\n",
      "step 520000 \t return [-62.499317 -72.35115 ], ([0.4869939  0.65501064]) \t loss 5.523E+01\n",
      "step 530000 \t return [-62.32737 -72.26398], ([0.45033455 0.58911294]) \t loss 5.155E+01\n",
      "step 540000 \t return [-65.54326  -75.469826], ([0.55930513 0.7181884 ]) \t loss 3.679E+01\n",
      "step 550000 \t return [-64.336655 -74.21377 ], ([0.54383075 0.68277085]) \t loss 5.005E+01\n",
      "step 560000 \t return [-65.08657 -74.96677], ([0.48338455 0.6397861 ]) \t loss 3.924E+01\n",
      "step 570000 \t return [-65.27826 -75.16004], ([0.51886344 0.7083242 ]) \t loss 4.177E+01\n",
      "step 580000 \t return [-62.368404 -72.2773  ], ([0.47921222 0.6060979 ]) \t loss 2.517E+01\n",
      "step 590000 \t return [-62.840004 -72.73672 ], ([0.4913229 0.650533 ]) \t loss 2.518E+01\n",
      "step 600000 \t return [-61.046474 -70.979866], ([0.5083884 0.6533647]) \t loss 2.324E+01\n",
      "step 610000 \t return [-61.72398 -71.61536], ([0.48225868 0.6381768 ]) \t loss 2.541E+01\n",
      "step 620000 \t return [-62.17997 -72.09389], ([0.49492806 0.64533824]) \t loss 2.382E+01\n",
      "step 630000 \t return [-60.686123 -70.593895], ([0.51531   0.6849035]) \t loss 2.288E+01\n",
      "step 640000 \t return [-60.564014 -70.44544 ], ([0.5012324 0.6619909]) \t loss 2.408E+01\n",
      "step 650000 \t return [-63.122536 -72.99458 ], ([0.48810104 0.687283  ]) \t loss 2.483E+01\n",
      "step 660000 \t return [-60.67735  -70.523384], ([0.53220576 0.7314983 ]) \t loss 2.589E+01\n",
      "step 670000 \t return [-59.491302 -69.38434 ], ([0.5408115 0.7391967]) \t loss 2.508E+01\n",
      "step 680000 \t return [-59.998547 -69.94083 ], ([0.44607982 0.6149206 ]) \t loss 2.400E+01\n",
      "step 690000 \t return [-59.465714 -69.36963 ], ([0.53905696 0.6990599 ]) \t loss 2.563E+01\n",
      "step 700000 \t return [-61.162003 -71.096565], ([0.49759138 0.6497313 ]) \t loss 2.351E+01\n",
      "step 710000 \t return [-60.66908 -70.54628], ([0.4726827 0.6148937]) \t loss 2.404E+01\n",
      "step 720000 \t return [-57.575512 -67.45097 ], ([0.45320508 0.6192149 ]) \t loss 2.443E+01\n",
      "step 730000 \t return [-58.08719 -67.98821], ([0.49513295 0.68297714]) \t loss 2.260E+01\n",
      "step 740000 \t return [-59.57878  -69.482956], ([0.5012992  0.66353685]) \t loss 2.377E+01\n",
      "step 750000 \t return [-55.206444 -65.106476], ([0.50670207 0.70403904]) \t loss 2.421E+01\n",
      "step 760000 \t return [-57.5417  -67.44344], ([0.510414  0.6600452]) \t loss 2.506E+01\n",
      "step 770000 \t return [-56.572876 -66.42745 ], ([0.4724744  0.63076663]) \t loss 2.657E+01\n",
      "step 780000 \t return [-57.96122 -67.81447], ([0.49945226 0.6788442 ]) \t loss 2.341E+01\n",
      "step 790000 \t return [-55.315063 -65.231834], ([0.4591401 0.6255916]) \t loss 2.386E+01\n",
      "step 800000 \t return [-56.938423 -66.84573 ], ([0.46155754 0.6477457 ]) \t loss 2.397E+01\n",
      "step 810000 \t return [-56.89307 -66.76085], ([0.5115328  0.72510535]) \t loss 2.427E+01\n",
      "step 820000 \t return [-56.725346 -66.600105], ([0.528658  0.7139666]) \t loss 2.319E+01\n",
      "step 830000 \t return [-56.59488 -66.52132], ([0.54792356 0.7351332 ]) \t loss 2.198E+01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\liamm\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 840000 \t return [-55.402267 -65.267006], ([0.46432814 0.61157835]) \t loss 2.271E+01\n",
      "step 850000 \t return [-53.725784 -63.60833 ], ([0.4633996  0.63298315]) \t loss 2.315E+01\n",
      "step 860000 \t return [-55.971176 -65.90171 ], ([0.49086496 0.6499096 ]) \t loss 2.388E+01\n",
      "step 870000 \t return [-53.83125  -63.712234], ([0.44798398 0.6115964 ]) \t loss 2.408E+01\n",
      "step 880000 \t return [-53.01641 -62.84609], ([0.3786063 0.5292116]) \t loss 2.279E+01\n",
      "step 890000 \t return [-53.334263 -63.19247 ], ([0.3798976 0.5885916]) \t loss 2.359E+01\n",
      "step 900000 \t return [-53.69057  -63.568676], ([0.4646434 0.6338685]) \t loss 2.192E+01\n",
      "step 910000 \t return [-52.436325 -62.31899 ], ([0.4212108 0.5985394]) \t loss 2.232E+01\n",
      "step 920000 \t return [-52.137604 -62.014805], ([0.4558754 0.6330795]) \t loss 2.330E+01\n",
      "step 930000 \t return [-52.59768  -62.510235], ([0.4107001 0.5603351]) \t loss 2.276E+01\n",
      "step 940000 \t return [-53.132275 -63.02625 ], ([0.39151353 0.555881  ]) \t loss 2.268E+01\n",
      "step 950000 \t return [-53.324097 -63.177902], ([0.5171232 0.7107484]) \t loss 2.325E+01\n",
      "step 960000 \t return [-51.91543 -61.77399], ([0.4814437  0.66266125]) \t loss 2.244E+01\n",
      "step 970000 \t return [-50.461143 -60.33131 ], ([0.44319963 0.61265904]) \t loss 2.264E+01\n",
      "step 980000 \t return [-51.22485  -61.084293], ([0.4708293 0.6488877]) \t loss 2.225E+01\n",
      "step 990000 \t return [-50.49913 -60.36726], ([0.4491225 0.6425551]) \t loss 2.377E+01\n",
      "step 1000000 \t return [-52.26127  -62.112583], ([0.44785485 0.5876023 ]) \t loss 2.339E+01\n",
      "step 1010000 \t return [-49.89891  -59.781303], ([0.4212263 0.5877514]) \t loss 2.394E+01\n",
      "step 1020000 \t return [-50.79834  -60.689327], ([0.44771096 0.65782034]) \t loss 2.296E+01\n",
      "step 1030000 \t return [-49.498295 -59.359028], ([0.48148417 0.66381   ]) \t loss 2.272E+01\n",
      "step 1040000 \t return [-49.06681  -58.902016], ([0.38738272 0.5511553 ]) \t loss 2.131E+01\n",
      "step 1050000 \t return [-49.119717 -58.93374 ], ([0.46162823 0.64411855]) \t loss 2.205E+01\n",
      "step 1060000 \t return [-48.97168  -58.820484], ([0.4076918 0.649423 ]) \t loss 2.227E+01\n",
      "step 1070000 \t return [-47.691628 -57.520153], ([0.4179818  0.58988756]) \t loss 2.136E+01\n",
      "step 1080000 \t return [-46.343403 -56.219032], ([0.42331174 0.591527  ]) \t loss 2.120E+01\n",
      "step 1090000 \t return [-48.47281  -58.346664], ([0.41950905 0.5978999 ]) \t loss 2.353E+01\n",
      "step 1100000 \t return [-47.34166  -57.217285], ([0.43871456 0.64399225]) \t loss 2.198E+01\n",
      "step 1110000 \t return [-47.071487 -56.884132], ([0.4420629 0.6517017]) \t loss 2.278E+01\n",
      "step 1120000 \t return [-45.787056 -55.655586], ([0.4760216  0.71477056]) \t loss 2.207E+01\n",
      "step 1130000 \t return [-46.589756 -56.425095], ([0.40418836 0.577522  ]) \t loss 2.148E+01\n",
      "step 1140000 \t return [-44.90432 -54.72069], ([0.40833807 0.57250535]) \t loss 2.229E+01\n",
      "step 1150000 \t return [-45.63342  -55.485302], ([0.37083313 0.56119883]) \t loss 2.132E+01\n",
      "step 1160000 \t return [-44.84104  -54.705353], ([0.40331534 0.5915821 ]) \t loss 2.226E+01\n",
      "step 1170000 \t return [-43.82973  -53.689472], ([0.40953267 0.6034911 ]) \t loss 2.213E+01\n",
      "step 1180000 \t return [-45.888218 -55.80374 ], ([0.45193586 0.647071  ]) \t loss 2.279E+01\n",
      "step 1190000 \t return [-43.58384 -53.47175], ([0.39677614 0.58240145]) \t loss 2.250E+01\n",
      "step 1200000 \t return [-43.995785 -53.820045], ([0.39052203 0.620502  ]) \t loss 2.223E+01\n",
      "step 1210000 \t return [-44.70673  -54.552155], ([0.39227533 0.5989502 ]) \t loss 2.133E+01\n",
      "step 1220000 \t return [-44.0816   -53.892426], ([0.33843726 0.4902718 ]) \t loss 2.246E+01\n",
      "step 1230000 \t return [-42.589054 -52.38722 ], ([0.379533  0.5811158]) \t loss 2.117E+01\n",
      "step 1240000 \t return [-44.03039 -53.86238], ([0.4021534  0.61288446]) \t loss 2.123E+01\n",
      "step 1250000 \t return [-43.794643 -53.621666], ([0.37213734 0.5208401 ]) \t loss 2.167E+01\n",
      "step 1260000 \t return [-43.037632 -52.871326], ([0.398745   0.57955897]) \t loss 2.148E+01\n",
      "step 1270000 \t return [-43.392467 -53.191254], ([0.42246208 0.6311627 ]) \t loss 2.090E+01\n",
      "step 1280000 \t return [-43.161312 -53.01209 ], ([0.36426517 0.539482  ]) \t loss 2.134E+01\n",
      "step 1290000 \t return [-42.33494  -52.122837], ([0.35951293 0.5731288 ]) \t loss 2.059E+01\n",
      "step 1300000 \t return [-43.076717 -52.95909 ], ([0.3860586 0.5965565]) \t loss 2.094E+01\n",
      "step 1310000 \t return [-41.72383 -51.61135], ([0.38050348 0.55649894]) \t loss 2.107E+01\n",
      "step 1320000 \t return [-42.610825 -52.421913], ([0.3996678  0.60338587]) \t loss 2.116E+01\n",
      "step 1330000 \t return [-41.13285  -50.956497], ([0.3625919 0.583554 ]) \t loss 2.053E+01\n",
      "step 1340000 \t return [-43.406322 -53.257763], ([0.4191976 0.6202767]) \t loss 2.125E+01\n",
      "step 1350000 \t return [-41.453148 -51.23909 ], ([0.39686498 0.6083678 ]) \t loss 2.217E+01\n",
      "step 1360000 \t return [-40.980537 -50.78132 ], ([0.38554403 0.6064679 ]) \t loss 2.151E+01\n",
      "step 1370000 \t return [-38.98888 -48.81883], ([0.3060334 0.4818879]) \t loss 2.065E+01\n",
      "step 1380000 \t return [-39.26338  -49.096954], ([0.36369446 0.5713954 ]) \t loss 2.173E+01\n",
      "step 1390000 \t return [-39.686165 -49.53444 ], ([0.34513837 0.5488395 ]) \t loss 2.171E+01\n",
      "step 1400000 \t return [-41.08791  -50.919907], ([0.380277   0.55643505]) \t loss 2.242E+01\n",
      "step 1410000 \t return [-38.954254 -48.73479 ], ([0.37223664 0.5639587 ]) \t loss 2.065E+01\n",
      "step 1420000 \t return [-40.775665 -50.629562], ([0.37943104 0.59981626]) \t loss 2.193E+01\n",
      "step 1430000 \t return [-37.49486  -47.240894], ([0.32067007 0.5107542 ]) \t loss 2.170E+01\n",
      "step 1440000 \t return [-37.21967  -46.963276], ([0.3339826 0.5442975]) \t loss 2.186E+01\n",
      "step 1450000 \t return [-37.597256 -47.365257], ([0.36392877 0.5758095 ]) \t loss 2.174E+01\n",
      "step 1460000 \t return [-39.08925  -48.874165], ([0.30976304 0.5427645 ]) \t loss 2.079E+01\n",
      "step 1470000 \t return [-37.212963 -46.960106], ([0.36156115 0.54938823]) \t loss 2.034E+01\n",
      "step 1480000 \t return [-37.504467 -47.259087], ([0.34426185 0.5602087 ]) \t loss 1.969E+01\n",
      "step 1490000 \t return [-36.30712  -46.084747], ([0.3348952  0.57003045]) \t loss 2.140E+01\n",
      "step 1500000 \t return [-38.15374 -47.93117], ([0.3487071 0.5771051]) \t loss 2.042E+01\n",
      "step 1510000 \t return [-36.67512 -46.54245], ([0.35338023 0.50812435]) \t loss 2.038E+01\n",
      "step 1520000 \t return [-36.29171  -46.065174], ([0.31165883 0.533664  ]) \t loss 2.025E+01\n",
      "step 1530000 \t return [-34.763214 -44.45688 ], ([0.3057379  0.52517074]) \t loss 1.939E+01\n",
      "step 1540000 \t return [-35.31642  -45.115463], ([0.32198447 0.54981935]) \t loss 1.974E+01\n",
      "step 1550000 \t return [-35.8359   -45.635014], ([0.3507189 0.5639936]) \t loss 2.176E+01\n",
      "step 1560000 \t return [-35.51058  -45.263214], ([0.3438822  0.58638865]) \t loss 1.925E+01\n",
      "step 1570000 \t return [-36.047897 -45.881958], ([0.30180085 0.4912926 ]) \t loss 2.035E+01\n",
      "step 1580000 \t return [-34.377953 -44.138535], ([0.3419428  0.49985853]) \t loss 1.794E+01\n",
      "step 1590000 \t return [-34.37201  -44.154785], ([0.3850474  0.56930244]) \t loss 2.036E+01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\liamm\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:179: RuntimeWarning: overflow encountered in reduce\n",
      "  ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1600000 \t return [-34.025047 -43.737404], ([0.29558623 0.4948434 ]) \t loss 2.003E+01\n",
      "step 1610000 \t return [-33.56792  -43.310596], ([0.31336656 0.5499534 ]) \t loss 2.185E+01\n",
      "step 1620000 \t return [-35.443317 -45.260185], ([0.34664   0.5663986]) \t loss 2.001E+01\n",
      "step 1630000 \t return [-33.645878 -43.392574], ([0.31461626 0.4953853 ]) \t loss 1.963E+01\n",
      "step 1640000 \t return [-33.474842 -43.247963], ([0.31336454 0.5200145 ]) \t loss 1.958E+01\n",
      "step 1650000 \t return [-33.82868 -43.52603], ([0.33340642 0.5668681 ]) \t loss 1.947E+01\n",
      "step 1660000 \t return [-34.485783 -44.218674], ([0.338762  0.5985713]) \t loss 1.921E+01\n",
      "step 1670000 \t return [-33.93528 -43.74801], ([0.31300133 0.54244167]) \t loss 1.962E+01\n",
      "step 1680000 \t return [-33.44269  -43.139416], ([0.30747712 0.5365642 ]) \t loss 2.027E+01\n",
      "step 1690000 \t return [-32.420948 -42.234886], ([0.32804152 0.4978401 ]) \t loss 2.020E+01\n",
      "step 1700000 \t return [-31.818039 -41.535835], ([0.30712447 0.5163745 ]) \t loss 2.082E+01\n",
      "step 1710000 \t return [-32.251034 -42.034893], ([0.3461189  0.55359226]) \t loss 2.003E+01\n",
      "step 1720000 \t return [-31.26173 -40.9669 ], ([0.27531266 0.48949814]) \t loss 1.984E+01\n",
      "step 1730000 \t return [-31.322363 -41.089256], ([0.31447357 0.57557786]) \t loss 1.865E+01\n",
      "step 1740000 \t return [-31.28091  -41.020573], ([0.3315352 0.5311431]) \t loss 1.925E+01\n",
      "step 1750000 \t return [-31.282194 -40.97462 ], ([0.27637556 0.4709786 ]) \t loss 1.915E+01\n",
      "step 1760000 \t return [-31.186338 -40.850292], ([0.29224223 0.41172153]) \t loss 1.953E+01\n",
      "step 1770000 \t return [-31.07544  -40.745346], ([0.28405678 0.4864448 ]) \t loss 1.748E+01\n",
      "step 1780000 \t return [-30.947039 -40.561287], ([0.29804802 0.5077953 ]) \t loss 1.880E+01\n",
      "step 1790000 \t return [-30.791855 -40.47562 ], ([0.26660314 0.4717118 ]) \t loss 1.846E+01\n",
      "step 1800000 \t return [-30.23848  -39.906887], ([0.30194795 0.4877636 ]) \t loss 1.871E+01\n",
      "step 1810000 \t return [-30.924654 -40.61919 ], ([0.30718    0.52368414]) \t loss 1.897E+01\n",
      "step 1820000 \t return [-30.35691  -40.009964], ([0.25063026 0.55323684]) \t loss 1.887E+01\n",
      "step 1830000 \t return [-30.196762 -39.84982 ], ([0.26130214 0.51123625]) \t loss 1.989E+01\n",
      "step 1840000 \t return [-30.7056   -40.366352], ([0.30901575 0.5487563 ]) \t loss 1.888E+01\n",
      "step 1850000 \t return [-30.65543 -40.30919], ([0.33969215 0.59115034]) \t loss 1.788E+01\n",
      "step 1860000 \t return [-31.043379 -40.66996 ], ([0.28798884 0.50744015]) \t loss 2.050E+01\n",
      "step 1870000 \t return [-30.294086 -39.96532 ], ([0.26893422 0.52049947]) \t loss 1.935E+01\n",
      "step 1880000 \t return [-30.568518 -40.216175], ([0.30205274 0.5181935 ]) \t loss 1.961E+01\n",
      "step 1890000 \t return [-31.79751  -41.513325], ([0.30353084 0.5072951 ]) \t loss 1.948E+01\n",
      "step 1900000 \t return [-30.325424 -40.04216 ], ([0.30882922 0.518006  ]) \t loss 1.949E+01\n",
      "step 1910000 \t return [-29.968542 -39.69714 ], ([0.3229284  0.59366435]) \t loss 1.878E+01\n",
      "step 1920000 \t return [-30.397688 -39.91723 ], ([0.31771246 0.60222894]) \t loss 1.792E+01\n",
      "step 1930000 \t return [-29.579487 -39.226597], ([0.30399328 0.5503213 ]) \t loss 1.960E+01\n",
      "step 1940000 \t return [-29.945887 -39.519337], ([0.28928825 0.59255487]) \t loss 1.805E+01\n",
      "step 1950000 \t return [-29.963188 -39.459095], ([0.26331502 0.5698475 ]) \t loss 1.900E+01\n",
      "step 1960000 \t return [-31.53941  -41.242508], ([0.30541965 0.45335418]) \t loss 1.860E+01\n",
      "step 1970000 \t return [-29.455256 -39.00831 ], ([0.28848422 0.5060291 ]) \t loss 1.869E+01\n",
      "step 1980000 \t return [-29.222256 -38.992603], ([0.28002888 0.50914884]) \t loss 1.806E+01\n",
      "step 1990000 \t return [-29.928091 -39.555008], ([0.2995253 0.5231017]) \t loss 1.965E+01\n",
      "step 2000000 \t return [-29.985283 -39.62402 ], ([0.30245444 0.57532126]) \t loss 1.858E+01\n",
      "step 2010000 \t return [-30.041634 -39.748627], ([0.27591673 0.47632977]) \t loss 1.781E+01\n",
      "step 2020000 \t return [-30.742579 -40.39155 ], ([0.29175466 0.60473925]) \t loss 1.886E+01\n",
      "step 2030000 \t return [-30.2445   -39.906666], ([0.294824 0.502296]) \t loss 1.755E+01\n",
      "step 2040000 \t return [-30.163445 -39.81812 ], ([0.27285996 0.5517285 ]) \t loss 1.801E+01\n",
      "step 2050000 \t return [-29.609774 -39.268932], ([0.2854289 0.5863324]) \t loss 1.893E+01\n",
      "step 2060000 \t return [-29.872377 -39.541794], ([0.26834816 0.5142021 ]) \t loss 1.858E+01\n",
      "step 2070000 \t return [-29.330542 -38.86005 ], ([0.3363916 0.5422021]) \t loss 1.777E+01\n",
      "step 2080000 \t return [-30.411636 -39.908165], ([0.3094218  0.49894103]) \t loss 1.795E+01\n",
      "step 2090000 \t return [-28.523113 -38.07296 ], ([0.28205305 0.5456045 ]) \t loss 1.852E+01\n",
      "step 2100000 \t return [-29.64636  -39.194557], ([0.33688486 0.6131955 ]) \t loss 1.930E+01\n",
      "step 2110000 \t return [-30.117231 -39.65682 ], ([0.33588973 0.5128601 ]) \t loss 1.900E+01\n",
      "step 2120000 \t return [-28.537914 -38.021725], ([0.3371366 0.600118 ]) \t loss 1.782E+01\n",
      "step 2130000 \t return [-28.69429 -38.24563], ([0.35103548 0.50968874]) \t loss 1.823E+01\n",
      "step 2140000 \t return [-29.659023 -39.30345 ], ([0.32537845 0.50106514]) \t loss 1.851E+01\n",
      "step 2150000 \t return [-29.26894 -38.84074], ([0.29273847 0.55959237]) \t loss 1.819E+01\n",
      "step 2160000 \t return [-29.129244 -38.68374 ], ([0.31640893 0.5128274 ]) \t loss 1.768E+01\n",
      "step 2170000 \t return [-28.301079 -37.847355], ([0.33706406 0.55682296]) \t loss 1.746E+01\n",
      "step 2180000 \t return [-28.021946 -37.636337], ([0.30298465 0.575494  ]) \t loss 1.791E+01\n",
      "step 2190000 \t return [-29.032446 -38.65455 ], ([0.29637924 0.53523475]) \t loss 1.810E+01\n",
      "step 2200000 \t return [-29.036785 -38.510216], ([0.3128722 0.5196474]) \t loss 1.835E+01\n",
      "step 2210000 \t return [-28.279808 -37.811714], ([0.31847987 0.5227661 ]) \t loss 1.736E+01\n",
      "step 2220000 \t return [-27.946915 -37.48218 ], ([0.3264642  0.52196753]) \t loss 1.661E+01\n",
      "step 2230000 \t return [-28.92294  -38.467182], ([0.35015804 0.51045734]) \t loss 1.723E+01\n",
      "step 2240000 \t return [-28.32046 -37.83638], ([0.3300718 0.5825844]) \t loss 1.727E+01\n",
      "step 2250000 \t return [-27.74837 -37.28795], ([0.34149265 0.6200614 ]) \t loss 1.692E+01\n",
      "step 2260000 \t return [-28.74434  -38.341816], ([0.3077376 0.5270414]) \t loss 1.820E+01\n",
      "step 2270000 \t return [-28.164682 -37.641953], ([0.36030427 0.6242826 ]) \t loss 1.782E+01\n",
      "step 2280000 \t return [-28.914251 -38.486313], ([0.32983488 0.5026808 ]) \t loss 1.758E+01\n",
      "step 2290000 \t return [-27.437937 -37.016922], ([0.33172983 0.50498134]) \t loss 1.758E+01\n",
      "step 2300000 \t return [-27.319181 -36.830273], ([0.36486793 0.5081097 ]) \t loss 1.796E+01\n",
      "step 2310000 \t return [-28.044685 -37.57887 ], ([0.3265006 0.5792702]) \t loss 1.695E+01\n",
      "step 2320000 \t return [-28.328638 -37.715103], ([0.34096515 0.60149384]) \t loss 1.707E+01\n",
      "step 2330000 \t return [-28.694458 -38.27168 ], ([0.3353968 0.5481971]) \t loss 1.726E+01\n",
      "step 2340000 \t return [-27.833887 -37.391743], ([0.3101209  0.52635294]) \t loss 1.721E+01\n",
      "step 2350000 \t return [-27.166471 -36.715115], ([0.35791835 0.5164385 ]) \t loss 1.683E+01\n",
      "step 2360000 \t return [-26.872662 -36.385914], ([0.33334422 0.5670736 ]) \t loss 1.725E+01\n",
      "step 2370000 \t return [-27.425438 -36.854057], ([0.33444348 0.5922872 ]) \t loss 1.826E+01\n",
      "step 2380000 \t return [-25.76307 -35.08735], ([0.36669216 0.62658554]) \t loss 1.744E+01\n",
      "step 2390000 \t return [-25.789165 -35.18698 ], ([0.33553824 0.5485665 ]) \t loss 1.784E+01\n",
      "step 2400000 \t return [-26.573915 -35.95856 ], ([0.39283305 0.614213  ]) \t loss 1.668E+01\n",
      "step 2410000 \t return [-25.527302 -35.034554], ([0.3483944  0.60323983]) \t loss 1.685E+01\n",
      "step 2420000 \t return [-25.016644 -34.36612 ], ([0.38978148 0.62321097]) \t loss 1.768E+01\n",
      "step 2430000 \t return [-25.721216 -35.0474  ], ([0.40673456 0.6543415 ]) \t loss 1.692E+01\n",
      "step 2440000 \t return [-26.263361 -35.615116], ([0.3860618  0.59498936]) \t loss 1.545E+01\n",
      "step 2450000 \t return [-24.463894 -33.65101 ], ([0.5000099  0.73769176]) \t loss 1.694E+01\n",
      "step 2460000 \t return [-25.766218 -35.24158 ], ([0.33217087 0.5913134 ]) \t loss 1.693E+01\n",
      "step 2470000 \t return [-26.01972  -35.278774], ([0.4689542  0.57984936]) \t loss 1.820E+01\n",
      "step 2480000 \t return [-25.171133 -34.60375 ], ([0.38789263 0.62608844]) \t loss 1.694E+01\n",
      "step 2490000 \t return [-24.87444  -34.193363], ([0.46415526 0.68782836]) \t loss 1.691E+01\n",
      "step 2500000 \t return [-24.30455 -33.47916], ([0.5492729  0.64917487]) \t loss 1.678E+01\n",
      "step 2510000 \t return [-25.867264 -35.266052], ([0.39060655 0.6042124 ]) \t loss 1.627E+01\n",
      "step 2520000 \t return [-24.534813 -33.900925], ([0.45257512 0.60735923]) \t loss 1.638E+01\n",
      "step 2530000 \t return [-24.138256 -33.3704  ], ([0.5125806  0.68540686]) \t loss 1.696E+01\n",
      "step 2540000 \t return [-24.062328 -33.21805 ], ([0.5469215  0.70380306]) \t loss 1.689E+01\n",
      "step 2550000 \t return [-24.522415 -33.563335], ([0.5554547  0.69965845]) \t loss 1.675E+01\n",
      "step 2560000 \t return [-24.435747 -33.686558], ([0.55947334 0.6643972 ]) \t loss 1.642E+01\n",
      "step 2570000 \t return [-24.79262 -33.68728], ([0.552822  0.7297487]) \t loss 1.582E+01\n",
      "step 2580000 \t return [-24.951912 -34.12826 ], ([0.5044414  0.67729586]) \t loss 1.629E+01\n",
      "step 2590000 \t return [-24.446611 -33.533665], ([0.52479285 0.7072969 ]) \t loss 1.692E+01\n",
      "step 2600000 \t return [-24.510237 -33.529526], ([0.51999706 0.7339866 ]) \t loss 1.597E+01\n",
      "step 2610000 \t return [-24.32333 -33.53886], ([0.46972418 0.67671984]) \t loss 1.702E+01\n",
      "step 2620000 \t return [-24.077473 -33.18894 ], ([0.5292951 0.7466783]) \t loss 1.645E+01\n",
      "step 2630000 \t return [-24.013645 -33.130245], ([0.59059703 0.73291814]) \t loss 1.512E+01\n",
      "step 2640000 \t return [-23.760483 -32.753365], ([0.6042817  0.72012705]) \t loss 1.604E+01\n",
      "step 2650000 \t return [-23.496584 -32.457287], ([0.69086003 0.7732476 ]) \t loss 1.631E+01\n",
      "step 2660000 \t return [-23.590256 -32.622597], ([0.5934303 0.754218 ]) \t loss 1.602E+01\n",
      "step 2670000 \t return [-24.297966 -33.370987], ([0.52463865 0.6733001 ]) \t loss 1.593E+01\n",
      "step 2680000 \t return [-23.848635 -32.989094], ([0.5286463 0.6913107]) \t loss 1.586E+01\n",
      "step 2690000 \t return [-23.638672 -32.71409 ], ([0.5820397  0.73961437]) \t loss 1.602E+01\n",
      "step 2700000 \t return [-23.710466 -32.687645], ([0.70548975 0.728722  ]) \t loss 1.514E+01\n",
      "step 2710000 \t return [-23.836472 -32.886814], ([0.59103346 0.7416945 ]) \t loss 1.517E+01\n",
      "step 2720000 \t return [-23.817741 -33.064327], ([0.4936699 0.6516772]) \t loss 1.690E+01\n",
      "step 2730000 \t return [-23.736256 -32.653664], ([0.64862525 0.80732083]) \t loss 1.631E+01\n",
      "step 2740000 \t return [-23.358213 -32.382023], ([0.6203366 0.8037897]) \t loss 1.615E+01\n",
      "step 2750000 \t return [-23.477085 -32.472034], ([0.6158913  0.81584406]) \t loss 1.568E+01\n",
      "step 2760000 \t return [-22.995441 -32.174057], ([0.5910229  0.70937806]) \t loss 1.537E+01\n",
      "step 2770000 \t return [-23.386242 -32.496395], ([0.5947866  0.78106964]) \t loss 1.557E+01\n",
      "step 2780000 \t return [-23.281212 -32.219112], ([0.5858093 0.8108415]) \t loss 1.619E+01\n",
      "step 2790000 \t return [-22.648071 -30.997883], ([0.8948881 1.0314337]) \t loss 1.477E+01\n",
      "step 2800000 \t return [-23.411604 -32.454582], ([0.6066465  0.79738814]) \t loss 1.503E+01\n",
      "step 2810000 \t return [-22.816397 -31.757603], ([0.57441735 0.7603905 ]) \t loss 1.655E+01\n",
      "step 2820000 \t return [-22.641663 -31.210693], ([0.7373517 0.8676321]) \t loss 1.574E+01\n",
      "step 2830000 \t return [-21.961155 -30.774199], ([0.7589348  0.90005285]) \t loss 1.520E+01\n",
      "step 2840000 \t return [-22.398647 -31.346031], ([0.60010856 0.8299698 ]) \t loss 1.482E+01\n",
      "step 2850000 \t return [-23.073938 -31.711863], ([0.70472115 0.8652894 ]) \t loss 1.476E+01\n",
      "step 2860000 \t return [-22.394106 -31.47747 ], ([0.64879626 0.8200439 ]) \t loss 1.505E+01\n",
      "step 2870000 \t return [-22.0799   -30.922947], ([0.6630971 0.8893455]) \t loss 1.492E+01\n",
      "step 2880000 \t return [-22.605669 -31.426336], ([0.78339845 0.9670692 ]) \t loss 1.459E+01\n",
      "step 2890000 \t return [-23.22043  -31.888916], ([0.7690612 0.9491222]) \t loss 1.493E+01\n",
      "step 2900000 \t return [-22.234713 -31.1619  ], ([0.6285328  0.83646685]) \t loss 1.516E+01\n",
      "step 2910000 \t return [-23.601154 -32.459747], ([0.6105927  0.93329054]) \t loss 1.503E+01\n",
      "step 2920000 \t return [-22.149864 -31.050434], ([0.7543854 0.8278254]) \t loss 1.424E+01\n",
      "step 2930000 \t return [-22.434294 -31.399656], ([0.6822748  0.91748846]) \t loss 1.444E+01\n",
      "step 2940000 \t return [-22.026443 -30.5811  ], ([0.8895434 0.8967605]) \t loss 1.490E+01\n",
      "step 2950000 \t return [-21.262571 -29.902546], ([0.7803478  0.88834774]) \t loss 1.482E+01\n",
      "step 2960000 \t return [-21.866707 -30.599663], ([0.8229651 0.8999344]) \t loss 1.495E+01\n",
      "step 2970000 \t return [-22.15456  -30.783884], ([0.8227847  0.90436876]) \t loss 1.519E+01\n",
      "step 2980000 \t return [-21.801767 -30.658049], ([0.7609021  0.83483434]) \t loss 1.616E+01\n",
      "step 2990000 \t return [-21.322092 -29.740139], ([0.87344253 0.91504025]) \t loss 1.525E+01\n",
      "step 3000000 \t return [-21.658447 -30.563692], ([0.6687425 0.7488545]) \t loss 1.578E+01\n",
      "step 3010000 \t return [-21.876087 -30.413994], ([0.88131   0.9470557]) \t loss 1.418E+01\n",
      "step 3020000 \t return [-21.186852 -29.90965 ], ([0.9533185 0.9251701]) \t loss 1.469E+01\n",
      "step 3030000 \t return [-21.473265 -30.4821  ], ([0.6923575  0.83090985]) \t loss 1.507E+01\n",
      "step 3040000 \t return [-21.824236 -30.338842], ([0.77299243 1.0081166 ]) \t loss 1.488E+01\n",
      "step 3050000 \t return [-21.121653 -30.01163 ], ([0.7889217  0.89714444]) \t loss 1.506E+01\n",
      "step 3060000 \t return [-21.638855 -30.215254], ([0.86658555 1.0040765 ]) \t loss 1.494E+01\n",
      "step 3070000 \t return [-21.301655 -30.195293], ([0.7965432 0.9364235]) \t loss 1.436E+01\n",
      "step 3080000 \t return [-21.271738 -29.531261], ([1.00609   0.9780292]) \t loss 1.471E+01\n",
      "step 3090000 \t return [-20.2262 -29.1977], ([0.78869706 0.88126034]) \t loss 1.450E+01\n",
      "step 3100000 \t return [-20.939629 -29.926147], ([0.7247755 0.9227362]) \t loss 1.516E+01\n",
      "step 3110000 \t return [-20.968355 -29.57944 ], ([0.9867434 0.8896834]) \t loss 1.408E+01\n",
      "step 3120000 \t return [-20.63931  -29.475496], ([0.81309485 0.87648726]) \t loss 1.430E+01\n",
      "step 3130000 \t return [-21.28021  -29.832087], ([0.80191267 1.0110631 ]) \t loss 1.512E+01\n",
      "step 3140000 \t return [-20.735878 -28.826988], ([1.1347606 1.2871116]) \t loss 1.468E+01\n",
      "step 3150000 \t return [-19.757679 -27.783018], ([1.1754985 1.1177174]) \t loss 1.493E+01\n",
      "step 3160000 \t return [-19.676533 -27.93884 ], ([1.0122947 1.2501388]) \t loss 1.495E+01\n",
      "step 3170000 \t return [-20.060675 -28.409418], ([1.0756664 1.1709589]) \t loss 1.384E+01\n",
      "step 3180000 \t return [-20.265942 -28.60833 ], ([0.86389726 1.0521344 ]) \t loss 1.502E+01\n",
      "step 3190000 \t return [-20.304655 -28.69614 ], ([0.9942547 1.1167802]) \t loss 1.420E+01\n",
      "step 3200000 \t return [-19.381207 -27.897072], ([1.044841  1.0289993]) \t loss 1.455E+01\n",
      "step 3210000 \t return [-19.555218 -27.824541], ([1.0527306 1.0500431]) \t loss 1.446E+01\n",
      "step 3220000 \t return [-19.733929 -28.465672], ([0.97106695 0.9132664 ]) \t loss 1.391E+01\n",
      "step 3230000 \t return [-19.54592  -28.013506], ([0.9453716 1.0718504]) \t loss 1.391E+01\n",
      "step 3240000 \t return [-19.832466 -27.713657], ([1.1030288 1.2053357]) \t loss 1.377E+01\n",
      "step 3250000 \t return [-19.902136 -27.690428], ([1.3421999 1.1772006]) \t loss 1.383E+01\n",
      "step 3260000 \t return [-19.751348 -27.891434], ([1.0538485 1.1175792]) \t loss 1.489E+01\n",
      "step 3270000 \t return [-19.888702 -27.906923], ([1.1711853 1.2921022]) \t loss 1.336E+01\n",
      "step 3280000 \t return [-19.98793  -28.319763], ([1.0230306 1.1866969]) \t loss 1.343E+01\n",
      "step 3290000 \t return [-19.687927 -27.963213], ([1.1972048 1.0244994]) \t loss 1.443E+01\n",
      "step 3300000 \t return [-19.3275   -27.331768], ([1.2834436 1.3167168]) \t loss 1.423E+01\n",
      "step 3310000 \t return [-18.926458 -27.322027], ([1.0743707 1.2503076]) \t loss 1.504E+01\n",
      "step 3320000 \t return [-20.26969  -28.277012], ([1.1901459 1.4090042]) \t loss 1.435E+01\n",
      "step 3330000 \t return [-19.818504 -27.485472], ([1.2287084 1.2785864]) \t loss 1.341E+01\n",
      "step 3340000 \t return [-19.176594 -27.396141], ([1.3668745 1.1378298]) \t loss 1.382E+01\n",
      "step 3350000 \t return [-19.981285 -27.630014], ([1.1946659 1.4720505]) \t loss 1.448E+01\n",
      "step 3360000 \t return [-19.614607 -28.011707], ([1.1212829 1.1025226]) \t loss 1.363E+01\n",
      "step 3370000 \t return [-19.663713 -27.904608], ([1.2148616 1.1727663]) \t loss 1.498E+01\n",
      "step 3380000 \t return [-20.175081 -28.221275], ([1.0292968 1.2737614]) \t loss 1.420E+01\n",
      "step 3390000 \t return [-18.14988  -26.064333], ([1.4147063 1.1540518]) \t loss 1.384E+01\n",
      "step 3400000 \t return [-19.948307 -28.165615], ([0.9967786 1.363957 ]) \t loss 1.434E+01\n",
      "step 3410000 \t return [-18.952803 -26.903662], ([1.2587025 1.3785181]) \t loss 1.352E+01\n",
      "step 3420000 \t return [-18.365166 -25.910202], ([1.6261513 1.289576 ]) \t loss 1.409E+01\n",
      "step 3430000 \t return [-18.149313 -25.325628], ([1.8637812 1.4605337]) \t loss 1.422E+01\n",
      "step 3440000 \t return [-18.532488 -26.094864], ([1.2694453 1.4301407]) \t loss 1.358E+01\n",
      "step 3450000 \t return [-17.714907 -25.463203], ([1.6363294 1.2345308]) \t loss 1.311E+01\n",
      "step 3460000 \t return [-17.87066 -24.4598 ], ([2.0427752 1.9032893]) \t loss 1.442E+01\n",
      "step 3470000 \t return [-17.213308 -25.0184  ], ([1.6625432 1.331939 ]) \t loss 1.457E+01\n",
      "step 3480000 \t return [-17.88038 -25.7958 ], ([1.4035072 1.3414624]) \t loss 1.426E+01\n",
      "step 3490000 \t return [-17.242155 -24.083298], ([1.981544  1.6181731]) \t loss 1.316E+01\n",
      "step 3500000 \t return [-17.378279 -24.561745], ([1.8411453 1.4445434]) \t loss 1.293E+01\n",
      "step 3510000 \t return [-17.427038 -24.738802], ([1.7128253 1.6214384]) \t loss 1.391E+01\n",
      "step 3520000 \t return [-17.950294 -23.635988], ([3.5400727 1.932707 ]) \t loss 1.363E+01\n",
      "step 3530000 \t return [-22.953747 -31.380465], ([0.8281729 1.4466978]) \t loss 1.472E+01\n",
      "step 3540000 \t return [-16.78506 -24.46051], ([2.1967807 1.4256525]) \t loss 1.651E+01\n",
      "step 3550000 \t return [-16.285002 -22.825354], ([3.1021144 1.4500757]) \t loss 1.467E+01\n",
      "step 3560000 \t return [-16.929346 -23.46541 ], ([2.173559  2.0280454]) \t loss 1.447E+01\n",
      "step 3570000 \t return [-17.441715 -22.879494], ([3.6079185 2.13883  ]) \t loss 1.408E+01\n",
      "step 3580000 \t return [-17.860912 -25.127783], ([1.4950483 1.8672557]) \t loss 1.429E+01\n",
      "step 3590000 \t return [-18.289356 -24.24692 ], ([3.4866366 2.503107 ]) \t loss 1.436E+01\n",
      "step 3600000 \t return [-17.832201 -25.080906], ([1.8430274 2.1083663]) \t loss 1.493E+01\n",
      "step 3610000 \t return [-16.200953 -23.245491], ([2.3174999 2.0040388]) \t loss 1.525E+01\n",
      "step 3620000 \t return [-18.253122 -21.726788], ([7.317318  2.3008082]) \t loss 1.460E+01\n",
      "step 3630000 \t return [-24.280062 -19.476923], ([16.787764   2.1410804]) \t loss 1.589E+01\n",
      "step 3640000 \t return [-15.347173 -21.09819 ], ([4.587499  2.0742364]) \t loss 1.787E+01\n",
      "step 3650000 \t return [-28.385515 -36.464947], ([0.79224515 1.6503975 ]) \t loss 1.729E+01\n",
      "step 3660000 \t return [-20.012459 -26.257195], ([3.714     2.8249893]) \t loss 2.814E+01\n",
      "step 3670000 \t return [-16.867928 -21.207657], ([5.8222284 2.662234 ]) \t loss 1.793E+01\n",
      "step 3680000 \t return [-18.339746 -19.780373], ([9.166027  2.2579262]) \t loss 1.663E+01\n",
      "step 3690000 \t return [-17.724342 -23.459288], ([4.63017   2.7951775]) \t loss 1.606E+01\n",
      "step 3700000 \t return [-21.720345 -27.995604], ([1.9475185 3.046982 ]) \t loss 1.674E+01\n",
      "step 3710000 \t return [-21.601225 -19.279379], ([15.322109  2.166715]) \t loss 2.106E+01\n",
      "step 3720000 \t return [-16.295437 -18.597301], ([9.857079  1.6378648]) \t loss 1.830E+01\n",
      "step 3730000 \t return [-21.426191 -18.398863], ([13.610755   1.8456757]) \t loss 1.630E+01\n",
      "step 3740000 \t return [-21.551296 -26.508512], ([2.4617324 3.3441684]) \t loss 1.704E+01\n",
      "step 3750000 \t return [-15.132425 -19.791405], ([6.2321067 1.2082132]) \t loss 2.052E+01\n",
      "step 3760000 \t return [-16.410482 -19.421043], ([7.9489155 1.8728491]) \t loss 1.708E+01\n",
      "step 3770000 \t return [-13.920978 -19.197329], ([5.47638   1.7183183]) \t loss 1.574E+01\n",
      "step 3780000 \t return [-17.585312 -19.116375], ([10.486073  2.343157]) \t loss 1.430E+01\n",
      "step 3790000 \t return [-21.180243 -18.819859], ([16.384958   2.2717125]) \t loss 1.480E+01\n",
      "step 3800000 \t return [-20.047098 -17.824118], ([17.230757   1.7110498]) \t loss 1.722E+01\n",
      "step 3810000 \t return [-20.477612 -17.439264], ([15.837058   0.9085952]) \t loss 1.739E+01\n",
      "step 3820000 \t return [-24.423979 -19.613642], ([17.842676   3.3397474]) \t loss 1.719E+01\n",
      "step 3830000 \t return [-29.111319 -19.140522], ([19.917618  3.78065 ]) \t loss 1.821E+01\n",
      "step 3840000 \t return [-14.810128 -18.73565 ], ([5.5641046 1.5063125]) \t loss 1.884E+01\n",
      "step 3850000 \t return [-32.214584 -36.290382], ([2.744689  4.1257105]) \t loss 1.683E+01\n",
      "step 3860000 \t return [-13.5362215 -18.12936  ], ([5.662813   0.45273218]) \t loss 4.175E+01\n",
      "step 3870000 \t return [-17.064362 -20.544832], ([6.482666  2.9326715]) \t loss 1.781E+01\n",
      "step 3880000 \t return [-24.116388 -16.712109], ([18.411512   0.9529014]) \t loss 1.780E+01\n",
      "step 3890000 \t return [-14.822063 -17.265726], ([6.838552  0.4254052]) \t loss 1.697E+01\n",
      "step 3900000 \t return [-16.526087 -17.372868], ([7.8547955  0.43378657]) \t loss 1.667E+01\n",
      "step 3910000 \t return [-17.054878 -17.6519  ], ([9.604418  1.6258659]) \t loss 1.662E+01\n",
      "step 3920000 \t return [-19.534798 -17.34163 ], ([7.6162972  0.45288968]) \t loss 1.659E+01\n",
      "step 3930000 \t return [-31.78799  -15.970087], ([18.914343   0.7685692]) \t loss 1.631E+01\n",
      "step 3940000 \t return [-17.425583 -17.744705], ([9.3843775 0.9724974]) \t loss 1.785E+01\n",
      "step 3950000 \t return [-20.06884  -17.221281], ([9.531821  0.8336535]) \t loss 1.596E+01\n",
      "step 3960000 \t return [-39.009884 -28.132353], ([16.581324  9.123689]) \t loss 1.578E+01\n",
      "step 3970000 \t return [-41.27061 -30.99835], ([16.462866 10.022952]) \t loss 4.365E+01\n",
      "step 3980000 \t return [-44.50357  -20.152334], ([24.497465  5.355424]) \t loss 5.726E+01\n",
      "step 3990000 \t return [-44.199364 -16.43639 ], ([24.043596    0.32996193]) \t loss 3.143E+01\n",
      "step 4000000 \t return [-18.370306 -16.942911], ([12.935745   0.5135812]) \t loss 1.835E+01\n",
      "step 4010000 \t return [-38.953724 -16.450851], ([22.104244   0.3561269]) \t loss 1.760E+01\n",
      "step 4020000 \t return [-24.662678 -19.449862], ([13.412849   3.4066818]) \t loss 1.709E+01\n",
      "step 4030000 \t return [-39.991173 -31.931337], ([10.434419  9.787632]) \t loss 2.004E+01\n",
      "step 4040000 \t return [-46.872387 -17.009628], ([23.63269     0.32250723]) \t loss 5.995E+01\n",
      "step 4050000 \t return [-20.194338 -19.47944 ], ([11.041698   2.8399153]) \t loss 1.959E+01\n",
      "step 4060000 \t return [-23.206497 -20.760927], ([11.513807   3.7278183]) \t loss 2.034E+01\n",
      "step 4070000 \t return [-26.000767 -17.004799], ([15.27309     0.70299834]) \t loss 2.420E+01\n",
      "step 4080000 \t return [-36.231644 -26.931128], ([14.142453  8.127904]) \t loss 1.589E+01\n",
      "step 4090000 \t return [-23.803816 -29.166166], ([2.7314172 3.5103102]) \t loss 3.343E+01\n",
      "step 4100000 \t return [-44.686134 -18.797789], ([19.278616   3.3647811]) \t loss 5.118E+01\n",
      "step 4110000 \t return [-12.966986 -17.921494], ([6.5876822  0.44319487]) \t loss 2.575E+01\n",
      "step 4120000 \t return [-21.032087 -25.134985], ([3.6933138 3.703639 ]) \t loss 1.646E+01\n",
      "step 4130000 \t return [-18.98933  -20.374449], ([5.2681303 2.9987922]) \t loss 3.562E+01\n",
      "step 4140000 \t return [-26.981926 -17.196833], ([15.060317   1.9636878]) \t loss 2.347E+01\n",
      "step 4150000 \t return [-20.004026 -16.746845], ([12.673903    0.70841146]) \t loss 1.710E+01\n",
      "step 4160000 \t return [-46.173477 -22.380232], ([12.672168   7.3109574]) \t loss 1.546E+01\n",
      "step 4170000 \t return [-30.559803 -36.15268 ], ([1.7136191 3.654363 ]) \t loss 3.621E+01\n",
      "step 4180000 \t return [-13.275327 -18.469955], ([5.3840246 0.8170791]) \t loss 7.915E+01\n",
      "step 4190000 \t return [-15.886562 -17.875042], ([10.873852   0.6993326]) \t loss 1.939E+01\n",
      "step 4200000 \t return [-26.123127 -23.546762], ([6.7924194 4.5862546]) \t loss 1.608E+01\n",
      "step 4210000 \t return [-37.58147  -37.963966], ([3.039509  7.0817494]) \t loss 3.976E+01\n",
      "step 4220000 \t return [-26.637096 -25.481081], ([6.1391945 5.701479 ]) \t loss 1.016E+02\n",
      "step 4230000 \t return [-69.74588  -18.636206], ([9.021557  4.3027186]) \t loss 4.524E+01\n",
      "step 4240000 \t return [-57.755505 -17.899303], ([12.033043   3.6794944]) \t loss 2.519E+01\n",
      "step 4250000 \t return [-49.528183 -17.994574], ([13.448508  4.270205]) \t loss 2.428E+01\n",
      "step 4260000 \t return [-28.480759 -20.053738], ([10.090546  4.009816]) \t loss 1.773E+01\n",
      "step 4270000 \t return [-25.73507  -17.152637], ([19.04074     0.40603212]) \t loss 2.073E+01\n",
      "step 4280000 \t return [-29.381994 -22.38168 ], ([5.2708735 4.5016413]) \t loss 1.416E+01\n",
      "step 4290000 \t return [-31.372034 -17.174494], ([21.368681    0.35488352]) \t loss 2.504E+01\n",
      "step 4300000 \t return [-17.59744 -18.14706], ([7.5513926 1.8809028]) \t loss 1.463E+01\n",
      "step 4310000 \t return [-18.768536 -17.528244], ([13.415882   0.4255031]) \t loss 1.503E+01\n",
      "step 4320000 \t return [-14.429002 -17.55896 ], ([8.899397  0.6047782]) \t loss 1.396E+01\n",
      "step 4330000 \t return [-46.55116  -19.595736], ([5.33756   3.9681227]) \t loss 1.426E+01\n",
      "step 4340000 \t return [-19.75731  -18.163982], ([8.332523 2.232603]) \t loss 1.673E+01\n",
      "step 4350000 \t return [-33.496475 -16.964804], ([21.381102   0.3450111]) \t loss 1.606E+01\n",
      "step 4360000 \t return [-45.22937  -17.284304], ([11.280022   2.6630988]) \t loss 1.438E+01\n",
      "step 4370000 \t return [-15.624973 -17.551582], ([10.095129    0.34994513]) \t loss 1.525E+01\n",
      "step 4380000 \t return [-14.174524 -17.849873], ([8.145055  1.1214473]) \t loss 1.327E+01\n",
      "step 4390000 \t return [-33.61815  -18.081018], ([5.4366503 2.71659  ]) \t loss 1.409E+01\n",
      "step 4400000 \t return [-27.04923  -17.042475], ([17.825527    0.29005772]) \t loss 1.673E+01\n",
      "step 4410000 \t return [-24.911493 -16.928759], ([19.175484   0.3703651]) \t loss 1.316E+01\n",
      "step 4420000 \t return [-14.400498 -17.359251], ([8.833084   0.85166085]) \t loss 1.385E+01\n",
      "step 4430000 \t return [-44.84332  -19.688423], ([3.8859801 3.2566206]) \t loss 1.338E+01\n",
      "step 4440000 \t return [-15.446851 -17.332727], ([11.034285    0.32129356]) \t loss 2.090E+01\n",
      "step 4450000 \t return [-13.692291 -17.510393], ([7.511392   0.32777867]) \t loss 1.400E+01\n",
      "step 4460000 \t return [-48.210014 -19.77371 ], ([3.863889  3.6098394]) \t loss 1.345E+01\n",
      "step 4470000 \t return [-15.139216 -17.311049], ([7.8216166  0.48033226]) \t loss 2.027E+01\n",
      "step 4480000 \t return [-37.636665 -23.779652], ([1.8428966 3.4265323]) \t loss 1.391E+01\n",
      "step 4490000 \t return [-15.788187 -16.928896], ([10.952022    0.26188928]) \t loss 2.410E+01\n",
      "step 4500000 \t return [-63.60521  -18.393957], ([4.4796386 3.5871408]) \t loss 1.299E+01\n",
      "step 4510000 \t return [-11.945866 -17.649786], ([5.074259   0.49394128]) \t loss 1.541E+01\n",
      "step 4520000 \t return [-43.633   -22.36413], ([1.4732454 3.3666732]) \t loss 1.477E+01\n",
      "step 4530000 \t return [-40.08446  -18.713818], ([2.4942415 2.6705973]) \t loss 2.088E+01\n",
      "step 4540000 \t return [-29.028397 -17.452316], ([5.506309  1.6902566]) \t loss 1.826E+01\n",
      "step 4550000 \t return [-48.87001  -20.524864], ([1.7286178 3.311706 ]) \t loss 1.527E+01\n",
      "step 4560000 \t return [-33.040604 -20.39367 ], ([1.7735846 2.527958 ]) \t loss 1.993E+01\n",
      "step 4570000 \t return [-10.421796 -17.291311], ([4.73224    0.40935794]) \t loss 1.832E+01\n",
      "step 4580000 \t return [-24.200928 -15.992864], ([12.449626  1.188168]) \t loss 1.557E+01\n",
      "step 4590000 \t return [-14.294598 -16.260015], ([10.049063    0.29537573]) \t loss 1.434E+01\n",
      "step 4600000 \t return [-16.016148 -16.635786], ([13.051187    0.22486319]) \t loss 1.384E+01\n",
      "step 4610000 \t return [-46.226055 -15.025863], ([8.403556  0.4061791]) \t loss 1.404E+01\n",
      "step 4620000 \t return [-67.367516 -17.012787], ([2.1194415 2.8438284]) \t loss 1.359E+01\n",
      "step 4630000 \t return [-36.923195 -15.082869], ([6.2048492  0.80334014]) \t loss 1.547E+01\n",
      "step 4640000 \t return [-68.915855 -16.180164], ([1.7515597 2.0853238]) \t loss 1.378E+01\n",
      "step 4650000 \t return [-21.839006 -16.44687 ], ([14.768622   1.1627895]) \t loss 1.594E+01\n",
      "step 4660000 \t return [-24.104092 -16.218575], ([18.612091   0.2588103]) \t loss 1.472E+01\n",
      "step 4670000 \t return [-26.20596  -16.909164], ([13.901199   2.4271595]) \t loss 1.430E+01\n",
      "step 4680000 \t return [-21.329418 -18.48698 ], ([9.7783165 2.8453279]) \t loss 1.446E+01\n",
      "step 4690000 \t return [-16.980719 -16.194744], ([12.848846   0.4033557]) \t loss 1.517E+01\n",
      "step 4700000 \t return [-21.427292 -17.006598], ([9.722706  2.1193376]) \t loss 1.262E+01\n",
      "step 4710000 \t return [-18.456818 -15.286487], ([16.896513   0.2757681]) \t loss 1.468E+01\n",
      "step 4720000 \t return [-19.850065 -15.395024], ([16.074593    0.24386634]) \t loss 1.399E+01\n",
      "step 4730000 \t return [-34.491318 -17.515684], ([3.3233156 2.4159544]) \t loss 1.429E+01\n",
      "step 4740000 \t return [-15.079818 -16.322285], ([9.104149  0.9154643]) \t loss 1.549E+01\n",
      "step 4750000 \t return [-71.45945  -17.190025], ([0.7260627 2.1832001]) \t loss 1.335E+01\n",
      "step 4760000 \t return [-22.927563 -18.879719], ([5.5868115 3.1286528]) \t loss 1.599E+01\n",
      "step 4770000 \t return [-34.674206 -15.627525], ([3.193916  1.0262121]) \t loss 1.703E+01\n",
      "step 4780000 \t return [-15.175915  -15.6143675], ([10.749951   0.4262301]) \t loss 1.305E+01\n",
      "step 4790000 \t return [-32.877487 -19.313026], ([0.7541822 1.7667619]) \t loss 1.356E+01\n",
      "step 4800000 \t return [-21.811106 -17.121723], ([9.928148  2.6735299]) \t loss 1.602E+01\n",
      "step 4810000 \t return [-13.614951 -15.374628], ([11.158406    0.34620476]) \t loss 1.431E+01\n",
      "step 4820000 \t return [-22.038986 -17.77051 ], ([9.628438  3.0571697]) \t loss 1.391E+01\n",
      "step 4830000 \t return [-35.97702  -15.941992], ([5.6745796 1.2597058]) \t loss 1.689E+01\n",
      "step 4840000 \t return [-15.77867  -15.216217], ([11.7609825   0.17004363]) \t loss 1.403E+01\n",
      "step 4850000 \t return [-35.669506 -17.906586], ([1.4545096 1.7731797]) \t loss 1.361E+01\n",
      "step 4860000 \t return [-70.80916  -20.799158], ([0.908016 4.020721]) \t loss 1.520E+01\n",
      "step 4870000 \t return [-67.870224 -18.579296], ([0.54751265 2.55083   ]) \t loss 2.064E+01\n",
      "step 4880000 \t return [-19.06332  -18.078232], ([9.402135  3.1882598]) \t loss 1.445E+01\n",
      "step 4890000 \t return [-67.735756 -18.485054], ([0.49013653 2.353251  ]) \t loss 1.536E+01\n",
      "step 4900000 \t return [-40.98436  -16.452387], ([2.5895662 1.4676063]) \t loss 1.436E+01\n",
      "step 4910000 \t return [-52.291737 -16.049196], ([1.9981728 1.8112566]) \t loss 1.400E+01\n",
      "step 4920000 \t return [-16.096375 -19.072718], ([5.775075  3.2577152]) \t loss 1.357E+01\n",
      "step 4930000 \t return [-21.582338 -18.148277], ([10.451051   2.9427543]) \t loss 1.476E+01\n",
      "step 4940000 \t return [-21.763296 -17.299511], ([9.530955 2.968613]) \t loss 1.391E+01\n",
      "step 4950000 \t return [-14.677738 -15.517534], ([8.952384   0.27705863]) \t loss 1.581E+01\n",
      "step 4960000 \t return [-69.363045 -18.161924], ([0.34010115 1.9504927 ]) \t loss 1.502E+01\n",
      "step 4970000 \t return [-19.796394 -18.425055], ([3.9172847 2.502046 ]) \t loss 1.509E+01\n",
      "step 4980000 \t return [-23.67286 -16.78839], ([7.914295  1.9388101]) \t loss 1.563E+01\n",
      "step 4990000 \t return [-19.991789  -15.2840605], ([15.20517     0.12962954]) \t loss 1.414E+01\n",
      "step 5000000 \t return [-17.954626 -15.697223], ([10.000131    0.58361816]) \t loss 1.439E+01\n",
      "step 5010000 \t return [-49.498802 -19.983807], ([0.30236694 1.8576826 ]) \t loss 1.668E+01\n",
      "step 5020000 \t return [-20.494827 -14.899781], ([14.042537    0.24473192]) \t loss 1.381E+01\n",
      "step 5030000 \t return [-35.630756 -18.03355 ], ([0.28353348 1.0082648 ]) \t loss 1.335E+01\n",
      "step 5040000 \t return [-11.738033 -15.582983], ([7.108115   0.41828316]) \t loss 1.155E+01\n",
      "step 5050000 \t return [-67.99369  -18.189337], ([0.5557373 2.7578082]) \t loss 1.218E+01\n",
      "step 5060000 \t return [-34.137394 -15.6925  ], ([4.160853  0.9074068]) \t loss 1.232E+01\n",
      "step 5070000 \t return [-14.2951355 -15.968775 ], ([7.1597877  0.64981776]) \t loss 1.131E+01\n",
      "step 5080000 \t return [-53.032135 -15.741973], ([0.93194264 1.4813116 ]) \t loss 1.365E+01\n",
      "step 5090000 \t return [-15.158833 -15.954913], ([7.518901  0.6879299]) \t loss 1.041E+01\n",
      "step 5100000 \t return [-69.8311   -17.194956], ([0.13878982 0.79097354]) \t loss 1.412E+01\n",
      "step 5110000 \t return [-19.162865 -15.094801], ([12.871638    0.08362708]) \t loss 1.041E+01\n",
      "step 5120000 \t return [-41.852585 -16.350582], ([1.3043506 1.390635 ]) \t loss 9.698E+00\n",
      "step 5130000 \t return [-53.80621  -15.721201], ([1.1766534 1.7219409]) \t loss 9.983E+00\n",
      "step 5140000 \t return [-19.454916 -19.97922 ], ([3.5590558 2.9610825]) \t loss 8.800E+00\n",
      "step 5150000 \t return [-18.03381  -19.351671], ([6.437462 3.169539]) \t loss 1.243E+01\n",
      "step 5160000 \t return [-55.32964  -15.231501], ([1.2495815 1.0611494]) \t loss 1.299E+01\n",
      "step 5170000 \t return [-16.353916 -15.974425], ([9.550869  0.5497495]) \t loss 1.070E+01\n",
      "step 5180000 \t return [-25.728369 -17.720922], ([6.421814  1.6060098]) \t loss 1.522E+01\n",
      "step 5190000 \t return [-51.822533 -15.428141], ([1.4744127 1.0077866]) \t loss 1.857E+01\n",
      "step 5200000 \t return [-16.686928 -15.865866], ([9.620728   0.36914983]) \t loss 8.767E+00\n",
      "step 5210000 \t return [-53.578224 -15.29092 ], ([0.56545985 1.3356429 ]) \t loss 1.357E+01\n",
      "step 5220000 \t return [-23.399847 -16.589108], ([9.353706  2.1906137]) \t loss 9.493E+00\n",
      "step 5230000 \t return [-18.253231 -15.766097], ([11.729678    0.38703802]) \t loss 1.222E+01\n",
      "step 5240000 \t return [-53.511276 -15.110234], ([0.80152273 1.0549781 ]) \t loss 1.478E+01\n",
      "step 5250000 \t return [-36.210133 -15.894107], ([0.49598333 0.7762968 ]) \t loss 8.421E+00\n",
      "step 5260000 \t return [-17.915802 -18.173338], ([6.674492 2.218703]) \t loss 7.714E+00\n",
      "step 5270000 \t return [-36.725662 -15.136364], ([1.7447098  0.55062723]) \t loss 1.178E+01\n",
      "step 5280000 \t return [-22.69411  -16.234642], ([10.116793   1.2405027]) \t loss 7.885E+00\n",
      "step 5290000 \t return [-22.541557 -16.64975 ], ([8.2006445 1.5777177]) \t loss 1.871E+01\n",
      "step 5300000 \t return [-53.006264 -15.78309 ], ([0.16382639 0.55070007]) \t loss 1.043E+01\n",
      "step 5310000 \t return [-53.05772  -15.186006], ([0.21056485 0.56728315]) \t loss 9.399E+00\n",
      "step 5320000 \t return [-35.593178 -15.051464], ([2.986293 0.600023]) \t loss 8.690E+00\n",
      "step 5330000 \t return [-15.002003 -16.99058 ], ([5.9027214  0.29804078]) \t loss 9.113E+00\n",
      "step 5340000 \t return [-39.420692 -15.550614], ([1.3305529 0.7395246]) \t loss 1.938E+01\n",
      "step 5350000 \t return [-17.67345  -15.865129], ([10.206368    0.72239053]) \t loss 5.926E+00\n",
      "step 5360000 \t return [-49.618275 -15.259487], ([0.25438717 0.42080066]) \t loss 1.061E+01\n",
      "step 5370000 \t return [-31.44443  -15.268694], ([2.4165604 0.9140281]) \t loss 6.176E+00\n",
      "step 5380000 \t return [-19.18225  -16.708935], ([6.685489  1.2809674]) \t loss 6.438E+00\n",
      "step 5390000 \t return [-40.34011  -15.044652], ([1.6216981  0.63282037]) \t loss 9.349E+00\n",
      "step 5400000 \t return [-19.099316 -16.021133], ([11.058381   0.5482825]) \t loss 5.617E+00\n",
      "step 5410000 \t return [-16.664576 -17.50458 ], ([7.8655777 1.0049497]) \t loss 1.583E+01\n",
      "step 5420000 \t return [-52.274063 -14.126539], ([1.376155  0.2716922]) \t loss 2.052E+01\n",
      "step 5430000 \t return [-54.02454  -13.973476], ([1.234665   0.37854344]) \t loss 5.642E+00\n",
      "step 5440000 \t return [-39.370987 -14.499081], ([0.75452363 0.676165  ]) \t loss 6.354E+00\n",
      "step 5450000 \t return [-40.79829  -14.277752], ([0.6294972 0.4273149]) \t loss 5.686E+00\n",
      "step 5460000 \t return [-54.138622 -13.995459], ([1.123743   0.40016517]) \t loss 4.898E+00\n",
      "step 5470000 \t return [-54.577843  -14.1015625], ([0.6524461 0.5353351]) \t loss 5.766E+00\n",
      "step 5480000 \t return [-18.73633  -17.162823], ([5.5111876  0.75055796]) \t loss 6.013E+00\n",
      "step 5490000 \t return [-40.468327 -14.194894], ([3.3074458  0.26285145]) \t loss 1.886E+01\n",
      "step 5500000 \t return [-25.8912   -14.882785], ([10.088007   0.7093031]) \t loss 8.215E+00\n",
      "step 5510000 \t return [-19.92885  -15.278372], ([7.3073497  0.42356002]) \t loss 7.661E+00\n",
      "step 5520000 \t return [-18.282478 -16.872375], ([5.6835437  0.79117936]) \t loss 1.052E+01\n",
      "step 5530000 \t return [-53.679844 -13.968961], ([2.098765   0.32831356]) \t loss 1.988E+01\n",
      "step 5540000 \t return [-18.564974 -16.145613], ([9.427193  0.6096149]) \t loss 6.270E+00\n",
      "step 5550000 \t return [-17.214758 -17.418928], ([6.375644  0.6635306]) \t loss 8.991E+00\n",
      "step 5560000 \t return [-53.60566  -13.665911], ([2.2796178  0.25574946]) \t loss 1.586E+01\n",
      "step 5570000 \t return [-19.290539 -15.031369], ([10.167236    0.48219803]) \t loss 6.314E+00\n",
      "step 5580000 \t return [-44.81711  -14.091147], ([0.47165757 0.21539827]) \t loss 8.040E+00\n",
      "step 5590000 \t return [-20.926748 -14.560042], ([6.4203186  0.43285885]) \t loss 5.411E+00\n",
      "step 5600000 \t return [-20.805239 -15.860071], ([6.677034  0.8381719]) \t loss 9.347E+00\n",
      "step 5610000 \t return [-29.977959 -14.384391], ([6.030758  0.6252488]) \t loss 1.595E+01\n",
      "step 5620000 \t return [-27.550135 -14.07819 ], ([2.3456364  0.29205793]) \t loss 1.124E+01\n",
      "step 5630000 \t return [-30.067717 -13.930426], ([5.0790896  0.37417307]) \t loss 7.131E+00\n",
      "step 5640000 \t return [-23.864172 -14.657365], ([10.8772955  0.830639 ]) \t loss 6.416E+00\n",
      "step 5650000 \t return [-13.676253 -15.708862], ([9.31238    0.40915814]) \t loss 1.565E+01\n",
      "step 5660000 \t return [-55.79818  -13.628632], ([6.2842946 0.4857029]) \t loss 1.353E+01\n",
      "step 5670000 \t return [-14.386896 -14.877972], ([10.5935755   0.06437354]) \t loss 9.470E+00\n",
      "step 5680000 \t return [-14.586256 -15.820608], ([6.407728  0.6070871]) \t loss 6.526E+00\n",
      "step 5690000 \t return [-20.047237 -17.549627], ([4.166921  0.6356364]) \t loss 6.202E+00\n",
      "step 5700000 \t return [-50.92963  -13.510237], ([1.6936537  0.10785581]) \t loss 1.995E+01\n",
      "step 5710000 \t return [-17.935352 -14.321192], ([9.667536   0.30801213]) \t loss 5.261E+00\n",
      "step 5720000 \t return [-16.285385  -15.7058935], ([5.642011  0.8858816]) \t loss 5.749E+00\n",
      "step 5730000 \t return [-53.70722 -13.08135], ([0.4564252  0.05182109]) \t loss 1.021E+01\n",
      "step 5740000 \t return [-52.64882  -13.332379], ([0.6006015  0.04973261]) \t loss 5.420E+00\n",
      "step 5750000 \t return [-20.480059 -13.963004], ([8.283629   0.20592423]) \t loss 4.155E+00\n",
      "step 5760000 \t return [-20.63398  -13.842739], ([8.129427   0.52951527]) \t loss 6.064E+00\n",
      "step 5770000 \t return [-21.943901  -15.8680725], ([5.8286753 0.6062667]) \t loss 9.216E+00\n",
      "step 5780000 \t return [-15.019715 -14.006134], ([8.004549  1.0136425]) \t loss 1.795E+01\n",
      "step 5790000 \t return [-12.357618 -15.728079], ([5.4194384  0.48465574]) \t loss 1.124E+01\n",
      "step 5800000 \t return [-19.21249  -14.154536], ([8.282423   0.82494336]) \t loss 1.311E+01\n",
      "step 5810000 \t return [-18.461065 -16.87638 ], ([3.8731303 0.5110577]) \t loss 1.616E+01\n",
      "step 5820000 \t return [-12.8323765 -15.897957 ], ([4.1574597  0.79401773]) \t loss 1.920E+01\n",
      "step 5830000 \t return [-16.794884 -15.969809], ([2.4794922 0.8041261]) \t loss 1.720E+01\n",
      "step 5840000 \t return [-13.927359 -15.088703], ([9.333503   0.04060604]) \t loss 1.707E+01\n",
      "step 5850000 \t return [-19.220861 -13.954036], ([6.6643143 0.7927381]) \t loss 1.142E+01\n",
      "step 5860000 \t return [-11.495323 -15.45016 ], ([5.8861675 0.3817929]) \t loss 1.235E+01\n",
      "step 5870000 \t return [-23.098085  -14.4465065], ([6.819721   0.92900425]) \t loss 1.239E+01\n",
      "step 5880000 \t return [-11.63183  -15.486046], ([6.444692 0.305919]) \t loss 1.399E+01\n",
      "step 5890000 \t return [-20.300179 -16.077152], ([4.283295  0.7436953]) \t loss 1.065E+01\n",
      "step 5900000 \t return [-19.484983  -13.9851265], ([6.7001114 0.5980585]) \t loss 1.728E+01\n",
      "step 5910000 \t return [-13.201114 -15.300924], ([7.6917906  0.29364812]) \t loss 1.057E+01\n",
      "step 5920000 \t return [-16.729626 -15.135668], ([5.034577  0.7384465]) \t loss 9.825E+00\n",
      "step 5930000 \t return [-24.51871  -14.356986], ([8.874963   0.40266064]) \t loss 1.202E+01\n",
      "step 5940000 \t return [-20.35371  -13.731525], ([6.164326  0.5169507]) \t loss 1.306E+01\n",
      "step 5950000 \t return [-22.961992 -14.4008  ], ([7.515435  0.4913449]) \t loss 9.659E+00\n",
      "step 5960000 \t return [-15.351554 -14.575149], ([5.683676   0.66085625]) \t loss 1.203E+01\n",
      "step 5970000 \t return [-20.29934  -15.514885], ([4.16338   0.7285609]) \t loss 1.122E+01\n",
      "step 5980000 \t return [-19.119133 -16.915155], ([3.4368727 0.5282466]) \t loss 1.492E+01\n",
      "step 5990000 \t return [-23.075037 -14.335305], ([6.52355    0.44768128]) \t loss 1.343E+01\n",
      "step 6000000 \t return [-26.106777 -14.271589], ([7.852046   0.39903283]) \t loss 1.008E+01\n",
      "step 6010000 \t return [-15.860565 -14.61728 ], ([6.9406123 0.6866124]) \t loss 1.091E+01\n",
      "step 6020000 \t return [-21.268011 -13.538941], ([7.9762244  0.82254714]) \t loss 1.162E+01\n",
      "step 6030000 \t return [-23.425636 -14.009594], ([7.405327   0.42281967]) \t loss 1.269E+01\n",
      "step 6040000 \t return [-24.097359 -14.74111 ], ([8.353967   0.42025074]) \t loss 1.018E+01\n",
      "step 6050000 \t return [-14.150398 -15.034576], ([4.389933  0.7684271]) \t loss 1.077E+01\n",
      "step 6060000 \t return [-24.474121 -14.846688], ([6.9820976 0.5257794]) \t loss 1.115E+01\n",
      "step 6070000 \t return [-23.422626 -14.671674], ([5.9456553  0.31028765]) \t loss 1.292E+01\n",
      "step 6080000 \t return [-32.286484 -13.515586], ([8.258394  0.6067264]) \t loss 1.192E+01\n",
      "step 6090000 \t return [-26.27451  -14.570469], ([7.9192243 0.5795469]) \t loss 1.136E+01\n",
      "step 6100000 \t return [-23.2845   -14.964621], ([6.074529 0.39051 ]) \t loss 1.234E+01\n",
      "step 6110000 \t return [-21.08576  -13.994577], ([6.1983624 0.5590005]) \t loss 1.193E+01\n",
      "step 6120000 \t return [-22.991917 -14.865739], ([7.1613026  0.66775143]) \t loss 1.046E+01\n",
      "step 6130000 \t return [-23.584705 -15.211174], ([6.824148   0.33506703]) \t loss 1.088E+01\n",
      "step 6140000 \t return [-24.392756 -15.320047], ([7.074655  0.5443649]) \t loss 1.164E+01\n",
      "step 6150000 \t return [-25.955364 -14.205863], ([8.525646   0.40518025]) \t loss 1.299E+01\n",
      "step 6160000 \t return [-24.434195 -14.307949], ([7.7728853 0.4626975]) \t loss 1.184E+01\n",
      "step 6170000 \t return [-25.875032 -14.296567], ([9.201726  0.5750316]) \t loss 1.027E+01\n",
      "step 6180000 \t return [-24.699482 -15.422198], ([6.9831743  0.28133023]) \t loss 1.157E+01\n",
      "step 6190000 \t return [-22.03545  -14.861816], ([6.2340355  0.43864122]) \t loss 1.056E+01\n",
      "step 6200000 \t return [-22.760471 -15.119128], ([6.492501   0.36994368]) \t loss 1.020E+01\n",
      "step 6210000 \t return [-25.561644  -14.0393095], ([7.492783  0.4751227]) \t loss 1.076E+01\n",
      "step 6220000 \t return [-22.564215 -14.683307], ([6.9048495 0.5814902]) \t loss 1.078E+01\n",
      "step 6230000 \t return [-25.38673  -14.792042], ([7.1273165  0.82140106]) \t loss 1.050E+01\n",
      "step 6240000 \t return [-23.786097 -14.88166 ], ([6.7906895  0.49067992]) \t loss 1.178E+01\n",
      "step 6250000 \t return [-23.913147  -14.3397665], ([7.0811267  0.72086394]) \t loss 9.699E+00\n",
      "step 6260000 \t return [-23.618572 -13.278012], ([6.979662 0.609537]) \t loss 1.121E+01\n",
      "step 6270000 \t return [-27.712786 -14.105001], ([6.4131913 0.5630348]) \t loss 1.050E+01\n",
      "step 6280000 \t return [-19.927252 -14.867888], ([4.7632093  0.67079914]) \t loss 1.059E+01\n",
      "step 6290000 \t return [-24.34506  -14.659388], ([7.61011   0.5109643]) \t loss 1.059E+01\n",
      "step 6300000 \t return [-17.81143  -15.440526], ([3.2625334 0.9383536]) \t loss 1.219E+01\n",
      "step 6310000 \t return [-16.445534 -15.092773], ([6.4106245  0.58343184]) \t loss 1.233E+01\n",
      "step 6320000 \t return [-25.039585 -15.297055], ([6.1656    0.8061469]) \t loss 1.054E+01\n",
      "step 6330000 \t return [-24.800512 -14.59963 ], ([6.4219193  0.50095713]) \t loss 1.263E+01\n",
      "step 6340000 \t return [-21.299221 -15.435156], ([6.665523  0.4091887]) \t loss 1.028E+01\n",
      "step 6350000 \t return [-20.81421  -14.840342], ([6.5511317 0.3573668]) \t loss 1.064E+01\n",
      "step 6360000 \t return [-23.811691 -14.302532], ([6.407962  0.6115349]) \t loss 1.111E+01\n",
      "step 6370000 \t return [-20.19489  -14.058625], ([5.6648955  0.62805325]) \t loss 1.149E+01\n",
      "step 6380000 \t return [-22.737965 -13.79096 ], ([8.887363  0.8625386]) \t loss 9.433E+00\n",
      "step 6390000 \t return [-25.817268 -14.469607], ([7.1383915  0.85420424]) \t loss 1.116E+01\n",
      "step 6400000 \t return [-20.345562 -15.98417 ], ([4.298856  0.3335627]) \t loss 1.059E+01\n",
      "step 6410000 \t return [-23.54364  -15.568542], ([6.2543917 0.3723425]) \t loss 1.288E+01\n",
      "step 6420000 \t return [-21.869936 -14.468679], ([6.6798153  0.48271072]) \t loss 1.003E+01\n",
      "step 6430000 \t return [-22.464344 -14.652322], ([5.8746486  0.52118266]) \t loss 9.529E+00\n",
      "step 6440000 \t return [-20.892189 -14.997723], ([5.30959   0.3789722]) \t loss 9.341E+00\n",
      "step 6450000 \t return [-19.137497 -14.162323], ([7.233344  0.5968987]) \t loss 1.025E+01\n",
      "step 6460000 \t return [-20.672562 -14.183781], ([5.5860453  0.47264367]) \t loss 9.607E+00\n",
      "step 6470000 \t return [-17.303848 -15.729879], ([3.3522582  0.55340487]) \t loss 9.822E+00\n",
      "step 6480000 \t return [-15.843113 -14.42998 ], ([5.0771456  0.39872366]) \t loss 1.028E+01\n",
      "step 6490000 \t return [-20.519133 -14.827854], ([5.283092  0.4775969]) \t loss 8.973E+00\n",
      "step 6500000 \t return [-25.998928 -13.57656 ], ([7.4272394 0.4359982]) \t loss 9.659E+00\n",
      "step 6510000 \t return [-24.128424 -13.512152], ([7.160348   0.45026308]) \t loss 9.443E+00\n",
      "step 6520000 \t return [-21.935022 -13.675995], ([5.857817   0.41067725]) \t loss 8.637E+00\n",
      "step 6530000 \t return [-18.979816 -13.995278], ([4.95173    0.35327667]) \t loss 8.766E+00\n",
      "step 6540000 \t return [-23.402082 -13.628648], ([6.7082987 0.5227865]) \t loss 9.416E+00\n",
      "step 6550000 \t return [-19.250877 -13.587802], ([7.058222   0.47024396]) \t loss 9.977E+00\n",
      "step 6560000 \t return [-18.148182 -14.291699], ([5.222832   0.39076594]) \t loss 9.389E+00\n",
      "step 6570000 \t return [-19.390938 -13.983312], ([5.706797   0.34328577]) \t loss 9.239E+00\n",
      "step 6580000 \t return [-20.116226 -13.765545], ([5.738653   0.36962122]) \t loss 8.408E+00\n",
      "step 6590000 \t return [-18.891216 -14.122898], ([3.860828   0.30117822]) \t loss 9.299E+00\n",
      "step 6600000 \t return [-17.993685 -13.599949], ([7.0742087 0.3851047]) \t loss 7.965E+00\n",
      "step 6610000 \t return [-17.075817 -13.457808], ([8.555732   0.38266277]) \t loss 8.765E+00\n",
      "step 6620000 \t return [-21.690632 -13.814092], ([5.1360526 0.4069976]) \t loss 7.969E+00\n",
      "step 6630000 \t return [-20.997044 -13.27589 ], ([6.449256  0.3508485]) \t loss 8.629E+00\n",
      "step 6640000 \t return [-19.382025 -13.162656], ([6.124783   0.39591303]) \t loss 6.957E+00\n",
      "step 6650000 \t return [-20.622295 -13.055822], ([5.601182   0.34972626]) \t loss 5.902E+00\n",
      "step 6660000 \t return [-17.768507 -13.285364], ([7.0695996 0.3935803]) \t loss 7.023E+00\n",
      "step 6670000 \t return [-21.951012 -13.373833], ([7.029625   0.43587363]) \t loss 6.873E+00\n",
      "step 6680000 \t return [-16.812698 -14.491018], ([5.151024   0.38111776]) \t loss 7.116E+00\n",
      "step 6690000 \t return [-21.321316 -13.211129], ([7.205367   0.46478006]) \t loss 8.444E+00\n",
      "step 6700000 \t return [-17.736343  -14.0476675], ([5.2819247  0.30571607]) \t loss 7.772E+00\n",
      "step 6710000 \t return [-17.70112  -14.145696], ([6.858644   0.44865578]) \t loss 6.607E+00\n",
      "step 6720000 \t return [-23.245779 -13.359847], ([6.377182  0.4169949]) \t loss 8.208E+00\n",
      "step 6730000 \t return [-18.461914 -14.10332 ], ([6.0496955  0.32931882]) \t loss 6.875E+00\n",
      "step 6740000 \t return [-15.889507 -13.489574], ([7.084316   0.41814694]) \t loss 7.442E+00\n",
      "step 6750000 \t return [-17.187553 -14.289423], ([4.8244324 0.3058874]) \t loss 5.694E+00\n",
      "step 6760000 \t return [-23.552404 -13.166352], ([6.572463  0.3785253]) \t loss 7.640E+00\n",
      "step 6770000 \t return [-21.021973 -13.332695], ([8.599918   0.36359206]) \t loss 5.545E+00\n",
      "step 6780000 \t return [-13.588992 -14.289578], ([8.714014   0.37406683]) \t loss 6.763E+00\n",
      "step 6790000 \t return [-18.60156   -13.2787485], ([6.925185 0.438441]) \t loss 7.649E+00\n",
      "step 6800000 \t return [-15.936728 -14.411639], ([6.0404544 0.4113513]) \t loss 6.841E+00\n",
      "step 6810000 \t return [-17.493229 -13.329593], ([5.8556905  0.40457422]) \t loss 6.744E+00\n",
      "step 6820000 \t return [-20.527702  -13.0472975], ([6.872784  0.5078224]) \t loss 5.357E+00\n",
      "step 6830000 \t return [-13.106575 -14.187837], ([8.534654   0.32908738]) \t loss 5.075E+00\n",
      "step 6840000 \t return [-14.706493 -13.952397], ([6.742953   0.68159133]) \t loss 7.128E+00\n",
      "step 6850000 \t return [-18.163267 -14.413741], ([6.1506867 0.4656276]) \t loss 5.437E+00\n",
      "step 6860000 \t return [-20.12138  -13.199693], ([4.5145874  0.42647824]) \t loss 6.476E+00\n",
      "step 6870000 \t return [-17.18341  -13.233604], ([6.9992056  0.35152856]) \t loss 4.494E+00\n",
      "step 6880000 \t return [-19.205082 -13.638544], ([5.7920065  0.39106983]) \t loss 5.486E+00\n",
      "step 6890000 \t return [-13.451916 -14.121378], ([8.448912   0.39988682]) \t loss 4.837E+00\n",
      "step 6900000 \t return [-23.690708 -13.188081], ([6.5810556  0.33788264]) \t loss 5.389E+00\n",
      "step 6910000 \t return [-15.985312 -13.556845], ([12.047265   0.0821788]) \t loss 6.756E+00\n",
      "step 6920000 \t return [-17.538418 -13.308245], ([6.7056255  0.40742648]) \t loss 5.575E+00\n",
      "step 6930000 \t return [-17.898792 -13.542158], ([5.7052135  0.40735927]) \t loss 4.953E+00\n",
      "step 6940000 \t return [-16.895645 -13.475724], ([5.8586693  0.47385368]) \t loss 5.039E+00\n",
      "step 6950000 \t return [-14.062796 -13.720602], ([7.217359  0.6647883]) \t loss 4.376E+00\n",
      "step 6960000 \t return [-17.04104  -13.278289], ([5.7553616  0.56012994]) \t loss 4.474E+00\n",
      "step 6970000 \t return [-14.288657 -13.585891], ([6.3125505 0.5966701]) \t loss 3.872E+00\n",
      "step 6980000 \t return [-21.794338 -13.204695], ([6.0477633  0.36386147]) \t loss 3.792E+00\n",
      "step 6990000 \t return [-13.057356 -13.849471], ([7.627321   0.57217777]) \t loss 4.352E+00\n",
      "step 7000000 \t return [-14.782776 -13.912865], ([8.791372   0.54098797]) \t loss 4.158E+00\n",
      "step 7010000 \t return [-21.28798  -13.225886], ([4.859693   0.39796066]) \t loss 4.269E+00\n",
      "step 7020000 \t return [-15.814069 -13.689325], ([6.0431    0.6217391]) \t loss 4.289E+00\n",
      "step 7030000 \t return [-19.945948 -13.583623], ([5.1746144  0.41592717]) \t loss 4.885E+00\n",
      "step 7040000 \t return [-14.8162565 -13.482225 ], ([5.259598  0.6221139]) \t loss 3.806E+00\n",
      "step 7050000 \t return [-11.43136  -13.829833], ([7.991699   0.43884757]) \t loss 4.998E+00\n",
      "step 7060000 \t return [-19.380125 -13.321307], ([5.676062   0.49175474]) \t loss 3.610E+00\n",
      "step 7070000 \t return [-16.892145  -13.8214855], ([5.0670605  0.69063383]) \t loss 3.646E+00\n",
      "step 7080000 \t return [-19.198254 -13.561139], ([5.4775047 0.4819735]) \t loss 4.718E+00\n",
      "step 7090000 \t return [-15.845563 -13.622922], ([5.8364263 0.7020292]) \t loss 3.760E+00\n",
      "step 7100000 \t return [-21.180988 -12.777499], ([6.05062    0.43945253]) \t loss 3.762E+00\n",
      "step 7110000 \t return [-17.302422 -13.438319], ([5.8913183  0.77868557]) \t loss 4.301E+00\n",
      "step 7120000 \t return [-13.437411 -14.027004], ([7.7765064  0.52350545]) \t loss 4.491E+00\n",
      "step 7130000 \t return [-17.952843 -13.488491], ([5.9037695 0.6799662]) \t loss 3.930E+00\n",
      "step 7140000 \t return [-17.44086  -13.388023], ([5.169998   0.53020364]) \t loss 4.062E+00\n",
      "step 7150000 \t return [-15.302091 -13.741798], ([4.832796  0.5557358]) \t loss 4.068E+00\n",
      "step 7160000 \t return [-17.749035 -13.242285], ([5.9894795  0.47552773]) \t loss 3.976E+00\n",
      "step 7170000 \t return [-15.350611 -13.568393], ([6.750228  0.6490109]) \t loss 4.423E+00\n",
      "step 7180000 \t return [-12.321003 -14.05385 ], ([6.892652 0.554306]) \t loss 3.643E+00\n",
      "step 7190000 \t return [-15.400521 -13.82935 ], ([8.845746  0.5160871]) \t loss 3.932E+00\n",
      "step 7200000 \t return [-21.17282 -13.3465 ], ([5.855738   0.47921795]) \t loss 3.196E+00\n",
      "step 7210000 \t return [-16.85666  -13.643044], ([6.786775   0.58103335]) \t loss 4.543E+00\n",
      "step 7220000 \t return [-15.880307 -13.799702], ([6.5948696 0.8373326]) \t loss 3.290E+00\n",
      "step 7230000 \t return [-18.531784 -14.003282], ([4.962195   0.50889766]) \t loss 3.772E+00\n",
      "step 7240000 \t return [-16.794977 -14.080949], ([4.4616575 0.5599083]) \t loss 4.396E+00\n",
      "step 7250000 \t return [-18.118156 -13.174551], ([6.446432  0.6755144]) \t loss 4.196E+00\n",
      "step 7260000 \t return [-16.388494 -13.576809], ([5.5605297 0.6639919]) \t loss 3.454E+00\n",
      "step 7270000 \t return [-14.839573 -13.415562], ([6.9732122  0.61192346]) \t loss 3.735E+00\n",
      "step 7280000 \t return [-17.469793 -13.140508], ([7.2462225 0.5369583]) \t loss 4.246E+00\n",
      "step 7290000 \t return [-19.564909 -13.247977], ([6.0453653  0.55166346]) \t loss 3.669E+00\n",
      "step 7300000 \t return [-16.128975 -13.378805], ([8.079143  0.6836157]) \t loss 4.070E+00\n",
      "step 7310000 \t return [-16.53059  -13.162878], ([7.9488287  0.53175586]) \t loss 3.645E+00\n",
      "step 7320000 \t return [-21.654184 -12.451085], ([6.360621 0.400007]) \t loss 3.852E+00\n",
      "step 7330000 \t return [-19.988825  -13.2090645], ([6.149461  0.4819286]) \t loss 3.984E+00\n",
      "step 7340000 \t return [-17.100805 -13.319839], ([7.1016464 0.5905844]) \t loss 3.608E+00\n",
      "step 7350000 \t return [-13.99068  -14.224044], ([4.9942093  0.69777995]) \t loss 3.197E+00\n",
      "step 7360000 \t return [-16.794205 -13.346282], ([5.4005914  0.59890085]) \t loss 3.642E+00\n",
      "step 7370000 \t return [-20.83121  -13.284835], ([5.3850617 0.3944387]) \t loss 3.731E+00\n",
      "step 7380000 \t return [-18.84462 -13.00653], ([6.435818  0.4957127]) \t loss 3.044E+00\n",
      "step 7390000 \t return [-16.17841 -13.69696], ([5.8653965 0.621277 ]) \t loss 3.841E+00\n",
      "step 7400000 \t return [-13.127632 -13.830749], ([5.7006993  0.72654873]) \t loss 4.136E+00\n",
      "step 7410000 \t return [-17.670177 -13.289967], ([3.7937     0.36516392]) \t loss 4.439E+00\n",
      "step 7420000 \t return [-17.80299 -12.65432], ([8.162758   0.42461327]) \t loss 5.413E+00\n",
      "step 7430000 \t return [-17.105219 -12.874362], ([7.514934  0.4721042]) \t loss 3.182E+00\n",
      "step 7440000 \t return [-13.792547 -13.708163], ([8.812722  0.5912228]) \t loss 3.599E+00\n",
      "step 7450000 \t return [-16.487381 -13.385933], ([5.139399   0.45678788]) \t loss 3.482E+00\n",
      "step 7460000 \t return [-15.537736 -13.663473], ([5.150753  0.5231803]) \t loss 3.355E+00\n",
      "step 7470000 \t return [-13.226998 -14.177207], ([7.3840866  0.66435003]) \t loss 3.504E+00\n",
      "step 7480000 \t return [-13.472631 -13.514666], ([7.3766556  0.62696594]) \t loss 4.397E+00\n",
      "step 7490000 \t return [-18.341425 -13.074557], ([5.641268   0.45515528]) \t loss 3.519E+00\n",
      "step 7500000 \t return [-18.631014 -13.038408], ([4.9828577  0.47284085]) \t loss 4.660E+00\n",
      "step 7510000 \t return [-20.71405  -12.894822], ([5.914288   0.34537262]) \t loss 2.993E+00\n",
      "step 7520000 \t return [-12.976202 -13.747026], ([6.4778666 0.7224902]) \t loss 4.804E+00\n",
      "step 7530000 \t return [-14.257578 -13.988095], ([5.8001556  0.56091964]) \t loss 3.463E+00\n",
      "step 7540000 \t return [-13.35599  -13.640537], ([8.327635  0.5784593]) \t loss 3.933E+00\n",
      "step 7550000 \t return [-16.125544 -13.641288], ([5.7928987 0.4510261]) \t loss 3.918E+00\n",
      "step 7560000 \t return [-11.7953825 -14.096569 ], ([6.4652133 0.6089766]) \t loss 4.042E+00\n",
      "step 7570000 \t return [-14.020005 -13.845879], ([6.299258  0.7462969]) \t loss 2.667E+00\n",
      "step 7580000 \t return [-18.554672 -13.362548], ([6.2556796  0.52698505]) \t loss 3.103E+00\n",
      "step 7590000 \t return [-20.152817 -12.972326], ([5.625775   0.41952726]) \t loss 3.144E+00\n",
      "step 7600000 \t return [-15.495639 -13.574821], ([6.7897453  0.54156774]) \t loss 3.739E+00\n",
      "step 7610000 \t return [-18.023232 -13.340265], ([6.831147  0.7233037]) \t loss 4.017E+00\n",
      "step 7620000 \t return [-15.820278 -13.405961], ([6.3152514 0.5628642]) \t loss 3.892E+00\n",
      "step 7630000 \t return [-17.760597 -13.629346], ([5.9880247 0.5109206]) \t loss 3.053E+00\n",
      "step 7640000 \t return [-16.248632 -13.200597], ([5.9838185 0.5257855]) \t loss 3.369E+00\n",
      "step 7650000 \t return [-16.111975 -13.382717], ([6.195207  0.5844363]) \t loss 3.789E+00\n",
      "step 7660000 \t return [-16.1088   -13.406364], ([6.1343293  0.54442567]) \t loss 3.910E+00\n",
      "step 7670000 \t return [-15.483799 -14.038692], ([5.6768804  0.58012027]) \t loss 3.533E+00\n",
      "step 7680000 \t return [-14.1565   -13.489973], ([7.0899663  0.64832336]) \t loss 4.319E+00\n",
      "step 7690000 \t return [-19.640266 -12.501522], ([9.354667   0.36024657]) \t loss 3.611E+00\n",
      "step 7700000 \t return [-19.700214 -12.721675], ([6.5114374  0.42245042]) \t loss 3.403E+00\n",
      "step 7710000 \t return [-13.067174 -14.059778], ([6.6710215  0.67890686]) \t loss 3.633E+00\n",
      "step 7720000 \t return [-12.509097 -13.944759], ([6.4401193 0.6070679]) \t loss 3.518E+00\n",
      "step 7730000 \t return [-15.553378 -13.697458], ([9.205078   0.32330504]) \t loss 3.227E+00\n",
      "step 7740000 \t return [-16.58107  -13.527179], ([5.758514  0.4776163]) \t loss 4.081E+00\n",
      "step 7750000 \t return [-17.524185 -13.10086 ], ([5.55824   0.4862043]) \t loss 3.705E+00\n",
      "step 7760000 \t return [-15.723191 -13.618312], ([5.6247244 0.5903196]) \t loss 3.477E+00\n",
      "step 7770000 \t return [-16.231403 -13.413114], ([6.1736717  0.57900286]) \t loss 3.605E+00\n",
      "step 7780000 \t return [-15.269095 -13.760221], ([6.493328  0.5768421]) \t loss 2.599E+00\n",
      "step 7790000 \t return [-15.819946 -13.606983], ([6.2153935  0.49301594]) \t loss 4.021E+00\n",
      "step 7800000 \t return [-16.174229 -13.368165], ([6.15735    0.49697837]) \t loss 2.775E+00\n",
      "step 7810000 \t return [-16.647707 -13.026314], ([9.886598  0.5143022]) \t loss 3.775E+00\n",
      "step 7820000 \t return [-13.432281 -13.88563 ], ([7.9918523 0.5760216]) \t loss 2.768E+00\n",
      "step 7830000 \t return [-15.506919 -13.828029], ([4.5775924 0.4335292]) \t loss 3.973E+00\n",
      "step 7840000 \t return [-14.2948475 -13.86891  ], ([4.679286  0.4700495]) \t loss 3.334E+00\n",
      "step 7850000 \t return [-17.02524  -13.439745], ([4.9671283  0.40389094]) \t loss 4.200E+00\n",
      "step 7860000 \t return [-14.722179 -13.621657], ([5.4331355  0.50111496]) \t loss 2.485E+00\n",
      "step 7870000 \t return [-20.209255 -12.616035], ([6.862872  0.5059009]) \t loss 3.110E+00\n",
      "step 7880000 \t return [-16.982048 -13.318952], ([6.291109  0.5943572]) \t loss 2.748E+00\n",
      "step 7890000 \t return [-14.56719  -13.846607], ([6.553713  0.5822743]) \t loss 3.091E+00\n",
      "step 7900000 \t return [-14.387664 -13.405932], ([5.818248  0.6304265]) \t loss 4.149E+00\n",
      "step 7910000 \t return [-28.429825 -12.382199], ([2.3387246 0.5226554]) \t loss 2.769E+00\n",
      "step 7920000 \t return [-19.276173 -12.833181], ([6.1631823  0.53418505]) \t loss 4.372E+00\n",
      "step 7930000 \t return [-19.882774 -12.621143], ([5.017442   0.38690767]) \t loss 3.703E+00\n",
      "step 7940000 \t return [-17.358593 -12.906057], ([6.06459    0.43882093]) \t loss 3.039E+00\n",
      "step 7950000 \t return [-14.182404 -13.573524], ([6.5819283 0.6942702]) \t loss 2.818E+00\n",
      "step 7960000 \t return [-18.602648 -12.588408], ([5.4096923  0.42004937]) \t loss 3.411E+00\n",
      "step 7970000 \t return [-12.090201 -13.830121], ([7.4752007 0.6382887]) \t loss 3.157E+00\n",
      "step 7980000 \t return [-11.095842 -14.355621], ([6.770644   0.66506255]) \t loss 3.043E+00\n",
      "step 7990000 \t return [-17.646265 -12.994967], ([4.902999   0.35104507]) \t loss 3.454E+00\n",
      "step 8000000 \t return [-13.899027 -13.458851], ([6.5080733  0.64561844]) \t loss 3.543E+00\n",
      "step 8010000 \t return [-14.546574 -13.541613], ([5.747431  0.6620888]) \t loss 3.112E+00\n",
      "step 8020000 \t return [-22.364527 -12.629991], ([3.3175023 0.4555696]) \t loss 3.465E+00\n",
      "step 8030000 \t return [-19.334084 -13.308912], ([5.735865   0.40053225]) \t loss 3.699E+00\n",
      "step 8040000 \t return [-16.811188 -13.272283], ([5.5721855  0.50361365]) \t loss 2.615E+00\n",
      "step 8050000 \t return [-16.100134 -12.898598], ([5.5496597 0.4554673]) \t loss 4.373E+00\n",
      "step 8060000 \t return [-12.159943 -13.342359], ([8.645626   0.59388065]) \t loss 4.995E+00\n",
      "step 8070000 \t return [-10.950218 -13.966731], ([8.1688795 0.3446856]) \t loss 3.514E+00\n",
      "step 8080000 \t return [-24.832912 -12.369801], ([1.4340699  0.46255964]) \t loss 5.034E+00\n",
      "step 8090000 \t return [-13.569066 -13.459982], ([5.1286206 0.6451104]) \t loss 2.460E+00\n",
      "step 8100000 \t return [-10.882263 -13.701246], ([6.689552  0.5443271]) \t loss 2.494E+00\n",
      "step 8110000 \t return [-14.291077 -13.650339], ([4.1614647  0.41843805]) \t loss 2.824E+00\n",
      "step 8120000 \t return [-13.012406 -14.176047], ([5.2069116  0.93090576]) \t loss 3.601E+00\n",
      "step 8130000 \t return [-14.708329 -13.441941], ([4.767992   0.46968046]) \t loss 3.338E+00\n",
      "step 8140000 \t return [-15.435363 -12.847108], ([6.3086257  0.45887336]) \t loss 3.185E+00\n",
      "step 8150000 \t return [-16.171734 -12.774099], ([5.3452077 0.4249523]) \t loss 3.517E+00\n",
      "step 8160000 \t return [-14.789211  -13.3309355], ([4.7667828  0.44085944]) \t loss 3.100E+00\n",
      "step 8170000 \t return [-14.476611 -13.059801], ([5.7991323 0.5703896]) \t loss 2.608E+00\n",
      "step 8180000 \t return [-11.010803 -14.282046], ([6.004317   0.65827245]) \t loss 2.146E+00\n",
      "step 8190000 \t return [-14.5023   -13.048707], ([8.454398  0.7075572]) \t loss 3.226E+00\n",
      "step 8200000 \t return [-16.777536 -12.927184], ([5.1334267 0.5126815]) \t loss 2.560E+00\n",
      "step 8210000 \t return [-13.032357 -13.527161], ([5.41753    0.61715174]) \t loss 2.702E+00\n",
      "step 8220000 \t return [-14.783953 -13.623666], ([5.951381   0.67943096]) \t loss 3.081E+00\n",
      "step 8230000 \t return [-17.283934 -13.659848], ([5.0843515  0.39963862]) \t loss 3.374E+00\n",
      "step 8240000 \t return [-16.509502 -13.301647], ([5.882422   0.66626245]) \t loss 3.002E+00\n",
      "step 8250000 \t return [-16.394672 -13.3595  ], ([4.8791475  0.51530886]) \t loss 2.789E+00\n",
      "step 8260000 \t return [-14.181252 -13.281317], ([7.054979   0.82461554]) \t loss 2.958E+00\n",
      "step 8270000 \t return [-18.297527 -13.173977], ([5.2391663  0.44002897]) \t loss 2.691E+00\n",
      "step 8280000 \t return [-16.744532 -13.228938], ([5.3308444 0.5472436]) \t loss 2.733E+00\n",
      "step 8290000 \t return [-12.407358 -13.882573], ([7.910781   0.44063094]) \t loss 3.200E+00\n",
      "step 8300000 \t return [-12.303234 -13.814514], ([7.510764  0.5348209]) \t loss 3.140E+00\n",
      "step 8310000 \t return [-11.185282 -13.621072], ([8.040409  0.3318747]) \t loss 5.909E+00\n",
      "step 8320000 \t return [-10.8815775 -14.153916 ], ([6.389466   0.63875026]) \t loss 3.108E+00\n",
      "step 8330000 \t return [-11.8554125 -13.543937 ], ([5.44072    0.65869945]) \t loss 3.556E+00\n",
      "step 8340000 \t return [-14.7389345 -13.234836 ], ([4.6198535 0.4562466]) \t loss 3.237E+00\n",
      "step 8350000 \t return [-19.431395 -13.169784], ([5.1445093 0.4552358]) \t loss 3.366E+00\n",
      "step 8360000 \t return [-14.953561 -13.354886], ([6.056767   0.74229294]) \t loss 2.644E+00\n",
      "step 8370000 \t return [-15.133993 -12.928511], ([4.6226754  0.44806847]) \t loss 2.655E+00\n",
      "step 8380000 \t return [-14.286184 -13.818716], ([4.7051897 0.6242364]) \t loss 2.290E+00\n",
      "step 8390000 \t return [-19.424812 -13.274169], ([3.957633   0.49472582]) \t loss 4.062E+00\n",
      "step 8400000 \t return [-13.602705 -13.590523], ([6.434218  0.6719525]) \t loss 2.757E+00\n",
      "step 8410000 \t return [-14.102958 -13.383978], ([5.943588  0.6948572]) \t loss 2.136E+00\n",
      "step 8420000 \t return [-18.639969 -13.227198], ([5.1165547  0.45154744]) \t loss 2.494E+00\n",
      "step 8430000 \t return [-12.529891 -13.821883], ([5.442241   0.64998925]) \t loss 2.496E+00\n",
      "step 8440000 \t return [-12.641339 -14.28214 ], ([4.355914  0.8550498]) \t loss 2.915E+00\n",
      "step 8450000 \t return [-13.617566 -13.28118 ], ([5.2525544 0.5865178]) \t loss 5.081E+00\n",
      "step 8460000 \t return [-17.195913 -12.704445], ([5.194145   0.43728557]) \t loss 3.530E+00\n",
      "step 8470000 \t return [-15.677595  -13.6476345], ([4.985473   0.46913016]) \t loss 3.169E+00\n",
      "step 8480000 \t return [-14.63014  -13.488433], ([5.0230575 0.6216915]) \t loss 2.324E+00\n",
      "step 8490000 \t return [-11.093331 -13.978131], ([5.823504   0.56897926]) \t loss 2.626E+00\n",
      "step 8500000 \t return [-12.707525 -13.662387], ([5.6104517 0.6681055]) \t loss 3.298E+00\n",
      "step 8510000 \t return [-28.107805 -12.178623], ([2.1705894  0.41489047]) \t loss 3.415E+00\n",
      "step 8520000 \t return [-14.564364 -13.338451], ([4.579169   0.37155408]) \t loss 4.479E+00\n",
      "step 8530000 \t return [-15.932893 -13.273681], ([5.0097876  0.57180244]) \t loss 3.157E+00\n",
      "step 8540000 \t return [-14.020458 -13.625226], ([5.630154   0.64013904]) \t loss 3.588E+00\n",
      "step 8550000 \t return [-15.547949 -13.503226], ([4.6975913 0.4782193]) \t loss 4.162E+00\n",
      "step 8560000 \t return [-14.554046 -13.420751], ([5.0773697 0.5590561]) \t loss 3.627E+00\n",
      "step 8570000 \t return [-18.569948  -12.4524975], ([4.8201504  0.37918878]) \t loss 3.866E+00\n",
      "step 8580000 \t return [-23.152346 -12.198386], ([3.5103564 0.507975 ]) \t loss 3.485E+00\n",
      "step 8590000 \t return [-21.404404 -13.009946], ([5.255893   0.31583038]) \t loss 2.625E+00\n",
      "step 8600000 \t return [-20.794052 -13.196085], ([4.6078806  0.30533555]) \t loss 2.972E+00\n",
      "step 8610000 \t return [-13.070111 -13.361534], ([4.5826426  0.41617936]) \t loss 4.016E+00\n",
      "step 8620000 \t return [-14.629991 -13.769248], ([4.530365  0.7060034]) \t loss 3.104E+00\n",
      "step 8630000 \t return [-13.300946 -13.20003 ], ([4.84913   0.5769049]) \t loss 3.474E+00\n",
      "step 8640000 \t return [-13.542985 -13.018973], ([5.2062902  0.44604623]) \t loss 4.042E+00\n",
      "step 8650000 \t return [-11.187641 -14.072807], ([6.3011665  0.36605412]) \t loss 4.002E+00\n",
      "step 8660000 \t return [-15.595311 -12.64751 ], ([7.299148   0.33820918]) \t loss 3.447E+00\n",
      "step 8670000 \t return [-14.176922 -13.188306], ([5.922314 0.556416]) \t loss 3.059E+00\n",
      "step 8680000 \t return [-18.278193 -12.852708], ([4.9671597  0.35604924]) \t loss 3.662E+00\n",
      "step 8690000 \t return [-11.699083 -13.765024], ([6.2070436 0.5994285]) \t loss 3.119E+00\n",
      "step 8700000 \t return [-25.890175 -12.241416], ([3.7164521 0.4181529]) \t loss 3.903E+00\n",
      "step 8710000 \t return [-12.805309 -13.321486], ([6.631684  0.5069648]) \t loss 2.528E+00\n",
      "step 8720000 \t return [-11.359184 -13.94397 ], ([6.4141903 0.5279884]) \t loss 3.339E+00\n",
      "step 8730000 \t return [-12.68158  -13.975032], ([4.9580526 0.6232513]) \t loss 2.750E+00\n",
      "step 8740000 \t return [-15.336719 -13.270132], ([5.588088  0.6606917]) \t loss 2.746E+00\n",
      "step 8750000 \t return [-18.84164  -12.878039], ([4.6741533  0.39300698]) \t loss 2.856E+00\n",
      "step 8760000 \t return [-11.427364 -13.987765], ([5.7146597 0.5922294]) \t loss 2.775E+00\n",
      "step 8770000 \t return [-24.275623 -12.534433], ([3.2151537  0.46096843]) \t loss 2.676E+00\n",
      "step 8780000 \t return [-26.128601 -12.41081 ], ([3.9979382 0.4861765]) \t loss 2.466E+00\n",
      "step 8790000 \t return [-15.116485  -13.3803215], ([5.7793474  0.65158635]) \t loss 3.147E+00\n",
      "step 8800000 \t return [-26.00074  -13.057862], ([3.0961566  0.63957256]) \t loss 2.554E+00\n",
      "step 8810000 \t return [-14.006156 -13.322063], ([4.9463067 0.7531223]) \t loss 2.899E+00\n",
      "step 8820000 \t return [-20.883938 -13.209912], ([3.5653899  0.47340822]) \t loss 2.665E+00\n",
      "step 8830000 \t return [-18.427546 -13.116881], ([4.3307123  0.44028443]) \t loss 3.802E+00\n",
      "step 8840000 \t return [-19.652218 -12.701595], ([5.473474   0.43128693]) \t loss 3.271E+00\n",
      "step 8850000 \t return [-11.088895 -13.846326], ([5.1487937  0.43876308]) \t loss 2.964E+00\n",
      "step 8860000 \t return [-14.780044 -13.310124], ([4.938539 0.570036]) \t loss 3.572E+00\n",
      "step 8870000 \t return [-13.803374 -14.117001], ([5.057617   0.62835944]) \t loss 2.172E+00\n",
      "step 8880000 \t return [-11.474369 -13.961554], ([5.166878  0.6058984]) \t loss 3.368E+00\n",
      "step 8890000 \t return [-14.222743 -13.74429 ], ([5.1913795 0.594246 ]) \t loss 2.492E+00\n",
      "step 8900000 \t return [-11.955705 -13.505121], ([5.7136464 0.5911638]) \t loss 2.655E+00\n",
      "step 8910000 \t return [-14.058305 -13.251089], ([5.1421633  0.45681283]) \t loss 2.415E+00\n",
      "step 8920000 \t return [-18.862415 -12.931164], ([4.02437   0.4304887]) \t loss 1.558E+00\n",
      "step 8930000 \t return [-10.482877 -14.472832], ([3.8460891 0.6740905]) \t loss 2.794E+00\n",
      "step 8940000 \t return [-18.260103 -13.427891], ([5.8515506  0.33517867]) \t loss 4.263E+00\n",
      "step 8950000 \t return [-23.680653 -13.004259], ([2.4904757  0.64386946]) \t loss 3.353E+00\n",
      "step 8960000 \t return [-16.327442 -13.489428], ([4.9955306  0.46399498]) \t loss 2.512E+00\n",
      "step 8970000 \t return [-21.732214 -12.74257 ], ([4.453222 0.468917]) \t loss 2.709E+00\n",
      "step 8980000 \t return [-10.974954 -13.452598], ([4.993547   0.44696754]) \t loss 2.608E+00\n",
      "step 8990000 \t return [-13.138374 -13.377417], ([6.0362873  0.48497993]) \t loss 2.727E+00\n",
      "step 9000000 \t return [-14.667235 -13.429972], ([5.2679315  0.56627643]) \t loss 2.713E+00\n",
      "step 9010000 \t return [-10.881513 -13.510115], ([6.1202593 0.3928211]) \t loss 3.115E+00\n",
      "step 9020000 \t return [-14.796487 -13.45752 ], ([5.0731964 0.4405486]) \t loss 1.923E+00\n",
      "step 9030000 \t return [-16.24558  -12.689618], ([6.2619915 0.4811905]) \t loss 2.814E+00\n",
      "step 9040000 \t return [-26.463469 -12.539072], ([3.058958  0.5238328]) \t loss 2.389E+00\n",
      "step 9050000 \t return [-14.183707 -13.095551], ([4.589268   0.48085347]) \t loss 2.508E+00\n",
      "step 9060000 \t return [-23.9224   -12.647501], ([3.106009   0.48643526]) \t loss 2.573E+00\n",
      "step 9070000 \t return [-14.267996 -13.231665], ([6.049372  0.5329788]) \t loss 2.459E+00\n",
      "step 9080000 \t return [-18.05193  -13.067438], ([5.0347776  0.37690192]) \t loss 2.187E+00\n",
      "step 9090000 \t return [-18.419462 -13.307284], ([3.5179164  0.43164882]) \t loss 2.559E+00\n",
      "step 9100000 \t return [-12.00943  -13.784603], ([6.1176224  0.55443156]) \t loss 2.063E+00\n",
      "step 9110000 \t return [-12.5895605 -13.816052 ], ([5.432861   0.46210268]) \t loss 2.203E+00\n",
      "step 9120000 \t return [-16.132915 -13.758585], ([4.9553347 0.3774901]) \t loss 2.732E+00\n",
      "step 9130000 \t return [-11.527924 -13.758689], ([6.7080736 0.5388326]) \t loss 3.569E+00\n",
      "step 9140000 \t return [-12.53305  -14.091554], ([6.062594   0.58748645]) \t loss 1.749E+00\n",
      "step 9150000 \t return [-11.240844 -14.114768], ([5.0801725 0.5902041]) \t loss 2.598E+00\n",
      "step 9160000 \t return [-13.279218 -13.18549 ], ([4.8704953 0.4713331]) \t loss 3.112E+00\n",
      "step 9170000 \t return [-12.095189 -13.991039], ([7.3985467  0.46118465]) \t loss 3.183E+00\n",
      "step 9180000 \t return [-19.086208 -12.946834], ([5.995451  0.3641107]) \t loss 2.176E+00\n",
      "step 9190000 \t return [-15.129421 -13.166552], ([5.492327  0.3105553]) \t loss 2.510E+00\n",
      "step 9200000 \t return [-12.151722 -14.048574], ([5.9316125 0.6395103]) \t loss 2.741E+00\n",
      "step 9210000 \t return [-12.371442 -13.151569], ([6.5982103 0.3777419]) \t loss 2.848E+00\n",
      "step 9220000 \t return [-12.328007  -13.1148405], ([6.92395    0.28733173]) \t loss 1.917E+00\n",
      "step 9230000 \t return [-13.832044 -13.287457], ([4.212867  0.4330788]) \t loss 2.230E+00\n",
      "step 9240000 \t return [-16.970953 -12.766282], ([3.7465909  0.49691194]) \t loss 2.878E+00\n",
      "step 9250000 \t return [-24.248884 -12.622501], ([3.1047225  0.60507315]) \t loss 2.092E+00\n",
      "step 9260000 \t return [-10.880593 -14.248143], ([4.7943587 0.5326455]) \t loss 2.519E+00\n",
      "step 9270000 \t return [-14.971762 -13.043875], ([4.198008   0.41512865]) \t loss 2.698E+00\n",
      "step 9280000 \t return [-15.580584 -12.69713 ], ([5.982075   0.44517824]) \t loss 2.314E+00\n",
      "step 9290000 \t return [-12.2366905 -13.37833  ], ([5.480267   0.45592874]) \t loss 2.211E+00\n",
      "step 9300000 \t return [-10.538287 -13.718084], ([4.5107727 0.6086236]) \t loss 2.447E+00\n",
      "step 9310000 \t return [-14.569408 -12.695717], ([6.7803903  0.39318356]) \t loss 2.563E+00\n",
      "step 9320000 \t return [-11.43816  -13.783474], ([5.992165   0.45342258]) \t loss 1.697E+00\n",
      "step 9330000 \t return [-11.493822 -13.834207], ([5.0869465 0.6131604]) \t loss 2.375E+00\n",
      "step 9340000 \t return [-19.83382  -12.583144], ([3.986245  0.3898967]) \t loss 3.528E+00\n",
      "step 9350000 \t return [-13.2744255 -13.527205 ], ([6.871947   0.48377258]) \t loss 2.058E+00\n",
      "step 9360000 \t return [-13.314597 -13.003796], ([6.0816655 0.4740071]) \t loss 2.155E+00\n",
      "step 9370000 \t return [-11.144988 -13.740656], ([5.980202   0.45867556]) \t loss 2.005E+00\n",
      "step 9380000 \t return [-10.592629 -13.585293], ([7.4009843  0.23171057]) \t loss 2.455E+00\n",
      "step 9390000 \t return [-20.816544 -12.306666], ([4.1270256 0.5673229]) \t loss 2.854E+00\n",
      "step 9400000 \t return [-20.134499  -12.3421335], ([4.9049582 0.3753559]) \t loss 3.158E+00\n",
      "step 9410000 \t return [-16.513147 -12.654732], ([4.891923  0.4144837]) \t loss 1.606E+00\n",
      "step 9420000 \t return [-13.713222 -13.028753], ([6.7425785  0.57620794]) \t loss 2.685E+00\n",
      "step 9430000 \t return [-13.181505 -13.281288], ([5.572248   0.54736155]) \t loss 2.413E+00\n",
      "step 9440000 \t return [-11.554818 -13.3996  ], ([6.793531  0.4097971]) \t loss 2.847E+00\n",
      "step 9450000 \t return [-12.062452 -13.163735], ([6.456113   0.41859648]) \t loss 2.530E+00\n",
      "step 9460000 \t return [-16.212675 -12.610597], ([5.755378   0.40310198]) \t loss 1.998E+00\n",
      "step 9470000 \t return [ -9.9115095 -13.286171 ], ([7.415716   0.27436954]) \t loss 2.280E+00\n",
      "step 9480000 \t return [-15.35913 -12.96398], ([6.661373  0.4053592]) \t loss 2.104E+00\n",
      "step 9490000 \t return [-19.524694 -12.237624], ([5.270919   0.40483382]) \t loss 2.752E+00\n",
      "step 9500000 \t return [-10.7347975 -13.647034 ], ([6.402163   0.36949414]) \t loss 1.682E+00\n",
      "step 9510000 \t return [-12.694286 -13.1181  ], ([6.3294024  0.49293438]) \t loss 2.047E+00\n",
      "step 9520000 \t return [-14.288569 -12.868307], ([5.620895   0.51147074]) \t loss 2.324E+00\n",
      "step 9530000 \t return [-10.594368  -13.7349825], ([6.1513615  0.40507832]) \t loss 1.854E+00\n",
      "step 9540000 \t return [ -9.665707 -13.491526], ([5.4951444  0.55107105]) \t loss 2.373E+00\n",
      "step 9550000 \t return [-11.753449 -13.763485], ([5.1042356 0.5245732]) \t loss 1.632E+00\n",
      "step 9560000 \t return [-10.437272 -14.660757], ([4.2459106 0.5502487]) \t loss 2.993E+00\n",
      "step 9570000 \t return [ -9.912781 -13.546246], ([5.5231385  0.45080996]) \t loss 2.793E+00\n",
      "step 9580000 \t return [-17.22889  -12.553393], ([4.2145915 0.5703847]) \t loss 2.101E+00\n",
      "step 9590000 \t return [-11.569567 -13.258823], ([6.6271353  0.42615688]) \t loss 1.998E+00\n",
      "step 9600000 \t return [-12.353739 -13.696124], ([6.067393   0.39196154]) \t loss 2.090E+00\n",
      "step 9610000 \t return [-11.837746 -12.909635], ([5.702013  0.3540026]) \t loss 2.493E+00\n",
      "step 9620000 \t return [-13.276702 -12.821952], ([5.005921   0.40936893]) \t loss 3.157E+00\n",
      "step 9630000 \t return [-12.497131 -13.379799], ([5.6076536 0.6103704]) \t loss 2.628E+00\n",
      "step 9640000 \t return [-14.637006 -12.924479], ([4.65588   0.5297008]) \t loss 2.149E+00\n",
      "step 9650000 \t return [-11.583477 -13.113346], ([6.8497043  0.42916927]) \t loss 2.375E+00\n",
      "step 9660000 \t return [-15.226841 -12.578612], ([6.296724  0.5382889]) \t loss 2.324E+00\n",
      "step 9670000 \t return [-11.480059 -12.707654], ([6.022825  0.6186398]) \t loss 2.617E+00\n",
      "step 9680000 \t return [-15.8616495 -12.9870615], ([3.5349045  0.41348317]) \t loss 2.553E+00\n",
      "step 9690000 \t return [-13.990309 -12.894296], ([4.536587   0.46442708]) \t loss 1.964E+00\n",
      "step 9700000 \t return [-12.683902 -13.06197 ], ([5.9258223  0.54101455]) \t loss 1.871E+00\n",
      "step 9710000 \t return [-13.11995  -12.594477], ([5.4483547  0.52261907]) \t loss 1.416E+00\n",
      "step 9720000 \t return [-15.87191  -12.822226], ([3.4921389 0.5279819]) \t loss 2.107E+00\n",
      "step 9730000 \t return [-14.2703495 -12.908741 ], ([5.114658  0.5424074]) \t loss 2.543E+00\n",
      "step 9740000 \t return [-15.363309 -12.534781], ([4.7240634 0.4847352]) \t loss 1.821E+00\n",
      "step 9750000 \t return [-18.93435  -12.519515], ([2.9531415 0.5945704]) \t loss 1.870E+00\n",
      "step 9760000 \t return [-23.925066 -12.38464 ], ([2.544541   0.52157676]) \t loss 1.744E+00\n",
      "step 9770000 \t return [-23.302248 -12.631259], ([2.102782   0.49852735]) \t loss 2.181E+00\n",
      "step 9780000 \t return [-16.685003 -12.695687], ([3.5732188  0.55644625]) \t loss 1.851E+00\n",
      "step 9790000 \t return [-18.275393 -12.5445  ], ([4.161088  0.5591386]) \t loss 1.623E+00\n",
      "step 9800000 \t return [-24.185398 -12.437679], ([2.0804214 0.7147689]) \t loss 1.465E+00\n",
      "step 9810000 \t return [-20.050095 -12.563016], ([3.338849  0.5043429]) \t loss 1.829E+00\n",
      "step 9820000 \t return [-23.245922 -12.504748], ([1.7526934 0.674454 ]) \t loss 1.856E+00\n",
      "step 9830000 \t return [-14.535371 -12.644444], ([5.195452   0.48536646]) \t loss 1.363E+00\n",
      "step 9840000 \t return [-13.157314 -12.860592], ([4.5658126 0.5278474]) \t loss 2.466E+00\n",
      "step 9850000 \t return [-15.976967 -12.548711], ([3.9476483 0.5762712]) \t loss 1.607E+00\n",
      "step 9860000 \t return [-17.317568 -12.18209 ], ([5.104548   0.55521035]) \t loss 2.300E+00\n",
      "step 9870000 \t return [-10.9372225 -13.275402 ], ([6.5513186  0.52572346]) \t loss 1.221E+00\n",
      "step 9880000 \t return [-12.649245 -12.91307 ], ([3.5142124  0.51514465]) \t loss 2.227E+00\n",
      "step 9890000 \t return [-11.681571 -13.041853], ([5.662374  0.4974321]) \t loss 2.362E+00\n",
      "step 9900000 \t return [-11.562003 -13.027432], ([6.1538515 0.5117064]) \t loss 1.549E+00\n",
      "step 9910000 \t return [-13.0966   -12.950268], ([4.004879   0.56456894]) \t loss 2.091E+00\n",
      "step 9920000 \t return [-23.730402 -12.685194], ([1.7533134  0.82048506]) \t loss 1.354E+00\n",
      "step 9930000 \t return [-11.211291 -13.068337], ([7.1519403  0.34941378]) \t loss 1.767E+00\n",
      "step 9940000 \t return [-10.205208 -13.499845], ([6.534237  0.3781032]) \t loss 1.499E+00\n",
      "step 9950000 \t return [ -9.424215 -13.245245], ([6.683701   0.33038202]) \t loss 1.974E+00\n",
      "step 9960000 \t return [-11.791143 -12.784898], ([5.174928  0.4557125]) \t loss 2.880E+00\n",
      "step 9970000 \t return [-20.462458 -12.286931], ([3.4397733  0.57969433]) \t loss 1.971E+00\n",
      "step 9980000 \t return [-14.287531 -12.713534], ([4.0068583 0.5337819]) \t loss 2.038E+00\n",
      "step 9990000 \t return [-15.453665 -12.929788], ([3.386526  0.5890047]) \t loss 2.040E+00\n",
      "step 10000000 \t return [-14.956493 -12.707876], ([4.6433125  0.45751572]) \t loss 1.647E+00\n"
     ]
    }
   ],
   "source": [
    "env = mo_gym.make('water-reservoir-v0', normalized_action=False, nO=2, penalize=True, time_limit=100)\n",
    "\n",
    "env = ScaleReward(env)\n",
    "\n",
    "PCNAgent = PCN(env, np.array((0.01, 0.01, 0.1), dtype=np.float32), continuous_actions=True, noise=5.0)\n",
    "\n",
    "max_return = np.zeros(2)\n",
    "\n",
    "PCNAgent.train(10000000, env, np.array((-100.0,-100.0), dtype=np.float32), num_step_episodes=100,max_return=max_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_pf('./noise5_decay_smallScaling.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returns are slightly worse than when using no decay and higher scaling factor.\n",
    "\n",
    "Try to increase the scaling factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:qrs4zuft) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">water-reservoir-v0__PCN__None__1683891247</strong> at: <a href='https://wandb.ai/vub-ai/MORL-Baselines/runs/qrs4zuft' target=\"_blank\">https://wandb.ai/vub-ai/MORL-Baselines/runs/qrs4zuft</a><br/>Synced 7 W&B file(s), 100 media file(s), 100 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230512_133411-qrs4zuft\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:qrs4zuft). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fdb5d66a3b74cd2bc67033ba6e782d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01693333333338766, max=1.0)â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\liamm\\water-resource-management\\PCN\\wandb\\run-20230512_172500-pzpqeksf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/vub-ai/MORL-Baselines/runs/pzpqeksf' target=\"_blank\">water-reservoir-v0__PCN__None__1683905100</a></strong> to <a href='https://wandb.ai/vub-ai/MORL-Baselines' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/vub-ai/MORL-Baselines' target=\"_blank\">https://wandb.ai/vub-ai/MORL-Baselines</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/vub-ai/MORL-Baselines/runs/pzpqeksf' target=\"_blank\">https://wandb.ai/vub-ai/MORL-Baselines/runs/pzpqeksf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Found log directory outside of given root_logdir, dropping given root_logdir for event file in /tmp/PCN\n",
      "c:\\Users\\liamm\\anaconda3\\lib\\site-packages\\morl_baselines\\multi_policy\\pcn\\pcn.py:411: RuntimeWarning: overflow encountered in exp\n",
      "  ent = np.sum(-np.exp(lp) * lp)\n",
      "c:\\Users\\liamm\\anaconda3\\lib\\site-packages\\morl_baselines\\multi_policy\\pcn\\pcn.py:411: RuntimeWarning: overflow encountered in multiply\n",
      "  ent = np.sum(-np.exp(lp) * lp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 60000 \t return [-77.6146  -87.56922], ([0.5192871 0.6546343]) \t loss 9.284E+03\n",
      "step 70000 \t return [-79.19106 -89.12555], ([0.52777964 0.6466042 ]) \t loss 4.140E+03\n",
      "step 80000 \t return [-74.47269  -84.399765], ([0.48488185 0.65345955]) \t loss 3.139E+03\n",
      "step 90000 \t return [-74.3658  -84.34794], ([0.49908277 0.6473662 ]) \t loss 2.100E+03\n",
      "step 100000 \t return [-71.752556 -81.65216 ], ([0.45104003 0.6160512 ]) \t loss 1.039E+03\n",
      "step 110000 \t return [-69.598465 -79.51743 ], ([0.4084645 0.5454903]) \t loss 6.226E+02\n",
      "step 120000 \t return [-69.39671 -79.31772], ([0.3759389  0.48656884]) \t loss 4.678E+02\n",
      "step 130000 \t return [-69.71568 -79.66215], ([0.38566944 0.51001245]) \t loss 2.692E+02\n",
      "step 140000 \t return [-69.19676 -79.08874], ([0.36429018 0.5444002 ]) \t loss 1.739E+02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\liamm\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 150000 \t return [-69.14358  -79.078705], ([0.4138692  0.56714374]) \t loss 7.440E+01\n",
      "step 160000 \t return [-69.101036 -79.05263 ], ([0.29880813 0.42804167]) \t loss 5.974E+01\n",
      "step 170000 \t return [-68.29662 -78.26131], ([0.36749616 0.5238353 ]) \t loss 5.018E+01\n",
      "step 180000 \t return [-68.25035 -78.18594], ([0.29053345 0.4407057 ]) \t loss 4.537E+01\n",
      "step 190000 \t return [-67.23343 -77.1362 ], ([0.3269333  0.46278444]) \t loss 4.116E+01\n",
      "step 200000 \t return [-66.234085 -76.21499 ], ([0.3235398 0.4691497]) \t loss 5.451E+01\n",
      "step 210000 \t return [-67.78062 -77.70157], ([0.28520146 0.4232193 ]) \t loss 6.207E+01\n",
      "step 220000 \t return [-66.741936 -76.69667 ], ([0.2859972 0.4034878]) \t loss 4.402E+01\n",
      "step 230000 \t return [-66.68088 -76.59876], ([0.3151378  0.43456084]) \t loss 6.111E+01\n",
      "step 240000 \t return [-66.22589 -76.15383], ([0.3235377 0.4484442]) \t loss 4.325E+01\n",
      "step 250000 \t return [-66.70994 -76.64751], ([0.34231287 0.4435186 ]) \t loss 4.400E+01\n",
      "step 260000 \t return [-66.247475 -76.1615  ], ([0.26199877 0.32675704]) \t loss 3.579E+01\n",
      "step 270000 \t return [-66.39934 -76.31127], ([0.32474756 0.45871454]) \t loss 4.612E+01\n",
      "step 280000 \t return [-66.34993 -76.30019], ([0.28883737 0.4067961 ]) \t loss 3.884E+01\n",
      "step 290000 \t return [-66.003235 -75.90598 ], ([0.25813472 0.37697968]) \t loss 2.918E+01\n",
      "step 300000 \t return [-65.535    -75.469215], ([0.2809829 0.409607 ]) \t loss 3.484E+01\n",
      "step 310000 \t return [-65.107086 -75.000725], ([0.31930348 0.47667927]) \t loss 2.650E+01\n",
      "step 320000 \t return [-65.61387 -75.54318], ([0.25914317 0.39073744]) \t loss 2.524E+01\n",
      "step 330000 \t return [-65.44559 -75.41059], ([0.2648293 0.3791935]) \t loss 2.526E+01\n",
      "step 340000 \t return [-65.33531 -75.27573], ([0.26216218 0.36811185]) \t loss 2.630E+01\n",
      "step 350000 \t return [-64.61249 -74.51574], ([0.28489652 0.41931227]) \t loss 2.517E+01\n",
      "step 360000 \t return [-64.80437  -74.737366], ([0.26065716 0.3661076 ]) \t loss 2.576E+01\n",
      "step 370000 \t return [-64.614426 -74.56955 ], ([0.25938436 0.38294786]) \t loss 2.368E+01\n",
      "step 380000 \t return [-63.56871 -73.49537], ([0.24687096 0.34290197]) \t loss 2.337E+01\n",
      "step 390000 \t return [-63.37589  -73.336334], ([0.25762916 0.37631473]) \t loss 2.480E+01\n",
      "step 400000 \t return [-62.84751 -72.77767], ([0.2317309  0.33093154]) \t loss 2.474E+01\n",
      "step 410000 \t return [-62.383514 -72.25051 ], ([0.22647344 0.3488022 ]) \t loss 2.546E+01\n",
      "step 420000 \t return [-61.841034 -71.772415], ([0.25466445 0.40332606]) \t loss 2.314E+01\n",
      "step 430000 \t return [-61.53183 -71.47386], ([0.24905598 0.38381165]) \t loss 2.495E+01\n",
      "step 440000 \t return [-61.87631 -71.78232], ([0.24022703 0.343008  ]) \t loss 2.367E+01\n",
      "step 450000 \t return [-61.52365  -71.409515], ([0.23847197 0.33315933]) \t loss 2.473E+01\n",
      "step 460000 \t return [-61.352768 -71.286644], ([0.20447448 0.33455148]) \t loss 2.418E+01\n",
      "step 470000 \t return [-61.62916 -71.53569], ([0.2243873  0.34463474]) \t loss 2.399E+01\n",
      "step 480000 \t return [-60.82355 -70.70469], ([0.21407612 0.31842357]) \t loss 2.452E+01\n",
      "step 490000 \t return [-60.577805 -70.518524], ([0.19815283 0.3144888 ]) \t loss 2.581E+01\n",
      "step 500000 \t return [-59.68616 -69.5837 ], ([0.23910959 0.3571182 ]) \t loss 2.383E+01\n",
      "step 510000 \t return [-60.295578 -70.2168  ], ([0.20355082 0.34746844]) \t loss 2.677E+01\n",
      "step 520000 \t return [-59.67998 -69.6228 ], ([0.21291281 0.33200973]) \t loss 2.327E+01\n",
      "step 530000 \t return [-59.29186 -69.19008], ([0.2170276  0.36298892]) \t loss 2.508E+01\n",
      "step 540000 \t return [-59.438797 -69.34366 ], ([0.22801693 0.35653302]) \t loss 2.458E+01\n",
      "step 550000 \t return [-58.89413  -68.811615], ([0.21958172 0.3557536 ]) \t loss 2.520E+01\n",
      "step 560000 \t return [-59.04684 -68.94053], ([0.22396503 0.33518136]) \t loss 2.468E+01\n",
      "step 570000 \t return [-59.272552 -69.2186  ], ([0.22123072 0.33205312]) \t loss 2.433E+01\n",
      "step 580000 \t return [-58.889862 -68.81604 ], ([0.20975155 0.3185119 ]) \t loss 2.365E+01\n",
      "step 590000 \t return [-59.053486 -68.97523 ], ([0.19466439 0.30485198]) \t loss 2.418E+01\n",
      "step 600000 \t return [-58.099876 -67.993324], ([0.19799352 0.277621  ]) \t loss 2.330E+01\n",
      "step 610000 \t return [-58.103603 -68.00429 ], ([0.23050214 0.35272336]) \t loss 2.587E+01\n",
      "step 620000 \t return [-57.65836 -67.554  ], ([0.18778273 0.32747114]) \t loss 2.441E+01\n",
      "step 630000 \t return [-57.934566 -67.80874 ], ([0.23227711 0.32519144]) \t loss 2.431E+01\n",
      "step 640000 \t return [-57.491455 -67.39054 ], ([0.19425212 0.32781464]) \t loss 2.506E+01\n",
      "step 650000 \t return [-57.341816 -67.24416 ], ([0.20999184 0.29583198]) \t loss 2.381E+01\n",
      "step 660000 \t return [-57.271973 -67.16614 ], ([0.19120221 0.28912264]) \t loss 2.344E+01\n",
      "step 670000 \t return [-57.864   -67.78526], ([0.1944625  0.30622104]) \t loss 2.417E+01\n",
      "step 680000 \t return [-57.32758 -67.21   ], ([0.19712375 0.29959258]) \t loss 2.372E+01\n",
      "step 690000 \t return [-56.90313 -66.76798], ([0.19748597 0.31733233]) \t loss 2.451E+01\n",
      "step 700000 \t return [-56.669174 -66.58128 ], ([0.20152216 0.32391688]) \t loss 2.386E+01\n",
      "step 710000 \t return [-57.195473 -67.10656 ], ([0.1786313 0.3193112]) \t loss 2.450E+01\n",
      "step 720000 \t return [-56.372757 -66.28629 ], ([0.21578392 0.3459178 ]) \t loss 2.422E+01\n",
      "step 730000 \t return [-56.491768 -66.42559 ], ([0.20819883 0.30042866]) \t loss 2.438E+01\n",
      "step 740000 \t return [-56.19771 -66.0786 ], ([0.17506497 0.25231534]) \t loss 2.326E+01\n",
      "step 750000 \t return [-56.353043 -66.23524 ], ([0.19247253 0.31647235]) \t loss 2.247E+01\n",
      "step 760000 \t return [-55.86741 -65.78212], ([0.17465071 0.2745947 ]) \t loss 2.312E+01\n",
      "step 770000 \t return [-55.971153 -65.852   ], ([0.19814563 0.30992827]) \t loss 2.772E+01\n",
      "step 780000 \t return [-55.50185 -65.40099], ([0.1851504  0.27862164]) \t loss 2.322E+01\n",
      "step 790000 \t return [-55.703724 -65.607376], ([0.18265073 0.29421163]) \t loss 2.297E+01\n",
      "step 800000 \t return [-55.45598  -65.386765], ([0.17013949 0.27112326]) \t loss 2.295E+01\n",
      "step 810000 \t return [-55.569443 -65.48934 ], ([0.17170194 0.2912104 ]) \t loss 2.230E+01\n",
      "step 820000 \t return [-55.346767 -65.284355], ([0.17261975 0.28925732]) \t loss 2.293E+01\n",
      "step 830000 \t return [-55.271774 -65.17117 ], ([0.18458547 0.28773803]) \t loss 2.443E+01\n",
      "step 840000 \t return [-54.81173 -64.7357 ], ([0.19915725 0.3142961 ]) \t loss 2.565E+01\n",
      "step 850000 \t return [-54.6868   -64.618645], ([0.18045986 0.29143864]) \t loss 2.259E+01\n",
      "step 860000 \t return [-54.34401 -64.23524], ([0.1859976  0.30899674]) \t loss 2.208E+01\n",
      "step 870000 \t return [-54.025047 -63.959522], ([0.17889374 0.28998324]) \t loss 2.268E+01\n",
      "step 880000 \t return [-53.694405 -63.597584], ([0.18254958 0.30690452]) \t loss 2.139E+01\n",
      "step 890000 \t return [-54.19458 -64.12327], ([0.18720107 0.27737084]) \t loss 2.448E+01\n",
      "step 900000 \t return [-53.646954 -63.543427], ([0.18424577 0.27530137]) \t loss 2.228E+01\n",
      "step 910000 \t return [-53.35337 -63.27985], ([0.16393782 0.28639582]) \t loss 2.157E+01\n",
      "step 920000 \t return [-53.740875 -63.66377 ], ([0.18015854 0.24004447]) \t loss 2.415E+01\n",
      "step 930000 \t return [-53.663395 -63.576904], ([0.19498782 0.28499314]) \t loss 2.207E+01\n",
      "step 940000 \t return [-53.357784 -63.28679 ], ([0.17522986 0.2724065 ]) \t loss 2.300E+01\n",
      "step 950000 \t return [-53.68562 -63.59134], ([0.17702214 0.22100653]) \t loss 2.267E+01\n",
      "step 960000 \t return [-53.60601 -63.58477], ([0.18225563 0.24407417]) \t loss 2.358E+01\n",
      "step 970000 \t return [-53.42798  -63.349834], ([0.17994647 0.27298766]) \t loss 2.398E+01\n",
      "step 980000 \t return [-53.087387 -63.01772 ], ([0.18154041 0.2559972 ]) \t loss 2.228E+01\n",
      "step 990000 \t return [-52.839012 -62.752064], ([0.15384804 0.2629865 ]) \t loss 2.190E+01\n",
      "step 1000000 \t return [-53.11068 -62.99663], ([0.18560253 0.27304634]) \t loss 2.250E+01\n",
      "step 1010000 \t return [-52.316097 -62.240116], ([0.1943921  0.28051475]) \t loss 2.311E+01\n",
      "step 1020000 \t return [-52.76922  -62.685284], ([0.18601924 0.2776215 ]) \t loss 2.111E+01\n",
      "step 1030000 \t return [-52.587925 -62.485455], ([0.17537898 0.27074158]) \t loss 2.389E+01\n",
      "step 1040000 \t return [-52.687378 -62.60749 ], ([0.16016102 0.23726034]) \t loss 2.107E+01\n",
      "step 1050000 \t return [-52.559887 -62.43746 ], ([0.1723438  0.25950086]) \t loss 2.250E+01\n",
      "step 1060000 \t return [-52.325977 -62.229454], ([0.1834873 0.2768867]) \t loss 2.219E+01\n",
      "step 1070000 \t return [-52.596615 -62.524296], ([0.19433054 0.2741209 ]) \t loss 2.278E+01\n",
      "step 1080000 \t return [-52.429718 -62.337643], ([0.16014144 0.26018506]) \t loss 2.272E+01\n",
      "step 1090000 \t return [-52.13259  -62.114357], ([0.1573687  0.25934368]) \t loss 2.397E+01\n",
      "step 1100000 \t return [-52.332165 -62.29019 ], ([0.14575681 0.26913527]) \t loss 2.221E+01\n",
      "step 1110000 \t return [-51.78584  -61.611916], ([0.16198102 0.23027883]) \t loss 2.268E+01\n",
      "step 1120000 \t return [-51.65841 -61.60515], ([0.14304887 0.2382008 ]) \t loss 2.330E+01\n",
      "step 1130000 \t return [-51.70133  -61.630157], ([0.16847739 0.22179988]) \t loss 2.179E+01\n",
      "step 1140000 \t return [-51.976494 -61.845314], ([0.18686423 0.2629197 ]) \t loss 2.294E+01\n",
      "step 1150000 \t return [-52.0595  -61.98668], ([0.17998183 0.271231  ]) \t loss 2.216E+01\n",
      "step 1160000 \t return [-51.874496 -61.786495], ([0.1644213  0.26408488]) \t loss 2.119E+01\n",
      "step 1170000 \t return [-51.841324 -61.765263], ([0.16671602 0.24955179]) \t loss 2.246E+01\n",
      "step 1180000 \t return [-51.528885 -61.433895], ([0.15147287 0.24651435]) \t loss 2.108E+01\n",
      "step 1190000 \t return [-51.479443 -61.419   ], ([0.17168622 0.26433975]) \t loss 2.109E+01\n",
      "step 1200000 \t return [-51.855125 -61.75126 ], ([0.18581803 0.23436914]) \t loss 2.230E+01\n",
      "step 1210000 \t return [-51.52257 -61.4304 ], ([0.14068957 0.2227763 ]) \t loss 2.175E+01\n",
      "step 1220000 \t return [-52.02157 -61.93142], ([0.17922223 0.25601622]) \t loss 2.128E+01\n",
      "step 1230000 \t return [-51.286556 -61.170055], ([0.17020404 0.2644503 ]) \t loss 2.178E+01\n",
      "step 1240000 \t return [-51.401188 -61.250275], ([0.14490093 0.27630907]) \t loss 2.128E+01\n",
      "step 1250000 \t return [-51.32515 -61.24263], ([0.13919915 0.257436  ]) \t loss 2.155E+01\n",
      "step 1260000 \t return [-50.84101 -60.73366], ([0.15508507 0.25903994]) \t loss 2.133E+01\n",
      "step 1270000 \t return [-50.60531  -60.447266], ([0.15436983 0.21026662]) \t loss 2.028E+01\n",
      "step 1280000 \t return [-50.580517 -60.468033], ([0.16007751 0.2712462 ]) \t loss 2.135E+01\n",
      "step 1290000 \t return [-50.099754 -59.978798], ([0.1593449  0.23642035]) \t loss 2.150E+01\n",
      "step 1300000 \t return [-50.10331  -60.000767], ([0.17077346 0.21534844]) \t loss 2.274E+01\n",
      "step 1310000 \t return [-49.72673  -59.653614], ([0.14036316 0.23393439]) \t loss 2.095E+01\n",
      "step 1320000 \t return [-49.869904 -59.747276], ([0.18898489 0.24705921]) \t loss 2.191E+01\n",
      "step 1330000 \t return [-49.629898 -59.52462 ], ([0.1563981  0.23634219]) \t loss 2.072E+01\n",
      "step 1340000 \t return [-49.71876 -59.63288], ([0.15500456 0.25170186]) \t loss 2.175E+01\n",
      "step 1350000 \t return [-49.46038  -59.366776], ([0.14131914 0.22242458]) \t loss 2.055E+01\n",
      "step 1360000 \t return [-49.300247 -59.20925 ], ([0.14640993 0.24788916]) \t loss 2.058E+01\n",
      "step 1370000 \t return [-49.310753 -59.168137], ([0.17004144 0.2431296 ]) \t loss 2.125E+01\n",
      "step 1380000 \t return [-49.525738 -59.403633], ([0.15115611 0.22006176]) \t loss 2.036E+01\n",
      "step 1390000 \t return [-49.27214  -59.206776], ([0.17702214 0.27044436]) \t loss 2.093E+01\n",
      "step 1400000 \t return [-48.878387 -58.781864], ([0.15904915 0.2276261 ]) \t loss 2.028E+01\n",
      "step 1410000 \t return [-48.87329 -58.79138], ([0.15494567 0.25643826]) \t loss 2.244E+01\n",
      "step 1420000 \t return [-48.8765   -58.812252], ([0.17360596 0.2522634 ]) \t loss 2.088E+01\n",
      "step 1430000 \t return [-48.54414  -58.439316], ([0.14137253 0.22851872]) \t loss 2.109E+01\n",
      "step 1440000 \t return [-48.849174 -58.79652 ], ([0.15420303 0.23375463]) \t loss 2.129E+01\n",
      "step 1450000 \t return [-48.706642 -58.584248], ([0.17439073 0.24763694]) \t loss 2.218E+01\n",
      "step 1460000 \t return [-48.454647 -58.31316 ], ([0.15614013 0.2263631 ]) \t loss 2.209E+01\n",
      "step 1470000 \t return [-48.512627 -58.421017], ([0.16841218 0.24547733]) \t loss 2.049E+01\n",
      "step 1480000 \t return [-48.283607 -58.188976], ([0.15109569 0.21844013]) \t loss 2.075E+01\n",
      "step 1490000 \t return [-48.20127  -58.045723], ([0.18400948 0.23498425]) \t loss 2.051E+01\n",
      "step 1500000 \t return [-47.793457 -57.72763 ], ([0.14903763 0.24078016]) \t loss 1.992E+01\n",
      "step 1510000 \t return [-47.659664 -57.59517 ], ([0.14529052 0.24270722]) \t loss 2.048E+01\n",
      "step 1520000 \t return [-47.96538  -57.862972], ([0.17387828 0.22708711]) \t loss 1.934E+01\n",
      "step 1530000 \t return [-47.45687  -57.325314], ([0.16103423 0.25783288]) \t loss 2.009E+01\n",
      "step 1540000 \t return [-47.45804 -57.36418], ([0.1622996  0.21465513]) \t loss 2.060E+01\n",
      "step 1550000 \t return [-47.48662  -57.382324], ([0.14385839 0.2032016 ]) \t loss 2.018E+01\n",
      "step 1560000 \t return [-47.482662 -57.38695 ], ([0.15961073 0.23193407]) \t loss 1.887E+01\n",
      "step 1570000 \t return [-47.157787 -57.09068 ], ([0.12565486 0.23066157]) \t loss 2.034E+01\n",
      "step 1580000 \t return [-46.98252  -56.879307], ([0.1331297  0.21431406]) \t loss 2.031E+01\n",
      "step 1590000 \t return [-47.190136 -57.064354], ([0.16345991 0.21485555]) \t loss 2.040E+01\n",
      "step 1600000 \t return [-47.15959  -57.069725], ([0.16619106 0.2084517 ]) \t loss 2.076E+01\n",
      "step 1610000 \t return [-46.86258  -56.756447], ([0.15967415 0.23187469]) \t loss 1.981E+01\n",
      "step 1620000 \t return [-47.00391 -56.90997], ([0.15368344 0.22986956]) \t loss 2.181E+01\n",
      "step 1630000 \t return [-46.5847   -56.519455], ([0.13624746 0.22984932]) \t loss 1.876E+01\n",
      "step 1640000 \t return [-46.415966 -56.32485 ], ([0.14539844 0.22000073]) \t loss 1.977E+01\n",
      "step 1650000 \t return [-46.317207 -56.210205], ([0.14319271 0.25105527]) \t loss 2.041E+01\n",
      "step 1660000 \t return [-45.752422 -55.58953 ], ([0.16952352 0.26062512]) \t loss 2.021E+01\n",
      "step 1670000 \t return [-45.820362 -55.710403], ([0.16444865 0.2436496 ]) \t loss 2.060E+01\n",
      "step 1680000 \t return [-45.5615   -55.466496], ([0.17281091 0.2517898 ]) \t loss 1.859E+01\n",
      "step 1690000 \t return [-45.473213 -55.314106], ([0.13128479 0.21858056]) \t loss 1.963E+01\n",
      "step 1700000 \t return [-45.080376 -54.99632 ], ([0.14362182 0.22622856]) \t loss 1.785E+01\n",
      "step 1710000 \t return [-45.13921  -55.060043], ([0.16014574 0.25650185]) \t loss 1.890E+01\n",
      "step 1720000 \t return [-45.275497 -55.14586 ], ([0.16739742 0.21452068]) \t loss 1.926E+01\n",
      "step 1730000 \t return [-45.109676 -54.955685], ([0.17043401 0.22888085]) \t loss 2.107E+01\n",
      "step 1740000 \t return [-45.357662 -55.272335], ([0.16167685 0.19264504]) \t loss 1.963E+01\n",
      "step 1750000 \t return [-44.851612 -54.737446], ([0.1526591  0.23352371]) \t loss 2.104E+01\n",
      "step 1760000 \t return [-44.687637 -54.610523], ([0.14015406 0.2279454 ]) \t loss 1.941E+01\n",
      "step 1770000 \t return [-44.469547 -54.388615], ([0.16515468 0.21813938]) \t loss 1.968E+01\n",
      "step 1780000 \t return [-44.772392 -54.639793], ([0.17917101 0.20036532]) \t loss 1.928E+01\n",
      "step 1790000 \t return [-44.11347  -53.988705], ([0.14412007 0.22135302]) \t loss 1.876E+01\n",
      "step 1800000 \t return [-43.66365 -53.58049], ([0.1284652  0.22914076]) \t loss 2.118E+01\n",
      "step 1810000 \t return [-44.007973 -53.879288], ([0.16836056 0.23163098]) \t loss 1.959E+01\n",
      "step 1820000 \t return [-43.530502 -53.39375 ], ([0.159597   0.23029006]) \t loss 1.951E+01\n",
      "step 1830000 \t return [-43.30622  -53.166016], ([0.13473786 0.24088529]) \t loss 1.939E+01\n",
      "step 1840000 \t return [-43.3054   -53.204514], ([0.1332376  0.25852337]) \t loss 1.942E+01\n",
      "step 1850000 \t return [-42.899696 -52.787453], ([0.14697686 0.23215483]) \t loss 1.847E+01\n",
      "step 1860000 \t return [-42.712738 -52.58698 ], ([0.14585142 0.25534156]) \t loss 1.905E+01\n",
      "step 1870000 \t return [-42.597816 -52.54462 ], ([0.14824826 0.22893456]) \t loss 1.863E+01\n",
      "step 1880000 \t return [-42.56611 -52.39779], ([0.18702462 0.22613278]) \t loss 1.887E+01\n",
      "step 1890000 \t return [-42.524776 -52.40341 ], ([0.16506566 0.21728264]) \t loss 1.668E+01\n",
      "step 1900000 \t return [-42.50645  -52.434032], ([0.15534014 0.21868514]) \t loss 1.878E+01\n",
      "step 1910000 \t return [-42.42226  -52.310604], ([0.17057517 0.19942774]) \t loss 1.960E+01\n",
      "step 1920000 \t return [-42.477528 -52.357067], ([0.11945892 0.2338024 ]) \t loss 1.961E+01\n",
      "step 1930000 \t return [-42.184437 -52.098568], ([0.15668544 0.19947116]) \t loss 1.916E+01\n",
      "step 1940000 \t return [-41.821365 -51.766396], ([0.15813366 0.22323076]) \t loss 1.842E+01\n",
      "step 1950000 \t return [-41.920044 -51.809193], ([0.1542084  0.24231653]) \t loss 1.843E+01\n",
      "step 1960000 \t return [-41.544823 -51.436188], ([0.15047526 0.21967115]) \t loss 1.842E+01\n",
      "step 1970000 \t return [-41.60038  -51.469692], ([0.15334691 0.22844255]) \t loss 1.939E+01\n",
      "step 1980000 \t return [-41.497284 -51.377056], ([0.14320092 0.20728803]) \t loss 1.836E+01\n",
      "step 1990000 \t return [-41.22422  -51.108784], ([0.15668404 0.22629508]) \t loss 1.845E+01\n",
      "step 2000000 \t return [-41.051147 -50.903404], ([0.17109336 0.22219247]) \t loss 1.826E+01\n",
      "step 2010000 \t return [-40.887672 -50.747215], ([0.18434416 0.21533191]) \t loss 1.919E+01\n",
      "step 2020000 \t return [-40.649    -50.551178], ([0.13184637 0.19522497]) \t loss 1.845E+01\n",
      "step 2030000 \t return [-40.49403  -50.358746], ([0.13836162 0.21936482]) \t loss 1.784E+01\n",
      "step 2040000 \t return [-40.28812  -50.217007], ([0.15730913 0.19522655]) \t loss 1.781E+01\n",
      "step 2050000 \t return [-40.069496 -49.967846], ([0.1494201  0.20702228]) \t loss 1.789E+01\n",
      "step 2060000 \t return [-40.23824  -50.102135], ([0.15613694 0.23431969]) \t loss 1.713E+01\n",
      "step 2070000 \t return [-40.2733   -50.126484], ([0.20805787 0.23213659]) \t loss 1.850E+01\n",
      "step 2080000 \t return [-40.106762 -49.992607], ([0.14885856 0.22557375]) \t loss 1.828E+01\n",
      "step 2090000 \t return [-40.15062 -50.05377], ([0.1617158  0.23610547]) \t loss 1.783E+01\n",
      "step 2100000 \t return [-39.927124 -49.819424], ([0.14645372 0.20941116]) \t loss 1.715E+01\n",
      "step 2110000 \t return [-40.069424 -49.95246 ], ([0.1517735  0.20044044]) \t loss 1.720E+01\n",
      "step 2120000 \t return [-39.77139  -49.606636], ([0.15392148 0.21523593]) \t loss 1.852E+01\n",
      "step 2130000 \t return [-39.610596 -49.473812], ([0.1457857 0.2050626]) \t loss 1.874E+01\n",
      "step 2140000 \t return [-40.05634 -49.92977], ([0.19098555 0.19385111]) \t loss 1.833E+01\n",
      "step 2150000 \t return [-39.51852 -49.42273], ([0.13152105 0.17730077]) \t loss 2.008E+01\n",
      "step 2160000 \t return [-39.438446 -49.34821 ], ([0.15341766 0.19154625]) \t loss 1.682E+01\n",
      "step 2170000 \t return [-39.83515  -49.722076], ([0.15386    0.20248248]) \t loss 1.837E+01\n",
      "step 2180000 \t return [-39.349125 -49.252216], ([0.18075874 0.21478337]) \t loss 1.870E+01\n",
      "step 2190000 \t return [-39.13143  -49.052433], ([0.13861819 0.20595184]) \t loss 1.891E+01\n",
      "step 2200000 \t return [-39.031208 -48.906963], ([0.16710381 0.19157071]) \t loss 1.760E+01\n",
      "step 2210000 \t return [-38.919407 -48.822872], ([0.13628802 0.23158963]) \t loss 1.815E+01\n",
      "step 2220000 \t return [-38.85159  -48.756134], ([0.14774673 0.20130579]) \t loss 1.816E+01\n",
      "step 2230000 \t return [-38.93289  -48.821674], ([0.13925157 0.21190332]) \t loss 1.750E+01\n",
      "step 2240000 \t return [-38.659706 -48.512962], ([0.1285532  0.18898074]) \t loss 1.743E+01\n",
      "step 2250000 \t return [-38.49387  -48.357033], ([0.15312256 0.20143934]) \t loss 1.698E+01\n",
      "step 2260000 \t return [-38.7363  -48.64232], ([0.1679315  0.19621795]) \t loss 1.836E+01\n",
      "step 2270000 \t return [-38.44959  -48.390526], ([0.15998907 0.19683269]) \t loss 1.914E+01\n",
      "step 2280000 \t return [-38.211548 -48.147614], ([0.12341342 0.21862073]) \t loss 1.796E+01\n",
      "step 2290000 \t return [-37.992573 -47.890976], ([0.13420556 0.21588232]) \t loss 1.785E+01\n",
      "step 2300000 \t return [-37.97625 -47.86075], ([0.17759761 0.17061454]) \t loss 1.831E+01\n",
      "step 2310000 \t return [-38.065277 -47.97857 ], ([0.16854426 0.18016602]) \t loss 1.818E+01\n",
      "step 2320000 \t return [-37.914703 -47.794434], ([0.16890828 0.20720245]) \t loss 1.741E+01\n",
      "step 2330000 \t return [-37.70437  -47.592247], ([0.14548506 0.1698969 ]) \t loss 1.733E+01\n",
      "step 2340000 \t return [-37.83145 -47.72998], ([0.17590533 0.20131595]) \t loss 1.726E+01\n",
      "step 2350000 \t return [-37.56097  -47.504898], ([0.11594968 0.21548758]) \t loss 1.756E+01\n",
      "step 2360000 \t return [-37.45517  -47.356537], ([0.12125156 0.19326085]) \t loss 1.787E+01\n",
      "step 2370000 \t return [-37.62149  -47.525463], ([0.19007897 0.18124755]) \t loss 1.758E+01\n",
      "step 2380000 \t return [-37.54439  -47.408726], ([0.17726433 0.17583363]) \t loss 1.682E+01\n",
      "step 2390000 \t return [-37.22463 -47.13333], ([0.1378331 0.2033906]) \t loss 1.805E+01\n",
      "step 2400000 \t return [-37.06156  -46.985424], ([0.14871362 0.20496981]) \t loss 1.549E+01\n",
      "step 2410000 \t return [-36.909443 -46.81208 ], ([0.15378581 0.20422894]) \t loss 1.681E+01\n",
      "step 2420000 \t return [-36.701256 -46.617123], ([0.1824213  0.19305122]) \t loss 1.684E+01\n",
      "step 2430000 \t return [-36.538822 -46.451427], ([0.1514236  0.20815444]) \t loss 1.749E+01\n",
      "step 2440000 \t return [-36.621037 -46.48374 ], ([0.14807141 0.2290865 ]) \t loss 1.786E+01\n",
      "step 2450000 \t return [-36.58935  -46.516403], ([0.12035554 0.22030117]) \t loss 1.627E+01\n",
      "step 2460000 \t return [-36.420235 -46.29993 ], ([0.18556169 0.21180773]) \t loss 1.660E+01\n",
      "step 2470000 \t return [-36.390705 -46.339832], ([0.17438713 0.17152719]) \t loss 1.659E+01\n",
      "step 2480000 \t return [-36.2848  -46.15416], ([0.11558818 0.2141704 ]) \t loss 1.696E+01\n",
      "step 2490000 \t return [-36.25112  -46.145355], ([0.17071052 0.20155522]) \t loss 1.687E+01\n",
      "step 2500000 \t return [-36.00564  -45.925182], ([0.16249865 0.20658037]) \t loss 1.687E+01\n",
      "step 2510000 \t return [-35.840908 -45.728344], ([0.14533591 0.23696606]) \t loss 1.740E+01\n",
      "step 2520000 \t return [-35.692093 -45.55817 ], ([0.1363559 0.1733522]) \t loss 1.782E+01\n",
      "step 2530000 \t return [-35.697376 -45.601986], ([0.1716084  0.19694868]) \t loss 1.736E+01\n",
      "step 2540000 \t return [-35.57012 -45.47499], ([0.1842398  0.17685804]) \t loss 1.531E+01\n",
      "step 2550000 \t return [-35.255386 -45.137173], ([0.13631506 0.18287052]) \t loss 1.634E+01\n",
      "step 2560000 \t return [-34.987736 -44.940132], ([0.13735868 0.21830684]) \t loss 1.669E+01\n",
      "step 2570000 \t return [-34.791893 -44.70044 ], ([0.13780056 0.22356069]) \t loss 1.701E+01\n",
      "step 2580000 \t return [-34.670895 -44.562794], ([0.11370189 0.21359952]) \t loss 1.631E+01\n",
      "step 2590000 \t return [-34.563644 -44.47648 ], ([0.12959778 0.19456883]) \t loss 1.724E+01\n",
      "step 2600000 \t return [-34.484074 -44.304794], ([0.12672739 0.21156207]) \t loss 1.665E+01\n",
      "step 2610000 \t return [-34.23725 -44.18113], ([0.14671905 0.21600959]) \t loss 1.605E+01\n",
      "step 2620000 \t return [-34.252678 -44.12643 ], ([0.17103897 0.17780349]) \t loss 1.601E+01\n",
      "step 2630000 \t return [-34.103756 -44.02275 ], ([0.11951244 0.2425854 ]) \t loss 1.600E+01\n",
      "step 2640000 \t return [-34.066544 -43.990723], ([0.14777105 0.207963  ]) \t loss 1.662E+01\n",
      "step 2650000 \t return [-33.87883  -43.735764], ([0.13204107 0.2153274 ]) \t loss 1.624E+01\n",
      "step 2660000 \t return [-33.637802 -43.550777], ([0.13070007 0.20160069]) \t loss 1.693E+01\n",
      "step 2670000 \t return [-33.72087 -43.61923], ([0.19246969 0.19873455]) \t loss 1.604E+01\n",
      "step 2680000 \t return [-33.57682 -43.50005], ([0.12404034 0.16604206]) \t loss 1.647E+01\n",
      "step 2690000 \t return [-33.450222 -43.392033], ([0.13383545 0.19520813]) \t loss 1.632E+01\n",
      "step 2700000 \t return [-33.451004 -43.359352], ([0.1476196  0.20503958]) \t loss 1.718E+01\n",
      "step 2710000 \t return [-33.60081 -43.48931], ([0.12915958 0.2009222 ]) \t loss 1.610E+01\n",
      "step 2720000 \t return [-33.13593 -43.01353], ([0.17223112 0.18228528]) \t loss 1.556E+01\n",
      "step 2730000 \t return [-33.07045 -42.95105], ([0.16010092 0.19804543]) \t loss 1.714E+01\n",
      "step 2740000 \t return [-32.768593 -42.713356], ([0.13677044 0.19613767]) \t loss 1.679E+01\n",
      "step 2750000 \t return [-32.698357 -42.611828], ([0.14751284 0.1853167 ]) \t loss 1.760E+01\n",
      "step 2760000 \t return [-32.60924  -42.525303], ([0.1373526  0.22157776]) \t loss 1.549E+01\n",
      "step 2770000 \t return [-32.492336 -42.38686 ], ([0.1204167  0.19596983]) \t loss 1.608E+01\n",
      "step 2780000 \t return [-32.375965 -42.266834], ([0.13215509 0.20725897]) \t loss 1.661E+01\n",
      "step 2790000 \t return [-32.27679 -42.16429], ([0.14027329 0.23184668]) \t loss 1.586E+01\n",
      "step 2800000 \t return [-32.201115 -42.10666 ], ([0.18054946 0.16460219]) \t loss 1.520E+01\n",
      "step 2810000 \t return [-32.262257 -42.169838], ([0.17812656 0.21935558]) \t loss 1.628E+01\n",
      "step 2820000 \t return [-32.224052 -42.109116], ([0.1709617  0.19201691]) \t loss 1.655E+01\n",
      "step 2830000 \t return [-32.131786 -42.03129 ], ([0.17191462 0.16736446]) \t loss 1.588E+01\n",
      "step 2840000 \t return [-31.8548  -41.74919], ([0.13766521 0.19141792]) \t loss 1.569E+01\n",
      "step 2850000 \t return [-31.80711 -41.72318], ([0.16412063 0.21363422]) \t loss 1.765E+01\n",
      "step 2860000 \t return [-31.611412 -41.508614], ([0.14602362 0.20395465]) \t loss 1.681E+01\n",
      "step 2870000 \t return [-31.578485 -41.467876], ([0.23304184 0.18746805]) \t loss 1.602E+01\n",
      "step 2880000 \t return [-31.34278 -41.23246], ([0.15223    0.18738821]) \t loss 1.607E+01\n",
      "step 2890000 \t return [-31.187468 -41.08552 ], ([0.13752753 0.19983111]) \t loss 1.550E+01\n",
      "step 2900000 \t return [-31.086172 -40.942936], ([0.16436914 0.18931133]) \t loss 1.535E+01\n",
      "step 2910000 \t return [-30.871061 -40.765797], ([0.13611002 0.2131471 ]) \t loss 1.464E+01\n",
      "step 2920000 \t return [-30.866188 -40.774715], ([0.18530403 0.2253084 ]) \t loss 1.418E+01\n",
      "step 2930000 \t return [-30.731178 -40.596558], ([0.11924322 0.21021165]) \t loss 1.475E+01\n",
      "step 2940000 \t return [-30.797697 -40.66806 ], ([0.17095117 0.20701581]) \t loss 1.584E+01\n",
      "step 2950000 \t return [-30.767622 -40.645275], ([0.16037616 0.21495959]) \t loss 1.539E+01\n",
      "step 2960000 \t return [-30.574013 -40.434235], ([0.16175812 0.18830359]) \t loss 1.624E+01\n",
      "step 2970000 \t return [-30.451225 -40.3395  ], ([0.12475286 0.23180783]) \t loss 1.635E+01\n",
      "step 2980000 \t return [-30.473694 -40.34833 ], ([0.1868649  0.18595901]) \t loss 1.596E+01\n",
      "step 2990000 \t return [-30.267326 -40.196476], ([0.10281213 0.23141104]) \t loss 1.525E+01\n",
      "step 3000000 \t return [-30.133404 -39.99894 ], ([0.13053815 0.20517787]) \t loss 1.492E+01\n",
      "step 3010000 \t return [-30.379217 -40.32222 ], ([0.1636612 0.1973902]) \t loss 1.463E+01\n",
      "step 3020000 \t return [-30.14833  -40.064335], ([0.17234279 0.22242759]) \t loss 1.541E+01\n",
      "step 3030000 \t return [-30.0178   -39.880486], ([0.15194717 0.19961876]) \t loss 1.523E+01\n",
      "step 3040000 \t return [-30.03657 -39.88547], ([0.12006727 0.20956823]) \t loss 1.404E+01\n",
      "step 3050000 \t return [-29.9724   -39.858727], ([0.12774266 0.19966736]) \t loss 1.398E+01\n",
      "step 3060000 \t return [-29.982895 -39.888634], ([0.1120657  0.21162672]) \t loss 1.526E+01\n",
      "step 3070000 \t return [-29.908852 -39.77372 ], ([0.14937867 0.20846798]) \t loss 1.413E+01\n",
      "step 3080000 \t return [-29.91516  -39.795666], ([0.13688888 0.20779192]) \t loss 1.469E+01\n",
      "step 3090000 \t return [-29.882624 -39.797607], ([0.12658675 0.20167235]) \t loss 1.505E+01\n",
      "step 3100000 \t return [-29.94098  -39.801334], ([0.09498512 0.21866234]) \t loss 1.551E+01\n",
      "step 3110000 \t return [-29.654068 -39.54827 ], ([0.14484288 0.20999162]) \t loss 1.432E+01\n",
      "step 3120000 \t return [-29.834078 -39.743923], ([0.22006112 0.17206185]) \t loss 1.493E+01\n",
      "step 3130000 \t return [-29.509527 -39.415287], ([0.11256909 0.22611366]) \t loss 1.500E+01\n",
      "step 3140000 \t return [-29.397882 -39.29698 ], ([0.13131769 0.20493971]) \t loss 1.442E+01\n",
      "step 3150000 \t return [-29.327515 -39.252514], ([0.09144509 0.21435508]) \t loss 1.527E+01\n",
      "step 3160000 \t return [-29.353828 -39.266056], ([0.09217655 0.20485662]) \t loss 1.551E+01\n",
      "step 3170000 \t return [-29.267038 -39.15821 ], ([0.15375854 0.1807197 ]) \t loss 1.454E+01\n",
      "step 3180000 \t return [-29.083763 -38.996166], ([0.11947921 0.19334571]) \t loss 1.461E+01\n",
      "step 3190000 \t return [-29.01564  -38.945545], ([0.096922   0.21002539]) \t loss 1.481E+01\n",
      "step 3200000 \t return [-28.992212 -38.86    ], ([0.13107672 0.18897639]) \t loss 1.468E+01\n",
      "step 3210000 \t return [-28.89931  -38.811684], ([0.15390709 0.19454814]) \t loss 1.452E+01\n",
      "step 3220000 \t return [-29.090403 -38.95728 ], ([0.19617476 0.16188926]) \t loss 1.440E+01\n",
      "step 3230000 \t return [-28.852007 -38.754284], ([0.1339224  0.20793402]) \t loss 1.485E+01\n",
      "step 3240000 \t return [-28.769888 -38.64733 ], ([0.09270241 0.20023394]) \t loss 1.460E+01\n",
      "step 3250000 \t return [-28.805017 -38.727367], ([0.09535696 0.21796148]) \t loss 1.410E+01\n",
      "step 3260000 \t return [-28.750881 -38.639038], ([0.06739346 0.19594212]) \t loss 1.463E+01\n",
      "step 3270000 \t return [-28.61818  -38.523594], ([0.16593133 0.16618727]) \t loss 1.417E+01\n",
      "step 3280000 \t return [-28.508354 -38.452713], ([0.12484241 0.18767913]) \t loss 1.415E+01\n",
      "step 3290000 \t return [-28.483093 -38.425648], ([0.12644725 0.18977663]) \t loss 1.468E+01\n",
      "step 3300000 \t return [-28.490555 -38.373184], ([0.16205435 0.18789044]) \t loss 1.374E+01\n",
      "step 3310000 \t return [-28.707819 -38.608326], ([0.10495229 0.2202627 ]) \t loss 1.435E+01\n",
      "step 3320000 \t return [-28.5475   -38.385345], ([0.18594824 0.1571896 ]) \t loss 1.450E+01\n",
      "step 3330000 \t return [-28.754028 -38.59113 ], ([0.05454828 0.24667588]) \t loss 1.416E+01\n",
      "step 3340000 \t return [-28.372593 -38.33707 ], ([0.09980586 0.22754535]) \t loss 1.477E+01\n",
      "step 3350000 \t return [-28.433865 -38.31241 ], ([0.08135271 0.26481447]) \t loss 1.474E+01\n",
      "step 3360000 \t return [-28.430069 -38.322704], ([0.1187414  0.20995802]) \t loss 1.446E+01\n",
      "step 3370000 \t return [-28.575693 -38.432106], ([0.18639389 0.15252091]) \t loss 1.380E+01\n",
      "step 3380000 \t return [-28.44589  -38.400066], ([0.15157562 0.16796741]) \t loss 1.422E+01\n",
      "step 3390000 \t return [-28.332537 -38.262398], ([0.14936088 0.21913709]) \t loss 1.383E+01\n",
      "step 3400000 \t return [-28.75167 -38.62216], ([0.21052471 0.12925066]) \t loss 1.409E+01\n",
      "step 3410000 \t return [-28.29937 -38.16684], ([0.14529242 0.17200519]) \t loss 1.427E+01\n",
      "step 3420000 \t return [-28.332283 -38.260017], ([0.1190882 0.2505778]) \t loss 1.453E+01\n",
      "step 3430000 \t return [-28.341581 -38.276283], ([0.25368333 0.17011154]) \t loss 1.351E+01\n",
      "step 3440000 \t return [-28.406008 -38.30791 ], ([0.2008061  0.15175426]) \t loss 1.445E+01\n",
      "step 3450000 \t return [-28.283857 -38.15422 ], ([0.15035447 0.17131607]) \t loss 1.416E+01\n",
      "step 3460000 \t return [-28.235458 -38.115753], ([0.09949403 0.24665475]) \t loss 1.353E+01\n",
      "step 3470000 \t return [-28.152369 -38.03728 ], ([0.08328656 0.21860918]) \t loss 1.377E+01\n",
      "step 3480000 \t return [-28.15218  -38.002228], ([0.1009163  0.21335082]) \t loss 1.404E+01\n",
      "step 3490000 \t return [-28.1939   -38.118324], ([0.15034    0.17690943]) \t loss 1.377E+01\n",
      "step 3500000 \t return [-28.08966  -37.988007], ([0.08211319 0.22074008]) \t loss 1.400E+01\n",
      "step 3510000 \t return [-28.171728 -38.060543], ([0.11736278 0.22816893]) \t loss 1.377E+01\n",
      "step 3520000 \t return [-28.040508 -37.955067], ([0.10928    0.17308617]) \t loss 1.433E+01\n",
      "step 3530000 \t return [-27.987093 -37.923195], ([0.14261281 0.24142694]) \t loss 1.346E+01\n",
      "step 3540000 \t return [-27.990786 -37.88958 ], ([0.27657202 0.20661655]) \t loss 1.368E+01\n",
      "step 3550000 \t return [-27.915668 -37.73383 ], ([0.20281352 0.24182896]) \t loss 1.470E+01\n",
      "step 3560000 \t return [-27.833956 -37.71747 ], ([0.15577547 0.22147077]) \t loss 1.369E+01\n",
      "step 3570000 \t return [-28.059742 -37.988514], ([0.20701385 0.25216475]) \t loss 1.402E+01\n",
      "step 3580000 \t return [-27.91988  -37.846817], ([0.19137804 0.1706453 ]) \t loss 1.332E+01\n",
      "step 3590000 \t return [-27.848291 -37.715477], ([0.19586508 0.21475513]) \t loss 1.375E+01\n",
      "step 3600000 \t return [-27.691801 -37.60466 ], ([0.11883166 0.20105553]) \t loss 1.381E+01\n",
      "step 3610000 \t return [-27.745255 -37.653683], ([0.15321153 0.15012535]) \t loss 1.338E+01\n",
      "step 3620000 \t return [-27.672483 -37.525993], ([0.18489796 0.23581955]) \t loss 1.357E+01\n",
      "step 3630000 \t return [-27.58815  -37.484573], ([0.12605977 0.22663385]) \t loss 1.354E+01\n",
      "step 3640000 \t return [-27.831858 -37.718433], ([0.16732198 0.15787584]) \t loss 1.494E+01\n",
      "step 3650000 \t return [-27.682053 -37.477036], ([0.31271657 0.21102476]) \t loss 1.508E+01\n",
      "step 3660000 \t return [-27.455889 -37.349125], ([0.0877829  0.22682215]) \t loss 1.393E+01\n",
      "step 3670000 \t return [-27.430265 -37.329964], ([0.17355886 0.15934199]) \t loss 1.259E+01\n",
      "step 3680000 \t return [-27.36165  -37.232933], ([0.13869248 0.1891077 ]) \t loss 1.329E+01\n",
      "step 3690000 \t return [-27.32503 -37.22826], ([0.15183023 0.1615317 ]) \t loss 1.296E+01\n",
      "step 3700000 \t return [-27.495407 -37.38104 ], ([0.0894075  0.27533668]) \t loss 1.307E+01\n",
      "step 3710000 \t return [-27.24094  -37.109398], ([0.13187924 0.19110475]) \t loss 1.395E+01\n",
      "step 3720000 \t return [-27.26363  -37.198044], ([0.13943624 0.15112752]) \t loss 1.227E+01\n",
      "step 3730000 \t return [-27.334728 -37.239246], ([0.05842262 0.23891339]) \t loss 1.284E+01\n",
      "step 3740000 \t return [-27.157986 -37.08897 ], ([0.14777939 0.14507906]) \t loss 1.384E+01\n",
      "step 3750000 \t return [-26.957949 -36.859695], ([0.1434087  0.17305395]) \t loss 1.244E+01\n",
      "step 3760000 \t return [-26.876684 -36.802963], ([0.13369398 0.2029582 ]) \t loss 1.214E+01\n",
      "step 3770000 \t return [-26.750078 -36.676765], ([0.16661897 0.20109677]) \t loss 1.284E+01\n",
      "step 3780000 \t return [-26.911104 -36.79635 ], ([0.17521228 0.1508635 ]) \t loss 1.309E+01\n",
      "step 3790000 \t return [-26.651245 -36.566685], ([0.0978193  0.22625847]) \t loss 1.443E+01\n",
      "step 3800000 \t return [-26.591412 -36.52353 ], ([0.09339732 0.22403824]) \t loss 1.367E+01\n",
      "step 3810000 \t return [-26.61677  -36.493526], ([0.14307447 0.18465832]) \t loss 1.375E+01\n",
      "step 3820000 \t return [-26.512974 -36.4098  ], ([0.14400148 0.20316987]) \t loss 1.367E+01\n",
      "step 3830000 \t return [-26.474316 -36.355156], ([0.10653977 0.23440565]) \t loss 1.298E+01\n",
      "step 3840000 \t return [-26.498901 -36.44415 ], ([0.06828486 0.24298379]) \t loss 1.283E+01\n",
      "step 3850000 \t return [-26.43764 -36.30719], ([0.06349195 0.23159118]) \t loss 1.310E+01\n",
      "step 3860000 \t return [-26.47218  -36.383595], ([0.06652421 0.23581378]) \t loss 1.405E+01\n",
      "step 3870000 \t return [-26.308653 -36.168358], ([0.09419385 0.22928661]) \t loss 1.331E+01\n",
      "step 3880000 \t return [-26.466894 -36.341877], ([0.19610873 0.1611913 ]) \t loss 1.366E+01\n",
      "step 3890000 \t return [-26.232735 -36.09393 ], ([0.15935023 0.18509658]) \t loss 1.297E+01\n",
      "step 3900000 \t return [-26.17866 -36.10556], ([0.07257736 0.26191399]) \t loss 1.279E+01\n",
      "step 3910000 \t return [-26.143589 -36.04178 ], ([0.1347056  0.20575853]) \t loss 1.305E+01\n",
      "step 3920000 \t return [-26.17416  -36.028133], ([0.04681363 0.24789405]) \t loss 1.268E+01\n",
      "step 3930000 \t return [-26.090088 -35.95205 ], ([0.05584911 0.27589285]) \t loss 1.324E+01\n",
      "step 3940000 \t return [-25.971992 -35.92406 ], ([0.08511442 0.1982768 ]) \t loss 1.347E+01\n",
      "step 3950000 \t return [-26.039541 -35.905094], ([0.05679229 0.26533085]) \t loss 1.359E+01\n",
      "step 3960000 \t return [-26.015226 -35.855103], ([0.05871649 0.2662439 ]) \t loss 1.245E+01\n",
      "step 3970000 \t return [-25.919079 -35.77039 ], ([0.09581234 0.24677995]) \t loss 1.322E+01\n",
      "step 3980000 \t return [-26.009636 -35.879383], ([0.05875775 0.25146747]) \t loss 1.323E+01\n",
      "step 3990000 \t return [-25.85777  -35.740612], ([0.14970846 0.20894349]) \t loss 1.261E+01\n",
      "step 4000000 \t return [-25.795507 -35.650772], ([0.06371573 0.22712083]) \t loss 1.287E+01\n",
      "step 4010000 \t return [-25.718054 -35.612232], ([0.0694083  0.21214625]) \t loss 1.215E+01\n",
      "step 4020000 \t return [-25.721218 -35.65902 ], ([0.15328437 0.24812785]) \t loss 1.280E+01\n",
      "step 4030000 \t return [-25.72972  -35.566513], ([0.08828751 0.26037872]) \t loss 1.231E+01\n",
      "step 4040000 \t return [-25.728088 -35.606388], ([0.09508157 0.22245598]) \t loss 1.224E+01\n",
      "step 4050000 \t return [-25.751123 -35.66765 ], ([0.16670676 0.2292127 ]) \t loss 1.327E+01\n",
      "step 4060000 \t return [-25.712387 -35.589664], ([0.16138268 0.18148367]) \t loss 1.285E+01\n",
      "step 4070000 \t return [-25.684708 -35.57996 ], ([0.1190714 0.2325171]) \t loss 1.187E+01\n",
      "step 4080000 \t return [-25.541695 -35.45878 ], ([0.09632453 0.24488589]) \t loss 1.252E+01\n",
      "step 4090000 \t return [-25.615595 -35.450943], ([0.22627808 0.24394342]) \t loss 1.240E+01\n",
      "step 4100000 \t return [-25.619263 -35.427048], ([0.19678216 0.23226796]) \t loss 1.292E+01\n",
      "step 4110000 \t return [-25.566145 -35.450703], ([0.12674585 0.23901619]) \t loss 1.308E+01\n",
      "step 4120000 \t return [-25.59156 -35.45   ], ([0.236346   0.21426588]) \t loss 1.324E+01\n",
      "step 4130000 \t return [-25.460596 -35.38648 ], ([0.07646859 0.23087789]) \t loss 1.295E+01\n",
      "step 4140000 \t return [-25.53562  -35.463516], ([0.153526  0.1960496]) \t loss 1.202E+01\n",
      "step 4150000 \t return [-25.32571  -35.222298], ([0.07961697 0.22753184]) \t loss 1.273E+01\n",
      "step 4160000 \t return [-25.425098 -35.30147 ], ([0.12958544 0.2590857 ]) \t loss 1.282E+01\n",
      "step 4170000 \t return [-25.393976 -35.277084], ([0.24313134 0.2359504 ]) \t loss 1.286E+01\n",
      "step 4180000 \t return [-25.336082 -35.225266], ([0.34222227 0.26189232]) \t loss 1.272E+01\n",
      "step 4190000 \t return [-25.382893 -35.274963], ([0.06909078 0.2570321 ]) \t loss 1.308E+01\n",
      "step 4200000 \t return [-25.194038 -35.075348], ([0.09940344 0.22247429]) \t loss 1.297E+01\n",
      "step 4210000 \t return [-25.17687  -35.098415], ([0.15546964 0.1955396 ]) \t loss 1.290E+01\n",
      "step 4220000 \t return [-25.07508 -34.93022], ([0.10974627 0.22484007]) \t loss 1.169E+01\n",
      "step 4230000 \t return [-25.182402 -35.04655 ], ([0.1365516  0.17301112]) \t loss 1.229E+01\n",
      "step 4240000 \t return [-24.998945 -34.85236 ], ([0.0633608  0.22166774]) \t loss 1.222E+01\n",
      "step 4250000 \t return [-24.921577 -34.782448], ([0.14900823 0.19165772]) \t loss 1.201E+01\n",
      "step 4260000 \t return [-24.841066 -34.698845], ([0.06564408 0.24067482]) \t loss 1.225E+01\n",
      "step 4270000 \t return [-24.908978 -34.80728 ], ([0.07092786 0.21209887]) \t loss 1.138E+01\n",
      "step 4280000 \t return [-24.88186  -34.752472], ([0.05866889 0.23701057]) \t loss 1.154E+01\n",
      "step 4290000 \t return [-24.871655 -34.710712], ([0.06759834 0.25133318]) \t loss 1.119E+01\n",
      "step 4300000 \t return [-24.757036 -34.596176], ([0.10018976 0.22982669]) \t loss 1.207E+01\n",
      "step 4310000 \t return [-24.751272 -34.65653 ], ([0.0744019 0.2903476]) \t loss 1.158E+01\n",
      "step 4320000 \t return [-24.908072 -34.806812], ([0.06767603 0.24728988]) \t loss 1.143E+01\n",
      "step 4330000 \t return [-24.781277 -34.663998], ([0.15664656 0.1971391 ]) \t loss 1.131E+01\n",
      "step 4340000 \t return [-24.622622 -34.481342], ([0.10042994 0.20992799]) \t loss 1.171E+01\n",
      "step 4350000 \t return [-24.438414 -34.310493], ([0.1149421  0.21083146]) \t loss 1.209E+01\n",
      "step 4360000 \t return [-24.425217 -34.296307], ([0.13748382 0.19215928]) \t loss 1.086E+01\n",
      "step 4370000 \t return [-24.328089 -34.200436], ([0.11181522 0.22884487]) \t loss 1.090E+01\n",
      "step 4380000 \t return [-24.224592 -34.088787], ([0.09440597 0.2370143 ]) \t loss 1.127E+01\n",
      "step 4390000 \t return [-24.31271 -34.20147], ([0.17883652 0.16583763]) \t loss 1.173E+01\n",
      "step 4400000 \t return [-24.082176 -33.977955], ([0.10982284 0.26720017]) \t loss 1.185E+01\n",
      "step 4410000 \t return [-24.04494  -33.953693], ([0.05265209 0.24900104]) \t loss 1.194E+01\n",
      "step 4420000 \t return [-23.991243 -33.885292], ([0.0989282  0.23823059]) \t loss 1.154E+01\n",
      "step 4430000 \t return [-23.902266 -33.793457], ([0.1127431  0.22065204]) \t loss 1.076E+01\n",
      "step 4440000 \t return [-23.909922 -33.783936], ([0.06714672 0.27189994]) \t loss 1.107E+01\n",
      "step 4450000 \t return [-23.876133 -33.74708 ], ([0.053362   0.24907099]) \t loss 1.116E+01\n",
      "step 4460000 \t return [-23.78247  -33.706512], ([0.0547184  0.26895836]) \t loss 1.135E+01\n",
      "step 4470000 \t return [-23.705904 -33.554955], ([0.07110141 0.32505694]) \t loss 1.157E+01\n",
      "step 4480000 \t return [-23.668127 -33.526287], ([0.05369455 0.25589892]) \t loss 1.177E+01\n",
      "step 4490000 \t return [-23.640549 -33.50464 ], ([0.1078486  0.25702903]) \t loss 1.164E+01\n",
      "step 4500000 \t return [-23.570688 -33.43287 ], ([0.07660341 0.2609223 ]) \t loss 1.104E+01\n",
      "step 4510000 \t return [-23.483149 -33.3557  ], ([0.05573563 0.27762076]) \t loss 1.218E+01\n",
      "step 4520000 \t return [-23.464638 -33.34359 ], ([0.0693633  0.26038563]) \t loss 1.124E+01\n",
      "step 4530000 \t return [-23.428242 -33.267776], ([0.04841482 0.27132708]) \t loss 1.186E+01\n",
      "step 4540000 \t return [-23.461939 -33.320965], ([0.11426337 0.22240032]) \t loss 1.104E+01\n",
      "step 4550000 \t return [-23.350092 -33.24272 ], ([0.06455322 0.23788601]) \t loss 1.052E+01\n",
      "step 4560000 \t return [-23.306154 -33.20211 ], ([0.06818969 0.3035541 ]) \t loss 1.164E+01\n",
      "step 4570000 \t return [-23.319494 -33.26531 ], ([0.09141897 0.27054167]) \t loss 1.107E+01\n",
      "step 4580000 \t return [-23.229877 -33.082893], ([0.05587057 0.23623733]) \t loss 1.107E+01\n",
      "step 4590000 \t return [-23.141638 -32.996586], ([0.08548974 0.25274318]) \t loss 1.179E+01\n",
      "step 4600000 \t return [-23.16816  -33.024445], ([0.13121724 0.20224825]) \t loss 1.105E+01\n",
      "step 4610000 \t return [-23.117775 -32.95047 ], ([0.12567738 0.272208  ]) \t loss 1.060E+01\n",
      "step 4620000 \t return [-23.05436 -32.91643], ([0.12846313 0.22684567]) \t loss 1.118E+01\n",
      "step 4630000 \t return [-23.000252 -32.880646], ([0.12609532 0.21023753]) \t loss 1.110E+01\n",
      "step 4640000 \t return [-22.953375 -32.80564 ], ([0.21689732 0.18738206]) \t loss 1.038E+01\n",
      "step 4650000 \t return [-22.945469 -32.905354], ([0.17445327 0.16897012]) \t loss 1.131E+01\n",
      "step 4660000 \t return [-22.829521 -32.71129 ], ([0.10681224 0.2596096 ]) \t loss 1.115E+01\n",
      "step 4670000 \t return [-22.876423 -32.73745 ], ([0.23705646 0.20964356]) \t loss 1.111E+01\n",
      "step 4680000 \t return [-22.791252 -32.650448], ([0.04604038 0.29901624]) \t loss 1.046E+01\n",
      "step 4690000 \t return [-22.703049 -32.583763], ([0.08784541 0.24780877]) \t loss 1.113E+01\n",
      "step 4700000 \t return [-22.699444 -32.526653], ([0.12488759 0.22623281]) \t loss 1.151E+01\n",
      "step 4710000 \t return [-22.66864 -32.53686], ([0.147895   0.20074853]) \t loss 1.120E+01\n",
      "step 4720000 \t return [-22.705078 -32.58382 ], ([0.07700373 0.26376918]) \t loss 1.124E+01\n",
      "step 4730000 \t return [-22.8213   -32.660973], ([0.05467794 0.26801738]) \t loss 1.055E+01\n",
      "step 4740000 \t return [-22.766289 -32.60127 ], ([0.06009228 0.25591126]) \t loss 1.077E+01\n",
      "step 4750000 \t return [-22.646717 -32.465755], ([0.05524267 0.2890105 ]) \t loss 1.128E+01\n",
      "step 4760000 \t return [-22.588465 -32.450672], ([0.06416418 0.2751271 ]) \t loss 1.151E+01\n",
      "step 4770000 \t return [-22.630432 -32.53867 ], ([0.1066382  0.24144198]) \t loss 1.080E+01\n",
      "step 4780000 \t return [-22.483084 -32.32494 ], ([0.09936459 0.24633293]) \t loss 1.109E+01\n",
      "step 4790000 \t return [-22.324505 -32.170486], ([0.08123159 0.2646675 ]) \t loss 1.158E+01\n",
      "step 4800000 \t return [-22.391382 -32.211525], ([0.06684677 0.30626762]) \t loss 1.152E+01\n",
      "step 4810000 \t return [-22.440874 -32.27133 ], ([0.13104884 0.2594617 ]) \t loss 1.179E+01\n",
      "step 4820000 \t return [-22.338242 -32.18521 ], ([0.05760087 0.31336766]) \t loss 1.100E+01\n",
      "step 4830000 \t return [-22.347929 -32.21466 ], ([0.0667055 0.2827069]) \t loss 1.146E+01\n",
      "step 4840000 \t return [-22.258307 -32.096783], ([0.05759053 0.28125915]) \t loss 1.101E+01\n",
      "step 4850000 \t return [-22.23479  -32.049694], ([0.06723928 0.29877552]) \t loss 1.141E+01\n",
      "step 4860000 \t return [-22.205097 -32.028862], ([0.07675751 0.26609802]) \t loss 1.030E+01\n",
      "step 4870000 \t return [-22.140335 -32.01332 ], ([0.07883964 0.26308346]) \t loss 1.027E+01\n",
      "step 4880000 \t return [-22.16415  -32.050087], ([0.11549402 0.2355409 ]) \t loss 9.724E+00\n",
      "step 4890000 \t return [-22.257591 -32.078724], ([0.06218659 0.29942843]) \t loss 1.037E+01\n",
      "step 4900000 \t return [-22.07444  -31.910723], ([0.06524675 0.2564685 ]) \t loss 1.064E+01\n",
      "step 4910000 \t return [-21.997427 -31.851538], ([0.08507037 0.24870986]) \t loss 1.020E+01\n",
      "step 4920000 \t return [-22.174362 -32.069283], ([0.06699717 0.30018225]) \t loss 9.906E+00\n",
      "step 4930000 \t return [-22.040737 -31.897362], ([0.12695366 0.23962602]) \t loss 1.040E+01\n",
      "step 4940000 \t return [-21.907375 -31.80671 ], ([0.07113969 0.23892008]) \t loss 9.763E+00\n",
      "step 4950000 \t return [-21.945261 -31.79532 ], ([0.05594042 0.29055756]) \t loss 1.030E+01\n",
      "step 4960000 \t return [-21.85952  -31.731003], ([0.10217303 0.26897982]) \t loss 1.018E+01\n",
      "step 4970000 \t return [-21.84394  -31.710094], ([0.0668578 0.2782678]) \t loss 9.935E+00\n",
      "step 4980000 \t return [-21.687386 -31.491875], ([0.06886333 0.27182856]) \t loss 1.056E+01\n",
      "step 4990000 \t return [-21.665672 -31.514658], ([0.05058478 0.30363464]) \t loss 1.284E+01\n",
      "step 5000000 \t return [-21.654818 -31.486671], ([0.11462896 0.23620635]) \t loss 1.073E+01\n",
      "step 5010000 \t return [-21.75632 -31.57709], ([0.02538462 0.28162646]) \t loss 1.077E+01\n",
      "step 5020000 \t return [-21.555008 -31.455103], ([0.04086794 0.30435887]) \t loss 8.111E+00\n",
      "step 5030000 \t return [-21.634436 -31.41773 ], ([0.02359151 0.34940344]) \t loss 6.564E+00\n",
      "step 5040000 \t return [-21.51361 -31.36463], ([0.05326925 0.26499823]) \t loss 5.534E+00\n",
      "step 5050000 \t return [-21.628288 -31.425692], ([0.10015027 0.25283533]) \t loss 4.736E+00\n",
      "step 5060000 \t return [-21.631712 -31.425709], ([0.03206398 0.28675792]) \t loss 2.949E+00\n",
      "step 5070000 \t return [-21.5125   -31.300266], ([0.05419187 0.27178165]) \t loss 2.226E+00\n",
      "step 5080000 \t return [-21.50284  -31.273426], ([0.10011756 0.2510985 ]) \t loss 2.015E+00\n",
      "step 5090000 \t return [-21.559185 -31.397259], ([0.03982067 0.30498138]) \t loss 1.210E+00\n",
      "step 5100000 \t return [-21.598022 -31.456985], ([0.02086452 0.3182414 ]) \t loss 9.183E-01\n",
      "step 5110000 \t return [-21.617836 -31.414253], ([0.01377187 0.32718763]) \t loss 7.324E-01\n",
      "step 5120000 \t return [-21.487436 -31.322172], ([0.08186524 0.27017745]) \t loss 5.113E-01\n",
      "step 5130000 \t return [-21.550499 -31.332197], ([0.10286943 0.22902426]) \t loss 4.870E-01\n",
      "step 5140000 \t return [-21.463865 -31.279217], ([0.07273133 0.24238771]) \t loss 5.417E-01\n",
      "step 5150000 \t return [-21.508171 -31.324062], ([0.03740281 0.29621318]) \t loss 3.536E-01\n",
      "step 5160000 \t return [-21.532887 -31.295242], ([0.11447688 0.22754207]) \t loss 3.004E-01\n",
      "step 5170000 \t return [-21.565079 -31.450918], ([0.01878127 0.3299473 ]) \t loss 3.310E-01\n",
      "step 5180000 \t return [-21.482468 -31.27171 ], ([0.09696483 0.27594852]) \t loss 3.073E-01\n",
      "step 5190000 \t return [-21.714878 -31.565115], ([0.01340999 0.34098178]) \t loss 3.340E-01\n",
      "step 5200000 \t return [-21.444944 -31.246052], ([0.08433495 0.28723457]) \t loss 3.478E-01\n",
      "step 5210000 \t return [-21.359121 -31.183882], ([0.09657751 0.26326323]) \t loss 2.629E-01\n",
      "step 5220000 \t return [-21.328867 -31.169285], ([0.06607357 0.29693115]) \t loss 2.604E-01\n",
      "step 5230000 \t return [-21.439129 -31.214415], ([0.12504369 0.20203494]) \t loss 2.708E-01\n",
      "step 5240000 \t return [-21.43854  -31.294323], ([0.12686226 0.21932694]) \t loss 2.858E-01\n",
      "step 5250000 \t return [-21.30741 -31.0606 ], ([0.08118189 0.279765  ]) \t loss 3.199E-01\n",
      "step 5260000 \t return [-21.326653 -31.173151], ([0.0763926  0.24773248]) \t loss 1.907E-01\n",
      "step 5270000 \t return [-21.315325 -31.159317], ([0.06785464 0.2890553 ]) \t loss 2.363E-01\n",
      "step 5280000 \t return [-21.307747 -31.03604 ], ([0.05675885 0.275097  ]) \t loss 1.787E-01\n",
      "step 5290000 \t return [-21.367851 -31.207125], ([0.01997965 0.32245225]) \t loss 1.667E-01\n",
      "step 5300000 \t return [-21.357376 -31.148483], ([0.01646927 0.32325602]) \t loss 2.158E-01\n",
      "step 5310000 \t return [-21.33216  -31.143261], ([0.01705463 0.32242042]) \t loss 2.680E-01\n",
      "step 5320000 \t return [-21.369802 -31.269958], ([0.0175107  0.33063564]) \t loss 3.813E-01\n",
      "step 5330000 \t return [-21.302027 -31.102339], ([0.02460589 0.32332397]) \t loss 2.991E-01\n",
      "step 5340000 \t return [-21.303555 -31.074379], ([0.08721608 0.23520361]) \t loss 1.833E-01\n",
      "step 5350000 \t return [-21.298872 -31.128778], ([0.05710047 0.32853684]) \t loss 1.832E-01\n",
      "step 5360000 \t return [-21.33102  -31.191452], ([0.01889984 0.29400784]) \t loss 2.011E-01\n",
      "step 5370000 \t return [-21.295988 -31.092297], ([0.02881159 0.30915165]) \t loss 2.289E-01\n",
      "step 5380000 \t return [-21.249475 -31.061827], ([0.07594297 0.26519942]) \t loss 2.369E-01\n",
      "step 5390000 \t return [-21.331203 -31.18217 ], ([0.0194981 0.3053921]) \t loss 2.733E-01\n",
      "step 5400000 \t return [-21.324142 -31.08979 ], ([0.01852495 0.3079682 ]) \t loss 2.756E-01\n",
      "step 5410000 \t return [-21.223965 -31.057146], ([0.02731976 0.28368896]) \t loss 2.820E-01\n",
      "step 5420000 \t return [-21.217234 -31.040745], ([0.07963485 0.24571674]) \t loss 2.208E-01\n",
      "step 5430000 \t return [-21.225859 -31.072144], ([0.02833415 0.3162628 ]) \t loss 2.623E-01\n",
      "step 5440000 \t return [-21.245111 -31.049028], ([0.06829169 0.28036729]) \t loss 2.427E-01\n",
      "step 5450000 \t return [-21.303476 -31.147554], ([0.01600413 0.31796822]) \t loss 2.113E-01\n",
      "step 5460000 \t return [-21.22802  -31.073847], ([0.0644258  0.27865043]) \t loss 2.704E-01\n",
      "step 5470000 \t return [-21.213512 -31.062038], ([0.05950955 0.27565667]) \t loss 2.607E-01\n",
      "step 5480000 \t return [-21.194199 -31.024864], ([0.06008452 0.2925662 ]) \t loss 2.598E-01\n",
      "step 5490000 \t return [-21.288738 -31.075144], ([0.12507978 0.26975358]) \t loss 3.415E-01\n",
      "step 5500000 \t return [-21.203407 -31.020857], ([0.03226176 0.2945083 ]) \t loss 3.489E-01\n",
      "step 5510000 \t return [-21.190926 -30.973583], ([0.03221601 0.32116285]) \t loss 3.069E-01\n",
      "step 5520000 \t return [-21.230257 -30.96444 ], ([0.02478597 0.34563407]) \t loss 4.377E-01\n",
      "step 5530000 \t return [-21.155607 -30.990942], ([0.07821673 0.2689255 ]) \t loss 3.447E-01\n",
      "step 5540000 \t return [-21.177969 -30.989296], ([0.02312128 0.34958765]) \t loss 2.828E-01\n",
      "step 5550000 \t return [-21.16303  -30.916365], ([0.03021261 0.31661174]) \t loss 3.174E-01\n",
      "step 5560000 \t return [-21.295282 -31.107561], ([0.01551762 0.3812095 ]) \t loss 2.442E-01\n",
      "step 5570000 \t return [-21.286928 -31.066418], ([0.02033593 0.34328195]) \t loss 3.794E-01\n",
      "step 5580000 \t return [-21.146677 -30.985445], ([0.06653223 0.26328555]) \t loss 3.623E-01\n",
      "step 5590000 \t return [-21.103096 -30.944744], ([0.03787209 0.3410064 ]) \t loss 3.267E-01\n",
      "step 5600000 \t return [-21.09629  -30.912376], ([0.02881642 0.28998014]) \t loss 2.502E-01\n",
      "step 5610000 \t return [-21.110332 -30.896595], ([0.02912968 0.30296215]) \t loss 2.122E-01\n",
      "step 5620000 \t return [-21.121288 -30.983425], ([0.02307794 0.34512168]) \t loss 1.866E-01\n",
      "step 5630000 \t return [-21.128847 -30.945148], ([0.01856825 0.27578068]) \t loss 2.784E-01\n",
      "step 5640000 \t return [-21.097668 -30.903954], ([0.04654296 0.30653477]) \t loss 2.219E-01\n",
      "step 5650000 \t return [-21.12098  -30.880972], ([0.03811449 0.35027298]) \t loss 1.813E-01\n",
      "step 5660000 \t return [-21.11784  -30.962795], ([0.03012203 0.33893743]) \t loss 1.957E-01\n",
      "step 5670000 \t return [-21.098536 -30.981125], ([0.04759606 0.27336088]) \t loss 2.549E-01\n",
      "step 5680000 \t return [-21.124916 -30.998177], ([0.03101865 0.32312405]) \t loss 2.031E-01\n",
      "step 5690000 \t return [-21.147783 -30.994343], ([0.07107683 0.28098765]) \t loss 1.973E-01\n",
      "step 5700000 \t return [-21.186369 -31.00873 ], ([0.07956584 0.27613866]) \t loss 2.091E-01\n",
      "step 5710000 \t return [-21.123299 -30.966621], ([0.05899864 0.29810873]) \t loss 2.450E-01\n",
      "step 5720000 \t return [-21.086494 -30.922695], ([0.04476239 0.28819284]) \t loss 2.312E-01\n",
      "step 5730000 \t return [-21.11376  -30.904726], ([0.0336202  0.35556865]) \t loss 1.877E-01\n",
      "step 5740000 \t return [-21.155777 -30.986982], ([0.07307062 0.29461116]) \t loss 2.028E-01\n",
      "step 5750000 \t return [-21.067053 -30.872375], ([0.05766872 0.29361182]) \t loss 2.112E-01\n",
      "step 5760000 \t return [-21.055439 -30.850243], ([0.04896462 0.29408774]) \t loss 1.914E-01\n",
      "step 5770000 \t return [-21.048025 -30.826845], ([0.0636298 0.3193163]) \t loss 1.433E-01\n",
      "step 5780000 \t return [-21.10757  -30.947311], ([0.01882253 0.3077886 ]) \t loss 1.769E-01\n",
      "step 5790000 \t return [-21.03668  -30.809721], ([0.05734213 0.29509348]) \t loss 1.783E-01\n",
      "step 5800000 \t return [-21.108187 -30.893227], ([0.02306295 0.32102725]) \t loss 1.568E-01\n",
      "step 5810000 \t return [-20.973913 -30.8648  ], ([0.02892751 0.29382405]) \t loss 2.317E-01\n",
      "step 5820000 \t return [-20.95003  -30.802427], ([0.03262879 0.27692893]) \t loss 1.601E-01\n",
      "step 5830000 \t return [-20.947718 -30.717922], ([0.02359739 0.3499132 ]) \t loss 1.478E-01\n",
      "step 5840000 \t return [-21.039064 -30.872486], ([0.02151266 0.33457145]) \t loss 2.731E-01\n",
      "step 5850000 \t return [-20.932913 -30.788109], ([0.02098258 0.26854485]) \t loss 2.529E-01\n",
      "step 5860000 \t return [-20.949394 -30.760187], ([0.03309371 0.2968147 ]) \t loss 3.237E-01\n",
      "step 5870000 \t return [-20.892689 -30.741909], ([0.0294153 0.3365776]) \t loss 3.309E-01\n",
      "step 5880000 \t return [-20.86914  -30.718664], ([0.02918158 0.3298139 ]) \t loss 3.884E-01\n",
      "step 5890000 \t return [-20.828848 -30.635725], ([0.0277575 0.2958646]) \t loss 3.372E-01\n",
      "step 5900000 \t return [-20.870188 -30.725231], ([0.02249804 0.32686096]) \t loss 2.427E-01\n",
      "step 5910000 \t return [-20.85575  -30.724783], ([0.02935449 0.29092205]) \t loss 2.987E-01\n",
      "step 5920000 \t return [-20.802895 -30.67829 ], ([0.02741937 0.29557443]) \t loss 2.367E-01\n",
      "step 5930000 \t return [-20.81217 -30.62475], ([0.05224156 0.3242425 ]) \t loss 1.874E-01\n",
      "step 5940000 \t return [-20.827726 -30.614534], ([0.02203142 0.3202331 ]) \t loss 2.088E-01\n",
      "step 5950000 \t return [-20.731735 -30.528015], ([0.02970099 0.37094238]) \t loss 2.148E-01\n",
      "step 5960000 \t return [-20.696987 -30.526382], ([0.02403615 0.3462199 ]) \t loss 2.716E-01\n",
      "step 5970000 \t return [-20.693214 -30.484238], ([0.03503983 0.37911722]) \t loss 3.800E-01\n",
      "step 5980000 \t return [-20.675104 -30.493677], ([0.03642627 0.32281527]) \t loss 3.968E-01\n",
      "step 5990000 \t return [-20.638525 -30.451252], ([0.02446304 0.33088684]) \t loss 3.917E-01\n",
      "step 6000000 \t return [-20.616663 -30.478952], ([0.02674599 0.34262142]) \t loss 3.115E-01\n",
      "step 6010000 \t return [-20.606985 -30.365185], ([0.02645365 0.35061154]) \t loss 3.425E-01\n",
      "step 6020000 \t return [-20.584797 -30.387249], ([0.02209666 0.36638796]) \t loss 3.606E-01\n",
      "step 6030000 \t return [-20.610476 -30.362272], ([0.02382065 0.37067312]) \t loss 3.588E-01\n",
      "step 6040000 \t return [-20.559761 -30.437346], ([0.022275   0.28979236]) \t loss 3.118E-01\n",
      "step 6050000 \t return [-20.526506 -30.382612], ([0.02238871 0.32495603]) \t loss 4.203E-01\n",
      "step 6060000 \t return [-20.504297 -30.27169 ], ([0.03142667 0.3535791 ]) \t loss 2.952E-01\n",
      "step 6070000 \t return [-20.505066 -30.321243], ([0.02370046 0.30070546]) \t loss 2.338E-01\n",
      "step 6080000 \t return [-20.4647   -30.222885], ([0.02514958 0.32257047]) \t loss 2.095E-01\n",
      "step 6090000 \t return [-20.535376 -30.352083], ([0.02402101 0.3278682 ]) \t loss 1.716E-01\n",
      "step 6100000 \t return [-20.511106 -30.32904 ], ([0.0212557  0.33011767]) \t loss 2.330E-01\n",
      "step 6110000 \t return [-20.42884  -30.214842], ([0.02962924 0.3337029 ]) \t loss 3.320E-01\n",
      "step 6120000 \t return [-20.467314 -30.24842 ], ([0.0252903 0.3233029]) \t loss 2.316E-01\n",
      "step 6130000 \t return [-20.43312  -30.194992], ([0.02828265 0.35739574]) \t loss 2.480E-01\n",
      "step 6140000 \t return [-20.447634 -30.217949], ([0.03778778 0.31896314]) \t loss 2.371E-01\n",
      "step 6150000 \t return [-20.407463 -30.248877], ([0.0220846  0.35708717]) \t loss 2.105E-01\n",
      "step 6160000 \t return [-20.386452 -30.181416], ([0.02447649 0.31815886]) \t loss 3.913E-01\n",
      "step 6170000 \t return [-20.365528 -30.184092], ([0.02896797 0.3453705 ]) \t loss 1.892E-01\n",
      "step 6180000 \t return [-20.34388  -30.235502], ([0.03238017 0.29437894]) \t loss 1.517E-01\n",
      "step 6190000 \t return [-20.342154 -30.162947], ([0.01969019 0.30772567]) \t loss 1.389E-01\n",
      "step 6200000 \t return [-20.322922 -30.11468 ], ([0.02354645 0.3228245 ]) \t loss 1.528E-01\n",
      "step 6210000 \t return [-20.347893 -30.121021], ([0.02779208 0.36259902]) \t loss 1.487E-01\n",
      "step 6220000 \t return [-20.320688 -30.128935], ([0.02210524 0.33636084]) \t loss 1.402E-01\n",
      "step 6230000 \t return [-20.252417 -30.046852], ([0.03200548 0.3977862 ]) \t loss 1.633E-01\n",
      "step 6240000 \t return [-20.203468 -30.001919], ([0.02816213 0.32287002]) \t loss 1.524E-01\n",
      "step 6250000 \t return [-20.207308 -30.026304], ([0.03604527 0.33184123]) \t loss 1.736E-01\n",
      "step 6260000 \t return [-20.166729 -30.026638], ([0.0250462  0.29415426]) \t loss 1.670E-01\n",
      "step 6270000 \t return [-20.144295 -29.945694], ([0.02300389 0.31880024]) \t loss 1.244E-01\n",
      "step 6280000 \t return [-20.268028 -30.132652], ([0.02034198 0.31865168]) \t loss 1.811E-01\n",
      "step 6290000 \t return [-20.205788 -30.073803], ([0.02027015 0.3561815 ]) \t loss 2.303E-01\n",
      "step 6300000 \t return [-20.125782 -29.867508], ([0.01791884 0.35390666]) \t loss 2.474E-01\n",
      "step 6310000 \t return [-20.08676  -29.957722], ([0.02032478 0.33295625]) \t loss 2.046E-01\n",
      "step 6320000 \t return [-20.09602  -29.930525], ([0.01785655 0.32881907]) \t loss 2.231E-01\n",
      "step 6330000 \t return [-20.094486 -29.893715], ([0.02141071 0.3530721 ]) \t loss 1.801E-01\n",
      "step 6340000 \t return [-20.054102 -29.812334], ([0.02255374 0.33413762]) \t loss 1.905E-01\n",
      "step 6350000 \t return [-20.078432 -29.83033 ], ([0.01799153 0.36803448]) \t loss 1.740E-01\n",
      "step 6360000 \t return [-20.028337 -29.865402], ([0.02146667 0.36298123]) \t loss 4.576E-01\n",
      "step 6370000 \t return [-19.965754 -29.782906], ([0.02157661 0.31405124]) \t loss 5.925E-01\n",
      "step 6380000 \t return [-20.03902 -29.81968], ([0.02036704 0.40104944]) \t loss 2.567E-01\n",
      "step 6390000 \t return [-19.932137 -29.759956], ([0.02352908 0.37343124]) \t loss 3.302E-01\n",
      "step 6400000 \t return [-19.936638 -29.740942], ([0.01732463 0.33022174]) \t loss 3.303E-01\n",
      "step 6410000 \t return [-19.878098 -29.73421 ], ([0.01971621 0.3448181 ]) \t loss 2.893E-01\n",
      "step 6420000 \t return [-19.930086 -29.74217 ], ([0.01872171 0.37169975]) \t loss 2.644E-01\n",
      "step 6430000 \t return [-19.815313 -29.657791], ([0.01897173 0.3547274 ]) \t loss 3.103E-01\n",
      "step 6440000 \t return [-19.80796  -29.536972], ([0.01917676 0.39314848]) \t loss 2.965E-01\n",
      "step 6450000 \t return [-19.840754 -29.59343 ], ([0.02908674 0.34246638]) \t loss 3.118E-01\n",
      "step 6460000 \t return [-19.77252  -29.560677], ([0.01764384 0.35430023]) \t loss 3.028E-01\n",
      "step 6470000 \t return [-19.68839  -29.481392], ([0.02370474 0.36476848]) \t loss 2.769E-01\n",
      "step 6480000 \t return [-19.635845 -29.404917], ([0.02300821 0.33895597]) \t loss 5.726E-01\n",
      "step 6490000 \t return [-19.580696 -29.39918 ], ([0.02189208 0.34723562]) \t loss 4.157E-01\n",
      "step 6500000 \t return [-19.576958 -29.34356 ], ([0.01566147 0.35693315]) \t loss 3.205E-01\n",
      "step 6510000 \t return [-19.56448  -29.335752], ([0.02631793 0.2883153 ]) \t loss 2.358E-01\n",
      "step 6520000 \t return [-19.579592 -29.392212], ([0.01712487 0.3433243 ]) \t loss 2.003E-01\n",
      "step 6530000 \t return [-19.53556  -29.382017], ([0.02221414 0.40089473]) \t loss 2.580E-01\n",
      "step 6540000 \t return [-19.51451  -29.344639], ([0.02125929 0.34395736]) \t loss 3.992E-01\n",
      "step 6550000 \t return [-19.518688 -29.296143], ([0.0141774 0.3661068]) \t loss 3.203E-01\n",
      "step 6560000 \t return [-19.516045 -29.311207], ([0.01551459 0.3403599 ]) \t loss 2.766E-01\n",
      "step 6570000 \t return [-19.49075  -29.293045], ([0.01902333 0.36102375]) \t loss 3.997E-01\n",
      "step 6580000 \t return [-19.466534 -29.222305], ([0.01354575 0.372622  ]) \t loss 3.202E-01\n",
      "step 6590000 \t return [-19.489588 -29.285162], ([0.016821  0.3905178]) \t loss 3.606E-01\n",
      "step 6600000 \t return [-19.472715 -29.25468 ], ([0.01273502 0.3412781 ]) \t loss 4.476E-01\n",
      "step 6610000 \t return [-19.452358 -29.264631], ([0.01335406 0.3806987 ]) \t loss 4.477E-01\n",
      "step 6620000 \t return [-19.474808 -29.31639 ], ([0.01709356 0.33013177]) \t loss 4.609E-01\n",
      "step 6630000 \t return [-19.450268 -29.23864 ], ([0.01843003 0.36355948]) \t loss 5.904E-01\n",
      "step 6640000 \t return [-19.454542 -29.241003], ([0.01505437 0.3832842 ]) \t loss 4.175E-01\n",
      "step 6650000 \t return [-19.44143  -29.240547], ([0.01374039 0.39264274]) \t loss 4.978E-01\n",
      "step 6660000 \t return [-19.432917 -29.219196], ([0.01321734 0.37277427]) \t loss 5.480E-01\n",
      "step 6670000 \t return [-19.47999  -29.248825], ([0.01541296 0.3378634 ]) \t loss 6.047E-01\n",
      "step 6680000 \t return [-19.429081 -29.187044], ([0.01688958 0.36965093]) \t loss 6.870E-01\n",
      "step 6690000 \t return [-19.387651 -29.226294], ([0.01473324 0.36355212]) \t loss 6.689E-01\n",
      "step 6700000 \t return [-19.404245 -29.248192], ([0.02405111 0.34814116]) \t loss 4.996E-01\n",
      "step 6710000 \t return [-19.35223  -29.148354], ([0.02479037 0.33315983]) \t loss 4.709E-01\n",
      "step 6720000 \t return [-19.382126 -29.168266], ([0.0129168 0.406149 ]) \t loss 3.144E-01\n",
      "step 6730000 \t return [-19.339672 -29.07572 ], ([0.01857206 0.35055542]) \t loss 4.432E-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\liamm\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:179: RuntimeWarning: overflow encountered in reduce\n",
      "  ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 6740000 \t return [-19.344273 -29.16542 ], ([0.02072353 0.30655533]) \t loss 4.032E-01\n",
      "step 6750000 \t return [-19.334354 -29.13243 ], ([0.01785789 0.3399636 ]) \t loss 3.620E-01\n",
      "step 6760000 \t return [-19.332043 -29.106123], ([0.01281598 0.38035947]) \t loss 3.924E-01\n",
      "step 6770000 \t return [-19.341124 -29.095137], ([0.01064533 0.41202223]) \t loss 3.455E-01\n",
      "step 6780000 \t return [-19.31302  -29.085848], ([0.01523975 0.3603055 ]) \t loss 5.684E-01\n",
      "step 6790000 \t return [-19.32542  -29.118156], ([0.02134197 0.38380668]) \t loss 5.049E-01\n",
      "step 6800000 \t return [-19.310955 -29.105684], ([0.01754491 0.36859655]) \t loss 6.311E-01\n",
      "step 6810000 \t return [-19.297026 -29.132204], ([0.01759474 0.33058444]) \t loss 4.948E-01\n",
      "step 6820000 \t return [-19.301645 -29.13792 ], ([0.01278852 0.33218604]) \t loss 4.073E-01\n",
      "step 6830000 \t return [-19.263203 -29.051746], ([0.01723132 0.39374337]) \t loss 4.814E-01\n",
      "step 6840000 \t return [-19.260904 -28.978088], ([0.01590321 0.41768566]) \t loss 4.255E-01\n",
      "step 6850000 \t return [-19.23755  -29.122295], ([0.01195701 0.33941635]) \t loss 3.844E-01\n",
      "step 6860000 \t return [-19.244087 -29.095154], ([0.01335213 0.35111016]) \t loss 4.002E-01\n",
      "step 6870000 \t return [-19.214664 -29.019903], ([0.01270439 0.35723016]) \t loss 4.565E-01\n",
      "step 6880000 \t return [-19.192999 -28.98747 ], ([0.01854409 0.38145426]) \t loss 5.730E-01\n",
      "step 6890000 \t return [-19.233995 -29.07851 ], ([0.01624264 0.3604714 ]) \t loss 4.783E-01\n",
      "step 6900000 \t return [-19.172493 -28.925495], ([0.01304049 0.3849881 ]) \t loss 5.819E-01\n",
      "step 6910000 \t return [-19.163939 -28.967548], ([0.01615388 0.35057116]) \t loss 5.179E-01\n",
      "step 6920000 \t return [-19.17302  -28.951189], ([0.01611336 0.37591696]) \t loss 5.521E-01\n",
      "step 6930000 \t return [-19.15848 -28.90591], ([0.01757984 0.37238804]) \t loss 7.123E-01\n",
      "step 6940000 \t return [-19.138304 -28.952576], ([0.01155253 0.36576438]) \t loss 5.981E-01\n",
      "step 6950000 \t return [-19.14336  -28.922903], ([0.01385306 0.38400334]) \t loss 7.703E-01\n",
      "step 6960000 \t return [-19.147543 -28.9179  ], ([0.01475723 0.39241758]) \t loss 7.885E-01\n",
      "step 6970000 \t return [-19.109844 -28.911106], ([0.01125532 0.37284747]) \t loss 1.128E+00\n",
      "step 6980000 \t return [-19.10922  -28.859882], ([0.01391219 0.38738674]) \t loss 1.039E+00\n",
      "step 6990000 \t return [-19.097649 -28.942558], ([0.01547548 0.39055142]) \t loss 8.847E-01\n",
      "step 7000000 \t return [-19.154642 -28.90127 ], ([0.01884339 0.424534  ]) \t loss 7.900E-01\n",
      "step 7010000 \t return [-19.07929  -28.888832], ([0.01444149 0.37203476]) \t loss 8.291E-01\n",
      "step 7020000 \t return [-19.069757 -28.85355 ], ([0.01351761 0.36857682]) \t loss 9.986E-01\n",
      "step 7030000 \t return [-19.05449 -28.87854], ([0.01316305 0.3564467 ]) \t loss 1.119E+00\n",
      "step 7040000 \t return [-19.05057  -28.791481], ([0.01533366 0.3900407 ]) \t loss 9.248E-01\n",
      "step 7050000 \t return [-19.04633  -28.893305], ([0.01418967 0.37987274]) \t loss 1.009E+00\n",
      "step 7060000 \t return [-19.0492   -28.879847], ([0.013596   0.40243593]) \t loss 8.331E-01\n",
      "step 7070000 \t return [-19.019754 -28.824228], ([0.01136397 0.36735308]) \t loss 1.048E+00\n",
      "step 7080000 \t return [-19.007294 -28.813555], ([0.01061002 0.32343736]) \t loss 8.279E-01\n",
      "step 7090000 \t return [-19.024927 -28.806679], ([0.01377655 0.36912376]) \t loss 8.662E-01\n",
      "step 7100000 \t return [-18.97422 -28.71148], ([0.01173821 0.41263548]) \t loss 9.049E-01\n",
      "step 7110000 \t return [-18.950706 -28.753183], ([0.01421648 0.3569437 ]) \t loss 8.238E-01\n",
      "step 7120000 \t return [-18.938143 -28.763235], ([0.02248502 0.3940848 ]) \t loss 6.236E-01\n",
      "step 7130000 \t return [-18.895523 -28.67546 ], ([0.01248043 0.40432423]) \t loss 9.280E-01\n",
      "step 7140000 \t return [-18.88028  -28.635859], ([0.01678704 0.4032611 ]) \t loss 8.197E-01\n",
      "step 7150000 \t return [-18.870934 -28.592432], ([0.01227393 0.42348742]) \t loss 9.117E-01\n",
      "step 7160000 \t return [-18.842482 -28.65895 ], ([0.01452838 0.43939257]) \t loss 6.929E-01\n",
      "step 7170000 \t return [-18.893387 -28.702436], ([0.03206602 0.4078088 ]) \t loss 8.570E-01\n",
      "step 7180000 \t return [-18.82436 -28.60163], ([0.01784772 0.36852124]) \t loss 9.885E-01\n",
      "step 7190000 \t return [-18.800865 -28.570206], ([0.01549418 0.4228627 ]) \t loss 7.256E-01\n",
      "step 7200000 \t return [-18.789406 -28.64224 ], ([0.01914175 0.36650965]) \t loss 6.964E-01\n",
      "step 7210000 \t return [-18.75167  -28.591703], ([0.01331857 0.36315364]) \t loss 8.016E-01\n",
      "step 7220000 \t return [-18.713892 -28.446417], ([0.01281848 0.438878  ]) \t loss 6.440E-01\n",
      "step 7230000 \t return [-18.70157  -28.454361], ([0.01498578 0.42152137]) \t loss 7.581E-01\n",
      "step 7240000 \t return [-18.693022 -28.463646], ([0.02176642 0.38454008]) \t loss 1.014E+00\n",
      "step 7250000 \t return [-18.6544   -28.386608], ([0.01490968 0.4219541 ]) \t loss 7.230E-01\n",
      "step 7260000 \t return [-18.622395 -28.400566], ([0.01523369 0.42534408]) \t loss 6.735E-01\n",
      "step 7270000 \t return [-18.63357  -28.406887], ([0.02061224 0.4316253 ]) \t loss 9.027E-01\n",
      "step 7280000 \t return [-18.628408 -28.31012 ], ([0.01869791 0.45557183]) \t loss 1.022E+00\n",
      "step 7290000 \t return [-18.61512  -28.320974], ([0.01425258 0.47242028]) \t loss 9.784E-01\n",
      "step 7300000 \t return [-18.617956 -28.34516 ], ([0.01472313 0.42821312]) \t loss 1.073E+00\n",
      "step 7310000 \t return [-18.591442 -28.332964], ([0.01733503 0.41971055]) \t loss 1.070E+00\n",
      "step 7320000 \t return [-18.576336 -28.350178], ([0.01875901 0.38949278]) \t loss 9.565E-01\n",
      "step 7330000 \t return [-18.5665   -28.308582], ([0.01993703 0.4584948 ]) \t loss 9.146E-01\n",
      "step 7340000 \t return [-18.620436 -28.335983], ([0.02741993 0.44734955]) \t loss 1.286E+00\n",
      "step 7350000 \t return [-18.537817 -28.283308], ([0.02019038 0.41043794]) \t loss 1.062E+00\n",
      "step 7360000 \t return [-18.544632 -28.316648], ([0.01989199 0.37505546]) \t loss 9.497E-01\n",
      "step 7370000 \t return [-18.615791 -28.446068], ([0.02835188 0.3923487 ]) \t loss 9.503E-01\n",
      "step 7380000 \t return [-18.61771  -28.419302], ([0.01513029 0.39059177]) \t loss 1.072E+00\n",
      "step 7390000 \t return [-18.506723 -28.269852], ([0.0183106  0.42115107]) \t loss 1.480E+00\n",
      "step 7400000 \t return [-18.588974 -28.32597 ], ([0.01463887 0.41456905]) \t loss 1.289E+00\n",
      "step 7410000 \t return [-18.41912 -28.09105], ([0.01472465 0.48103985]) \t loss 1.278E+00\n",
      "step 7420000 \t return [-18.426298 -28.181568], ([0.02582667 0.43425617]) \t loss 1.212E+00\n",
      "step 7430000 \t return [-18.425386 -28.213072], ([0.02682932 0.45539317]) \t loss 1.182E+00\n",
      "step 7440000 \t return [-18.388456 -28.007654], ([0.01619929 0.43950033]) \t loss 1.192E+00\n",
      "step 7450000 \t return [-18.378464 -28.178867], ([0.01414145 0.3900283 ]) \t loss 1.225E+00\n",
      "step 7460000 \t return [-18.400936 -28.077732], ([0.02060733 0.4441993 ]) \t loss 1.272E+00\n",
      "step 7470000 \t return [-18.338665 -28.018118], ([0.01760916 0.46271747]) \t loss 1.110E+00\n",
      "step 7480000 \t return [-18.324295 -28.015007], ([0.0237175  0.48945743]) \t loss 1.046E+00\n",
      "step 7490000 \t return [-18.309206 -28.034111], ([0.02853899 0.45367473]) \t loss 1.347E+00\n",
      "step 7500000 \t return [-18.267189 -28.045801], ([0.01997494 0.43053377]) \t loss 1.167E+00\n",
      "step 7510000 \t return [-18.251907 -28.006279], ([0.01905983 0.45662096]) \t loss 1.362E+00\n",
      "step 7520000 \t return [-18.237614 -27.993444], ([0.01827782 0.4129075 ]) \t loss 1.228E+00\n",
      "step 7530000 \t return [-18.262548 -27.96233 ], ([0.01796657 0.44320425]) \t loss 1.158E+00\n",
      "step 7540000 \t return [-18.19836 -27.92728], ([0.02328721 0.44806278]) \t loss 1.222E+00\n",
      "step 7550000 \t return [-18.373327 -28.12716 ], ([0.03708974 0.43608597]) \t loss 1.261E+00\n",
      "step 7560000 \t return [-18.193975 -27.937527], ([0.0272926  0.47719377]) \t loss 1.247E+00\n",
      "step 7570000 \t return [-18.167522 -27.96971 ], ([0.02176516 0.36020142]) \t loss 9.148E-01\n",
      "step 7580000 \t return [-18.144869 -27.859793], ([0.01786121 0.43037117]) \t loss 8.463E-01\n",
      "step 7590000 \t return [-18.159035 -27.82047 ], ([0.03360731 0.49882182]) \t loss 9.235E-01\n",
      "step 7600000 \t return [-18.148327 -27.889088], ([0.0299099  0.40854052]) \t loss 9.234E-01\n",
      "step 7610000 \t return [-18.093311 -27.840525], ([0.02530024 0.476317  ]) \t loss 9.556E-01\n",
      "step 7620000 \t return [-18.075714 -27.738955], ([0.02732213 0.47037488]) \t loss 7.686E-01\n",
      "step 7630000 \t return [-18.035816 -27.838703], ([0.02507675 0.43268695]) \t loss 8.637E-01\n",
      "step 7640000 \t return [-18.00954  -27.603428], ([0.0274326 0.504303 ]) \t loss 7.203E-01\n",
      "step 7650000 \t return [-18.041187 -27.772175], ([0.04044986 0.5082521 ]) \t loss 8.915E-01\n",
      "step 7660000 \t return [-18.014277 -27.757536], ([0.02921761 0.46919346]) \t loss 1.053E+00\n",
      "step 7670000 \t return [-17.968641 -27.618853], ([0.01951871 0.4969644 ]) \t loss 9.745E-01\n",
      "step 7680000 \t return [-17.953093 -27.693098], ([0.02245479 0.47278947]) \t loss 9.207E-01\n",
      "step 7690000 \t return [-18.000957 -27.709621], ([0.04550803 0.5022515 ]) \t loss 8.600E-01\n",
      "step 7700000 \t return [-17.946705 -27.669504], ([0.01892442 0.46755284]) \t loss 1.108E+00\n",
      "step 7710000 \t return [-17.91337  -27.658916], ([0.02404644 0.43659356]) \t loss 1.195E+00\n",
      "step 7720000 \t return [-17.873983 -27.560616], ([0.03479688 0.50320196]) \t loss 8.973E-01\n",
      "step 7730000 \t return [-17.831005 -27.543262], ([0.02979762 0.5135639 ]) \t loss 9.616E-01\n",
      "step 7740000 \t return [-17.776644 -27.479673], ([0.02082494 0.5358222 ]) \t loss 9.430E-01\n",
      "step 7750000 \t return [-17.747833 -27.412983], ([0.023632   0.49518427]) \t loss 7.879E-01\n",
      "step 7760000 \t return [-17.782629 -27.446386], ([0.03881289 0.5168691 ]) \t loss 9.960E-01\n",
      "step 7770000 \t return [-17.743454 -27.42527 ], ([0.03195966 0.5259793 ]) \t loss 1.166E+00\n",
      "step 7780000 \t return [-17.732378 -27.412857], ([0.03192174 0.5551296 ]) \t loss 1.286E+00\n",
      "step 7790000 \t return [-17.715435 -27.334082], ([0.02830628 0.5511802 ]) \t loss 1.320E+00\n",
      "step 7800000 \t return [-17.739393 -27.354004], ([0.03996284 0.59480876]) \t loss 1.768E+00\n",
      "step 7810000 \t return [-17.702309 -27.34614 ], ([0.02777508 0.5786953 ]) \t loss 1.382E+00\n",
      "step 7820000 \t return [-17.747812 -27.40522 ], ([0.03497934 0.5860498 ]) \t loss 1.394E+00\n",
      "step 7830000 \t return [-17.725813 -27.44977 ], ([0.03218961 0.5212465 ]) \t loss 1.311E+00\n",
      "step 7840000 \t return [-17.7082   -27.424091], ([0.03129075 0.51658773]) \t loss 1.478E+00\n",
      "step 7850000 \t return [-17.68003 -27.30574], ([0.03287688 0.5207767 ]) \t loss 1.540E+00\n",
      "step 7860000 \t return [-17.814503 -27.45154 ], ([0.03841371 0.499268  ]) \t loss 1.415E+00\n",
      "step 7870000 \t return [-17.652454 -27.311718], ([0.0177305 0.5247511]) \t loss 1.553E+00\n",
      "step 7880000 \t return [-17.712214 -27.420662], ([0.02728879 0.5104234 ]) \t loss 1.332E+00\n",
      "step 7890000 \t return [-17.773554 -27.631582], ([0.03509907 0.4358504 ]) \t loss 1.328E+00\n",
      "step 7900000 \t return [-17.703245 -27.4383  ], ([0.03233744 0.5114262 ]) \t loss 1.385E+00\n",
      "step 7910000 \t return [-17.708036 -27.382925], ([0.03049605 0.52074105]) \t loss 1.324E+00\n",
      "step 7920000 \t return [-17.72443  -27.395395], ([0.03102823 0.4963719 ]) \t loss 1.125E+00\n",
      "step 7930000 \t return [-17.73885 -27.41762], ([0.03172813 0.54834   ]) \t loss 1.124E+00\n",
      "step 7940000 \t return [-17.699545 -27.304537], ([0.04121456 0.58430773]) \t loss 1.474E+00\n",
      "step 7950000 \t return [-17.627054 -27.195057], ([0.02498732 0.6498587 ]) \t loss 1.476E+00\n",
      "step 7960000 \t return [-17.602835 -27.26818 ], ([0.02257206 0.58942723]) \t loss 1.299E+00\n",
      "step 7970000 \t return [-17.650358 -27.293648], ([0.03981414 0.5411292 ]) \t loss 1.585E+00\n",
      "step 7980000 \t return [-17.638115 -27.390049], ([0.03694521 0.47283304]) \t loss 1.405E+00\n",
      "step 7990000 \t return [-17.609032 -27.19138 ], ([0.02884597 0.57503206]) \t loss 1.344E+00\n",
      "step 8000000 \t return [-17.603449 -27.285446], ([0.03109404 0.59482825]) \t loss 1.210E+00\n",
      "step 8010000 \t return [-17.572277 -27.173029], ([0.02439288 0.60474783]) \t loss 1.360E+00\n",
      "step 8020000 \t return [-17.588774 -27.242931], ([0.03495437 0.57826716]) \t loss 1.410E+00\n",
      "step 8030000 \t return [-17.647236 -27.332674], ([0.04743772 0.5763911 ]) \t loss 1.165E+00\n",
      "step 8040000 \t return [-17.604053 -27.234137], ([0.03931639 0.57014596]) \t loss 1.464E+00\n",
      "step 8050000 \t return [-17.604744 -27.296616], ([0.03691734 0.53240144]) \t loss 1.880E+00\n",
      "step 8060000 \t return [-17.534908 -27.178848], ([0.03028681 0.56056947]) \t loss 1.486E+00\n",
      "step 8070000 \t return [-17.559828 -27.23224 ], ([0.03000184 0.53692764]) \t loss 1.456E+00\n",
      "step 8080000 \t return [-17.455317 -27.107803], ([0.03396103 0.55386597]) \t loss 1.255E+00\n",
      "step 8090000 \t return [-17.56089 -27.16126], ([0.03714977 0.60546964]) \t loss 1.238E+00\n",
      "step 8100000 \t return [-17.560528 -27.216091], ([0.04186388 0.5644902 ]) \t loss 1.350E+00\n",
      "step 8110000 \t return [-17.656588 -27.433897], ([0.04652124 0.5226095 ]) \t loss 1.248E+00\n",
      "step 8120000 \t return [-17.434177 -27.066505], ([0.03481333 0.56525654]) \t loss 1.594E+00\n",
      "step 8130000 \t return [-17.374737 -27.083935], ([0.03037885 0.60097   ]) \t loss 1.430E+00\n",
      "step 8140000 \t return [-17.413937 -27.103376], ([0.04689903 0.57370734]) \t loss 1.542E+00\n",
      "step 8150000 \t return [-17.410955 -27.078442], ([0.04290903 0.5522151 ]) \t loss 1.476E+00\n",
      "step 8160000 \t return [-17.322884 -26.972708], ([0.02996812 0.63840353]) \t loss 1.581E+00\n",
      "step 8170000 \t return [-17.333744 -26.9981  ], ([0.03265586 0.5911804 ]) \t loss 1.559E+00\n",
      "step 8180000 \t return [-17.33244  -26.943825], ([0.03716823 0.586273  ]) \t loss 1.654E+00\n",
      "step 8190000 \t return [-17.312397 -26.907988], ([0.03969701 0.5953891 ]) \t loss 1.538E+00\n",
      "step 8200000 \t return [-17.32061  -26.919493], ([0.04720242 0.63589054]) \t loss 1.418E+00\n",
      "step 8210000 \t return [-17.46823  -27.009777], ([0.05778415 0.64124745]) \t loss 1.614E+00\n",
      "step 8220000 \t return [-17.264446 -26.866768], ([0.03412559 0.57687926]) \t loss 2.020E+00\n",
      "step 8230000 \t return [-17.236992 -26.896677], ([0.03771406 0.587624  ]) \t loss 1.570E+00\n",
      "step 8240000 \t return [-17.422483 -27.090967], ([0.04281878 0.55452967]) \t loss 1.484E+00\n",
      "step 8250000 \t return [-17.312471 -26.944973], ([0.06488132 0.6430625 ]) \t loss 1.831E+00\n",
      "step 8260000 \t return [-17.171694 -26.73139 ], ([0.04678182 0.62657374]) \t loss 1.749E+00\n",
      "step 8270000 \t return [-17.262367 -26.795076], ([0.04360431 0.62887853]) \t loss 1.539E+00\n",
      "step 8280000 \t return [-17.283175 -26.855812], ([0.03022264 0.6671802 ]) \t loss 1.258E+00\n",
      "step 8290000 \t return [-17.319803 -26.911938], ([0.03506655 0.6699743 ]) \t loss 1.226E+00\n",
      "step 8300000 \t return [-17.301838 -26.910278], ([0.03303275 0.5629169 ]) \t loss 1.503E+00\n",
      "step 8310000 \t return [-17.161215 -26.768282], ([0.0481602 0.6415092]) \t loss 1.293E+00\n",
      "step 8320000 \t return [-17.305567 -26.956055], ([0.04208837 0.5976286 ]) \t loss 1.523E+00\n",
      "step 8330000 \t return [-17.13581  -26.632324], ([0.0516178  0.67527235]) \t loss 1.402E+00\n",
      "step 8340000 \t return [-17.05145  -26.644775], ([0.04853305 0.66136557]) \t loss 1.346E+00\n",
      "step 8350000 \t return [-17.082273 -26.667376], ([0.05555743 0.64497477]) \t loss 1.260E+00\n",
      "step 8360000 \t return [-17.06696 -26.53565], ([0.07156644 0.69069034]) \t loss 1.080E+00\n",
      "step 8370000 \t return [-17.015736 -26.554558], ([0.06478515 0.73886955]) \t loss 1.219E+00\n",
      "step 8380000 \t return [-16.893723 -26.507708], ([0.0338651 0.5844597]) \t loss 1.029E+00\n",
      "step 8390000 \t return [-16.952015 -26.481987], ([0.07006966 0.7319126 ]) \t loss 1.049E+00\n",
      "step 8400000 \t return [-16.888538 -26.522768], ([0.0521637 0.6194601]) \t loss 1.179E+00\n",
      "step 8410000 \t return [-16.974443 -26.626953], ([0.05744311 0.66517395]) \t loss 1.187E+00\n",
      "step 8420000 \t return [-16.933514 -26.506458], ([0.05535818 0.6709869 ]) \t loss 1.443E+00\n",
      "step 8430000 \t return [-16.89809  -26.562933], ([0.05328877 0.62531036]) \t loss 1.113E+00\n",
      "step 8440000 \t return [-16.740475 -26.38063 ], ([0.08368003 0.63125205]) \t loss 1.176E+00\n",
      "step 8450000 \t return [-16.71136  -26.207954], ([0.04505479 0.6904401 ]) \t loss 1.240E+00\n",
      "step 8460000 \t return [-16.207132 -25.791862], ([0.07301701 0.6847836 ]) \t loss 1.000E+00\n",
      "step 8470000 \t return [-16.153076 -25.679316], ([0.07436697 0.77745086]) \t loss 8.873E-01\n",
      "step 8480000 \t return [-16.153622 -25.70743 ], ([0.0564409 0.7491996]) \t loss 1.133E+00\n",
      "step 8490000 \t return [-16.160313 -25.724714], ([0.06253104 0.718507  ]) \t loss 1.563E+00\n",
      "step 8500000 \t return [-16.20573 -25.7822 ], ([0.08888669 0.77972937]) \t loss 1.801E+00\n",
      "step 8510000 \t return [-16.23498  -25.746422], ([0.0980549 0.8180871]) \t loss 2.363E+00\n",
      "step 8520000 \t return [-16.03682  -25.526272], ([0.07092528 0.8448784 ]) \t loss 3.168E+00\n",
      "step 8530000 \t return [-16.027695 -25.38907 ], ([0.095054   0.95398283]) \t loss 1.744E+00\n",
      "step 8540000 \t return [-16.133228 -25.591097], ([0.09780006 0.8294067 ]) \t loss 2.040E+00\n",
      "step 8550000 \t return [-15.963702 -25.411638], ([0.07920154 0.87295586]) \t loss 3.065E+00\n",
      "step 8560000 \t return [-16.05488  -25.534845], ([0.09158876 0.8191325 ]) \t loss 2.761E+00\n",
      "step 8570000 \t return [-16.006151 -25.52531 ], ([0.09304724 0.89171255]) \t loss 3.036E+00\n",
      "step 8580000 \t return [-16.023565 -25.540142], ([0.09085222 0.8393465 ]) \t loss 3.157E+00\n",
      "step 8590000 \t return [-16.367403 -25.834627], ([0.10037431 0.85886675]) \t loss 4.311E+00\n",
      "step 8600000 \t return [-16.451546 -25.869217], ([0.09464642 0.8115576 ]) \t loss 7.409E+00\n",
      "step 8610000 \t return [-15.991669 -25.298748], ([0.11197864 1.018248  ]) \t loss 1.154E+01\n",
      "step 8620000 \t return [-16.078123 -25.388   ], ([0.10987923 0.89373034]) \t loss 6.920E+00\n",
      "step 8630000 \t return [-16.058687 -25.383184], ([0.10549474 0.9211151 ]) \t loss 6.523E+00\n",
      "step 8640000 \t return [-16.192318 -25.622103], ([0.11402541 0.92166525]) \t loss 5.953E+00\n",
      "step 8650000 \t return [-16.29444  -25.754122], ([0.11976591 0.93027014]) \t loss 6.885E+00\n",
      "step 8660000 \t return [-16.295692 -25.645454], ([0.13186024 0.94497085]) \t loss 7.783E+00\n",
      "step 8670000 \t return [-16.508268 -26.055098], ([0.12689234 0.8493326 ]) \t loss 8.531E+00\n",
      "step 8680000 \t return [-16.347559 -25.744524], ([0.14879507 0.9833631 ]) \t loss 1.204E+01\n",
      "step 8690000 \t return [-15.881652 -25.071514], ([0.11575206 1.1679634 ]) \t loss 1.042E+01\n",
      "step 8700000 \t return [-15.816712 -25.107187], ([0.09672364 1.0276616 ]) \t loss 7.698E+00\n",
      "step 8710000 \t return [-15.568435 -25.036753], ([0.05566138 0.8894544 ]) \t loss 7.120E+00\n",
      "step 8720000 \t return [-16.215662 -25.613525], ([0.1033226  0.90189964]) \t loss 6.269E+00\n",
      "step 8730000 \t return [-16.284199 -25.520557], ([0.18163757 1.1144365 ]) \t loss 9.262E+00\n",
      "step 8740000 \t return [-16.06726  -25.465086], ([0.1658853 0.992681 ]) \t loss 1.088E+01\n",
      "step 8750000 \t return [-16.090452 -25.510452], ([0.17244262 0.98724097]) \t loss 8.666E+00\n",
      "step 8760000 \t return [-16.03371  -25.502243], ([0.12508294 0.95803034]) \t loss 8.822E+00\n",
      "step 8770000 \t return [-15.73529  -25.026785], ([0.13052957 1.0030471 ]) \t loss 7.314E+00\n",
      "step 8780000 \t return [-16.216877 -25.667198], ([0.12769201 0.90796846]) \t loss 6.247E+00\n",
      "step 8790000 \t return [-15.7603245 -25.127161 ], ([0.14494985 0.97343457]) \t loss 7.244E+00\n",
      "step 8800000 \t return [-16.240812 -25.639793], ([0.10568068 0.8298769 ]) \t loss 6.029E+00\n",
      "step 8810000 \t return [-16.468176 -25.902573], ([0.16093954 1.037627  ]) \t loss 9.120E+00\n",
      "step 8820000 \t return [-16.155798 -25.806753], ([0.12390622 0.86257017]) \t loss 1.052E+01\n",
      "step 8830000 \t return [-15.883082 -25.415688], ([0.14313132 0.89916   ]) \t loss 7.336E+00\n",
      "step 8840000 \t return [-16.016298 -25.309107], ([0.14639823 1.0703679 ]) \t loss 6.646E+00\n",
      "step 8850000 \t return [-16.505484 -25.054434], ([2.443626  1.7573868]) \t loss 7.071E+00\n",
      "step 8860000 \t return [-22.924282 -18.536108], ([0.86606896 1.1605946 ]) \t loss 1.108E+01\n",
      "step 8870000 \t return [-26.402119 -16.37812 ], ([1.3798869 0.6443749]) \t loss 1.106E+01\n",
      "step 8880000 \t return [-27.09172 -16.08718], ([1.2044452 0.6524308]) \t loss 9.727E+00\n",
      "step 8890000 \t return [-29.455557 -15.794141], ([0.78264946 0.9380647 ]) \t loss 8.219E+00\n",
      "step 8900000 \t return [-29.150549 -15.808031], ([1.180011  1.0420274]) \t loss 6.650E+00\n",
      "step 8910000 \t return [-27.94193  -16.040651], ([1.3928809  0.93937045]) \t loss 6.006E+00\n",
      "step 8920000 \t return [-26.802435 -16.653955], ([1.8128521 1.0494261]) \t loss 4.690E+00\n",
      "step 8930000 \t return [-26.873533 -16.590107], ([2.7814827 1.0177907]) \t loss 5.169E+00\n",
      "step 8940000 \t return [-27.641384 -16.118126], ([2.0805078 0.9608893]) \t loss 5.762E+00\n",
      "step 8950000 \t return [-27.520418 -16.584837], ([2.0669017  0.95548713]) \t loss 5.107E+00\n",
      "step 8960000 \t return [-28.236265 -16.430344], ([2.0445309 1.0981677]) \t loss 5.438E+00\n",
      "step 8970000 \t return [-27.086584 -16.33587 ], ([2.0850701  0.83864033]) \t loss 5.077E+00\n",
      "step 8980000 \t return [-29.141582 -14.914475], ([2.0726478 0.9885574]) \t loss 5.668E+00\n",
      "step 8990000 \t return [-27.691313 -14.960008], ([2.585723 1.211984]) \t loss 4.688E+00\n",
      "step 9000000 \t return [-17.130884 -17.4783  ], ([0.3283757  0.86651605]) \t loss 5.480E+00\n",
      "step 9010000 \t return [-24.337318 -15.335204], ([2.7425172 0.750879 ]) \t loss 7.255E+00\n",
      "step 9020000 \t return [-18.165035 -15.649006], ([2.1230729  0.42703313]) \t loss 3.873E+00\n",
      "step 9030000 \t return [-28.108526 -14.548911], ([0.73729146 0.6700695 ]) \t loss 3.616E+00\n",
      "step 9040000 \t return [-14.057787 -23.446379], ([0.11332312 0.85801184]) \t loss 2.241E+00\n",
      "step 9050000 \t return [-26.370173 -14.233953], ([0.74445826 0.56571585]) \t loss 2.495E+00\n",
      "step 9060000 \t return [-18.777275 -14.332239], ([0.73383886 0.577747  ]) \t loss 1.875E+00\n",
      "step 9070000 \t return [-18.026997 -14.352412], ([0.7791114 0.5568748]) \t loss 1.828E+00\n",
      "step 9080000 \t return [-12.754939 -19.163403], ([0.8826492 1.8687098]) \t loss 1.962E+00\n",
      "step 9090000 \t return [-18.581676 -14.268702], ([0.57748926 0.48998395]) \t loss 5.787E+00\n",
      "step 9100000 \t return [-16.627909 -16.154638], ([0.810585   0.45401284]) \t loss 1.720E+00\n",
      "step 9110000 \t return [-15.283597 -16.294912], ([0.77722067 0.5696278 ]) \t loss 2.182E+00\n",
      "step 9120000 \t return [-16.848421 -15.73162 ], ([0.81331617 0.56260717]) \t loss 2.154E+00\n",
      "step 9130000 \t return [-14.2599535 -17.279764 ], ([1.0808004 1.336672 ]) \t loss 2.242E+00\n",
      "step 9140000 \t return [-11.896227 -17.179985], ([0.92288196 0.8479801 ]) \t loss 2.782E+00\n",
      "step 9150000 \t return [-10.402527 -17.001305], ([0.37855121 0.64890885]) \t loss 2.979E+00\n",
      "step 9160000 \t return [-10.302638 -18.218718], ([4.3766813 1.2278214]) \t loss 3.045E+00\n",
      "step 9170000 \t return [ -9.235825 -15.928496], ([0.87515444 0.63797426]) \t loss 3.362E+00\n",
      "step 9180000 \t return [ -9.815415 -15.578391], ([1.2538738 0.5585796]) \t loss 3.300E+00\n",
      "step 9190000 \t return [-10.336244 -15.628158], ([1.9697977 0.4856467]) \t loss 2.540E+00\n",
      "step 9200000 \t return [-11.435064 -15.359797], ([1.21832    0.65443355]) \t loss 2.378E+00\n",
      "step 9210000 \t return [-18.860325 -14.772977], ([1.3712078  0.62547594]) \t loss 1.701E+00\n",
      "step 9220000 \t return [-13.134472 -14.970043], ([1.297942   0.58726543]) \t loss 1.825E+00\n",
      "step 9230000 \t return [-19.989004 -13.716791], ([1.1346877  0.40982616]) \t loss 1.669E+00\n",
      "step 9240000 \t return [-13.24349  -15.072488], ([1.3584534 0.6578034]) \t loss 1.156E+00\n",
      "step 9250000 \t return [ -9.792824 -15.330461], ([1.3423065  0.63861346]) \t loss 1.354E+00\n",
      "step 9260000 \t return [ -8.6911545 -14.872095 ], ([1.8325363  0.49624017]) \t loss 1.260E+00\n",
      "step 9270000 \t return [-11.017717 -14.81176 ], ([4.6618614 0.5210374]) \t loss 1.644E+00\n",
      "step 9280000 \t return [-23.487669 -13.12629 ], ([2.1199248 0.423992 ]) \t loss 1.908E+00\n",
      "step 9290000 \t return [-20.947294 -12.991616], ([1.6595186  0.23193876]) \t loss 1.435E+00\n",
      "step 9300000 \t return [-17.687864 -13.770394], ([1.4203043  0.44891775]) \t loss 1.273E+00\n",
      "step 9310000 \t return [ -9.905741 -14.399911], ([4.4981422  0.43366995]) \t loss 1.505E+00\n",
      "step 9320000 \t return [ -8.26274  -14.155866], ([5.5372066  0.43297616]) \t loss 1.773E+00\n",
      "step 9330000 \t return [ -9.090164 -13.596344], ([4.479219   0.35302654]) \t loss 1.550E+00\n",
      "step 9340000 \t return [-10.647564 -13.368357], ([5.152472  0.3655064]) \t loss 1.690E+00\n",
      "step 9350000 \t return [ -5.3911214 -13.7574215], ([2.0586941  0.64727813]) \t loss 1.886E+00\n",
      "step 9360000 \t return [-28.249363 -12.593018], ([3.0952368  0.31669983]) \t loss 1.550E+00\n",
      "step 9370000 \t return [ -8.089669 -13.392762], ([5.9821887 0.8450478]) \t loss 1.310E+00\n",
      "step 9380000 \t return [-11.195179 -13.51293 ], ([3.5925694  0.61951333]) \t loss 1.399E+00\n",
      "step 9390000 \t return [-18.070871 -12.999852], ([2.9488175  0.47852853]) \t loss 1.917E+00\n",
      "step 9400000 \t return [ -9.313367 -13.302621], ([3.9821317  0.33073446]) \t loss 1.507E+00\n",
      "step 9410000 \t return [ -4.770422 -13.322524], ([1.5274067 0.8984602]) \t loss 1.284E+00\n",
      "step 9420000 \t return [ -5.2406797 -12.909637 ], ([2.16868    0.73047626]) \t loss 1.050E+00\n",
      "step 9430000 \t return [ -5.279684 -12.942465], ([2.6934102 0.8238578]) \t loss 1.144E+00\n",
      "step 9440000 \t return [ -5.297296 -13.155627], ([2.3796544 0.8474811]) \t loss 9.872E-01\n",
      "step 9450000 \t return [ -6.3866425 -12.7185135], ([2.886499   0.37503657]) \t loss 9.097E-01\n",
      "step 9460000 \t return [ -4.2475343 -12.693579 ], ([1.3020021  0.84633404]) \t loss 7.747E-01\n",
      "step 9470000 \t return [ -4.0038667 -12.669539 ], ([1.0106337  0.81350154]) \t loss 7.279E-01\n",
      "step 9480000 \t return [ -4.8035626 -12.35368  ], ([2.635467   0.71387535]) \t loss 6.347E-01\n",
      "step 9490000 \t return [ -4.3671274 -12.201176 ], ([1.5233324  0.83042574]) \t loss 6.196E-01\n",
      "step 9500000 \t return [ -4.99884 -12.12053], ([3.1213973 0.7358217]) \t loss 6.152E-01\n",
      "step 9510000 \t return [ -3.7252448 -12.208762 ], ([1.0547905 0.8572893]) \t loss 5.690E-01\n",
      "step 9520000 \t return [ -3.8187962 -12.234855 ], ([1.7192512  0.89250803]) \t loss 6.987E-01\n",
      "step 9530000 \t return [ -3.7379794 -12.070522 ], ([1.4495844  0.87067145]) \t loss 7.190E-01\n",
      "step 9540000 \t return [ -5.813492 -11.529868], ([2.0592663  0.86625487]) \t loss 7.501E-01\n",
      "step 9550000 \t return [ -4.707298 -11.45116 ], ([1.3190452 0.7311574]) \t loss 7.290E-01\n",
      "step 9560000 \t return [ -4.247259 -11.492876], ([1.7455999  0.72534174]) \t loss 6.982E-01\n",
      "step 9570000 \t return [ -3.4816492 -11.553637 ], ([1.6857073  0.84867734]) \t loss 6.753E-01\n",
      "step 9580000 \t return [-11.260267 -11.080587], ([2.3230329 0.6301476]) \t loss 6.722E-01\n",
      "step 9590000 \t return [-20.880499 -10.906515], ([2.6805491 0.5948088]) \t loss 6.332E-01\n",
      "step 9600000 \t return [-11.158648 -11.058935], ([3.1013906  0.84090376]) \t loss 7.526E-01\n",
      "step 9610000 \t return [ -4.9958754 -11.825386 ], ([1.7027144 1.1523495]) \t loss 1.103E+00\n",
      "step 9620000 \t return [ -8.137765 -11.063951], ([1.7844411 0.7946748]) \t loss 1.951E+00\n",
      "step 9630000 \t return [-13.605682 -10.588235], ([3.1337774 0.5927531]) \t loss 1.020E+00\n",
      "step 9640000 \t return [ -3.1838427 -11.453647 ], ([1.4388475 0.8752794]) \t loss 7.137E-01\n",
      "step 9650000 \t return [ -4.6170387 -11.182981 ], ([2.7858186  0.72679466]) \t loss 7.104E-01\n",
      "step 9660000 \t return [ -3.621967 -11.516845], ([1.9546777 0.9693285]) \t loss 5.996E-01\n",
      "step 9670000 \t return [ -5.0646405 -11.10856  ], ([2.6670399  0.70084286]) \t loss 6.158E-01\n",
      "step 9680000 \t return [ -8.743569 -10.798144], ([3.0530005  0.60591877]) \t loss 5.309E-01\n",
      "step 9690000 \t return [ -4.869598  -10.9810915], ([2.03996   0.8384842]) \t loss 4.697E-01\n",
      "step 9700000 \t return [ -4.9013352 -11.135996 ], ([1.3958273 0.844297 ]) \t loss 5.989E-01\n",
      "step 9710000 \t return [ -4.854746 -10.985125], ([1.3626887  0.86716175]) \t loss 6.149E-01\n",
      "step 9720000 \t return [ -9.741742 -10.483393], ([2.869601  0.5712026]) \t loss 7.633E-01\n",
      "step 9730000 \t return [ -5.890533 -10.86514 ], ([1.6925317  0.66196835]) \t loss 5.035E-01\n",
      "step 9740000 \t return [ -7.363314 -10.654435], ([1.8340769  0.73000836]) \t loss 4.437E-01\n",
      "step 9750000 \t return [ -2.8532279 -11.463663 ], ([1.6586952 0.7967068]) \t loss 5.605E-01\n",
      "step 9760000 \t return [ -5.927891 -10.924169], ([1.8181524 0.6982052]) \t loss 4.534E-01\n",
      "step 9770000 \t return [ -5.1379046 -10.921935 ], ([1.817738 1.030454]) \t loss 4.229E-01\n",
      "step 9780000 \t return [ -4.864885 -11.001242], ([2.0237026 0.7200114]) \t loss 7.893E-01\n",
      "step 9790000 \t return [ -4.2561283 -11.004345 ], ([2.2750313  0.78249913]) \t loss 3.681E-01\n",
      "step 9800000 \t return [ -8.652529 -10.46402 ], ([2.384423 0.826747]) \t loss 4.211E-01\n",
      "step 9810000 \t return [ -3.557468 -11.040588], ([2.2559216 0.778031 ]) \t loss 4.368E-01\n",
      "step 9820000 \t return [ -5.0733104 -10.953056 ], ([3.0285492  0.70818585]) \t loss 3.624E-01\n",
      "step 9830000 \t return [ -3.3316967 -11.309112 ], ([2.0571663  0.78203106]) \t loss 7.847E-01\n",
      "step 9840000 \t return [ -6.206635 -10.600901], ([1.5930556  0.89362866]) \t loss 3.125E-01\n",
      "step 9850000 \t return [ -7.911161 -10.713182], ([1.7010882 0.9204242]) \t loss 4.577E-01\n",
      "step 9860000 \t return [-13.974532 -10.091547], ([4.664768   0.62470174]) \t loss 5.459E-01\n",
      "step 9870000 \t return [ -4.758468  -10.8776245], ([1.8422121  0.76325405]) \t loss 4.007E-01\n",
      "step 9880000 \t return [ -4.196708 -10.93167 ], ([2.0302036  0.88636005]) \t loss 4.102E-01\n",
      "step 9890000 \t return [ -3.4832795 -11.032152 ], ([2.0579789 0.7560813]) \t loss 4.762E-01\n",
      "step 9900000 \t return [ -4.0779696 -10.906589 ], ([2.8670146  0.80522096]) \t loss 4.649E-01\n",
      "step 9910000 \t return [ -3.2592163 -10.914862 ], ([2.1139865  0.79577315]) \t loss 2.856E-01\n",
      "step 9920000 \t return [-13.241875 -10.141635], ([3.334792  0.6442707]) \t loss 3.292E-01\n",
      "step 9930000 \t return [ -3.7783556 -10.759601 ], ([2.0838354  0.97399354]) \t loss 3.550E-01\n",
      "step 9940000 \t return [ -3.926287 -11.081221], ([2.9377816  0.75963753]) \t loss 4.951E-01\n",
      "step 9950000 \t return [ -3.6912212 -10.833263 ], ([2.7768724 0.8970992]) \t loss 3.370E-01\n",
      "step 9960000 \t return [ -7.685649 -10.635045], ([1.5521746 0.9071647]) \t loss 4.549E-01\n",
      "step 9970000 \t return [ -3.1291769 -10.743543 ], ([2.4147758  0.74614155]) \t loss 4.234E-01\n",
      "step 9980000 \t return [ -3.6839728 -10.722212 ], ([2.9912348 0.9293038]) \t loss 3.474E-01\n",
      "step 9990000 \t return [ -8.437276 -10.482547], ([2.3941524  0.87737095]) \t loss 3.771E-01\n",
      "step 10000000 \t return [ -3.5534716 -10.82378  ], ([3.1128123 0.9932983]) \t loss 3.540E-01\n"
     ]
    }
   ],
   "source": [
    "env = mo_gym.make('water-reservoir-v0', normalized_action=False, nO=2, penalize=True, time_limit=100)\n",
    "\n",
    "env = ScaleReward(env)\n",
    "\n",
    "PCNAgent = PCN(env, np.array((0.1, 0.1, 0.1), dtype=np.float32), continuous_actions=True, noise=5.0)\n",
    "\n",
    "max_return = np.zeros(2)\n",
    "\n",
    "PCNAgent.train(10000000, env, np.array((-100.0,-100.0), dtype=np.float32), num_step_episodes=100,max_return=max_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_pf('./noise5_decay.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a scaling of 0.1 for the returns improves them by a significant amount, but there is still room for improvement if we want to reach the same performance as gpi...\n",
    "\n",
    "Try to increase the scaling factor even more:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:pzpqeksf) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">water-reservoir-v0__PCN__None__1683905100</strong> at: <a href='https://wandb.ai/vub-ai/MORL-Baselines/runs/pzpqeksf' target=\"_blank\">https://wandb.ai/vub-ai/MORL-Baselines/runs/pzpqeksf</a><br/>Synced 7 W&B file(s), 100 media file(s), 100 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230512_172500-pzpqeksf\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:pzpqeksf). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7c0b4183be14b5e9c772912c0383bd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016933333332902596, max=1.0â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\liamm\\water-resource-management\\PCN\\wandb\\run-20230512_193928-3tlo5gne</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/vub-ai/MORL-Baselines/runs/3tlo5gne' target=\"_blank\">water-reservoir-v0__PCN__None__1683913168</a></strong> to <a href='https://wandb.ai/vub-ai/MORL-Baselines' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/vub-ai/MORL-Baselines' target=\"_blank\">https://wandb.ai/vub-ai/MORL-Baselines</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/vub-ai/MORL-Baselines/runs/3tlo5gne' target=\"_blank\">https://wandb.ai/vub-ai/MORL-Baselines/runs/3tlo5gne</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Found log directory outside of given root_logdir, dropping given root_logdir for event file in /tmp/PCN\n",
      "c:\\Users\\liamm\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "c:\\Users\\liamm\\anaconda3\\lib\\site-packages\\morl_baselines\\multi_policy\\pcn\\pcn.py:411: RuntimeWarning: overflow encountered in exp\n",
      "  ent = np.sum(-np.exp(lp) * lp)\n",
      "c:\\Users\\liamm\\anaconda3\\lib\\site-packages\\morl_baselines\\multi_policy\\pcn\\pcn.py:411: RuntimeWarning: overflow encountered in multiply\n",
      "  ent = np.sum(-np.exp(lp) * lp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 60000 \t return [-71.99667 -81.93241], ([0.43030995 0.5780136 ]) \t loss 8.776E+03\n",
      "step 70000 \t return [-78.04584 -87.99494], ([0.40900967 0.48895338]) \t loss 4.281E+03\n",
      "step 80000 \t return [-70.53062 -80.46731], ([0.30726284 0.4242694 ]) \t loss 3.215E+03\n",
      "step 90000 \t return [-66.817696 -76.75815 ], ([0.19571498 0.30260658]) \t loss 2.205E+03\n",
      "step 100000 \t return [-66.24229 -76.18389], ([0.17172727 0.29622653]) \t loss 1.083E+03\n",
      "step 110000 \t return [-66.680534 -76.64029 ], ([0.16138366 0.2657518 ]) \t loss 5.077E+02\n",
      "step 120000 \t return [-66.1188  -76.05401], ([0.14426862 0.21125801]) \t loss 1.596E+02\n",
      "step 130000 \t return [-65.635956 -75.53744 ], ([0.11425579 0.21633823]) \t loss 8.232E+01\n",
      "step 140000 \t return [-65.96396 -75.89455], ([0.14464469 0.20212756]) \t loss 6.022E+01\n",
      "step 150000 \t return [-65.7296  -75.67471], ([0.13864219 0.22043188]) \t loss 4.015E+01\n",
      "step 160000 \t return [-65.434715 -75.372215], ([0.14000195 0.21538502]) \t loss 4.109E+01\n",
      "step 170000 \t return [-65.223656 -75.18277 ], ([0.15098608 0.23244573]) \t loss 3.130E+01\n",
      "step 180000 \t return [-65.609436 -75.509125], ([0.15171973 0.22315304]) \t loss 3.301E+01\n",
      "step 190000 \t return [-65.41303 -75.33101], ([0.1362256  0.23013675]) \t loss 3.831E+01\n",
      "step 200000 \t return [-65.38104 -75.27594], ([0.14128764 0.22562267]) \t loss 2.933E+01\n",
      "step 210000 \t return [-65.449615 -75.381516], ([0.13577826 0.20917165]) \t loss 3.092E+01\n",
      "step 220000 \t return [-65.35345  -75.322945], ([0.13374987 0.2218407 ]) \t loss 3.599E+01\n",
      "step 230000 \t return [-65.4807   -75.417244], ([0.14251556 0.21335298]) \t loss 3.533E+01\n",
      "step 240000 \t return [-65.33487  -75.301506], ([0.12726173 0.21231334]) \t loss 2.881E+01\n",
      "step 250000 \t return [-65.35893 -75.29294], ([0.14100769 0.22519311]) \t loss 3.996E+01\n",
      "step 260000 \t return [-65.42305  -75.373634], ([0.14100367 0.23744483]) \t loss 3.667E+01\n",
      "step 270000 \t return [-65.19158 -75.11829], ([0.14010608 0.23499897]) \t loss 4.135E+01\n",
      "step 280000 \t return [-65.17059 -75.10256], ([0.12912643 0.21849017]) \t loss 3.801E+01\n",
      "step 290000 \t return [-65.55258  -75.520096], ([0.14085238 0.21300477]) \t loss 3.002E+01\n",
      "step 300000 \t return [-65.359055 -75.308426], ([0.13828215 0.23454908]) \t loss 4.032E+01\n",
      "step 310000 \t return [-65.32827 -75.28105], ([0.13321045 0.20636739]) \t loss 4.165E+01\n",
      "step 320000 \t return [-65.3296  -75.20624], ([0.12975916 0.20298436]) \t loss 3.475E+01\n",
      "step 330000 \t return [-65.19281 -75.1368 ], ([0.12672667 0.23585428]) \t loss 3.089E+01\n",
      "step 340000 \t return [-65.07639 -75.00752], ([0.12136592 0.2562551 ]) \t loss 3.357E+01\n",
      "step 350000 \t return [-65.13448 -75.06144], ([0.13460375 0.2383151 ]) \t loss 4.399E+01\n",
      "step 360000 \t return [-65.495735 -75.39879 ], ([0.1237912  0.23592663]) \t loss 3.269E+01\n",
      "step 370000 \t return [-65.205315 -75.11422 ], ([0.14230247 0.20858777]) \t loss 3.716E+01\n",
      "step 380000 \t return [-65.436775 -75.38796 ], ([0.12918842 0.21671118]) \t loss 3.364E+01\n",
      "step 390000 \t return [-65.28504  -75.207985], ([0.12517199 0.23243484]) \t loss 2.430E+01\n",
      "step 400000 \t return [-65.63901 -75.56671], ([0.14726362 0.21561536]) \t loss 3.273E+01\n",
      "step 410000 \t return [-64.97016 -74.9261 ], ([0.13182537 0.20924664]) \t loss 3.037E+01\n",
      "step 420000 \t return [-65.10287 -75.04782], ([0.13349393 0.24251375]) \t loss 2.899E+01\n",
      "step 430000 \t return [-64.99739 -74.91946], ([0.1421023  0.22160521]) \t loss 3.372E+01\n",
      "step 440000 \t return [-64.90314  -74.803635], ([0.12740044 0.21241128]) \t loss 2.984E+01\n",
      "step 450000 \t return [-65.07395 -75.0122 ], ([0.13441953 0.24223806]) \t loss 4.391E+01\n",
      "step 460000 \t return [-65.33356 -75.24693], ([0.11566323 0.2411816 ]) \t loss 3.757E+01\n",
      "step 470000 \t return [-64.931404 -74.85488 ], ([0.13792539 0.24152671]) \t loss 3.395E+01\n",
      "step 480000 \t return [-64.94814 -74.85605], ([0.1406696  0.21924561]) \t loss 3.851E+01\n",
      "step 490000 \t return [-65.06952 -74.9952 ], ([0.12367003 0.24259087]) \t loss 3.901E+01\n",
      "step 500000 \t return [-64.97352 -74.9106 ], ([0.11626859 0.21996231]) \t loss 3.259E+01\n",
      "step 510000 \t return [-65.036545 -74.963425], ([0.13159059 0.2204339 ]) \t loss 2.748E+01\n",
      "step 520000 \t return [-64.60653 -74.53653], ([0.13039605 0.21497951]) \t loss 4.753E+01\n",
      "step 530000 \t return [-64.7593  -74.71909], ([0.14480151 0.17850284]) \t loss 3.320E+01\n",
      "step 540000 \t return [-64.85855 -74.80061], ([0.13136539 0.1902779 ]) \t loss 3.976E+01\n",
      "step 550000 \t return [-64.63724 -74.58165], ([0.11815918 0.1999053 ]) \t loss 4.253E+01\n",
      "step 560000 \t return [-64.51224  -74.446365], ([0.12553033 0.21286847]) \t loss 2.761E+01\n",
      "step 570000 \t return [-64.657845 -74.579285], ([0.13896684 0.20624115]) \t loss 3.102E+01\n",
      "step 580000 \t return [-64.32675 -74.24007], ([0.14487477 0.20345984]) \t loss 3.219E+01\n",
      "step 590000 \t return [-63.92107  -73.822464], ([0.13836761 0.21514863]) \t loss 3.996E+01\n",
      "step 600000 \t return [-64.31477 -74.22183], ([0.13328727 0.20024472]) \t loss 3.200E+01\n",
      "step 610000 \t return [-64.0438  -73.98855], ([0.13590151 0.22187904]) \t loss 4.028E+01\n",
      "step 620000 \t return [-63.690556 -73.63188 ], ([0.11108395 0.23088934]) \t loss 3.020E+01\n",
      "step 630000 \t return [-63.4022  -73.33232], ([0.11246696 0.22022328]) \t loss 4.096E+01\n",
      "step 640000 \t return [-63.717525 -73.656   ], ([0.13610059 0.20003149]) \t loss 3.609E+01\n",
      "step 650000 \t return [-63.438847 -73.37673 ], ([0.11628167 0.2205574 ]) \t loss 2.893E+01\n",
      "step 660000 \t return [-63.49604 -73.46105], ([0.11501817 0.20562364]) \t loss 2.506E+01\n",
      "step 670000 \t return [-63.32962 -73.27552], ([0.11628086 0.24385059]) \t loss 2.638E+01\n",
      "step 680000 \t return [-63.389687 -73.33908 ], ([0.13338934 0.23204117]) \t loss 2.436E+01\n",
      "step 690000 \t return [-63.354477 -73.25564 ], ([0.10657799 0.22378092]) \t loss 2.603E+01\n",
      "step 700000 \t return [-63.437572 -73.383606], ([0.13449736 0.21419512]) \t loss 2.449E+01\n",
      "step 710000 \t return [-63.03333 -72.99148], ([0.11291991 0.21960083]) \t loss 2.745E+01\n",
      "step 720000 \t return [-62.801178 -72.725975], ([0.10715733 0.20554513]) \t loss 2.480E+01\n",
      "step 730000 \t return [-63.03425 -72.90497], ([0.11181471 0.20043193]) \t loss 2.481E+01\n",
      "step 740000 \t return [-62.759342 -72.69159 ], ([0.12276249 0.23490885]) \t loss 2.387E+01\n",
      "step 750000 \t return [-62.5524  -72.46528], ([0.10344259 0.19773766]) \t loss 2.277E+01\n",
      "step 760000 \t return [-62.551464 -72.46077 ], ([0.11884721 0.1908697 ]) \t loss 2.303E+01\n",
      "step 770000 \t return [-62.417305 -72.35297 ], ([0.10026872 0.21933053]) \t loss 2.245E+01\n",
      "step 780000 \t return [-62.400013 -72.27988 ], ([0.13347712 0.21511059]) \t loss 2.307E+01\n",
      "step 790000 \t return [-62.51937 -72.40564], ([0.12388839 0.21422927]) \t loss 2.367E+01\n",
      "step 800000 \t return [-62.470055 -72.35274 ], ([0.11679734 0.19479059]) \t loss 2.322E+01\n",
      "step 810000 \t return [-62.48701 -72.41875], ([0.10634976 0.2347193 ]) \t loss 2.355E+01\n",
      "step 820000 \t return [-62.269302 -72.18322 ], ([0.10584783 0.1930098 ]) \t loss 2.346E+01\n",
      "step 830000 \t return [-62.32321  -72.292145], ([0.12654066 0.17994747]) \t loss 2.334E+01\n",
      "step 840000 \t return [-62.30324 -72.20363], ([0.09378417 0.18849468]) \t loss 2.378E+01\n",
      "step 850000 \t return [-62.268517 -72.179535], ([0.12604573 0.20389014]) \t loss 2.493E+01\n",
      "step 860000 \t return [-62.071407 -72.00811 ], ([0.10131829 0.19802651]) \t loss 2.507E+01\n",
      "step 870000 \t return [-61.85084 -71.7687 ], ([0.10900129 0.2018811 ]) \t loss 2.389E+01\n",
      "step 880000 \t return [-61.712925 -71.608475], ([0.11945517 0.18367945]) \t loss 2.500E+01\n",
      "step 890000 \t return [-61.831615 -71.78935 ], ([0.10718719 0.20787263]) \t loss 2.464E+01\n",
      "step 900000 \t return [-61.668125 -71.59962 ], ([0.10874638 0.17972882]) \t loss 2.464E+01\n",
      "step 910000 \t return [-61.715996 -71.59964 ], ([0.11972556 0.21714616]) \t loss 2.352E+01\n",
      "step 920000 \t return [-61.425426 -71.345695], ([0.09304918 0.2043123 ]) \t loss 2.460E+01\n",
      "step 930000 \t return [-61.430702 -71.31169 ], ([0.13245581 0.1885682 ]) \t loss 2.407E+01\n",
      "step 940000 \t return [-61.37783 -71.3054 ], ([0.13185638 0.2059492 ]) \t loss 2.177E+01\n",
      "step 950000 \t return [-61.418514 -71.34287 ], ([0.15264757 0.19367056]) \t loss 2.217E+01\n",
      "step 960000 \t return [-61.328823 -71.22212 ], ([0.12645681 0.16999568]) \t loss 2.367E+01\n",
      "step 970000 \t return [-61.289906 -71.20288 ], ([0.09163123 0.20510155]) \t loss 2.324E+01\n",
      "step 980000 \t return [-61.084488 -71.02811 ], ([0.12239888 0.18844894]) \t loss 2.283E+01\n",
      "step 990000 \t return [-60.927624 -70.87999 ], ([0.11857338 0.17970428]) \t loss 2.170E+01\n",
      "step 1000000 \t return [-60.84808 -70.71032], ([0.16481426 0.17860779]) \t loss 2.279E+01\n",
      "step 1010000 \t return [-60.829056 -70.75093 ], ([0.0843844  0.20060018]) \t loss 2.223E+01\n",
      "step 1020000 \t return [-60.810932 -70.68469 ], ([0.15113997 0.1612785 ]) \t loss 2.117E+01\n",
      "step 1030000 \t return [-60.739548 -70.637985], ([0.12739393 0.16383474]) \t loss 2.293E+01\n",
      "step 1040000 \t return [-60.835136 -70.756256], ([0.08192496 0.19017228]) \t loss 2.302E+01\n",
      "step 1050000 \t return [-60.837593 -70.80616 ], ([0.08544198 0.24132161]) \t loss 2.212E+01\n",
      "step 1060000 \t return [-60.717358 -70.65279 ], ([0.1237745  0.16441262]) \t loss 2.150E+01\n",
      "step 1070000 \t return [-60.746147 -70.65943 ], ([0.13756467 0.16413751]) \t loss 2.213E+01\n",
      "step 1080000 \t return [-60.667416 -70.543816], ([0.14525883 0.15314819]) \t loss 2.252E+01\n",
      "step 1090000 \t return [-60.469727 -70.417175], ([0.08773197 0.1978532 ]) \t loss 2.216E+01\n",
      "step 1100000 \t return [-60.642475 -70.59607 ], ([0.1592468  0.13695271]) \t loss 2.271E+01\n",
      "step 1110000 \t return [-60.38061  -70.336525], ([0.10441232 0.20211126]) \t loss 2.329E+01\n",
      "step 1120000 \t return [-60.340626 -70.2295  ], ([0.13703233 0.17166445]) \t loss 2.285E+01\n",
      "step 1130000 \t return [-60.2066  -70.13681], ([0.07969863 0.19708087]) \t loss 2.120E+01\n",
      "step 1140000 \t return [-60.063267 -69.989235], ([0.09104021 0.20028336]) \t loss 2.227E+01\n",
      "step 1150000 \t return [-59.98294 -69.94597], ([0.09108344 0.19169714]) \t loss 2.173E+01\n",
      "step 1160000 \t return [-60.000908 -69.897736], ([0.11267398 0.18553057]) \t loss 2.106E+01\n",
      "step 1170000 \t return [-59.866627 -69.74314 ], ([0.09913543 0.18350357]) \t loss 2.244E+01\n",
      "step 1180000 \t return [-59.81055 -69.67524], ([0.07980206 0.24455392]) \t loss 2.181E+01\n",
      "step 1190000 \t return [-59.573795 -69.512985], ([0.11231516 0.21501091]) \t loss 2.136E+01\n",
      "step 1200000 \t return [-59.483883 -69.43726 ], ([0.09561898 0.17645766]) \t loss 2.399E+01\n",
      "step 1210000 \t return [-59.587563 -69.489975], ([0.12662289 0.1763482 ]) \t loss 2.255E+01\n",
      "step 1220000 \t return [-59.49024 -69.41092], ([0.13683638 0.14965007]) \t loss 2.248E+01\n",
      "step 1230000 \t return [-59.454536 -69.39224 ], ([0.08257018 0.18942228]) \t loss 2.364E+01\n",
      "step 1240000 \t return [-59.432877 -69.35123 ], ([0.08546241 0.21263413]) \t loss 2.288E+01\n",
      "step 1250000 \t return [-59.44438  -69.334206], ([0.13558859 0.14353837]) \t loss 2.362E+01\n",
      "step 1260000 \t return [-59.42849 -69.34801], ([0.18244399 0.14462598]) \t loss 2.240E+01\n",
      "step 1270000 \t return [-59.33474 -69.24366], ([0.1253042  0.19021514]) \t loss 2.258E+01\n",
      "step 1280000 \t return [-59.220497 -69.15036 ], ([0.11296407 0.19350718]) \t loss 2.347E+01\n",
      "step 1290000 \t return [-59.33293 -69.23379], ([0.08054442 0.22533679]) \t loss 2.130E+01\n",
      "step 1300000 \t return [-59.216015 -69.11026 ], ([0.06582156 0.2094045 ]) \t loss 2.162E+01\n",
      "step 1310000 \t return [-59.148636 -69.061745], ([0.11379047 0.19982025]) \t loss 2.261E+01\n",
      "step 1320000 \t return [-59.081165 -69.03243 ], ([0.13181688 0.16311853]) \t loss 2.144E+01\n",
      "step 1330000 \t return [-59.14497 -69.03973], ([0.06992421 0.21502976]) \t loss 2.174E+01\n",
      "step 1340000 \t return [-59.016464 -68.91187 ], ([0.15202406 0.17829467]) \t loss 2.078E+01\n",
      "step 1350000 \t return [-58.949905 -68.858826], ([0.13884787 0.17101742]) \t loss 2.228E+01\n",
      "step 1360000 \t return [-58.9542  -68.86193], ([0.08363126 0.20430793]) \t loss 2.235E+01\n",
      "step 1370000 \t return [-59.015785 -68.955986], ([0.1534724  0.14298014]) \t loss 2.207E+01\n",
      "step 1380000 \t return [-58.842525 -68.81273 ], ([0.1358451  0.16767558]) \t loss 2.030E+01\n",
      "step 1390000 \t return [-58.815006 -68.74538 ], ([0.13435979 0.15184894]) \t loss 2.006E+01\n",
      "step 1400000 \t return [-58.66736  -68.571915], ([0.13469382 0.18242326]) \t loss 2.112E+01\n",
      "step 1410000 \t return [-58.60094 -68.50043], ([0.07096552 0.21007603]) \t loss 2.117E+01\n",
      "step 1420000 \t return [-58.689716 -68.606674], ([0.17566109 0.14976181]) \t loss 2.042E+01\n",
      "step 1430000 \t return [-58.635597 -68.562904], ([0.16532518 0.14456214]) \t loss 1.969E+01\n",
      "step 1440000 \t return [-58.50724 -68.44395], ([0.08111898 0.18645367]) \t loss 2.070E+01\n",
      "step 1450000 \t return [-58.39026  -68.339355], ([0.10517403 0.1749359 ]) \t loss 2.102E+01\n",
      "step 1460000 \t return [-58.319717 -68.219475], ([0.12909493 0.16878755]) \t loss 2.096E+01\n",
      "step 1470000 \t return [-58.27106 -68.15771], ([0.14182438 0.16687559]) \t loss 2.150E+01\n",
      "step 1480000 \t return [-58.18225 -68.13028], ([0.12179659 0.1983371 ]) \t loss 1.996E+01\n",
      "step 1490000 \t return [-58.17234 -68.11675], ([0.11958539 0.18565604]) \t loss 2.126E+01\n",
      "step 1500000 \t return [-58.141094 -68.10229 ], ([0.09919219 0.21062458]) \t loss 2.131E+01\n",
      "step 1510000 \t return [-58.18729 -68.12158], ([0.12623008 0.18559606]) \t loss 2.064E+01\n",
      "step 1520000 \t return [-58.333797 -68.24967 ], ([0.15230098 0.12343141]) \t loss 2.087E+01\n",
      "step 1530000 \t return [-58.136036 -68.07343 ], ([0.08907184 0.20365812]) \t loss 2.113E+01\n",
      "step 1540000 \t return [-58.159542 -68.095116], ([0.1602363  0.16105881]) \t loss 2.037E+01\n",
      "step 1550000 \t return [-58.00373 -67.92918], ([0.11591554 0.16371073]) \t loss 2.086E+01\n",
      "step 1560000 \t return [-57.999718 -67.885635], ([0.14727004 0.17892231]) \t loss 1.996E+01\n",
      "step 1570000 \t return [-57.931183 -67.8811  ], ([0.15103827 0.18685713]) \t loss 2.094E+01\n",
      "step 1580000 \t return [-57.876812 -67.880325], ([0.09480632 0.1833634 ]) \t loss 2.214E+01\n",
      "step 1590000 \t return [-58.05488 -68.00356], ([0.193694   0.20140386]) \t loss 2.027E+01\n",
      "step 1600000 \t return [-57.94882 -67.87103], ([0.11285757 0.18691508]) \t loss 2.171E+01\n",
      "step 1610000 \t return [-57.83869 -67.75379], ([0.1429185  0.17224585]) \t loss 2.139E+01\n",
      "step 1620000 \t return [-57.75949 -67.70205], ([0.12945229 0.15883487]) \t loss 2.136E+01\n",
      "step 1630000 \t return [-57.673996 -67.602325], ([0.10980234 0.16708381]) \t loss 1.987E+01\n",
      "step 1640000 \t return [-57.60648 -67.53778], ([0.13247019 0.1740259 ]) \t loss 2.023E+01\n",
      "step 1650000 \t return [-57.508717 -67.470764], ([0.10544871 0.18671922]) \t loss 2.013E+01\n",
      "step 1660000 \t return [-57.411133 -67.31216 ], ([0.0815336  0.18355842]) \t loss 2.017E+01\n",
      "step 1670000 \t return [-57.397373 -67.2977  ], ([0.13499227 0.18980755]) \t loss 1.988E+01\n",
      "step 1680000 \t return [-57.24607  -67.142456], ([0.08671256 0.22274667]) \t loss 2.091E+01\n",
      "step 1690000 \t return [-57.26817 -67.23124], ([0.14524569 0.16597222]) \t loss 1.950E+01\n",
      "step 1700000 \t return [-57.190887 -67.12182 ], ([0.11086001 0.17683436]) \t loss 1.953E+01\n",
      "step 1710000 \t return [-57.143917 -67.07534 ], ([0.13086936 0.16002591]) \t loss 2.314E+01\n",
      "step 1720000 \t return [-57.054955 -66.94801 ], ([0.13178597 0.18326296]) \t loss 2.163E+01\n",
      "step 1730000 \t return [-57.320606 -67.26477 ], ([0.19038035 0.14105625]) \t loss 2.161E+01\n",
      "step 1740000 \t return [-56.94587 -66.8843 ], ([0.08978368 0.24428274]) \t loss 2.079E+01\n",
      "step 1750000 \t return [-56.919266 -66.841805], ([0.07841093 0.228937  ]) \t loss 2.089E+01\n",
      "step 1760000 \t return [-56.936813 -66.822945], ([0.1342752  0.16019912]) \t loss 2.036E+01\n",
      "step 1770000 \t return [-56.7692   -66.686066], ([0.09464201 0.17371921]) \t loss 2.099E+01\n",
      "step 1780000 \t return [-56.734985 -66.64253 ], ([0.07178927 0.23010175]) \t loss 2.090E+01\n",
      "step 1790000 \t return [-56.66176  -66.522125], ([0.07870398 0.2647972 ]) \t loss 1.919E+01\n",
      "step 1800000 \t return [-56.437798 -66.330505], ([0.07962317 0.2475849 ]) \t loss 2.008E+01\n",
      "step 1810000 \t return [-56.641582 -66.57676 ], ([0.14837703 0.16874959]) \t loss 2.013E+01\n",
      "step 1820000 \t return [-56.276485 -66.15649 ], ([0.07110497 0.2376336 ]) \t loss 2.202E+01\n",
      "step 1830000 \t return [-56.211803 -66.14548 ], ([0.08289943 0.25289947]) \t loss 1.982E+01\n",
      "step 1840000 \t return [-56.268467 -66.14842 ], ([0.07314444 0.22337745]) \t loss 1.935E+01\n",
      "step 1850000 \t return [-56.149868 -66.02243 ], ([0.07396619 0.23176491]) \t loss 2.059E+01\n",
      "step 1860000 \t return [-56.107452 -66.0185  ], ([0.08627384 0.23320903]) \t loss 2.042E+01\n",
      "step 1870000 \t return [-56.137535 -66.0314  ], ([0.07555585 0.2571699 ]) \t loss 2.001E+01\n",
      "step 1880000 \t return [-56.117477 -66.02929 ], ([0.13078819 0.17390634]) \t loss 1.982E+01\n",
      "step 1890000 \t return [-55.99752 -65.91036], ([0.07427163 0.19358745]) \t loss 1.860E+01\n",
      "step 1900000 \t return [-56.02251 -65.9227 ], ([0.07493431 0.19562595]) \t loss 1.922E+01\n",
      "step 1910000 \t return [-55.867165 -65.779686], ([0.10805982 0.22001946]) \t loss 1.741E+01\n",
      "step 1920000 \t return [-55.830025 -65.67809 ], ([0.09609742 0.21582705]) \t loss 1.827E+01\n",
      "step 1930000 \t return [-55.64206 -65.54221], ([0.07650039 0.2315979 ]) \t loss 1.962E+01\n",
      "step 1940000 \t return [-55.69175 -65.59827], ([0.11933304 0.18173589]) \t loss 1.982E+01\n",
      "step 1950000 \t return [-55.282047 -65.18706 ], ([0.10044955 0.29832494]) \t loss 1.965E+01\n",
      "step 1960000 \t return [-54.23929 -64.14909], ([0.21975    0.43170962]) \t loss 1.961E+01\n",
      "step 1970000 \t return [-53.76045  -63.626915], ([0.08173542 0.23186684]) \t loss 2.155E+01\n",
      "step 1980000 \t return [-53.866146 -63.815758], ([0.07033943 0.24161468]) \t loss 2.189E+01\n",
      "step 1990000 \t return [-53.643463 -63.55333 ], ([0.07631689 0.26089063]) \t loss 2.051E+01\n",
      "step 2000000 \t return [-53.37355 -63.26564], ([0.08851337 0.28158787]) \t loss 2.050E+01\n",
      "step 2010000 \t return [-53.361862 -63.228394], ([0.08706665 0.20965892]) \t loss 2.042E+01\n",
      "step 2020000 \t return [-53.32366  -63.251503], ([0.06940193 0.2669782 ]) \t loss 2.060E+01\n",
      "step 2030000 \t return [-53.298706 -63.245975], ([0.06318856 0.28772122]) \t loss 2.007E+01\n",
      "step 2040000 \t return [-53.208893 -63.05006 ], ([0.06493419 0.24344757]) \t loss 1.920E+01\n",
      "step 2050000 \t return [-53.13757  -63.016434], ([0.08283158 0.21790576]) \t loss 2.083E+01\n",
      "step 2060000 \t return [-53.21062  -63.100185], ([0.09854163 0.23809902]) \t loss 1.819E+01\n",
      "step 2070000 \t return [-53.228626 -63.120438], ([0.06600595 0.2441983 ]) \t loss 1.777E+01\n",
      "step 2080000 \t return [-52.93132  -62.818134], ([0.0565587  0.27625918]) \t loss 1.870E+01\n",
      "step 2090000 \t return [-52.83818 -62.68452], ([0.05430901 0.2458892 ]) \t loss 2.129E+01\n",
      "step 2100000 \t return [-52.791435 -62.71493 ], ([0.05361111 0.23028244]) \t loss 2.059E+01\n",
      "step 2110000 \t return [-52.92607  -62.835396], ([0.04403286 0.26056775]) \t loss 1.984E+01\n",
      "step 2120000 \t return [-52.830666 -62.749977], ([0.04678374 0.2374153 ]) \t loss 1.972E+01\n",
      "step 2130000 \t return [-52.729923 -62.618843], ([0.05726364 0.24376512]) \t loss 1.804E+01\n",
      "step 2140000 \t return [-52.905895 -62.762413], ([0.04091164 0.27691102]) \t loss 1.880E+01\n",
      "step 2150000 \t return [-52.864548 -62.777245], ([0.18816681 0.13337506]) \t loss 1.963E+01\n",
      "step 2160000 \t return [-52.97334  -62.882725], ([0.03964617 0.2871667 ]) \t loss 1.930E+01\n",
      "step 2170000 \t return [-52.73816 -62.59761], ([0.09992559 0.18421337]) \t loss 1.983E+01\n",
      "step 2180000 \t return [-52.668533 -62.56491 ], ([0.08510391 0.23282737]) \t loss 1.872E+01\n",
      "step 2190000 \t return [-52.718433 -62.638214], ([0.04595223 0.28068098]) \t loss 1.816E+01\n",
      "step 2200000 \t return [-52.66418  -62.587337], ([0.12019537 0.19221532]) \t loss 1.983E+01\n",
      "step 2210000 \t return [-52.958485 -62.848   ], ([0.03986321 0.24947912]) \t loss 1.971E+01\n",
      "step 2220000 \t return [-52.59012  -62.546154], ([0.08100905 0.21226607]) \t loss 1.919E+01\n",
      "step 2230000 \t return [-52.643078 -62.545902], ([0.05755023 0.24027656]) \t loss 1.882E+01\n",
      "step 2240000 \t return [-52.688393 -62.565292], ([0.13017786 0.21888115]) \t loss 1.867E+01\n",
      "step 2250000 \t return [-52.67485 -62.63008], ([0.04410318 0.28754985]) \t loss 1.923E+01\n",
      "step 2260000 \t return [-52.67152  -62.539806], ([0.06262038 0.26097998]) \t loss 1.907E+01\n",
      "step 2270000 \t return [-52.70347 -62.61161], ([0.05307283 0.27668348]) \t loss 1.913E+01\n",
      "step 2280000 \t return [-52.613914 -62.512794], ([0.14189996 0.18208596]) \t loss 1.828E+01\n",
      "step 2290000 \t return [-52.804474 -62.646847], ([0.18798746 0.13212639]) \t loss 1.801E+01\n",
      "step 2300000 \t return [-52.622295 -62.55816 ], ([0.06209243 0.23701434]) \t loss 1.862E+01\n",
      "step 2310000 \t return [-52.76027 -62.66233], ([0.04584773 0.2696421 ]) \t loss 1.855E+01\n",
      "step 2320000 \t return [-52.60237  -62.506657], ([0.12139019 0.16396946]) \t loss 1.938E+01\n",
      "step 2330000 \t return [-52.47879  -62.436283], ([0.09462205 0.23016974]) \t loss 1.889E+01\n",
      "step 2340000 \t return [-52.504307 -62.373802], ([0.14140311 0.16738711]) \t loss 1.852E+01\n",
      "step 2350000 \t return [-52.471664 -62.35045 ], ([0.1168796  0.20630781]) \t loss 1.933E+01\n",
      "step 2360000 \t return [-52.73699  -62.601524], ([0.1856247  0.11295898]) \t loss 1.930E+01\n",
      "step 2370000 \t return [-52.3819  -62.27406], ([0.08196742 0.1933747 ]) \t loss 2.007E+01\n",
      "step 2380000 \t return [-52.384    -62.262997], ([0.13788565 0.17866065]) \t loss 1.817E+01\n",
      "step 2390000 \t return [-52.44264  -62.358204], ([0.04819867 0.24141243]) \t loss 1.827E+01\n",
      "step 2400000 \t return [-52.349136 -62.27285 ], ([0.14085023 0.18587802]) \t loss 1.878E+01\n",
      "step 2410000 \t return [-52.435013 -62.358997], ([0.0550959  0.24096794]) \t loss 1.845E+01\n",
      "step 2420000 \t return [-52.34279 -62.23462], ([0.0572532 0.2102554]) \t loss 1.835E+01\n",
      "step 2430000 \t return [-52.18186  -62.087852], ([0.04925084 0.2425211 ]) \t loss 1.767E+01\n",
      "step 2440000 \t return [-52.089474 -61.969204], ([0.05544374 0.2622641 ]) \t loss 1.841E+01\n",
      "step 2450000 \t return [-51.91316 -61.77319], ([0.06110038 0.2534003 ]) \t loss 2.001E+01\n",
      "step 2460000 \t return [-51.895653 -61.84218 ], ([0.12457656 0.21422593]) \t loss 1.857E+01\n",
      "step 2470000 \t return [-51.88717 -61.71642], ([0.07030632 0.24481666]) \t loss 1.845E+01\n",
      "step 2480000 \t return [-51.87061 -61.72826], ([0.0565809  0.28137925]) \t loss 1.924E+01\n",
      "step 2490000 \t return [-51.80273  -61.695206], ([0.04817063 0.26741484]) \t loss 1.903E+01\n",
      "step 2500000 \t return [-51.848896 -61.714733], ([0.05099187 0.28362474]) \t loss 1.795E+01\n",
      "step 2510000 \t return [-51.796978 -61.6723  ], ([0.04496913 0.24823684]) \t loss 1.856E+01\n",
      "step 2520000 \t return [-51.82887 -61.7027 ], ([0.08879023 0.25652435]) \t loss 1.876E+01\n",
      "step 2530000 \t return [-51.791485 -61.6769  ], ([0.04966279 0.29996112]) \t loss 1.872E+01\n",
      "step 2540000 \t return [-51.80767 -61.68431], ([0.04534511 0.26558825]) \t loss 1.862E+01\n",
      "step 2550000 \t return [-51.51503  -61.395016], ([0.14173481 0.34850264]) \t loss 1.818E+01\n",
      "step 2560000 \t return [-50.635815 -60.573505], ([0.07531773 0.24491711]) \t loss 1.978E+01\n",
      "step 2570000 \t return [-50.60184 -60.42974], ([0.08781301 0.21356447]) \t loss 1.984E+01\n",
      "step 2580000 \t return [-50.61126 -60.50416], ([0.10176703 0.21710356]) \t loss 1.907E+01\n",
      "step 2590000 \t return [-50.557568 -60.47365 ], ([0.07321983 0.2424428 ]) \t loss 2.018E+01\n",
      "step 2600000 \t return [-50.5356   -60.423775], ([0.0758361  0.26104698]) \t loss 1.767E+01\n",
      "step 2610000 \t return [-50.597538 -60.47447 ], ([0.09468406 0.22890882]) \t loss 1.779E+01\n",
      "step 2620000 \t return [-50.41174  -60.328003], ([0.06138755 0.2637046 ]) \t loss 1.812E+01\n",
      "step 2630000 \t return [-50.68135  -60.558495], ([0.05476105 0.298166  ]) \t loss 1.798E+01\n",
      "step 2640000 \t return [-50.458546 -60.299404], ([0.10694202 0.23144536]) \t loss 2.011E+01\n",
      "step 2650000 \t return [-50.358433 -60.223934], ([0.06220923 0.25647318]) \t loss 1.878E+01\n",
      "step 2660000 \t return [-50.39584  -60.276997], ([0.10810998 0.19265406]) \t loss 1.746E+01\n",
      "step 2670000 \t return [-50.29541 -60.13205], ([0.10843443 0.31234187]) \t loss 1.711E+01\n",
      "step 2680000 \t return [-49.611237 -59.548874], ([0.11017801 0.22499478]) \t loss 1.817E+01\n",
      "step 2690000 \t return [-49.508793 -59.429985], ([0.09528195 0.20825559]) \t loss 1.940E+01\n",
      "step 2700000 \t return [-49.48284  -59.290924], ([0.0820404  0.22008926]) \t loss 1.812E+01\n",
      "step 2710000 \t return [-49.325317 -59.233585], ([0.07101745 0.264388  ]) \t loss 1.764E+01\n",
      "step 2720000 \t return [-49.2772   -59.135643], ([0.05816783 0.26769197]) \t loss 1.942E+01\n",
      "step 2730000 \t return [-49.22888  -59.091793], ([0.0490687  0.30641586]) \t loss 1.865E+01\n",
      "step 2740000 \t return [-49.133892 -59.011673], ([0.12043958 0.37884176]) \t loss 1.774E+01\n",
      "step 2750000 \t return [-48.45227  -58.302998], ([0.0667658  0.26189807]) \t loss 1.938E+01\n",
      "step 2760000 \t return [-48.39373  -58.228966], ([0.05848854 0.29186717]) \t loss 1.809E+01\n",
      "step 2770000 \t return [-48.333775 -58.200947], ([0.06852739 0.28567508]) \t loss 1.936E+01\n",
      "step 2780000 \t return [-48.399246 -58.2678  ], ([0.0863442  0.24448115]) \t loss 1.931E+01\n",
      "step 2790000 \t return [-48.276196 -58.140587], ([0.0680065  0.27800328]) \t loss 1.692E+01\n",
      "step 2800000 \t return [-48.2669  -58.13881], ([0.05860171 0.32842022]) \t loss 1.831E+01\n",
      "step 2810000 \t return [-48.260357 -58.12712 ], ([0.04494497 0.2793119 ]) \t loss 1.851E+01\n",
      "step 2820000 \t return [-48.32863  -58.189697], ([0.05044946 0.2750149 ]) \t loss 1.773E+01\n",
      "step 2830000 \t return [-48.29239 -58.19263], ([0.04636933 0.3002735 ]) \t loss 1.889E+01\n",
      "step 2840000 \t return [-48.190598 -58.068047], ([0.09562391 0.31018007]) \t loss 1.924E+01\n",
      "step 2850000 \t return [-47.62438 -57.43075], ([0.05875765 0.29838678]) \t loss 1.839E+01\n",
      "step 2860000 \t return [-47.63851 -57.49445], ([0.06506786 0.2376355 ]) \t loss 1.930E+01\n",
      "step 2870000 \t return [-47.653885 -57.50724 ], ([0.05979182 0.27029172]) \t loss 1.778E+01\n",
      "step 2880000 \t return [-47.614716 -57.46735 ], ([0.1024745  0.22502273]) \t loss 1.897E+01\n",
      "step 2890000 \t return [-47.521317 -57.375492], ([0.05825909 0.34250653]) \t loss 1.878E+01\n",
      "step 2900000 \t return [-47.376358 -57.244633], ([0.05395362 0.30631116]) \t loss 1.662E+01\n",
      "step 2910000 \t return [-47.340645 -57.237392], ([0.06380837 0.27974072]) \t loss 1.857E+01\n",
      "step 2920000 \t return [-47.34943 -57.24974], ([0.05285572 0.28539652]) \t loss 1.785E+01\n",
      "step 2930000 \t return [-47.34621  -57.206104], ([0.06497404 0.28394037]) \t loss 1.773E+01\n",
      "step 2940000 \t return [-47.389263 -57.31653 ], ([0.05951042 0.30366638]) \t loss 1.986E+01\n",
      "step 2950000 \t return [-47.33102  -57.234493], ([0.12614204 0.3586525 ]) \t loss 1.967E+01\n",
      "step 2960000 \t return [-46.3862  -56.30024], ([0.048537  0.2671284]) \t loss 1.876E+01\n",
      "step 2970000 \t return [-46.35938 -56.23108], ([0.04459523 0.35789174]) \t loss 1.702E+01\n",
      "step 2980000 \t return [-46.421368 -56.256973], ([0.05021136 0.34535423]) \t loss 1.836E+01\n",
      "step 2990000 \t return [-46.33252 -56.16619], ([0.05010971 0.30239156]) \t loss 2.258E+01\n",
      "step 3000000 \t return [-46.30288 -56.15247], ([0.06930125 0.28977406]) \t loss 2.027E+01\n",
      "step 3010000 \t return [-46.281147 -56.154537], ([0.08189671 0.29765624]) \t loss 1.812E+01\n",
      "step 3020000 \t return [-46.36888  -56.242798], ([0.09826452 0.2461108 ]) \t loss 1.739E+01\n",
      "step 3030000 \t return [-46.234756 -56.031612], ([0.06826875 0.34706613]) \t loss 1.788E+01\n",
      "step 3040000 \t return [-46.184193 -55.983067], ([0.05267137 0.27152276]) \t loss 1.768E+01\n",
      "step 3050000 \t return [-46.22822  -56.109352], ([0.0460579 0.3197393]) \t loss 1.688E+01\n",
      "step 3060000 \t return [-45.25968  -55.009655], ([0.91279894 1.1447742 ]) \t loss 1.855E+01\n",
      "step 3070000 \t return [-42.753628 -52.56161 ], ([0.08517223 0.31793103]) \t loss 2.050E+01\n",
      "step 3080000 \t return [-42.842194 -52.71772 ], ([0.06113493 0.2922378 ]) \t loss 1.974E+01\n",
      "step 3090000 \t return [-42.72642 -52.56552], ([0.08784259 0.2623508 ]) \t loss 1.996E+01\n",
      "step 3100000 \t return [-42.88105 -52.75172], ([0.04254223 0.33848697]) \t loss 1.822E+01\n",
      "step 3110000 \t return [-42.706978 -52.547626], ([0.07777029 0.31327122]) \t loss 1.969E+01\n",
      "step 3120000 \t return [-42.85594  -52.744396], ([0.05114274 0.36360916]) \t loss 2.225E+01\n",
      "step 3130000 \t return [-42.705776 -52.597645], ([0.08019702 0.2689716 ]) \t loss 1.865E+01\n",
      "step 3140000 \t return [-42.66007 -52.55033], ([0.05532985 0.31617263]) \t loss 1.850E+01\n",
      "step 3150000 \t return [-42.65083  -52.507446], ([0.07803296 0.284059  ]) \t loss 1.770E+01\n",
      "step 3160000 \t return [-42.602875 -52.47163 ], ([0.09843659 0.28537142]) \t loss 1.698E+01\n",
      "step 3170000 \t return [-42.579376 -52.465782], ([0.07920022 0.29494536]) \t loss 1.719E+01\n",
      "step 3180000 \t return [-42.63665 -52.51522], ([0.1351856  0.31805477]) \t loss 1.702E+01\n",
      "step 3190000 \t return [-42.72432 -52.59438], ([0.21987723 0.3131132 ]) \t loss 1.835E+01\n",
      "step 3200000 \t return [-42.595547 -52.452915], ([0.13218431 0.36801627]) \t loss 2.192E+01\n",
      "step 3210000 \t return [-42.53332 -52.35941], ([0.11514072 0.3396915 ]) \t loss 1.854E+01\n",
      "step 3220000 \t return [-39.422756 -49.248367], ([3.25324   3.5819647]) \t loss 1.615E+01\n",
      "step 3230000 \t return [-31.482864 -41.279667], ([0.29780546 0.5492664 ]) \t loss 6.130E+01\n",
      "step 3240000 \t return [-29.812994 -39.689953], ([0.26237583 0.56127226]) \t loss 8.440E+01\n",
      "step 3250000 \t return [-29.938665 -39.825634], ([0.3013356 0.5620726]) \t loss 6.499E+01\n",
      "step 3260000 \t return [-30.114574 -39.900707], ([0.39705542 0.67776245]) \t loss 5.517E+01\n",
      "step 3270000 \t return [-29.293806 -39.06766 ], ([0.27941793 0.45698446]) \t loss 5.591E+01\n",
      "step 3280000 \t return [-31.453873 -41.298603], ([0.75506777 0.93538123]) \t loss 3.739E+01\n",
      "step 3290000 \t return [-29.248753 -38.999847], ([0.5462122 0.814194 ]) \t loss 6.982E+01\n",
      "step 3300000 \t return [-31.233828 -41.090492], ([0.7957187 1.0629433]) \t loss 3.883E+01\n",
      "step 3310000 \t return [-28.623953 -38.399227], ([0.2358258  0.50562286]) \t loss 6.755E+01\n",
      "step 3320000 \t return [-28.589317 -38.430695], ([0.3542475 0.6191968]) \t loss 3.520E+01\n",
      "step 3330000 \t return [-28.291323 -38.08795 ], ([0.22933643 0.5206637 ]) \t loss 2.645E+01\n",
      "step 3340000 \t return [-28.296545 -38.080734], ([0.39063117 0.7410362 ]) \t loss 2.485E+01\n",
      "step 3350000 \t return [-27.769814 -37.630177], ([0.09081579 0.38588244]) \t loss 1.999E+01\n",
      "step 3360000 \t return [-28.49126  -38.300964], ([0.4887329 0.7929986]) \t loss 1.727E+01\n",
      "step 3370000 \t return [-27.86624  -37.707268], ([0.2818748 0.5372603]) \t loss 2.074E+01\n",
      "step 3380000 \t return [-28.228792 -38.024097], ([0.4180043  0.68909717]) \t loss 1.797E+01\n",
      "step 3390000 \t return [-27.470007 -37.27201 ], ([0.05683704 0.39331734]) \t loss 2.045E+01\n",
      "step 3400000 \t return [-27.321716 -37.109695], ([0.06791917 0.38517302]) \t loss 1.779E+01\n",
      "step 3410000 \t return [-27.36654  -37.142845], ([0.19164465 0.5135057 ]) \t loss 1.841E+01\n",
      "step 3420000 \t return [-27.549698 -37.38861 ], ([0.33729982 0.63773394]) \t loss 1.593E+01\n",
      "step 3430000 \t return [-27.455986 -37.27285 ], ([0.26665944 0.5885837 ]) \t loss 1.650E+01\n",
      "step 3440000 \t return [-27.420721 -37.233486], ([0.19155139 0.4308342 ]) \t loss 1.726E+01\n",
      "step 3450000 \t return [-27.622303 -37.43878 ], ([0.33830726 0.6539281 ]) \t loss 1.578E+01\n",
      "step 3460000 \t return [-27.23435  -37.041996], ([0.15914793 0.4282171 ]) \t loss 1.853E+01\n",
      "step 3470000 \t return [-27.286943 -37.065258], ([0.21085793 0.5343855 ]) \t loss 1.587E+01\n",
      "step 3480000 \t return [-27.343317 -37.224445], ([0.2061426  0.42924562]) \t loss 1.521E+01\n",
      "step 3490000 \t return [-27.026592 -36.80695 ], ([0.08728168 0.44968686]) \t loss 1.564E+01\n",
      "step 3500000 \t return [-27.024063 -36.82292 ], ([0.12109371 0.4621381 ]) \t loss 1.585E+01\n",
      "step 3510000 \t return [-26.945042 -36.738365], ([0.10313881 0.41556236]) \t loss 1.506E+01\n",
      "step 3520000 \t return [-26.853209 -36.70089 ], ([0.08934249 0.37144116]) \t loss 1.414E+01\n",
      "step 3530000 \t return [-26.810863 -36.600643], ([0.07442862 0.3801296 ]) \t loss 1.469E+01\n",
      "step 3540000 \t return [-26.909775 -36.820694], ([0.11354759 0.40372315]) \t loss 1.487E+01\n",
      "step 3550000 \t return [-27.02098 -36.86025], ([0.15833925 0.45930555]) \t loss 1.481E+01\n",
      "step 3560000 \t return [-26.73559  -36.420544], ([0.06370141 0.39315516]) \t loss 1.376E+01\n",
      "step 3570000 \t return [-26.724472 -36.603844], ([0.09061823 0.33779868]) \t loss 1.471E+01\n",
      "step 3580000 \t return [-26.64411  -36.448647], ([0.08045841 0.368106  ]) \t loss 1.390E+01\n",
      "step 3590000 \t return [-26.567366 -36.388428], ([0.07421421 0.30492386]) \t loss 1.364E+01\n",
      "step 3600000 \t return [-26.542229 -36.350296], ([0.06991702 0.32562068]) \t loss 1.612E+01\n",
      "step 3610000 \t return [-26.803701 -36.654392], ([0.16692844 0.42886916]) \t loss 1.483E+01\n",
      "step 3620000 \t return [-26.637148 -36.457855], ([0.12339202 0.41327205]) \t loss 1.519E+01\n",
      "step 3630000 \t return [-26.498625 -36.250134], ([0.06796944 0.30712503]) \t loss 1.455E+01\n",
      "step 3640000 \t return [-26.552063 -36.318516], ([0.07248935 0.41400707]) \t loss 1.482E+01\n",
      "step 3650000 \t return [-26.440119 -36.226143], ([0.10394127 0.394551  ]) \t loss 1.596E+01\n",
      "step 3660000 \t return [-26.531704 -36.359234], ([0.06399347 0.4063664 ]) \t loss 1.463E+01\n",
      "step 3670000 \t return [-26.445301 -36.227444], ([0.13945936 0.3617454 ]) \t loss 1.366E+01\n",
      "step 3680000 \t return [-26.317375 -36.10908 ], ([0.07890823 0.3733834 ]) \t loss 1.393E+01\n",
      "step 3690000 \t return [-26.505049 -36.35793 ], ([0.13059315 0.46119726]) \t loss 1.407E+01\n",
      "step 3700000 \t return [-26.421179 -36.27281 ], ([0.10857375 0.35332915]) \t loss 1.491E+01\n",
      "step 3710000 \t return [-26.46083  -36.227055], ([0.21117005 0.4614723 ]) \t loss 1.594E+01\n",
      "step 3720000 \t return [-26.258205 -36.03591 ], ([0.08481512 0.3420379 ]) \t loss 1.508E+01\n",
      "step 3730000 \t return [-26.261982 -36.03535 ], ([0.09241494 0.34839994]) \t loss 1.391E+01\n",
      "step 3740000 \t return [-26.267656 -36.067993], ([0.06186701 0.3500748 ]) \t loss 1.578E+01\n",
      "step 3750000 \t return [-26.21955 -36.0444 ], ([0.07281595 0.36807692]) \t loss 1.521E+01\n",
      "step 3760000 \t return [-26.304113 -36.176075], ([0.04702536 0.33628848]) \t loss 1.440E+01\n",
      "step 3770000 \t return [-26.215775 -35.985916], ([0.08362481 0.3325367 ]) \t loss 1.322E+01\n",
      "step 3780000 \t return [-25.91542  -35.685585], ([0.4427028  0.76961756]) \t loss 1.433E+01\n",
      "step 3790000 \t return [-25.17086 -35.00383], ([0.5837465  0.89059526]) \t loss 1.806E+01\n",
      "step 3800000 \t return [-23.79263  -33.566452], ([0.07394708 0.33683088]) \t loss 2.696E+01\n",
      "step 3810000 \t return [-23.850122 -33.59444 ], ([0.27641886 0.40872476]) \t loss 2.251E+01\n",
      "step 3820000 \t return [-24.121363 -33.89703 ], ([0.3365762 0.6199507]) \t loss 2.238E+01\n",
      "step 3830000 \t return [-23.779108 -33.601006], ([0.04507142 0.36457562]) \t loss 1.951E+01\n",
      "step 3840000 \t return [-23.653357 -33.382088], ([0.03250111 0.39653176]) \t loss 1.767E+01\n",
      "step 3850000 \t return [-23.640797 -33.353615], ([0.05684689 0.40981945]) \t loss 1.519E+01\n",
      "step 3860000 \t return [-23.7323   -33.474987], ([0.04270927 0.459467  ]) \t loss 1.448E+01\n",
      "step 3870000 \t return [-23.575756 -33.342133], ([0.11032432 0.3964078 ]) \t loss 1.424E+01\n",
      "step 3880000 \t return [-23.574806 -33.401714], ([0.08478465 0.39582333]) \t loss 1.561E+01\n",
      "step 3890000 \t return [-23.574793 -33.328735], ([0.03603325 0.41850644]) \t loss 1.585E+01\n",
      "step 3900000 \t return [-22.874811 -32.631596], ([0.10377361 0.34884498]) \t loss 1.535E+01\n",
      "step 3910000 \t return [-22.951632 -32.693913], ([0.26321006 0.42239586]) \t loss 1.585E+01\n",
      "step 3920000 \t return [-22.792542 -32.58311 ], ([0.03842369 0.41451648]) \t loss 1.465E+01\n",
      "step 3930000 \t return [-22.765854 -32.627075], ([0.04256987 0.37222856]) \t loss 1.479E+01\n",
      "step 3940000 \t return [-22.750587 -32.52531 ], ([0.09906959 0.3651198 ]) \t loss 1.430E+01\n",
      "step 3950000 \t return [-22.648552 -32.42294 ], ([0.44293684 0.73076856]) \t loss 1.524E+01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\liamm\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:179: RuntimeWarning: overflow encountered in reduce\n",
      "  ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3960000 \t return [-20.899832 -30.269175], ([0.7743384 0.7364228]) \t loss 1.507E+01\n",
      "step 3970000 \t return [-20.503695 -29.589487], ([1.586933   0.56272095]) \t loss 2.017E+01\n",
      "step 3980000 \t return [-21.044714 -30.010857], ([1.4560974  0.52648836]) \t loss 2.641E+01\n",
      "step 3990000 \t return [-19.922667 -29.628714], ([0.6197499  0.30386358]) \t loss 2.228E+01\n",
      "step 4000000 \t return [-20.47128  -29.675983], ([1.7102114 0.3779776]) \t loss 1.892E+01\n",
      "step 4010000 \t return [-19.712814 -29.375069], ([0.8613413  0.29407465]) \t loss 1.637E+01\n",
      "step 4020000 \t return [-19.111992 -28.773954], ([0.44662392 0.22884303]) \t loss 1.343E+01\n",
      "step 4030000 \t return [-19.474459 -28.66414 ], ([1.5914315  0.34207606]) \t loss 1.418E+01\n",
      "step 4040000 \t return [-18.398266 -28.097706], ([0.8736507  0.25267217]) \t loss 1.475E+01\n",
      "step 4050000 \t return [-18.180403 -27.775576], ([0.48586693 0.2651876 ]) \t loss 1.417E+01\n",
      "step 4060000 \t return [-18.625702 -27.978657], ([1.3721228  0.36383617]) \t loss 1.368E+01\n",
      "step 4070000 \t return [-18.435574 -27.777748], ([1.1344916  0.29548082]) \t loss 1.361E+01\n",
      "step 4080000 \t return [-17.671833 -27.456324], ([0.24835941 0.27513748]) \t loss 1.216E+01\n",
      "step 4090000 \t return [-17.536716 -27.297382], ([0.2368994  0.25637835]) \t loss 1.302E+01\n",
      "step 4100000 \t return [-17.555504 -27.215488], ([0.301326  0.3461859]) \t loss 1.252E+01\n",
      "step 4110000 \t return [-17.654888 -27.210342], ([1.0714918  0.32074642]) \t loss 1.256E+01\n",
      "step 4120000 \t return [-17.475256 -27.269829], ([0.07766855 0.3491513 ]) \t loss 1.333E+01\n",
      "step 4130000 \t return [-17.480444 -27.221857], ([0.09888853 0.37033334]) \t loss 1.333E+01\n",
      "step 4140000 \t return [-17.482943 -27.19625 ], ([0.5364109  0.31988305]) \t loss 1.437E+01\n",
      "step 4150000 \t return [-17.21451  -26.929688], ([0.11329889 0.3562067 ]) \t loss 1.268E+01\n",
      "step 4160000 \t return [-17.135406 -26.91142 ], ([0.10909583 0.33819878]) \t loss 1.223E+01\n",
      "step 4170000 \t return [-17.055801 -26.792242], ([0.11982418 0.3771592 ]) \t loss 1.470E+01\n",
      "step 4180000 \t return [-17.246134 -26.971315], ([0.06648507 0.3930731 ]) \t loss 1.363E+01\n",
      "step 4190000 \t return [-17.081135 -26.811148], ([0.11933097 0.38549995]) \t loss 1.346E+01\n",
      "step 4200000 \t return [-16.852448 -26.546442], ([0.16199216 0.36586797]) \t loss 1.292E+01\n",
      "step 4210000 \t return [-16.860254 -26.572483], ([0.10515498 0.36808157]) \t loss 1.257E+01\n",
      "step 4220000 \t return [-16.332827 -26.09451 ], ([0.09837437 0.3492557 ]) \t loss 1.278E+01\n",
      "step 4230000 \t return [-16.401434 -26.180437], ([0.13822497 0.30597216]) \t loss 1.303E+01\n",
      "step 4240000 \t return [-16.401873 -26.171253], ([0.06803797 0.41698444]) \t loss 1.205E+01\n",
      "step 4250000 \t return [-16.334602 -26.109358], ([0.0798782  0.37569392]) \t loss 1.177E+01\n",
      "step 4260000 \t return [-16.280756 -25.991018], ([0.10541752 0.37428063]) \t loss 1.200E+01\n",
      "step 4270000 \t return [-15.93877  -25.697395], ([0.22572167 0.37123322]) \t loss 1.302E+01\n",
      "step 4280000 \t return [-15.6437235 -25.349976 ], ([0.1711288  0.31133753]) \t loss 1.320E+01\n",
      "step 4290000 \t return [-15.684506 -25.382788], ([0.10644782 0.45640984]) \t loss 1.366E+01\n",
      "step 4300000 \t return [-15.610997 -25.35145 ], ([0.17273115 0.2936593 ]) \t loss 1.381E+01\n",
      "step 4310000 \t return [-15.522164 -25.173393], ([0.15788952 0.34624144]) \t loss 1.265E+01\n",
      "step 4320000 \t return [-15.15553  -24.779976], ([0.22106661 0.42764038]) \t loss 1.307E+01\n",
      "step 4330000 \t return [-14.606381 -24.278576], ([0.11091365 0.3985119 ]) \t loss 1.330E+01\n",
      "step 4340000 \t return [-14.77479 -24.52204], ([0.04590084 0.43553817]) \t loss 1.339E+01\n",
      "step 4350000 \t return [-14.561637 -24.223207], ([0.16028582 0.5620201 ]) \t loss 1.309E+01\n",
      "step 4360000 \t return [-14.302576 -24.02259 ], ([0.22449735 0.5260182 ]) \t loss 1.315E+01\n",
      "step 4370000 \t return [-13.846797 -23.545282], ([0.17055063 0.50022495]) \t loss 1.382E+01\n",
      "step 4380000 \t return [-12.846184 -22.634855], ([0.08699605 0.36464998]) \t loss 1.328E+01\n",
      "step 4390000 \t return [-13.115348 -22.87175 ], ([0.03824459 0.45550627]) \t loss 1.219E+01\n",
      "step 4400000 \t return [-12.846212 -22.605497], ([0.06114223 0.4092376 ]) \t loss 1.239E+01\n",
      "step 4410000 \t return [-12.967151 -22.658157], ([0.04485781 0.4598001 ]) \t loss 1.234E+01\n",
      "step 4420000 \t return [-12.805568 -22.546999], ([0.07120353 0.40062204]) \t loss 1.239E+01\n",
      "step 4430000 \t return [-12.820854 -22.6073  ], ([0.03930857 0.46662867]) \t loss 1.208E+01\n",
      "step 4440000 \t return [-12.716714 -22.470253], ([0.07527927 0.43617326]) \t loss 1.262E+01\n",
      "step 4450000 \t return [-12.76655  -22.559992], ([0.03261478 0.4036385 ]) \t loss 1.195E+01\n",
      "step 4460000 \t return [-12.693953 -22.524868], ([0.05129908 0.40851164]) \t loss 1.295E+01\n",
      "step 4470000 \t return [-12.316471 -22.074175], ([0.07675888 0.39023846]) \t loss 1.212E+01\n",
      "step 4480000 \t return [-12.300851 -22.10158 ], ([0.04195272 0.42815536]) \t loss 1.221E+01\n",
      "step 4490000 \t return [-12.318457 -22.018396], ([0.07862002 0.3981426 ]) \t loss 1.228E+01\n",
      "step 4500000 \t return [-12.251544 -22.045086], ([0.05577105 0.4024    ]) \t loss 1.240E+01\n",
      "step 4510000 \t return [-12.31377  -22.132055], ([0.04746362 0.46101385]) \t loss 1.285E+01\n",
      "step 4520000 \t return [-12.264278 -21.99258 ], ([0.02983433 0.45687917]) \t loss 1.189E+01\n",
      "step 4530000 \t return [-12.234291 -22.01054 ], ([0.03808781 0.4312868 ]) \t loss 1.262E+01\n",
      "step 4540000 \t return [-12.239441 -21.9559  ], ([0.09415017 0.3695586 ]) \t loss 1.213E+01\n",
      "step 4550000 \t return [-12.214403 -21.978945], ([0.1327995 0.3396245]) \t loss 1.229E+01\n",
      "step 4560000 \t return [-12.253325 -21.992847], ([0.04431435 0.41834927]) \t loss 1.263E+01\n",
      "step 4570000 \t return [-12.262455 -21.998241], ([0.047688   0.42970705]) \t loss 1.231E+01\n",
      "step 4580000 \t return [-12.309005 -22.0844  ], ([0.0639278  0.43447894]) \t loss 1.253E+01\n",
      "step 4590000 \t return [-12.295619 -22.014214], ([0.04083735 0.4417598 ]) \t loss 1.249E+01\n",
      "step 4600000 \t return [-12.202819 -21.934546], ([0.04027409 0.49936342]) \t loss 1.290E+01\n",
      "step 4610000 \t return [-12.173413 -21.974365], ([0.13315591 0.35838354]) \t loss 1.263E+01\n",
      "step 4620000 \t return [-12.143386 -21.816511], ([0.13660476 0.38331047]) \t loss 1.299E+01\n",
      "step 4630000 \t return [-12.135512 -21.871758], ([0.16439666 0.30638692]) \t loss 1.302E+01\n",
      "step 4640000 \t return [-12.182439 -21.91562 ], ([0.044551   0.47606227]) \t loss 1.302E+01\n",
      "step 4650000 \t return [-12.170986 -21.759417], ([0.09058614 0.52949977]) \t loss 1.272E+01\n",
      "step 4660000 \t return [-12.035961 -21.707834], ([0.11502828 0.39474493]) \t loss 1.242E+01\n",
      "step 4670000 \t return [-11.990815 -21.777317], ([0.05296356 0.43921572]) \t loss 1.274E+01\n",
      "step 4680000 \t return [-12.595962 -22.252108], ([0.03964967 0.49260384]) \t loss 1.264E+01\n",
      "step 4690000 \t return [-11.974266 -21.712255], ([0.0438283 0.5732191]) \t loss 1.349E+01\n",
      "step 4700000 \t return [-12.031349 -21.75304 ], ([0.05253294 0.47473207]) \t loss 1.241E+01\n",
      "step 4710000 \t return [-11.90631  -21.362822], ([0.04714945 0.5304448 ]) \t loss 1.218E+01\n",
      "step 4720000 \t return [-11.863862 -21.456926], ([0.07018888 0.6285464 ]) \t loss 1.257E+01\n",
      "step 4730000 \t return [-12.367435 -18.66897 ], ([1.6146739  0.92566836]) \t loss 1.208E+01\n",
      "step 4740000 \t return [-10.045297 -19.72898 ], ([0.10017309 0.57779485]) \t loss 1.510E+01\n",
      "step 4750000 \t return [-13.610788 -16.834488], ([2.9958124 0.9460371]) \t loss 1.410E+01\n",
      "step 4760000 \t return [-16.180672 -16.004633], ([7.9101214  0.93072766]) \t loss 1.497E+01\n",
      "step 4770000 \t return [-24.574951 -14.311226], ([12.019491   0.9748899]) \t loss 1.578E+01\n",
      "step 4780000 \t return [ -7.908947 -17.162043], ([0.69309807 0.33227724]) \t loss 1.707E+01\n",
      "step 4790000 \t return [-20.39477  -14.768608], ([3.8994033  0.44512138]) \t loss 1.667E+01\n",
      "step 4800000 \t return [ -7.568608 -16.212936], ([0.77091664 0.3147896 ]) \t loss 1.518E+01\n",
      "step 4810000 \t return [-13.66782  -14.414482], ([7.569548 0.605496]) \t loss 1.512E+01\n",
      "step 4820000 \t return [-13.477287 -13.584733], ([7.708043   0.63743335]) \t loss 1.390E+01\n",
      "step 4830000 \t return [-14.24009  -13.838005], ([8.189201  0.6187542]) \t loss 1.317E+01\n",
      "step 4840000 \t return [-12.951208 -13.767374], ([6.4246373 0.5990647]) \t loss 1.288E+01\n",
      "step 4850000 \t return [-32.193714 -13.012282], ([10.71288    0.9151605]) \t loss 1.290E+01\n",
      "step 4860000 \t return [ -7.7423487 -13.18353  ], ([3.986355   0.69071287]) \t loss 1.184E+01\n",
      "step 4870000 \t return [-11.792178 -12.889874], ([6.40151   0.8167128]) \t loss 1.187E+01\n",
      "step 4880000 \t return [-25.828424 -12.41933 ], ([3.2563052  0.78885746]) \t loss 1.153E+01\n",
      "step 4890000 \t return [-18.35425  -13.152589], ([9.305957  1.0443584]) \t loss 1.198E+01\n",
      "step 4900000 \t return [-31.254993 -12.374038], ([3.7259624  0.91184574]) \t loss 1.176E+01\n",
      "step 4910000 \t return [-26.133484 -12.88178 ], ([10.482737   1.2315742]) \t loss 1.114E+01\n",
      "step 4920000 \t return [-26.223066 -12.611911], ([11.268079   1.1691496]) \t loss 1.065E+01\n",
      "step 4930000 \t return [-34.75713  -13.235682], ([9.94069   1.1324031]) \t loss 1.116E+01\n",
      "step 4940000 \t return [-14.762935 -12.779205], ([10.883364   0.8222054]) \t loss 1.057E+01\n",
      "step 4950000 \t return [-16.201881 -12.591975], ([11.0972595  0.9637776]) \t loss 1.086E+01\n",
      "step 4960000 \t return [-22.13401 -13.0357 ], ([7.3396673 1.0002215]) \t loss 1.038E+01\n",
      "step 4970000 \t return [-23.082573 -12.665118], ([11.1111965  1.0451754]) \t loss 1.058E+01\n",
      "step 4980000 \t return [-23.859121 -12.912542], ([10.349159   1.0284678]) \t loss 1.063E+01\n",
      "step 4990000 \t return [-12.821393 -13.206412], ([7.562068   0.82868654]) \t loss 1.036E+01\n",
      "step 5000000 \t return [-10.59906 -12.16425], ([6.6305532 0.9725662]) \t loss 1.055E+01\n",
      "step 5010000 \t return [-22.29791  -11.279991], ([2.3260467 0.8344413]) \t loss 1.054E+01\n",
      "step 5020000 \t return [-13.622541 -11.877994], ([9.037292  1.1219295]) \t loss 8.520E+00\n",
      "step 5030000 \t return [-16.999338 -12.021711], ([7.0227733 1.2752522]) \t loss 8.057E+00\n",
      "step 5040000 \t return [ -9.602077 -11.956378], ([3.0813994 0.877445 ]) \t loss 6.777E+00\n",
      "step 5050000 \t return [ -7.804465  -11.5188465], ([1.3672341 0.8704958]) \t loss 6.375E+00\n",
      "step 5060000 \t return [-10.195546 -11.639509], ([3.3988981 0.9331693]) \t loss 5.125E+00\n",
      "step 5070000 \t return [-12.26518  -11.067716], ([1.0377275  0.72375506]) \t loss 4.242E+00\n",
      "step 5080000 \t return [-12.297341 -12.153877], ([8.255087  1.0451846]) \t loss 3.462E+00\n",
      "step 5090000 \t return [-19.666882  -11.2487955], ([2.3787417 0.8280436]) \t loss 3.050E+00\n",
      "step 5100000 \t return [ -6.99201 -11.45838], ([3.2357266 0.8609446]) \t loss 2.898E+00\n",
      "step 5110000 \t return [ -6.7646666 -11.813657 ], ([2.294845   0.82061493]) \t loss 2.252E+00\n",
      "step 5120000 \t return [-11.485205 -11.693826], ([5.3069468  0.91622263]) \t loss 2.143E+00\n",
      "step 5130000 \t return [-12.841596 -11.815398], ([8.829387  1.1544843]) \t loss 1.489E+00\n",
      "step 5140000 \t return [-11.262655 -11.966439], ([7.853125  1.1057705]) \t loss 1.213E+00\n",
      "step 5150000 \t return [ -7.573474 -11.215842], ([1.5339307 0.7668686]) \t loss 1.059E+00\n",
      "step 5160000 \t return [-20.919445 -11.189483], ([2.5529833  0.84309745]) \t loss 8.891E-01\n",
      "step 5170000 \t return [ -9.797312 -11.838614], ([6.5518007 1.0185457]) \t loss 7.793E-01\n",
      "step 5180000 \t return [-11.487495 -11.488229], ([5.1654234 0.9856093]) \t loss 7.366E-01\n",
      "step 5190000 \t return [-12.306989 -11.571045], ([6.810993  1.1585486]) \t loss 6.531E-01\n",
      "step 5200000 \t return [-16.592537 -12.049297], ([11.088204   1.1609899]) \t loss 4.480E-01\n",
      "step 5210000 \t return [-17.971062 -12.174434], ([10.395433   1.2122637]) \t loss 3.285E-01\n",
      "step 5220000 \t return [-10.194307  -11.8769045], ([3.5983186  0.98595405]) \t loss 3.360E-01\n",
      "step 5230000 \t return [-12.202649 -11.760903], ([9.901802  1.0841364]) \t loss 3.443E-01\n",
      "step 5240000 \t return [-12.876761 -11.665206], ([4.6411247 1.0296129]) \t loss 2.430E-01\n",
      "step 5250000 \t return [-10.149708 -11.654118], ([5.2169394 1.0890515]) \t loss 2.426E-01\n",
      "step 5260000 \t return [-12.065003 -11.80253 ], ([7.8965054 1.1260765]) \t loss 2.633E-01\n",
      "step 5270000 \t return [-17.804977 -11.023072], ([2.007264   0.85216635]) \t loss 2.255E-01\n",
      "step 5280000 \t return [-10.023879 -11.448034], ([3.9060442 0.9546009]) \t loss 2.696E-01\n",
      "step 5290000 \t return [ -9.267297 -11.332877], ([2.5148861  0.89440125]) \t loss 2.014E-01\n",
      "step 5300000 \t return [ -7.9942064 -11.207191 ], ([0.9425627  0.81392473]) \t loss 2.461E-01\n",
      "step 5310000 \t return [-18.073214 -11.94981 ], ([9.647181  1.1381261]) \t loss 2.552E-01\n",
      "step 5320000 \t return [ -8.23689 -11.70017], ([4.0639977  0.90539974]) \t loss 3.308E-01\n",
      "step 5330000 \t return [-10.199457 -11.32611 ], ([3.5465035 0.9118502]) \t loss 2.618E-01\n",
      "step 5340000 \t return [ -8.017591 -11.5404  ], ([4.622326  0.9285109]) \t loss 1.775E-01\n",
      "step 5350000 \t return [ -9.296553 -11.609767], ([4.542425   0.93805015]) \t loss 2.286E-01\n",
      "step 5360000 \t return [ -9.866208 -11.538804], ([3.7800827 0.9115839]) \t loss 2.290E-01\n",
      "step 5370000 \t return [-16.72292  -12.147836], ([10.578645   1.2034203]) \t loss 1.913E-01\n",
      "step 5380000 \t return [-15.063915  -11.7090225], ([8.601708  1.1490601]) \t loss 2.534E-01\n",
      "step 5390000 \t return [-15.443049 -11.896237], ([9.898889  1.1313807]) \t loss 2.127E-01\n",
      "step 5400000 \t return [ -9.727922 -11.186684], ([2.3318355 0.9058853]) \t loss 1.785E-01\n",
      "step 5410000 \t return [-11.337502 -11.597573], ([4.832241  0.9133635]) \t loss 2.348E-01\n",
      "step 5420000 \t return [ -8.720755 -11.497662], ([3.423275  1.0208426]) \t loss 1.836E-01\n",
      "step 5430000 \t return [ -5.605681 -11.147595], ([1.5248241  0.88215435]) \t loss 1.865E-01\n",
      "step 5440000 \t return [ -7.1427402 -11.558968 ], ([2.0052917  0.92325854]) \t loss 2.377E-01\n",
      "step 5450000 \t return [-10.834197 -11.545328], ([1.9287645 1.0115293]) \t loss 2.153E-01\n",
      "step 5460000 \t return [ -8.10438  -11.415654], ([3.1361134  0.95618284]) \t loss 2.044E-01\n",
      "step 5470000 \t return [ -9.227535 -11.593381], ([6.724084  1.0799707]) \t loss 1.762E-01\n",
      "step 5480000 \t return [-11.845676 -11.757225], ([9.351455  1.0248679]) \t loss 1.631E-01\n",
      "step 5490000 \t return [-15.856584 -11.807032], ([10.494564   1.1776483]) \t loss 2.199E-01\n",
      "step 5500000 \t return [-16.041704  -11.0957155], ([1.7469928 0.8328823]) \t loss 1.577E-01\n",
      "step 5510000 \t return [ -8.457643 -11.290466], ([2.9998493 0.9951154]) \t loss 1.800E-01\n",
      "step 5520000 \t return [ -9.098734 -11.547859], ([7.043967  1.0142944]) \t loss 1.476E-01\n",
      "step 5530000 \t return [ -9.606788 -11.71562 ], ([6.357326 1.051106]) \t loss 1.687E-01\n",
      "step 5540000 \t return [ -7.0965657 -11.425681 ], ([2.3101828 0.977014 ]) \t loss 1.657E-01\n",
      "step 5550000 \t return [-16.669603 -11.542379], ([10.375324   1.1655889]) \t loss 1.891E-01\n",
      "step 5560000 \t return [-12.275309 -11.707563], ([9.083224  1.1906806]) \t loss 1.687E-01\n",
      "step 5570000 \t return [ -7.292378 -11.379553], ([1.1479958 1.0230478]) \t loss 1.795E-01\n",
      "step 5580000 \t return [ -8.2319565 -11.203777 ], ([2.0715206 1.0197898]) \t loss 2.477E-01\n",
      "step 5590000 \t return [-11.637918 -11.272854], ([8.516597  1.1224276]) \t loss 1.833E-01\n",
      "step 5600000 \t return [ -8.108288 -11.299385], ([6.7552423  0.94495445]) \t loss 1.785E-01\n",
      "step 5610000 \t return [ -9.074292 -11.458879], ([7.869457 1.01827 ]) \t loss 2.127E-01\n",
      "step 5620000 \t return [-19.592022 -10.875725], ([2.0814996 0.9420769]) \t loss 1.789E-01\n",
      "step 5630000 \t return [ -8.214242 -11.364623], ([2.3394732  0.97235143]) \t loss 2.366E-01\n",
      "step 5640000 \t return [ -6.74098  -11.361503], ([4.444666  0.8864379]) \t loss 1.730E-01\n",
      "step 5650000 \t return [ -8.106273 -11.285871], ([1.781667   0.89866096]) \t loss 1.829E-01\n",
      "step 5660000 \t return [ -4.659242 -10.925612], ([2.0357878 0.9962679]) \t loss 2.318E-01\n",
      "step 5670000 \t return [ -5.5807695 -10.984956 ], ([1.3035777 1.1488475]) \t loss 5.848E-01\n",
      "step 5680000 \t return [-32.247375 -10.618605], ([7.275527   0.87701404]) \t loss 3.592E-01\n",
      "step 5690000 \t return [ -3.7944987 -11.493248 ], ([1.3292067 1.1722958]) \t loss 4.117E-01\n",
      "step 5700000 \t return [ -7.2761183 -11.332677 ], ([2.2918866 1.1372561]) \t loss 4.510E-01\n",
      "step 5710000 \t return [ -8.041677  -11.4495735], ([5.535251  0.9985911]) \t loss 2.527E-01\n",
      "step 5720000 \t return [ -6.1445904 -11.253792 ], ([4.383852  0.8501249]) \t loss 2.485E-01\n",
      "step 5730000 \t return [ -6.5451818 -11.1756935], ([5.3495383 0.8723661]) \t loss 2.258E-01\n",
      "step 5740000 \t return [ -5.3316784 -11.104257 ], ([3.3220713 0.8162371]) \t loss 2.010E-01\n",
      "step 5750000 \t return [ -9.399437 -11.084586], ([6.9077206 1.0848901]) \t loss 2.123E-01\n",
      "step 5760000 \t return [ -5.443068 -11.030811], ([3.6345012 0.7371681]) \t loss 2.406E-01\n",
      "step 5770000 \t return [ -7.9505835 -11.3187685], ([5.5850096 1.0313286]) \t loss 5.155E-01\n",
      "step 5780000 \t return [ -8.201589 -10.946917], ([2.2480516 1.0623616]) \t loss 2.665E-01\n",
      "step 5790000 \t return [ -7.8156214 -10.849371 ], ([3.7785714 1.0399641]) \t loss 2.429E-01\n",
      "step 5800000 \t return [ -5.5526204 -10.964529 ], ([1.102455   0.97711885]) \t loss 3.004E-01\n",
      "step 5810000 \t return [ -5.0585465 -11.26228  ], ([1.1903784 1.1818902]) \t loss 2.693E-01\n",
      "step 5820000 \t return [ -7.40943  -11.529559], ([1.0711544 1.0903065]) \t loss 2.832E-01\n",
      "step 5830000 \t return [ -6.4641857 -11.053019 ], ([2.0648272 1.1404078]) \t loss 3.352E-01\n",
      "step 5840000 \t return [ -6.601122 -11.133677], ([0.9162299 1.1426742]) \t loss 3.145E-01\n",
      "step 5850000 \t return [ -5.569426 -10.978183], ([0.90439016 0.9707951 ]) \t loss 4.300E-01\n",
      "step 5860000 \t return [ -7.938566 -11.020337], ([2.9854236 1.0353646]) \t loss 3.697E-01\n",
      "step 5870000 \t return [ -5.0016747 -10.69392  ], ([1.9442511 1.0359371]) \t loss 3.546E-01\n",
      "step 5880000 \t return [ -4.8835583 -11.002489 ], ([2.4459617 0.9103015]) \t loss 2.966E-01\n",
      "step 5890000 \t return [ -3.8387547 -10.875448 ], ([2.0461063 0.7237   ]) \t loss 2.333E-01\n",
      "step 5900000 \t return [ -3.5121098 -10.925758 ], ([1.9648842 0.7308959]) \t loss 2.002E-01\n",
      "step 5910000 \t return [ -4.4598966 -10.654364 ], ([1.8040457 0.8725863]) \t loss 1.980E-01\n",
      "step 5920000 \t return [ -5.2663593 -10.780561 ], ([1.9956887  0.93759847]) \t loss 2.771E-01\n",
      "step 5930000 \t return [ -3.3281512 -10.893311 ], ([1.7106563  0.76118785]) \t loss 2.045E-01\n",
      "step 5940000 \t return [ -3.6407602 -10.713344 ], ([2.1052692  0.80213314]) \t loss 3.158E-01\n",
      "step 5950000 \t return [ -4.0562754 -10.696948 ], ([1.9333955  0.88528174]) \t loss 2.608E-01\n",
      "step 5960000 \t return [ -4.717796 -10.783112], ([1.8507223 1.02443  ]) \t loss 1.744E-01\n",
      "step 5970000 \t return [ -4.551277 -10.815518], ([2.2952952 0.9359633]) \t loss 2.154E-01\n",
      "step 5980000 \t return [ -3.4320846 -10.482279 ], ([2.6179066 0.8832901]) \t loss 1.884E-01\n",
      "step 5990000 \t return [ -5.840099 -10.592347], ([2.4230728 1.0358477]) \t loss 3.060E-01\n",
      "step 6000000 \t return [ -4.346836 -10.643044], ([1.7659527 0.9131442]) \t loss 1.967E-01\n",
      "step 6010000 \t return [ -3.1711168 -10.855259 ], ([1.4676362 0.9919056]) \t loss 1.652E-01\n",
      "step 6020000 \t return [ -2.7909198 -10.850266 ], ([2.0090113 1.0135708]) \t loss 2.081E-01\n",
      "step 6030000 \t return [ -4.0612154 -10.680151 ], ([1.6464424 0.9608272]) \t loss 2.516E-01\n",
      "step 6040000 \t return [ -5.6480346 -10.52146  ], ([2.6312733  0.98473597]) \t loss 1.631E-01\n",
      "step 6050000 \t return [ -3.2353556 -10.544339 ], ([2.4317622 1.059694 ]) \t loss 1.784E-01\n",
      "step 6060000 \t return [ -2.731434 -10.716034], ([2.0403411 1.0279787]) \t loss 2.488E-01\n",
      "step 6070000 \t return [ -3.3831856 -10.511453 ], ([2.2210622 0.9291696]) \t loss 2.587E-01\n",
      "step 6080000 \t return [ -4.83812  -10.480546], ([3.3578901 0.8917299]) \t loss 1.872E-01\n",
      "step 6090000 \t return [ -3.3080528 -10.716248 ], ([2.3534515 1.1805092]) \t loss 1.947E-01\n",
      "step 6100000 \t return [ -4.2146297 -10.583091 ], ([2.8740063 1.043349 ]) \t loss 3.126E-01\n",
      "step 6110000 \t return [ -2.9456654 -10.760963 ], ([2.094801  1.0654196]) \t loss 2.904E-01\n",
      "step 6120000 \t return [ -5.560554 -10.477224], ([3.2758727  0.88191134]) \t loss 2.553E-01\n",
      "step 6130000 \t return [ -3.0365555 -10.869296 ], ([1.7961307 1.0457463]) \t loss 2.371E-01\n",
      "step 6140000 \t return [ -3.0911071 -10.69727  ], ([2.263614   0.99480283]) \t loss 2.400E-01\n",
      "step 6150000 \t return [ -3.1748817 -10.585962 ], ([2.41672   0.9964626]) \t loss 2.173E-01\n",
      "step 6160000 \t return [ -6.168454 -10.421858], ([3.2264483  0.89861673]) \t loss 2.233E-01\n",
      "step 6170000 \t return [ -3.050811 -10.494184], ([2.3565283  0.92959714]) \t loss 1.710E-01\n",
      "step 6180000 \t return [ -3.1589036 -10.575692 ], ([2.3600323 1.0778216]) \t loss 1.767E-01\n",
      "step 6190000 \t return [ -3.9060874 -10.962652 ], ([2.5731723 1.3294688]) \t loss 2.146E-01\n",
      "step 6200000 \t return [ -5.7564497 -10.323852 ], ([3.0153143 0.7916395]) \t loss 2.924E-01\n",
      "step 6210000 \t return [ -4.1880627 -10.528883 ], ([5.1483464 1.0106627]) \t loss 1.741E-01\n",
      "step 6220000 \t return [ -3.5839572 -10.389566 ], ([2.858664   0.79367733]) \t loss 2.637E-01\n",
      "step 6230000 \t return [ -6.550062  -10.3866625], ([5.1205125  0.77596897]) \t loss 1.717E-01\n",
      "step 6240000 \t return [ -6.4833198 -10.446594 ], ([5.405734   0.81367904]) \t loss 1.744E-01\n",
      "step 6250000 \t return [ -3.6042097 -10.447136 ], ([3.6306505  0.75418264]) \t loss 1.863E-01\n",
      "step 6260000 \t return [ -2.820884 -10.416466], ([2.544668  0.8409792]) \t loss 1.591E-01\n",
      "step 6270000 \t return [ -3.7526062 -10.908545 ], ([1.7072062 1.2969184]) \t loss 1.877E-01\n",
      "step 6280000 \t return [ -3.428094 -10.44665 ], ([2.7546911  0.77784586]) \t loss 2.195E-01\n",
      "step 6290000 \t return [ -3.903948 -11.214243], ([1.5689496 1.2987802]) \t loss 1.732E-01\n",
      "step 6300000 \t return [ -4.722837 -10.31798 ], ([2.3451607  0.76034397]) \t loss 2.621E-01\n",
      "step 6310000 \t return [ -4.213262 -10.683822], ([3.043513  1.3333969]) \t loss 1.670E-01\n",
      "step 6320000 \t return [ -7.0262914 -10.255077 ], ([3.666854   0.81698966]) \t loss 2.559E-01\n",
      "step 6330000 \t return [ -6.3423    -10.4149475], ([2.6333547 0.865926 ]) \t loss 2.399E-01\n",
      "step 6340000 \t return [ -3.7497778 -10.212631 ], ([2.7138042  0.77320486]) \t loss 1.560E-01\n",
      "step 6350000 \t return [ -3.1696398 -10.750857 ], ([1.9163837 1.1589038]) \t loss 1.352E-01\n",
      "step 6360000 \t return [ -9.947917 -10.574255], ([3.6825583 0.7934665]) \t loss 2.108E-01\n",
      "step 6370000 \t return [ -3.1659992 -11.04755  ], ([1.6948406 1.2813061]) \t loss 1.559E-01\n",
      "step 6380000 \t return [ -2.9309537 -10.854581 ], ([1.9820178 1.1077896]) \t loss 2.827E-01\n",
      "step 6390000 \t return [ -6.0334764 -10.236894 ], ([4.121668   0.78916645]) \t loss 2.971E-01\n",
      "step 6400000 \t return [ -2.834169 -10.345331], ([2.3590517 0.9116557]) \t loss 1.629E-01\n",
      "step 6410000 \t return [ -4.616647 -10.469819], ([2.065279  0.8539211]) \t loss 1.708E-01\n",
      "step 6420000 \t return [ -3.488027 -10.305621], ([2.9868984 0.8911714]) \t loss 1.187E-01\n",
      "step 6430000 \t return [ -2.7217584 -10.346047 ], ([2.354538   0.77415854]) \t loss 1.284E-01\n",
      "step 6440000 \t return [ -2.9782302 -10.710788 ], ([2.1965432 1.052009 ]) \t loss 1.274E-01\n",
      "step 6450000 \t return [ -3.8319733 -10.374629 ], ([3.7752624  0.85761154]) \t loss 1.855E-01\n",
      "step 6460000 \t return [ -5.8897395 -10.70319  ], ([1.8916644 0.8723959]) \t loss 1.328E-01\n",
      "step 6470000 \t return [ -3.3030097 -10.325897 ], ([2.90382    0.99070674]) \t loss 1.474E-01\n",
      "step 6480000 \t return [ -5.045524 -10.261431], ([4.768299   0.70549566]) \t loss 1.353E-01\n",
      "step 6490000 \t return [ -4.9497895 -10.265942 ], ([3.459706  0.7851678]) \t loss 1.139E-01\n",
      "step 6500000 \t return [ -7.3350887 -10.261748 ], ([4.0975304  0.78622687]) \t loss 1.051E-01\n",
      "step 6510000 \t return [ -5.9780593 -10.369848 ], ([3.1039686 0.7734211]) \t loss 1.122E-01\n",
      "step 6520000 \t return [ -2.88447  -10.456758], ([2.7544014 0.9535077]) \t loss 1.502E-01\n",
      "step 6530000 \t return [ -3.489827 -10.238348], ([2.9312434 0.8943969]) \t loss 1.589E-01\n",
      "step 6540000 \t return [ -3.228649 -10.47044 ], ([2.8695154 0.993374 ]) \t loss 1.104E-01\n",
      "step 6550000 \t return [ -3.907037 -10.289373], ([4.2586155 0.8919275]) \t loss 1.311E-01\n",
      "step 6560000 \t return [ -4.215303 -10.355746], ([5.4829493  0.67895055]) \t loss 1.363E-01\n",
      "step 6570000 \t return [ -3.7558393 -10.185773 ], ([4.1349735 0.7376319]) \t loss 1.158E-01\n",
      "step 6580000 \t return [ -5.485557 -10.371708], ([2.1569078 1.1959164]) \t loss 1.028E-01\n",
      "step 6590000 \t return [ -4.385189 -10.515778], ([3.5345721 0.9115022]) \t loss 1.477E-01\n",
      "step 6600000 \t return [ -4.6302476 -10.441026 ], ([4.1669593 0.92442  ]) \t loss 1.359E-01\n",
      "step 6610000 \t return [ -4.320676 -10.440505], ([5.362536   0.65836746]) \t loss 1.210E-01\n",
      "step 6620000 \t return [ -5.60724  -10.262695], ([5.7978683  0.66719764]) \t loss 1.168E-01\n",
      "step 6630000 \t return [ -5.909241 -10.256312], ([6.7379684 0.8265807]) \t loss 1.206E-01\n",
      "step 6640000 \t return [ -3.2803519 -10.490866 ], ([2.2833664 1.0473491]) \t loss 1.609E-01\n",
      "step 6650000 \t return [ -4.830473  -10.2181225], ([4.419508  0.7547449]) \t loss 1.924E-01\n",
      "step 6660000 \t return [ -3.3741364 -10.254463 ], ([2.3015568  0.79285884]) \t loss 1.126E-01\n",
      "step 6670000 \t return [ -2.3298194 -10.612279 ], ([1.7385088  0.90019524]) \t loss 9.953E-02\n",
      "step 6680000 \t return [ -4.5623493 -10.2969475], ([4.8844676 0.7011198]) \t loss 1.450E-01\n",
      "step 6690000 \t return [ -5.7287135 -10.1522   ], ([5.4545646  0.68326235]) \t loss 1.131E-01\n",
      "step 6700000 \t return [ -3.4843376 -10.380967 ], ([3.3377986 0.8830728]) \t loss 9.873E-02\n",
      "step 6710000 \t return [ -3.5123818 -10.373958 ], ([3.0350842 0.8782074]) \t loss 1.133E-01\n",
      "step 6720000 \t return [ -4.210273 -10.460557], ([4.6689477 0.8700313]) \t loss 1.194E-01\n",
      "step 6730000 \t return [ -8.949549 -10.581077], ([11.568914   0.6137125]) \t loss 1.108E-01\n",
      "step 6740000 \t return [ -4.2949715 -10.440851 ], ([2.781747  1.0399226]) \t loss 1.293E-01\n",
      "step 6750000 \t return [ -8.488295 -10.370628], ([10.450746   0.6734032]) \t loss 1.502E-01\n",
      "step 6760000 \t return [-14.943468 -10.338886], ([14.722584    0.41992098]) \t loss 2.483E-01\n",
      "step 6770000 \t return [ -2.943441 -10.281195], ([3.3726926 0.7837027]) \t loss 1.237E-01\n",
      "step 6780000 \t return [ -4.206163 -10.380266], ([5.5715704 0.7627029]) \t loss 9.612E-02\n",
      "step 6790000 \t return [ -3.6203756 -10.487367 ], ([3.9427526  0.94874763]) \t loss 8.927E-02\n",
      "step 6800000 \t return [ -9.100064 -10.556261], ([11.325029    0.50481826]) \t loss 1.112E-01\n",
      "step 6810000 \t return [ -3.8830466 -10.259707 ], ([3.7345588 0.919058 ]) \t loss 1.010E-01\n",
      "step 6820000 \t return [ -4.8644104 -10.268982 ], ([5.751823  0.7383294]) \t loss 1.101E-01\n",
      "step 6830000 \t return [-13.585463 -10.415925], ([13.844793    0.40575647]) \t loss 9.289E-02\n",
      "step 6840000 \t return [-10.75135  -10.311707], ([10.655858   0.4984304]) \t loss 3.261E-01\n",
      "step 6850000 \t return [ -4.576696 -10.538083], ([6.030682  0.8039108]) \t loss 1.075E-01\n",
      "step 6860000 \t return [ -5.9793787 -10.467554 ], ([7.9363375 0.6599447]) \t loss 9.718E-02\n",
      "step 6870000 \t return [ -5.2932386 -10.341255 ], ([7.1657567  0.64508694]) \t loss 1.001E-01\n",
      "step 6880000 \t return [ -6.1954937 -10.163931 ], ([6.8271427 0.7252995]) \t loss 8.661E-02\n",
      "step 6890000 \t return [ -4.054938 -10.667922], ([2.728994  1.2156274]) \t loss 7.730E-02\n",
      "step 6900000 \t return [ -3.5407305 -11.148773 ], ([1.7612534 1.0351415]) \t loss 2.005E-01\n",
      "step 6910000 \t return [ -4.912265 -10.238686], ([5.344135   0.60862726]) \t loss 1.364E-01\n",
      "step 6920000 \t return [ -3.0317724 -10.624914 ], ([2.088368  1.0137081]) \t loss 1.091E-01\n",
      "step 6930000 \t return [ -3.3031101 -10.417944 ], ([3.510091  0.7115148]) \t loss 1.452E-01\n",
      "step 6940000 \t return [ -3.490505 -10.270824], ([3.2385595  0.77605593]) \t loss 9.273E-02\n",
      "step 6950000 \t return [ -4.250755 -10.915775], ([2.2781055 1.2196436]) \t loss 1.164E-01\n",
      "step 6960000 \t return [ -7.1304803 -10.789631 ], ([2.673445  1.1276735]) \t loss 1.626E-01\n",
      "step 6970000 \t return [ -3.975839 -10.475354], ([2.3543859 0.9590828]) \t loss 1.264E-01\n",
      "step 6980000 \t return [ -5.516797 -10.183362], ([6.7374763 0.5801308]) \t loss 1.034E-01\n",
      "step 6990000 \t return [ -4.4158835 -10.217599 ], ([4.3453517 0.7548446]) \t loss 1.438E-01\n",
      "step 7000000 \t return [ -4.0540423 -10.246143 ], ([4.285762  0.8428228]) \t loss 9.204E-02\n",
      "step 7010000 \t return [ -5.814612 -10.445558], ([7.01501   0.6508012]) \t loss 8.458E-02\n",
      "step 7020000 \t return [ -7.1443458 -10.449242 ], ([8.951831   0.73008114]) \t loss 9.130E-02\n",
      "step 7030000 \t return [ -3.8046677 -10.513904 ], ([2.5139549 1.0655979]) \t loss 1.118E-01\n",
      "step 7040000 \t return [ -6.319329 -10.478367], ([8.442822  0.5763888]) \t loss 1.232E-01\n",
      "step 7050000 \t return [ -4.15297 -10.15674], ([4.1011825  0.72427243]) \t loss 2.115E-01\n",
      "step 7060000 \t return [ -4.1175456 -10.37962  ], ([4.7165227 0.7814246]) \t loss 1.066E-01\n",
      "step 7070000 \t return [ -2.5512516 -10.867053 ], ([1.923087  1.0658107]) \t loss 7.383E-02\n",
      "step 7080000 \t return [ -3.2735548 -10.774139 ], ([2.4871264 1.1397436]) \t loss 1.575E-01\n",
      "step 7090000 \t return [ -6.3728566 -10.243251 ], ([3.249507 0.716056]) \t loss 1.715E-01\n",
      "step 7100000 \t return [ -5.2434025 -10.186321 ], ([5.1511226 0.7266104]) \t loss 1.506E-01\n",
      "step 7110000 \t return [ -5.03138 -10.51471], ([6.330731  0.7049149]) \t loss 1.090E-01\n",
      "step 7120000 \t return [ -8.123432 -10.244889], ([9.788271 0.705536]) \t loss 1.255E-01\n",
      "step 7130000 \t return [ -6.959467 -10.321362], ([8.899679  0.6371291]) \t loss 1.054E-01\n",
      "step 7140000 \t return [ -3.413716  -10.3695345], ([3.0882797 1.0179173]) \t loss 9.338E-02\n",
      "step 7150000 \t return [ -7.3114996 -10.35893  ], ([9.098923   0.53128153]) \t loss 1.060E-01\n",
      "step 7160000 \t return [ -6.322668 -10.35842 ], ([8.039868  0.6138478]) \t loss 8.335E-02\n",
      "step 7170000 \t return [ -5.015001 -10.289643], ([5.814488  0.7782103]) \t loss 1.119E-01\n",
      "step 7180000 \t return [ -4.405153  -10.1813965], ([5.285561   0.77548486]) \t loss 1.071E-01\n",
      "step 7190000 \t return [ -5.833267 -10.326146], ([6.9857697 0.7062962]) \t loss 9.446E-02\n",
      "step 7200000 \t return [ -4.1332607 -10.277707 ], ([3.9605982 0.9546708]) \t loss 7.690E-02\n",
      "step 7210000 \t return [ -6.1540895 -10.137104 ], ([7.4455853  0.71102065]) \t loss 1.161E-01\n",
      "step 7220000 \t return [ -5.237511 -10.358074], ([6.341226   0.70434433]) \t loss 9.572E-02\n",
      "step 7230000 \t return [ -4.6932945 -10.116025 ], ([5.39339    0.78828514]) \t loss 1.168E-01\n",
      "step 7240000 \t return [ -6.7012477 -10.332798 ], ([7.835217  0.7176968]) \t loss 9.256E-02\n",
      "step 7250000 \t return [ -3.7175164 -10.405813 ], ([2.621428   0.97194636]) \t loss 1.396E-01\n",
      "step 7260000 \t return [ -5.4497848 -10.208439 ], ([5.8719835 0.6427934]) \t loss 1.391E-01\n",
      "step 7270000 \t return [ -8.266666 -10.05497 ], ([9.816018   0.66168225]) \t loss 7.629E-02\n",
      "step 7280000 \t return [ -2.9703202 -10.302446 ], ([2.6110935  0.93735987]) \t loss 1.053E-01\n",
      "step 7290000 \t return [ -5.105911 -10.390973], ([6.804691   0.76190776]) \t loss 1.437E-01\n",
      "step 7300000 \t return [ -4.466956 -10.198119], ([3.5940213 1.0371491]) \t loss 1.346E-01\n",
      "step 7310000 \t return [ -4.2413044 -10.105429 ], ([2.5905848 0.7151914]) \t loss 1.606E-01\n",
      "step 7320000 \t return [ -3.7259073 -10.365099 ], ([3.8354023 0.9931993]) \t loss 1.061E-01\n",
      "step 7330000 \t return [ -6.060025 -10.323138], ([8.216788  0.5253219]) \t loss 1.360E-01\n",
      "step 7340000 \t return [ -5.5174675 -10.144023 ], ([5.6823444  0.73431104]) \t loss 1.088E-01\n",
      "step 7350000 \t return [-11.510197 -10.105396], ([10.83126    0.5486944]) \t loss 1.044E-01\n",
      "step 7360000 \t return [ -3.2124496 -10.199123 ], ([2.7237577  0.99438983]) \t loss 1.217E-01\n",
      "step 7370000 \t return [ -3.5536556 -10.224326 ], ([3.6563385  0.92859066]) \t loss 1.172E-01\n",
      "step 7380000 \t return [ -4.08675  -10.043078], ([3.3478801 0.7286754]) \t loss 1.351E-01\n",
      "step 7390000 \t return [ -3.24332  -10.218843], ([3.404121  0.8737947]) \t loss 1.308E-01\n",
      "step 7400000 \t return [ -2.9288344 -10.327763 ], ([2.473867  0.8997823]) \t loss 1.282E-01\n",
      "step 7410000 \t return [ -5.367132 -10.182368], ([3.2276256 0.8725402]) \t loss 1.495E-01\n",
      "step 7420000 \t return [ -5.6626415 -10.263534 ], ([2.2892118  0.77544695]) \t loss 1.321E-01\n",
      "step 7430000 \t return [ -6.198793 -10.188558], ([7.388488  0.7827989]) \t loss 1.702E-01\n",
      "step 7440000 \t return [-4.58763  -9.998129], ([4.222438   0.85118896]) \t loss 1.109E-01\n",
      "step 7450000 \t return [ -4.9873247 -10.260911 ], ([5.452628  1.0201749]) \t loss 1.536E-01\n",
      "step 7460000 \t return [-10.4999075 -10.021464 ], ([4.196964   0.70987767]) \t loss 2.147E-01\n",
      "step 7470000 \t return [ -6.77779 -10.20473], ([6.4314547 0.6043601]) \t loss 1.392E-01\n",
      "step 7480000 \t return [ -5.777934 -10.191204], ([6.7811804  0.69482374]) \t loss 1.577E-01\n",
      "step 7490000 \t return [ -4.542609 -10.087268], ([4.56614    0.80823404]) \t loss 1.113E-01\n",
      "step 7500000 \t return [ -8.590418 -10.143224], ([9.782793  0.6607949]) \t loss 1.150E-01\n",
      "step 7510000 \t return [-5.375569 -9.930295], ([5.456783   0.72472924]) \t loss 1.307E-01\n",
      "step 7520000 \t return [ -7.5170503 -10.381123 ], ([10.872253    0.61170363]) \t loss 1.227E-01\n",
      "step 7530000 \t return [ -3.5655107 -10.201422 ], ([3.122959   0.93532985]) \t loss 1.728E-01\n",
      "step 7540000 \t return [ -6.6690693 -10.122621 ], ([8.009201   0.68164957]) \t loss 1.526E-01\n",
      "step 7550000 \t return [ -4.802147 -10.046   ], ([4.67003   0.8172973]) \t loss 1.148E-01\n",
      "step 7560000 \t return [ -3.9598727 -10.11032  ], ([2.9533856 0.8621703]) \t loss 1.162E-01\n",
      "step 7570000 \t return [ -4.4628134 -10.216542 ], ([1.929182  0.8381444]) \t loss 1.251E-01\n",
      "step 7580000 \t return [ -9.346977 -10.056338], ([3.9769104  0.75761485]) \t loss 2.214E-01\n",
      "step 7590000 \t return [ -6.137754 -10.353185], ([7.586368  0.6527884]) \t loss 1.788E-01\n",
      "step 7600000 \t return [ -3.3863802 -10.172171 ], ([3.8378565  0.72602725]) \t loss 1.399E-01\n",
      "step 7610000 \t return [ -6.5244656 -10.116041 ], ([6.3255415 0.8849874]) \t loss 1.017E-01\n",
      "step 7620000 \t return [ -4.6419606 -10.321256 ], ([4.3392396  0.93524075]) \t loss 1.638E-01\n",
      "step 7630000 \t return [ -5.773343 -10.160966], ([5.769897 0.615843]) \t loss 1.937E-01\n",
      "step 7640000 \t return [ -3.5754735 -10.122405 ], ([3.9515052 0.744755 ]) \t loss 1.176E-01\n",
      "step 7650000 \t return [ -7.6612525 -10.186415 ], ([8.194731   0.55430007]) \t loss 1.084E-01\n",
      "step 7660000 \t return [ -7.250512 -10.356642], ([8.814837  0.6960148]) \t loss 1.072E-01\n",
      "step 7670000 \t return [ -5.521812 -10.400488], ([6.779725 0.789936]) \t loss 1.166E-01\n",
      "step 7680000 \t return [ -8.201102  -10.0079155], ([1.8465354  0.69412005]) \t loss 1.454E-01\n",
      "step 7690000 \t return [ -4.897843 -10.380432], ([5.544496  1.0096008]) \t loss 1.763E-01\n",
      "step 7700000 \t return [ -3.1914115 -10.54948  ], ([2.2398186  0.94957876]) \t loss 2.109E-01\n",
      "step 7710000 \t return [ -6.716758 -10.022992], ([4.5237026 0.6971385]) \t loss 1.633E-01\n",
      "step 7720000 \t return [ -9.1395645 -10.1556425], ([2.2894132  0.83653575]) \t loss 1.481E-01\n",
      "step 7730000 \t return [ -9.934129 -10.370554], ([10.103859    0.66519785]) \t loss 2.759E-01\n",
      "step 7740000 \t return [ -5.3347406 -10.155564 ], ([4.944756  0.7806968]) \t loss 3.682E-01\n",
      "step 7750000 \t return [ -6.0796046 -10.216803 ], ([6.672554  0.5634123]) \t loss 1.395E-01\n",
      "step 7760000 \t return [ -4.242891 -10.132626], ([4.5974107 0.7994353]) \t loss 1.091E-01\n",
      "step 7770000 \t return [ -3.4990418 -10.225206 ], ([3.664605  0.9817713]) \t loss 1.389E-01\n",
      "step 7780000 \t return [ -4.62179  -10.125881], ([4.695945   0.66311526]) \t loss 1.768E-01\n",
      "step 7790000 \t return [ -3.8695889 -10.178961 ], ([2.9041271 0.9714582]) \t loss 1.314E-01\n",
      "step 7800000 \t return [ -3.8514452 -10.226417 ], ([3.5462234 0.9763507]) \t loss 2.102E-01\n",
      "step 7810000 \t return [ -3.4363198 -10.280722 ], ([2.4613187 1.013895 ]) \t loss 2.130E-01\n",
      "step 7820000 \t return [ -5.942771 -10.224813], ([6.150843  0.4132079]) \t loss 1.664E-01\n",
      "step 7830000 \t return [ -4.504253 -10.074525], ([4.0913444 0.8045229]) \t loss 4.483E-01\n",
      "step 7840000 \t return [ -6.1891522 -10.284784 ], ([7.1105175  0.47726882]) \t loss 1.148E-01\n",
      "step 7850000 \t return [ -3.274484 -10.47331 ], ([2.4989302 1.1625028]) \t loss 1.131E-01\n",
      "step 7860000 \t return [ -5.013464 -10.302905], ([6.15095    0.64306206]) \t loss 3.349E-01\n",
      "step 7870000 \t return [ -5.9408793 -10.193918 ], ([7.092307  0.7628123]) \t loss 1.052E-01\n",
      "step 7880000 \t return [ -5.212195 -10.337952], ([6.16422   0.6983715]) \t loss 1.249E-01\n",
      "step 7890000 \t return [ -4.9915037 -10.093479 ], ([2.937794   0.87963337]) \t loss 1.240E-01\n",
      "step 7900000 \t return [ -5.850176 -10.105054], ([5.9957643 0.5685685]) \t loss 2.166E-01\n",
      "step 7910000 \t return [ -5.9166694 -10.126646 ], ([6.103644   0.87675047]) \t loss 1.177E-01\n",
      "step 7920000 \t return [ -2.983239 -10.866699], ([1.8631985 1.3412693]) \t loss 1.676E-01\n",
      "step 7930000 \t return [ -5.784589 -10.047605], ([6.091387  0.5536326]) \t loss 4.682E-01\n",
      "step 7940000 \t return [ -2.9883769 -10.23894  ], ([2.9317954 0.8397997]) \t loss 1.177E-01\n",
      "step 7950000 \t return [ -4.0025606 -10.038385 ], ([3.8239827 0.7954037]) \t loss 1.407E-01\n",
      "step 7960000 \t return [ -5.2902455 -10.009045 ], ([5.360934   0.53408945]) \t loss 1.220E-01\n",
      "step 7970000 \t return [ -3.3351276 -10.072075 ], ([3.0174022 0.880343 ]) \t loss 8.665E-02\n",
      "step 7980000 \t return [ -4.6763563 -10.136534 ], ([4.3199153  0.94558334]) \t loss 1.411E-01\n",
      "step 7990000 \t return [ -3.381554 -10.16486 ], ([3.474248   0.69883436]) \t loss 2.101E-01\n",
      "step 8000000 \t return [ -5.5792503 -10.277306 ], ([6.7839174 0.5803731]) \t loss 1.187E-01\n",
      "step 8010000 \t return [ -4.1322627 -10.1483755], ([5.0214334 0.7673474]) \t loss 9.957E-02\n",
      "step 8020000 \t return [ -3.1920264 -10.28001  ], ([3.3695736 0.9595503]) \t loss 1.289E-01\n",
      "step 8030000 \t return [ -3.0540717 -10.093215 ], ([2.9732141  0.72901005]) \t loss 2.642E-01\n",
      "step 8040000 \t return [ -3.605403 -10.139616], ([4.036701  0.6356821]) \t loss 1.252E-01\n",
      "step 8050000 \t return [ -5.7052546 -10.181942 ], ([6.167904   0.76139414]) \t loss 1.341E-01\n",
      "step 8060000 \t return [ -4.4485106 -10.301187 ], ([4.755247  1.0685034]) \t loss 1.367E-01\n",
      "step 8070000 \t return [ -3.733455 -10.024377], ([3.7810602 0.7272979]) \t loss 2.629E-01\n",
      "step 8080000 \t return [ -4.778172 -10.171164], ([3.105973  1.0416876]) \t loss 1.058E-01\n",
      "step 8090000 \t return [-14.477693  -9.836026], ([3.5723553  0.61421436]) \t loss 2.548E-01\n",
      "step 8100000 \t return [ -5.9315534 -10.158689 ], ([6.4045973 0.589229 ]) \t loss 2.179E-01\n",
      "step 8110000 \t return [-5.833642 -9.96141 ], ([2.0022388 0.7491649]) \t loss 1.385E-01\n",
      "step 8120000 \t return [ -5.2725825 -10.348569 ], ([5.8238688 0.7898289]) \t loss 1.193E-01\n",
      "step 8130000 \t return [ -8.789901 -10.083095], ([3.0345497 0.7479174]) \t loss 2.017E-01\n",
      "step 8140000 \t return [ -2.9963408 -10.682246 ], ([2.1561584 1.1366292]) \t loss 1.656E-01\n",
      "step 8150000 \t return [ -6.524303 -10.153677], ([6.1380053  0.61796546]) \t loss 2.809E-01\n",
      "step 8160000 \t return [ -7.656969 -10.006806], ([2.7087333 0.6833176]) \t loss 1.491E-01\n",
      "step 8170000 \t return [ -5.27219  -10.212387], ([5.2749577 0.5620118]) \t loss 1.413E-01\n",
      "step 8180000 \t return [ -4.125457 -10.259747], ([4.67563   0.6175424]) \t loss 1.223E-01\n",
      "step 8190000 \t return [ -4.1031966 -10.246961 ], ([4.5470986  0.70295095]) \t loss 1.240E-01\n",
      "step 8200000 \t return [ -4.0777187 -10.038701 ], ([4.2844815 0.6948471]) \t loss 1.087E-01\n",
      "step 8210000 \t return [ -4.463066 -10.179531], ([5.1071386  0.60162276]) \t loss 1.157E-01\n",
      "step 8220000 \t return [ -3.365035 -10.160474], ([3.126617  0.8723808]) \t loss 1.054E-01\n",
      "step 8230000 \t return [-7.494542 -9.980298], ([7.810358  0.6460733]) \t loss 1.563E-01\n",
      "step 8240000 \t return [ -5.0729055 -10.295551 ], ([4.9060345 1.1064768]) \t loss 1.383E-01\n",
      "step 8250000 \t return [ -5.396903 -10.312308], ([7.27956    0.51664513]) \t loss 2.599E-01\n",
      "step 8260000 \t return [ -3.432128 -10.080239], ([3.2133448  0.83231944]) \t loss 1.568E-01\n",
      "step 8270000 \t return [ -3.2065742 -10.074619 ], ([3.7185285  0.71288455]) \t loss 1.012E-01\n",
      "step 8280000 \t return [ -4.144336 -10.102333], ([4.9666915 0.6544403]) \t loss 1.032E-01\n",
      "step 8290000 \t return [ -3.0137606 -10.18883  ], ([2.3778944 0.9346392]) \t loss 1.256E-01\n",
      "step 8300000 \t return [-4.4200463 -9.96185  ], ([4.1630545 0.66534  ]) \t loss 1.565E-01\n",
      "step 8310000 \t return [ -4.4799356 -10.522586 ], ([2.3274941 1.3372823]) \t loss 1.306E-01\n",
      "step 8320000 \t return [ -5.73579  -10.146763], ([6.441462  0.5410816]) \t loss 3.537E-01\n",
      "step 8330000 \t return [ -3.510457 -10.264019], ([3.4807827 0.84183  ]) \t loss 1.502E-01\n",
      "step 8340000 \t return [ -8.502397 -10.289176], ([3.144453   0.92819834]) \t loss 1.334E-01\n",
      "step 8350000 \t return [ -4.239833  -11.4165945], ([1.5919358 1.6683726]) \t loss 2.618E-01\n",
      "step 8360000 \t return [ -5.0707283 -11.051766 ], ([5.4122844 1.354806 ]) \t loss 7.152E-01\n",
      "step 8370000 \t return [ -7.5126534 -10.135705 ], ([5.4916306  0.77994645]) \t loss 4.030E-01\n",
      "step 8380000 \t return [-16.534378 -10.149136], ([3.869556  0.7250726]) \t loss 1.743E-01\n",
      "step 8390000 \t return [-4.696971 -9.870291], ([2.501409   0.63204366]) \t loss 1.930E-01\n",
      "step 8400000 \t return [ -5.968434 -10.022748], ([3.2716718 0.7681757]) \t loss 1.400E-01\n",
      "step 8410000 \t return [ -3.9793057 -10.09294  ], ([4.292883  0.6986509]) \t loss 1.365E-01\n",
      "step 8420000 \t return [ -4.5859675 -10.174171 ], ([2.751308   0.81790996]) \t loss 1.546E-01\n",
      "step 8430000 \t return [ -3.4429874 -10.123168 ], ([2.7630842 0.7741107]) \t loss 1.143E-01\n",
      "step 8440000 \t return [ -4.778347 -10.337314], ([1.9914352 1.0917664]) \t loss 1.108E-01\n",
      "step 8450000 \t return [ -4.0173354 -10.150683 ], ([4.1361027 0.6469941]) \t loss 3.004E-01\n",
      "step 8460000 \t return [ -6.2493315 -10.229686 ], ([2.857661  0.7723366]) \t loss 1.386E-01\n",
      "step 8470000 \t return [-6.7742753 -9.993953 ], ([6.957744  0.6120399]) \t loss 1.610E-01\n",
      "step 8480000 \t return [ -8.403504 -10.052504], ([3.3240526  0.66092515]) \t loss 1.168E-01\n",
      "step 8490000 \t return [ -3.2291572 -10.385835 ], ([2.437864  1.0093011]) \t loss 1.718E-01\n",
      "step 8500000 \t return [-10.009379 -10.149872], ([2.9783492  0.68404365]) \t loss 2.840E-01\n",
      "step 8510000 \t return [ -3.2197595 -10.082196 ], ([2.5570085 0.7093509]) \t loss 1.282E-01\n",
      "step 8520000 \t return [ -4.4468427 -10.092916 ], ([4.6767254 0.6795385]) \t loss 1.398E-01\n",
      "step 8530000 \t return [ -3.8388736 -10.20232  ], ([2.4312809 0.7344947]) \t loss 1.009E-01\n",
      "step 8540000 \t return [-4.1438026 -9.984045 ], ([3.8791554 0.731914 ]) \t loss 1.126E-01\n",
      "step 8550000 \t return [ -3.581119 -10.178603], ([3.9968896 0.7300702]) \t loss 1.116E-01\n",
      "step 8560000 \t return [ -3.1673577 -10.296949 ], ([3.1007698  0.97821075]) \t loss 1.148E-01\n",
      "step 8570000 \t return [ -4.223666 -10.12798 ], ([2.9534113 0.6508039]) \t loss 2.033E-01\n",
      "step 8580000 \t return [ -3.4341962 -10.214334 ], ([3.428207   0.86410415]) \t loss 1.184E-01\n",
      "step 8590000 \t return [ -3.719139 -10.532662], ([4.876324  0.8566299]) \t loss 1.486E-01\n",
      "step 8600000 \t return [ -3.3053064 -10.296902 ], ([2.2729547 1.0564669]) \t loss 1.448E-01\n",
      "step 8610000 \t return [ -9.2708   -10.208389], ([3.2201412 0.6539699]) \t loss 1.890E-01\n",
      "step 8620000 \t return [ -6.3812604 -10.104741 ], ([6.6905303 0.6697796]) \t loss 1.730E-01\n",
      "step 8630000 \t return [ -3.9396813 -10.260963 ], ([2.9134505 1.0957972]) \t loss 1.288E-01\n",
      "step 8640000 \t return [ -3.8748715 -10.275485 ], ([4.605079   0.75051117]) \t loss 2.764E-01\n",
      "step 8650000 \t return [ -4.814845 -10.114149], ([4.581316  0.8939271]) \t loss 1.085E-01\n",
      "step 8660000 \t return [ -7.0932417 -11.127613 ], ([3.1859083 1.9816048]) \t loss 1.251E-01\n",
      "step 8670000 \t return [ -9.426922 -10.166571], ([11.301233    0.53320444]) \t loss 1.066E+00\n",
      "step 8680000 \t return [ -4.356824 -10.080411], ([5.1883864  0.74901336]) \t loss 2.071E-01\n",
      "step 8690000 \t return [ -3.9202738 -10.223044 ], ([2.8380368 0.7591613]) \t loss 1.162E-01\n",
      "step 8700000 \t return [ -9.855468 -10.248948], ([10.433821    0.42458847]) \t loss 1.269E-01\n",
      "step 8710000 \t return [ -4.840781 -10.134223], ([3.8793578  0.73219025]) \t loss 1.516E-01\n",
      "step 8720000 \t return [ -5.662096 -10.010235], ([4.3842416 0.9073309]) \t loss 1.215E-01\n",
      "step 8730000 \t return [ -3.6147375 -10.1998825], ([3.5266068 0.8516082]) \t loss 1.592E-01\n",
      "step 8740000 \t return [ -4.7826033 -10.071646 ], ([4.933382  0.7007982]) \t loss 1.166E-01\n",
      "step 8750000 \t return [ -5.2822742 -10.178873 ], ([5.888748 0.718039]) \t loss 1.144E-01\n",
      "step 8760000 \t return [ -4.301464 -10.109526], ([4.3888464  0.71749026]) \t loss 1.208E-01\n",
      "step 8770000 \t return [ -3.3837519 -10.226792 ], ([3.5892243 0.90422  ]) \t loss 1.166E-01\n",
      "step 8780000 \t return [ -3.1526482 -10.247195 ], ([3.1315935  0.86707675]) \t loss 1.520E-01\n",
      "step 8790000 \t return [-10.056167 -10.271135], ([10.715802    0.47279122]) \t loss 1.593E-01\n",
      "step 8800000 \t return [-4.9090347 -9.97066  ], ([5.077801   0.71578926]) \t loss 1.220E-01\n",
      "step 8810000 \t return [ -4.434433 -10.343568], ([5.177751  0.7481416]) \t loss 1.194E-01\n",
      "step 8820000 \t return [ -4.0593114 -10.918362 ], ([2.5836804 1.5484747]) \t loss 1.357E-01\n",
      "step 8830000 \t return [ -7.529679 -10.050028], ([7.973108  0.5902584]) \t loss 5.712E-01\n",
      "step 8840000 \t return [ -5.2556887 -10.021529 ], ([4.9864035 0.6030673]) \t loss 1.479E-01\n",
      "step 8850000 \t return [ -4.0284953 -10.23551  ], ([4.443425   0.90811855]) \t loss 1.274E-01\n",
      "step 8860000 \t return [ -3.6899936 -10.184963 ], ([3.2665606  0.93855286]) \t loss 1.128E-01\n",
      "step 8870000 \t return [ -5.1537395 -10.3462   ], ([6.541559   0.62711585]) \t loss 1.476E-01\n",
      "step 8880000 \t return [ -5.258774 -10.35768 ], ([3.3294325 1.1919686]) \t loss 1.394E-01\n",
      "step 8890000 \t return [ -6.88291  -10.270398], ([7.2974296  0.52541935]) \t loss 2.775E-01\n",
      "step 8900000 \t return [ -3.20896  -10.371568], ([3.1290932 1.1271598]) \t loss 1.209E-01\n",
      "step 8910000 \t return [ -3.7238984 -10.028202 ], ([3.54566   0.8567645]) \t loss 2.318E-01\n",
      "step 8920000 \t return [ -3.9569268 -10.405285 ], ([5.0838685 0.8670197]) \t loss 1.686E-01\n",
      "step 8930000 \t return [ -6.0597277 -10.113055 ], ([5.8567295 0.8513716]) \t loss 1.606E-01\n",
      "step 8940000 \t return [ -4.8926105 -10.336823 ], ([5.681954   0.69606847]) \t loss 1.914E-01\n",
      "step 8950000 \t return [ -7.6687007 -10.45057  ], ([6.6529713 1.3303741]) \t loss 1.939E-01\n",
      "step 8960000 \t return [ -3.7607925 -10.366925 ], ([2.7546618 1.1810578]) \t loss 4.655E-01\n",
      "step 8970000 \t return [ -5.3213563 -11.075602 ], ([3.4790146 2.0951695]) \t loss 2.689E-01\n",
      "step 8980000 \t return [-11.903012 -10.319289], ([12.886614   0.6543469]) \t loss 1.367E+00\n",
      "step 8990000 \t return [ -4.476637 -10.572051], ([4.8549504 1.2778652]) \t loss 1.803E-01\n",
      "step 9000000 \t return [ -9.883559 -10.249761], ([10.441081   0.4677946]) \t loss 2.678E-01\n",
      "step 9010000 \t return [ -4.160773 -10.625979], ([2.3755796 1.4246025]) \t loss 1.495E-01\n",
      "step 9020000 \t return [ -3.8077338 -10.531071 ], ([2.7151346 1.3332893]) \t loss 3.630E-01\n",
      "step 9030000 \t return [ -4.2725964 -10.190876 ], ([3.9906485 1.0614219]) \t loss 3.227E-01\n",
      "step 9040000 \t return [ -3.4024463 -10.485157 ], ([2.9893563 1.0981885]) \t loss 1.869E-01\n",
      "step 9050000 \t return [ -8.630378 -10.278181], ([8.953605  0.4709524]) \t loss 1.788E-01\n",
      "step 9060000 \t return [ -5.240721 -10.326207], ([5.484149  0.8715418]) \t loss 4.480E-01\n",
      "step 9070000 \t return [ -4.057683 -10.39079 ], ([3.586735  1.2238731]) \t loss 1.555E-01\n",
      "step 9080000 \t return [ -4.142485 -10.844856], ([2.5735552 1.6785865]) \t loss 2.181E-01\n",
      "step 9090000 \t return [ -6.9801507 -10.395527 ], ([8.236883  0.7399535]) \t loss 5.577E-01\n",
      "step 9100000 \t return [ -3.5944629 -10.855713 ], ([2.6784294 1.5742496]) \t loss 1.620E-01\n",
      "step 9110000 \t return [-12.470089 -10.158674], ([9.7553     0.42969567]) \t loss 4.368E-01\n",
      "step 9120000 \t return [ -4.7268257 -10.459365 ], ([4.251563  1.2677864]) \t loss 1.883E-01\n",
      "step 9130000 \t return [ -4.9142976 -10.389161 ], ([4.890639 1.248149]) \t loss 2.318E-01\n",
      "step 9140000 \t return [ -5.8199477 -10.310027 ], ([5.729905  0.9972428]) \t loss 2.540E-01\n",
      "step 9150000 \t return [ -4.3503036 -10.344605 ], ([4.2103615 1.1680386]) \t loss 1.537E-01\n",
      "step 9160000 \t return [ -4.0169153 -10.492371 ], ([3.3278265 1.3393432]) \t loss 1.747E-01\n",
      "step 9170000 \t return [ -3.2181537 -10.815235 ], ([2.377783  1.5029294]) \t loss 3.810E-01\n",
      "step 9180000 \t return [ -5.2191057 -10.733201 ], ([4.066918  1.3730193]) \t loss 3.565E-01\n",
      "step 9190000 \t return [-10.8457985 -10.336378 ], ([11.139314    0.78711003]) \t loss 4.546E-01\n",
      "step 9200000 \t return [-21.028902 -10.545261], ([15.962474  1.387606]) \t loss 6.037E-01\n",
      "step 9210000 \t return [-18.515102 -11.08927 ], ([20.9997     1.6175151]) \t loss 8.082E-01\n",
      "step 9220000 \t return [-10.657994 -11.395649], ([12.726276   2.2865229]) \t loss 7.385E-01\n",
      "step 9230000 \t return [-11.781111 -11.134274], ([13.87173    1.9395459]) \t loss 9.192E-01\n",
      "step 9240000 \t return [-18.865784 -10.448364], ([20.83742    0.8691621]) \t loss 8.425E-01\n",
      "step 9250000 \t return [-11.751112 -10.786304], ([15.833074   1.3510597]) \t loss 3.685E-01\n",
      "step 9260000 \t return [-17.573614 -10.537745], ([18.328781   1.8073369]) \t loss 4.088E-01\n",
      "step 9270000 \t return [ -9.482545 -12.030661], ([9.739648  2.6023195]) \t loss 6.427E-01\n",
      "step 9280000 \t return [-18.307684 -11.461834], ([18.473467   2.4180064]) \t loss 1.262E+00\n",
      "step 9290000 \t return [ -7.662802 -14.438971], ([4.4033117 3.8823786]) \t loss 1.188E+00\n",
      "step 9300000 \t return [ -7.8714576 -11.470089 ], ([9.448375  2.1025107]) \t loss 3.590E+00\n",
      "step 9310000 \t return [-21.612637 -10.292573], ([22.836344   0.6288896]) \t loss 8.912E-01\n",
      "step 9320000 \t return [ -7.153429  -10.6307535], ([9.32921   1.3932471]) \t loss 3.103E-01\n",
      "step 9330000 \t return [-13.3061695 -10.8143835], ([16.350569   1.1346118]) \t loss 3.617E-01\n",
      "step 9340000 \t return [ -7.248361 -11.16218 ], ([8.758679 2.137401]) \t loss 3.077E-01\n",
      "step 9350000 \t return [ -9.560502 -11.653928], ([10.439456  2.535507]) \t loss 5.843E-01\n",
      "step 9360000 \t return [-10.298008 -11.856312], ([10.906343   2.7096262]) \t loss 8.186E-01\n",
      "step 9370000 \t return [-10.303013 -11.384109], ([11.549778   2.2374315]) \t loss 1.019E+00\n",
      "step 9380000 \t return [ -6.735509 -13.170246], ([4.101609  3.5011344]) \t loss 6.561E-01\n",
      "step 9390000 \t return [ -7.5421515 -13.451574 ], ([4.8771977 3.4544055]) \t loss 1.796E+00\n",
      "step 9400000 \t return [ -9.936696 -11.945189], ([11.158615   2.5031297]) \t loss 1.741E+00\n",
      "step 9410000 \t return [ -8.210132 -11.532378], ([9.89175   2.8208098]) \t loss 8.466E-01\n",
      "step 9420000 \t return [ -8.589321 -11.40115 ], ([8.430808 2.666039]) \t loss 1.026E+00\n",
      "step 9430000 \t return [ -6.8757167 -12.571628 ], ([5.9541764 3.4879518]) \t loss 9.305E-01\n",
      "step 9440000 \t return [ -8.823341 -11.852741], ([7.757803  2.7427516]) \t loss 1.431E+00\n",
      "step 9450000 \t return [-12.424727 -11.768934], ([14.454979   3.1716359]) \t loss 6.911E-01\n",
      "step 9460000 \t return [ -7.9439416 -15.392552 ], ([2.6539133 4.5971074]) \t loss 6.092E-01\n",
      "step 9470000 \t return [-11.234772 -18.215256], ([2.0609703 5.348439 ]) \t loss 2.388E+00\n",
      "step 9480000 \t return [-19.906965 -10.709463], ([20.050686   1.5538714]) \t loss 2.513E+00\n",
      "step 9490000 \t return [-10.678933 -12.35494 ], ([10.790798   3.6263914]) \t loss 1.796E+00\n",
      "step 9500000 \t return [-12.286968 -11.060824], ([12.962081   2.4523351]) \t loss 9.500E-01\n",
      "step 9510000 \t return [ -9.443988 -14.022898], ([7.7440906 4.613279 ]) \t loss 7.335E-01\n",
      "step 9520000 \t return [ -7.4795823 -14.927532 ], ([2.2489724 4.0887594]) \t loss 1.814E+00\n",
      "step 9530000 \t return [-11.451659 -18.653133], ([2.8533862 5.200349 ]) \t loss 2.492E+00\n",
      "step 9540000 \t return [-13.308379 -11.461861], ([15.786584   2.7718124]) \t loss 2.401E+00\n",
      "step 9550000 \t return [ -9.18358  -12.534475], ([9.071575  3.7924454]) \t loss 8.744E-01\n",
      "step 9560000 \t return [ -9.977996 -16.567574], ([2.9089959 5.4721107]) \t loss 1.129E+00\n",
      "step 9570000 \t return [-12.38729 -19.26397], ([2.5789685 6.1094537]) \t loss 2.482E+00\n",
      "step 9580000 \t return [-13.396366 -11.268059], ([16.599564   2.2806783]) \t loss 1.681E+00\n",
      "step 9590000 \t return [ -7.622652 -14.265495], ([3.132834  4.6582727]) \t loss 5.541E-01\n",
      "step 9600000 \t return [-10.240109 -16.72424 ], ([3.0929756 5.38823  ]) \t loss 1.722E+00\n",
      "step 9610000 \t return [ -9.678605 -14.769092], ([6.064     4.9700246]) \t loss 1.931E+00\n",
      "step 9620000 \t return [-12.303387 -12.719418], ([11.551741   4.2273984]) \t loss 1.407E+00\n",
      "step 9630000 \t return [-10.572862 -12.992418], ([8.40212  4.272316]) \t loss 7.414E-01\n",
      "step 9640000 \t return [-11.626667 -13.052604], ([9.410811  4.6666408]) \t loss 6.948E-01\n",
      "step 9650000 \t return [-15.440797 -11.738082], ([16.448828   3.1211953]) \t loss 8.155E-01\n",
      "step 9660000 \t return [-10.22894  -12.487134], ([10.690975  3.612495]) \t loss 8.048E-01\n",
      "step 9670000 \t return [ -8.721027 -16.109236], ([3.5839524 5.6054626]) \t loss 8.307E-01\n",
      "step 9680000 \t return [ -8.990447  -12.2958555], ([7.8833485 3.841751 ]) \t loss 8.585E-01\n",
      "step 9690000 \t return [ -9.032962 -14.142179], ([6.502295 5.231884]) \t loss 6.049E-01\n",
      "step 9700000 \t return [ -9.39495   -14.4579935], ([5.311728 5.196463]) \t loss 1.925E+00\n",
      "step 9710000 \t return [-11.43201  -14.219846], ([9.094543 5.335737]) \t loss 1.995E+00\n",
      "step 9720000 \t return [ -9.218615 -16.861431], ([3.595217 5.733178]) \t loss 2.045E+00\n",
      "step 9730000 \t return [ -7.7996306 -13.795058 ], ([4.828386 4.868411]) \t loss 1.540E+00\n",
      "step 9740000 \t return [-10.207618 -12.377446], ([10.269657   3.8022249]) \t loss 8.790E-01\n",
      "step 9750000 \t return [ -7.712816 -15.01675 ], ([3.9720974 5.347914 ]) \t loss 9.128E-01\n",
      "step 9760000 \t return [-10.815466 -14.334799], ([6.73685  5.494205]) \t loss 1.275E+00\n",
      "step 9770000 \t return [-10.678824 -13.076618], ([9.371535  4.8252997]) \t loss 8.146E-01\n",
      "step 9780000 \t return [-10.112894 -14.532339], ([5.4471483 5.910621 ]) \t loss 1.263E+00\n",
      "step 9790000 \t return [-10.519775 -16.074675], ([5.8798776 6.674816 ]) \t loss 1.244E+00\n",
      "step 9800000 \t return [-11.127832  -11.9297495], ([10.853375  3.18163 ]) \t loss 8.942E-01\n",
      "step 9810000 \t return [-13.434445 -12.02022 ], ([13.050894   3.4628398]) \t loss 5.600E-01\n",
      "step 9820000 \t return [ -9.301879 -12.048127], ([8.339153  3.5105543]) \t loss 4.612E-01\n",
      "step 9830000 \t return [-10.005431 -12.594265], ([7.7213035 4.7005863]) \t loss 5.958E-01\n",
      "step 9840000 \t return [ -9.579288 -16.372913], ([5.1079516 7.317262 ]) \t loss 6.420E-01\n",
      "step 9850000 \t return [ -9.366693 -15.66239 ], ([4.9235053 5.624132 ]) \t loss 7.578E-01\n",
      "step 9860000 \t return [-12.865617 -11.989483], ([10.596084   3.7326994]) \t loss 1.359E+00\n",
      "step 9870000 \t return [ -8.146015 -12.232797], ([6.6836734 3.8686926]) \t loss 8.255E-01\n",
      "step 9880000 \t return [ -8.611663 -15.807908], ([4.713052 6.349908]) \t loss 7.120E-01\n",
      "step 9890000 \t return [-12.044739  -12.1780815], ([11.555021  3.770384]) \t loss 8.792E-01\n",
      "step 9900000 \t return [ -8.989106 -14.984219], ([4.5244985 5.9234376]) \t loss 4.955E-01\n",
      "step 9910000 \t return [ -8.070329 -14.46256 ], ([4.7881985 4.956611 ]) \t loss 1.206E+00\n",
      "step 9920000 \t return [ -8.598953 -15.955962], ([3.1919932 5.3418927]) \t loss 1.259E+00\n",
      "step 9930000 \t return [-10.126143 -11.978452], ([8.649974 3.716491]) \t loss 1.405E+00\n",
      "step 9940000 \t return [ -8.7568445 -12.565857 ], ([7.633567  3.9564345]) \t loss 7.590E-01\n",
      "step 9950000 \t return [ -7.7757335 -13.2965555], ([5.2899065 4.484429 ]) \t loss 3.841E-01\n",
      "step 9960000 \t return [-10.039875 -11.167276], ([10.145755   2.6593637]) \t loss 5.891E-01\n",
      "step 9970000 \t return [ -7.5212016 -13.540282 ], ([3.8041608 4.6917176]) \t loss 4.395E-01\n",
      "step 9980000 \t return [ -9.944934 -15.857104], ([4.697317  5.7297587]) \t loss 8.453E-01\n",
      "step 9990000 \t return [ -8.935553 -15.277731], ([4.8323097 5.259063 ]) \t loss 8.040E-01\n",
      "step 10000000 \t return [ -8.951394 -14.48888 ], ([5.0961556 4.4354334]) \t loss 5.803E-01\n"
     ]
    }
   ],
   "source": [
    "env = mo_gym.make('water-reservoir-v0', normalized_action=False, nO=2, penalize=True, time_limit=100)\n",
    "\n",
    "env = ScaleReward(env)\n",
    "\n",
    "PCNAgent = PCN(env, np.array((0.5, 0.5, 0.1), dtype=np.float32), continuous_actions=True, noise=5.0)\n",
    "\n",
    "max_return = np.zeros(2)\n",
    "\n",
    "PCNAgent.train(10000000, env, np.array((-100.0,-100.0), dtype=np.float32), num_step_episodes=100,max_return=max_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_pf('./noise5_decay_largeScaling.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
